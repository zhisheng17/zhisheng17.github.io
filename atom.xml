<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2021-11-11T15:33:45.652Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>宕机一台机器，结果一百多个 Flink 作业挂了</title>
    <link href="http://www.54tianzhisheng.cn/2021/11/11/flink-akka-framesize/"/>
    <id>http://www.54tianzhisheng.cn/2021/11/11/flink-akka-framesize/</id>
    <published>2021-11-10T16:00:00.000Z</published>
    <updated>2021-11-11T15:33:45.652Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下：</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2021-11-09 16:01:11</span><br><span class="line">java.util.concurrent.CompletionException: java.lang.reflect.UndeclaredThrowableException</span><br><span class="line"> at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)</span><br><span class="line"> at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)</span><br><span class="line"> at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592)</span><br><span class="line"> at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line"> at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)</span><br><span class="line"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)</span><br><span class="line"> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line"> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line"> at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.lang.reflect.UndeclaredThrowableException</span><br><span class="line"> at com.sun.proxy.$Proxy54.submitTask(Unknown Source)</span><br><span class="line"> at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72)</span><br><span class="line"> at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756)</span><br><span class="line"> at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)</span><br><span class="line"> ... 7 more</span><br><span class="line">Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize.</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.createRpcInvocationMessage(AkkaInvocationHandler.java:276)</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:205)</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:134)</span><br><span class="line"> ... 11 more</span><br></pre></td></tr></table></figure><h3 id="解决异常过程"><a href="#解决异常过程" class="headerlink" title="解决异常过程"></a>解决异常过程</h3><p>从上面的异常日志中我们提取到关键信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize.</span><br></pre></td></tr></table></figure><p>看起来是 RPC 的消息大小超过了默认的 akka framesize 的最大值了，所以我们来了解一下这个值的默认值，从 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.12/deployment/config.html#akka-framesize">官网</a> 我们可以看的到该值的默认大小为 “10485760b”，并且该参数的描述为：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwbm72hedkj31i806imya.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier.</span><br></pre></td></tr></table></figure><p>翻译过来的意思就是：这个参数是 JobManager 和 TaskManagers 之间通信允许的最大消息大小，如果 Flink 作业因为通信消息大小超过了该值，你可以通过增加该值的大小来解决，该参数需要指定一个单位。</p><h3 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h3><p>Flink 使用 Akka 作为组件（JobManager/TaskManager/ResourceManager）之间的 RPC 框架，在 JobManager 和 TaskManagers 之间发送的消息的最大大小默认为 10485760b，如果消息超过这个限制就会失败，报错。这个可以看下抛出异常处的源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">protected RpcInvocation createRpcInvocationMessage(String methodName, Class&lt;?&gt;[] parameterTypes, Object[] args) throws IOException &#123;</span><br><span class="line">    Object rpcInvocation;</span><br><span class="line">    if (this.isLocal) &#123;</span><br><span class="line">        rpcInvocation = new LocalRpcInvocation(methodName, parameterTypes, args);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            RemoteRpcInvocation remoteRpcInvocation = new RemoteRpcInvocation(methodName, parameterTypes, args);</span><br><span class="line">            if (remoteRpcInvocation.getSize() &gt; this.maximumFramesize) &#123;</span><br><span class="line">                // 异常所在位置</span><br><span class="line">                throw new IOException(&quot;The rpc invocation size exceeds the maximum akka framesize.&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            rpcInvocation = remoteRpcInvocation;</span><br><span class="line">        &#125; catch (IOException var6) &#123;</span><br><span class="line">            LOG.warn(&quot;Could not create remote rpc invocation message. Failing rpc invocation because...&quot;, var6);</span><br><span class="line">            throw var6;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return (RpcInvocation)rpcInvocation;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至于为什么 JobManager 和 TaskManager 之间的 RPC 消息大小会如此之大，初步的解释是在 task 出现异常之后，它需要调用 updateTaskExecutionState(TaskExecutionState，taskExecutionState) 这个 RPC 接口去通知 Flink Jobmanager 去改变对应 task 的状态并且重启 task。但是呢，taskExecutionState 这个参数里面有个 error 属性，当我的 task 打出来的错误栈太多的时候，在序列化的之后超过了 rpc 接口要求的最大数据大小（也就是 maximum akka framesize），导致调用 updateTaskExecutionState 这个 rpc 接口失败，Jobmanager 无法获知这个 task 已经处于 fail 的状态，也无法重启，然后就导致了一系列连锁反应。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>任务停止，在 <code>flink-conf.yaml</code> 中加入 <code>akka.framesize</code> 参数，调大该值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">akka.framesize: &quot;62914560b&quot;</span><br></pre></td></tr></table></figure><p>然后将任务重启，可以观察 Jobmanager Configration 看看参数是否生效。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下：&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>实时平台如何管理多个 Flink 版本？—— 为啥会出现多个版本？</title>
    <link href="http://www.54tianzhisheng.cn/2021/09/26/realtime-platform-flink-version/"/>
    <id>http://www.54tianzhisheng.cn/2021/09/26/realtime-platform-flink-version/</id>
    <published>2021-09-25T16:00:00.000Z</published>
    <updated>2021-11-14T03:12:49.099Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为啥会出现多个版本？"><a href="#为啥会出现多个版本？" class="headerlink" title="为啥会出现多个版本？"></a>为啥会出现多个版本？</h3><a id="more"></a><ul><li><p><strong>Flink 社区</strong>本身迭代速度非常快，目前阿里云有一大波的人专职做 Flink 开源，另外还拥有活跃的社区贡献者，所以功能开发较快，bug 修复速度较快，几乎每 4 个月一个大版本，每个大版本之间迭代的功能非常多，代码变动非常大，API 接口变动也大，动不动就干翻自己了。</p></li><li><p>社区迭代快就快呗，为什么<strong>公司</strong>也要要不断跟着社区鼻子走？社区迭代快意味着功能多，修复的 bug 多，相对于早期版本意味着稳定性也高些。除了国内一二线公司有特别多的专职人去负责这块，大多数中小公司最简单最快捷体验到稳定性最高、功能性最多、性能最好的 Flink 版本无非是直接使用最新的 Flink 版本。举个例子：Flink SQL 从最早期（1.9）的功能、性能到目前 1.14，差别真的大很多，优化了特别多的地方，增强了很多功能。原先使用 Flink SQL 完成一个流处理任务非常麻烦，还不如直接写几十行代码来的快，目前我情愿写 SQL 去处理一个流任务。那么自然会跟着升级到新版本。</p></li><li><p><strong>用户 A</strong> 问 Flink SQL 支持单独设置并行度吗？<strong>用户 B</strong> 问实时平台现在支持 Flink 1.13 版本的 Window TVF？这个要 Flink xxx 版本才能支持，要不你升级一下 Flink 版本到 xxx？这样就能支持了，类似的场景还有很多，对于<strong>中小公司的实时平台负责人</strong>来说，这无非最省事；对于<strong>大公司的负责实时开发的人</strong>来说，这无疑是一个噩梦，每次升级新版本都要将在老版本开发的各种功能都想尽办法移植到新版本上来，碰到 API 接口变动大的无非相当于重写了，或者将新版本的某些特别需要的功能通过打 patch 的方式打到老版本里面去。</p></li><li><p>新版本香是真的香，可是为啥有的人不用呢？问题就是，实时作业大多数是长期运行的，如果一个作业没啥错误，在生产运行的好好的，也不出啥故障，稳定性和性能也都能接受（并不是所有作业数据量都很大，会遇到性能问题），那么<strong>用户</strong>为啥要使用新版本？用户才不管你新版本功能多牛逼，性能多屌呢，老子升级还要改依赖版本、改接口代码、测试联调、性能测试（谁知道你说的性能提升是不是吹牛逼的）、稳定性测试（可能上线双跑一段时间验证），这些不需要时间呀，你叫我升级就升级，滚犊子吧，你知道我还有多少业务需求要做吗？</p></li></ul><p>那么就落下这个场地了，又要使用新版本的功能去解决问题，老作业的用户跟他各种扯皮也打动不了他升级作业的版本，那么自然就不断的出现了多个版本了。</p><p>这样，如果不对版本做好规划，那么摊子就逐渐越来越大，越来越难收拾了？</p><p>那么该如何管理公司的 Flink 版本？如果管理和兼容多个 Flink 版本的作业提交？如何兼容 Jar 包和 SQL 作业的提交</p><h3 id="怎么管理多个-Flink-版本的作业提交？"><a href="#怎么管理多个-Flink-版本的作业提交？" class="headerlink" title="怎么管理多个 Flink 版本的作业提交？"></a>怎么管理多个 Flink 版本的作业提交？</h3><p>尽请期待下篇文章</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;为啥会出现多个版本？&quot;&gt;&lt;a href=&quot;#为啥会出现多个版本？&quot; class=&quot;headerlink&quot; title=&quot;为啥会出现多个版本？&quot;&gt;&lt;/a&gt;为啥会出现多个版本？&lt;/h3&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 扩展库——Gelly</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/02/flink-in-action-6.5/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/02/flink-in-action-6.5/</id>
    <published>2021-08-01T16:00:00.000Z</published>
    <updated>2021-12-26T11:40:16.522Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-5-Flink-扩展库——Gelly"><a href="#6-5-Flink-扩展库——Gelly" class="headerlink" title="6.5 Flink 扩展库——Gelly"></a>6.5 Flink 扩展库——Gelly</h2><p>在 1.9 版本中还剩最后一个扩展库就是 Gelly，本节将带你了解一下 Gelly 的功能以及如何使用。</p><a id="more"></a><h3 id="6-5-1-Gelly-简介"><a href="#6-5-1-Gelly-简介" class="headerlink" title="6.5.1 Gelly 简介"></a>6.5.1 Gelly 简介</h3><p>Gelly 是 Flink 的图 API 库，它包含了一组旨在简化 Flink 中图形分析应用程序开发的方法和实用程序。在 Gelly 中，可以使用类似于批处理 API 提供的高级函数来转换和修改图。Gelly 提供了创建、转换和修改图的方法以及图算法库。</p><h3 id="6-5-2-使用-Gelly"><a href="#6-5-2-使用-Gelly" class="headerlink" title="6.5.2 使用 Gelly"></a>6.5.2 使用 Gelly</h3><p>因为 Gelly 是 Flink 项目中库的一部分，它本身不在 Flink 的二进制包中，所以运行 Gelly 项目（Java 应用程序）是需要将 <code>opt/flink-gelly_2.11-1.9.0.jar</code> 移动到 <code>lib</code> 目录中，如果是 Scala 应用程序则需要将 <code>opt/flink-gelly-scala_2.11-1.9.0.jar</code> 移动到 <code>lib</code> 中，接着运行下面的命令就可以运行一个 flink-gelly-examples 项目。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run examples/gelly/flink-gelly-examples_2.11-1.9.0.jar \</span><br><span class="line">    --algorithm GraphMetrics --order directed \</span><br><span class="line">    --input RMatGraph --type integer --scale 20 --simplify directed \</span><br><span class="line">    --output print</span><br></pre></td></tr></table></figure><p>接下来可以在 UI 上看到运行的结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-155600.png" alt=""></p><p>如果是自己创建的 Gelly Java 应用程序，则需要添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-gelly_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是 Gelly Scala 应用程序，添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-gelly-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="6-5-3-Gelly-API"><a href="#6-5-3-Gelly-API" class="headerlink" title="6.5.3 Gelly API"></a>6.5.3 Gelly API</h3><p>引入好依赖后，接着将介绍一下 Gelly 该如何使用。</p><h4 id="Graph-介绍"><a href="#Graph-介绍" class="headerlink" title="Graph 介绍"></a>Graph 介绍</h4><p>在 Gelly 中，一个图（Graph）由顶点的数据集（DataSet）和边的数据集（DataSet）组成。图中的顶点由 Vertex 类型来表示，一个 Vertex 由唯一的 ID 和一个值来表示。其中 Vertex 的 ID 必须是全局唯一的值，且实现了 Comparable 接口。如果节点不需要由任何值，则该值类型可以声明成 NullValue 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 Vertex&lt;Long，String&gt;</span></span><br><span class="line">Vertex&lt;Long, String&gt; v = <span class="keyword">new</span> Vertex&lt;Long, String&gt;(<span class="number">1L</span>, <span class="string">"foo"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个 Vertex&lt;Long，NullValue&gt;</span></span><br><span class="line">Vertex&lt;Long, NullValue&gt; v = <span class="keyword">new</span> Vertex&lt;Long, NullValue&gt;(<span class="number">1L</span>, NullValue.getInstance());</span><br></pre></td></tr></table></figure><p>Graph 中的边由 Edge 类型来表示，一个 Edge 通常由源顶点的 ID，目标顶点的 ID 以及一个可选的值来表示。其中源顶点和目标顶点的类型必须与 Vertex 的 ID 类型相同。同样的，如果边不需要由任何值，则该值类型可以声明成 NullValue 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Edge&lt;Long, Double&gt; e = <span class="keyword">new</span> Edge&lt;Long, Double&gt;(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">0.5</span>);</span><br><span class="line"><span class="comment">//反转此 edge 的源和目标</span></span><br><span class="line">Edge&lt;Long, Double&gt; reversed = e.reverse();</span><br><span class="line">Double weight = e.getValue(); <span class="comment">// weight = 0.5</span></span><br></pre></td></tr></table></figure><p>在 Gelly 中，一个 Edge 总是从源顶点指向目标顶点。如果图中每条边都能匹配一个从目标顶点到源顶点的 Edge，那么这个图可能是个无向图。同样地，无向图可以用这个方式来表示。</p><h4 id="创建-Graph"><a href="#创建-Graph" class="headerlink" title="创建 Graph"></a>创建 Graph</h4><p>可以通过以下几种方式创建一个 Graph：</p><ul><li>从一个 Edge 数据集合和一个 Vertex 数据集合中创建图。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Vertex&lt;String, Long&gt;&gt; vertices = ...</span><br><span class="line">DataSet&lt;Edge&lt;String, Double&gt;&gt; edges = ...</span><br><span class="line"></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromDataSet(vertices, edges, env);</span><br></pre></td></tr></table></figure><ul><li>从一个表示边的 Tuple2 数据集合中创建图。Gelly 会将每个 Tuple2 转换成一个 Edge，其中第一个元素表示源顶点的 ID，第二个元素表示目标顶点的 ID，图中的顶点和边的 value 值均被设置为 NullValue。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; edges = ...</span><br><span class="line"></span><br><span class="line">Graph&lt;String, NullValue, NullValue&gt; graph = Graph.fromTuple2DataSet(edges, env);</span><br></pre></td></tr></table></figure><ul><li>从一个 Tuple3 数据集和一个可选的 Tuple2 数据集中生成图。在这种情况下，Gelly 会将每个 Tuple3 转换成 Edge，其中第一个元素域是源顶点 ID，第二个域是目标顶点 ID，第三个域是边的值。同样的，每个 Tuple2 会转换成一个顶点 Vertex，其中第一个域是顶点的 ID，第二个域是顶点的 value。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Long&gt;&gt; vertexTuples = env.readCsvFile(<span class="string">"path/to/vertex/input"</span>).types(String.class, Long.class);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; edgeTuples = env.readCsvFile(<span class="string">"path/to/edge/input"</span>).types(String.class, String.class, Double.class);</span><br><span class="line"></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env);</span><br></pre></td></tr></table></figure><ul><li>从一个表示边数据的CSV文件和一个可选的表示节点的CSV文件中生成图。在这种情况下，Gelly会将表示边的CSV文件中的每一行转换成一个Edge，其中第一个域表示源顶点ID，第二个域表示目标顶点ID，第三个域表示边的值。同样的，表示节点的CSV中的每一行都被转换成一个Vertex，其中第一个域表示顶点的ID，第二个域表示顶点的值。为了通过GraphCsvReader生成图，需要指定每个域的类型，可以使用 types、edgeTypes、vertexTypes、keyType 中的方法。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个具有字符串 Vertex id、Long Vertex 和双边缘的图</span></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromCsvReader(<span class="string">"path/to/vertex/input"</span>, <span class="string">"path/to/edge/input"</span>, env)</span><br><span class="line">                    .types(String.class, Long.class, Double.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个既没有顶点值也没有边值的图</span></span><br><span class="line">Graph&lt;Long, NullValue, NullValue&gt; simpleGraph = Graph.fromCsvReader(<span class="string">"path/to/edge/input"</span>, env).keyType(Long.class);</span><br></pre></td></tr></table></figure><ul><li>从一个边的集合和一个可选的顶点的集合中生成图。如果在图创建的时候顶点的集合没有传入，Gelly 会依据数据的边数据集合自动地生成一个 Vertex 集合。这种情况下，创建的节点是没有值的。或者也可以像下面一样，在创建图的时候提供一个 MapFunction 方法来初始化节点的值。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Vertex&lt;Long, Long&gt;&gt; vertexList = <span class="keyword">new</span> ArrayList...</span><br><span class="line"></span><br><span class="line">List&lt;Edge&lt;Long, String&gt;&gt; edgeList = <span class="keyword">new</span> ArrayList...</span><br><span class="line"></span><br><span class="line">Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(vertexList, edgeList, env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将顶点值初始化为顶点ID</span></span><br><span class="line">Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(edgeList,</span><br><span class="line">                <span class="keyword">new</span> MapFunction&lt;Long, Long&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(Long value)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;, env);</span><br></pre></td></tr></table></figure><h4 id="Graph-属性"><a href="#Graph-属性" class="headerlink" title="Graph 属性"></a>Graph 属性</h4><p>Gelly 提供了下列方法来查询图的属性和指标：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Vertex&lt;K, VV&gt;&gt; getVertices()</span><br><span class="line"><span class="comment">//获取边缘数据集</span></span><br><span class="line">DataSet&lt;Edge&lt;K, EV&gt;&gt; getEdges()</span><br><span class="line"><span class="comment">//获取顶点的 id 数据集</span></span><br><span class="line"><span class="function">DataSet&lt;K&gt; <span class="title">getVertexIds</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, K&gt;&gt; <span class="title">getEdgeIds</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">inDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">outDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">getDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">numberOfVertices</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">numberOfEdges</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Triplet&lt;K, VV, EV&gt;&gt; <span class="title">getTriplets</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><h4 id="Graph-转换"><a href="#Graph-转换" class="headerlink" title="Graph 转换"></a>Graph 转换</h4><p>Graph 转换方式有下面几种方式：</p><ul><li>Map：Gelly 提供了专门的用于转换顶点值和边值的方法。mapVertices 和 mapEdges 会返回一个新图，图中的每个顶点和边的 ID 不会改变，但是顶点和边的值会根据用户自定义的映射方法进行修改。这些映射方法同时也可以修改顶点和边的值的类型。</li><li>Translate：Gelly 还提供了专门用于根据用户定义的函数转换顶点和边的 ID 和值的值及类型的方法（translateGraphIDs/translateVertexValues/translateEdgeValues），是Map 功能的升级版，因为 Map 操作不支持修订顶点和边的 ID。</li><li>Filter：Gelly 支持在图中的顶点上或边上执行一个用户指定的 filter 转换。filterOnEdges 会根据提供的在边上的断言在原图的基础上生成一个新的子图，注意，顶点的数据不会被修改。同样的 filterOnVertices 在原图的顶点上进行 filter 转换，不满足断言条件的源节点或目标节点会在新的子图中移除。该子图方法支持同时对顶点和边应用 filter 函数，如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-165050.jpg" alt=""></p><ul><li>Reverse：Gelly中得reverse()方法用于在原图的基础上，生成一个所有边方向与原图相反的新图。</li><li>Undirected：在前面的内容中，我们提到过，Gelly中的图通常都是有向的，而无向图可以通过对所有边添加反向的边来实现，出于这个目的，Gelly提供了getUndirected()方法，用于获取原图的无向图。</li><li>Union：Gelly的union()操作用于联合当前图和指定的输入图，并生成一个新图，在输出的新图中，相同的节点只保留一份，但是重复的边会保留。如下图所示：</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-165224.jpg" alt=""></p><ul><li>Difference：Gelly提供了difference()方法用于发现当前图与指定的输入图之间的差异。</li><li>Intersect：Gelly提供了intersect()方法用于发现两个图中共同存在的边，并将相同的边以新图的方式返回。相同的边指的是具有相同的源顶点，相同的目标顶点和相同的边值。返回的新图中，所有的节点没有任何值，如果需要节点值，可以使用joinWithVertices()方法去任何一个输入图中检索。</li></ul><h4 id="Graph-变化"><a href="#Graph-变化" class="headerlink" title="Graph 变化"></a>Graph 变化</h4><p>Gelly 内置下列方法以支持对一个图进行节点和边的增加/移除操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addVertex</span><span class="params">(<span class="keyword">final</span> Vertex&lt;K, VV&gt; vertex)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addVertices</span><span class="params">(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToAdd)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addEdge</span><span class="params">(Vertex&lt;K, VV&gt; source, Vertex&lt;K, VV&gt; target, EV edgeValue)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addEdges</span><span class="params">(List&lt;Edge&lt;K, EV&gt;&gt; newEdges)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeVertex</span><span class="params">(Vertex&lt;K, VV&gt; vertex)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeVertices</span><span class="params">(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToBeRemoved)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeEdge</span><span class="params">(Edge&lt;K, EV&gt; edge)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeEdges</span><span class="params">(List&lt;Edge&lt;K, EV&gt;&gt; edgesToBeRemoved)</span></span></span><br></pre></td></tr></table></figure><h4 id="Neighborhood-Methods"><a href="#Neighborhood-Methods" class="headerlink" title="Neighborhood Methods"></a>Neighborhood Methods</h4><h4 id="Graph-验证"><a href="#Graph-验证" class="headerlink" title="Graph 验证"></a>Graph 验证</h4><h3 id="6-5-4-小结与反思"><a href="#6-5-4-小结与反思" class="headerlink" title="6.5.4 小结与反思"></a>6.5.4 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><p>本章所讲的内容属于 Flink 的扩展库，包含了 CEP 复杂事件处理、State Processor API、Machine Learning 和 Gelly，各种都有讲解一些样例，但是没有过多深入的讲，但还是希望你可以在书本外自己去扩充这些内容的知识点。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-5-Flink-扩展库——Gelly&quot;&gt;&lt;a href=&quot;#6-5-Flink-扩展库——Gelly&quot; class=&quot;headerlink&quot; title=&quot;6.5 Flink 扩展库——Gelly&quot;&gt;&lt;/a&gt;6.5 Flink 扩展库——Gelly&lt;/h2&gt;&lt;p&gt;在 1.9 版本中还剩最后一个扩展库就是 Gelly，本节将带你了解一下 Gelly 的功能以及如何使用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 扩展库——Machine Learning</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/01/flink-in-action-6.4/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/01/flink-in-action-6.4/</id>
    <published>2021-07-31T16:00:00.000Z</published>
    <updated>2021-12-26T11:38:23.421Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-4-Flink-扩展库——Machine-Learning"><a href="#6-4-Flink-扩展库——Machine-Learning" class="headerlink" title="6.4 Flink 扩展库——Machine Learning"></a>6.4 Flink 扩展库——Machine Learning</h2><p>随着人工智能的火热，机器学习这门技术也变得异常重要，Flink 作为一个数据处理的引擎，虽然目前在该方面还较弱，但是在 Flink Forward Asia 2019 北京站后，阿里开源了 <a href="https://github.com/alibaba/Alink">Alink</a> 平台的核心代码，并上传了一系列的算法库，该项目是基于 Flink 的通用算法平台，开发者和数据分析师可以利用 Alink 提供的一系列算法来构建软件功能，例如统计分析、机器学习、实时预测、个性化推荐和异常检测。相信未来 Flink 的机器学习库将会应用到更多的场景去，本节将带你了解一下 Flink 中的机器学习库。</p><a id="more"></a><h3 id="6-4-1-Flink-ML-简介"><a href="#6-4-1-Flink-ML-简介" class="headerlink" title="6.4.1 Flink-ML 简介"></a>6.4.1 Flink-ML 简介</h3><p>ML 是 Machine Learning 的简称，Flink-ML 是 Flink 的机器学习类库。在 Flink 1.9 之前该类库是存在 <code>flink-libraries</code> 模块下的，但是在 Flink 1.9 版本中，为了支持 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-39+Flink+ML+pipeline+and+ML+libs">FLIP-39</a> ，所以该类库被移除了。</p><p>建立 FLIP-39 的目的主要是增强 Flink-ML 的可伸缩性和易用性。通常使用机器学习的有两类人，一类是机器学习算法库的开发者，他们需要一套标准的 API 来实现算法，每个机器学习算法会在这些 API 的基础上实现；另一类用户是直接利用这些现有的机器学习算法库去训练数据模型，整个训练是要通过很多转换或者算法才能完成的，所以如果能够提供 ML Pipeline，那么对于后一类用户来说绝对是一种福音。虽然在 1.9 中移除了之前的 Flink-ML 模块，但是在 Flink 项目下出现了一个 <code>flink-ml-parent</code> 的模块，该模块有两个子模块 <code>flink-ml-api</code> 和 <code>flink-ml-lib</code>。</p><p><code>flink-ml-api</code> 模块增加了 ML Pipeline 和 MLLib 的接口，它的类结构图如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-22-124512.png" alt=""></p><p>主要接口如下所示：</p><ul><li>Transformer: Transformer 是一种可以将一个表转换成另一个表的算法</li><li>Model: Model 是一种特别的 Transformer，它继承自 Transformer。它通常是由 Estimator 生成，Model 用于推断，输入一个数据表会生成结果表。</li><li>Estimator: Estimator 是一个可以根据一个数据表生成一个模型的算法。</li><li>Pipeline: Pipeline 描述的是机器学习的工作流，它将很多 Transformer 和 Estimator 连接在一起成一个工作流。</li><li>PipelineStage: PipelineStage 是 Pipeline 的基础节点，Transformer 和 Estimator 两个都继承自 PipelineStage 接口。</li><li>Params: Params 是一个参数容器。</li><li>WithParams: WithParams 有一个保存参数的 Params 容器。通常会使用在 PipelineStage 里面，因为几乎所有的算法都需要参数。</li></ul><p>Flink-ML 的 pipeline 流程如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-22-135555.png" alt=""></p><p><code>flink-ml-lib</code> 模块包括了 DenseMatrix、DenseVector、SparseVector 等类的基本操作。这两个模块是 Flink-ML 的基础模块，相信社区在后面的稳定版本一定会带来更加完善的 Flink-ML 库。</p><h3 id="6-4-2-使用-Flink-ML"><a href="#6-4-2-使用-Flink-ML" class="headerlink" title="6.4.2 使用 Flink-ML"></a>6.4.2 使用 Flink-ML</h3><p>虽然在 Flink 1.9 中已经移除了 Flink-ML 模块，但是在之前的版本还是支持的，如果你们公司使用的是低于 1.9 的版本，那么还是可以使用的，在使用之前引入依赖（假设使用的是 Flink 1.8 版本）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-ml_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>另外如果是要运行的话还是要将 opt 目录下的 flink-ml_2.11-1.8.0.jar 移到 lib 目录下。下面演示下如何训练多元线性回归模型：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//带标签的特征向量</span></span><br><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataSet: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="comment">//使用 Splitter 将数据集拆分成训练数据和测试数据</span></span><br><span class="line"><span class="keyword">val</span> trainTestData: <span class="type">DataSet</span>[<span class="type">TrainTestDataSet</span>] = <span class="type">Splitter</span>.trainTestSplit(dataSet)</span><br><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = trainTestData.training</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = trainTestData.testing.map(lv =&gt; lv.vector)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mlr = <span class="type">MultipleLinearRegression</span>()</span><br><span class="line">  .setStepsize(<span class="number">1.0</span>)</span><br><span class="line">  .setIterations(<span class="number">100</span>)</span><br><span class="line">  .setConvergenceThreshold(<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">mlr.fit(trainingData)</span><br><span class="line"></span><br><span class="line"><span class="comment">//已经形成的模型可以用来预测数据了</span></span><br><span class="line"><span class="keyword">val</span> predictions: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = mlr.predict(testingData)</span><br></pre></td></tr></table></figure><h3 id="6-4-3-使用-Flink-ML-Pipeline"><a href="#6-4-3-使用-Flink-ML-Pipeline" class="headerlink" title="6.4.3 使用 Flink-ML Pipeline"></a>6.4.3 使用 Flink-ML Pipeline</h3><h3 id="6-4-4-Alink-介绍"><a href="#6-4-4-Alink-介绍" class="headerlink" title="6.4.4 Alink 介绍"></a>6.4.4 Alink 介绍</h3><h3 id="6-4-5-小结与反思"><a href="#6-4-5-小结与反思" class="headerlink" title="6.4.5 小结与反思"></a>6.4.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-4-Flink-扩展库——Machine-Learning&quot;&gt;&lt;a href=&quot;#6-4-Flink-扩展库——Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;6.4 Flink 扩展库——Machine Learning&quot;&gt;&lt;/a&gt;6.4 Flink 扩展库——Machine Learning&lt;/h2&gt;&lt;p&gt;随着人工智能的火热，机器学习这门技术也变得异常重要，Flink 作为一个数据处理的引擎，虽然目前在该方面还较弱，但是在 Flink Forward Asia 2019 北京站后，阿里开源了 &lt;a href=&quot;https://github.com/alibaba/Alink&quot;&gt;Alink&lt;/a&gt; 平台的核心代码，并上传了一系列的算法库，该项目是基于 Flink 的通用算法平台，开发者和数据分析师可以利用 Alink 提供的一系列算法来构建软件功能，例如统计分析、机器学习、实时预测、个性化推荐和异常检测。相信未来 Flink 的机器学习库将会应用到更多的场景去，本节将带你了解一下 Flink 中的机器学习库。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 扩展库——State Processor API</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/30/flink-in-action-6.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/30/flink-in-action-6.3/</id>
    <published>2021-07-29T16:00:00.000Z</published>
    <updated>2021-12-26T11:37:33.546Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-3-Flink-扩展库——State-Processor-API"><a href="#6-3-Flink-扩展库——State-Processor-API" class="headerlink" title="6.3 Flink 扩展库——State Processor API"></a>6.3 Flink 扩展库——State Processor API</h2><p>State Processor API 功能是在 1.9 版本中新增加的一个功能，本节将带你了解一下其功能和如何使用？</p><a id="more"></a><h3 id="6-3-1-State-Processor-API-简介"><a href="#6-3-1-State-Processor-API-简介" class="headerlink" title="6.3.1 State Processor API 简介"></a>6.3.1 State Processor API 简介</h3><p>能够从外部访问 Flink 作业的状态一直用户迫切需要的功能之一，在 Apache Flink 1.9.0 中新引入了 State Processor API，该 API 让用户可以通过  Flink DataSet 作业来灵活读取、写入和修改 Flink 的 Savepoint 和 Checkpoint。</p><h3 id="6-3-2-在-Flink-1-9-之前是如何处理状态的？"><a href="#6-3-2-在-Flink-1-9-之前是如何处理状态的？" class="headerlink" title="6.3.2 在 Flink 1.9 之前是如何处理状态的？"></a>6.3.2 在 Flink 1.9 之前是如何处理状态的？</h3><p>一般来说，大多数的 Flink 作业都是有状态的，并且随着作业运行的时间越来越久，就会累积越多越多的状态，如果因为故障导致作业崩溃可能会导致作业的状态都丢失，那么对于比较重要的状态来说，损失就会很大。为了保证作业状态的一致性和持久性，Flink 从一开始使用的就是 Checkpoint 和 Savepoint 来保存状态，并且可以从 Savepoint 中恢复状态。在 Flink 的每个新 Release 版本中，Flink 社区添加了越来越多与状态相关的功能以提高 Checkpoint 的速度和恢复速度。</p><p>有的时候，用户可能会有这些需求场景，比如从第三方外部系统访问作业的状态、将作业的状态信息迁移到另一个应用程序等，目前现有支持查询作业状态的功能 Queryable State，但是在 Flink 中目前该功能只支持根据 Key 查找，并且不能保证返回值的一致性。另外该功能不支持添加和修改作业的状态，所以适用的场景还是比较有限。</p><h3 id="6-3-3-使用-State-Processor-API-读写作业状态"><a href="#6-3-3-使用-State-Processor-API-读写作业状态" class="headerlink" title="6.3.3 使用 State Processor API 读写作业状态"></a>6.3.3 使用 State Processor API 读写作业状态</h3><p>在 1.9 版本中的 State Processor API，它完全和之前不一致，该功能使用 InputFormat 和 OutputFormat 扩展了 DataSet API 以读取和写入 Checkpoint 和 Savepoint 数据。由于 DataSet 和 Table API 的互通性，所以也可以使用 Table API 或者 SQL 查询和分析状态的数据。例如，再获取到正在运行的流作业状态的 Checkpoint 后，可以使用 DataSet 批处理程序对其进行分析，以验证该流作业的运行是否正确。另外 State Processor API 还可以修复不一致的状态信息，它提供了很多方法来开发有状态的应用程序，这些方法在以前的版本中因为设计的问题导致作业在启动后不能再修改，否则状态可能会丢失。现在，你可以任意修改状态的数据类型、调整算子的最大并行度、拆分或合并算子的状态、重新分配算子的 uid 等。</p><p>如果要使用 State Processor API 去读写作业的状态，你需要添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-state-processor-api_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="6-3-4-使用-DataSet-读取作业状态"><a href="#6-3-4-使用-DataSet-读取作业状态" class="headerlink" title="6.3.4 使用 DataSet 读取作业状态"></a>6.3.4 使用 DataSet 读取作业状态</h3><p>State Processor API 将作业的状态映射到一个或多个可以单独处理的数据集，为了能够使用该 API，需要先了解这个映射的工作方式，首先来看下有状态的 Flink 作业是什么样子的。Flink 作业是由很多算子组成，通常是一个或多个数据源（Source）、一些实际处理数据的算子（比如 Map／Filter／FlatMap 等）和一个或者多个 Sink。每个算子会在一个或者多个任务中并行运行（取决于并行度），并且可以使用不同类型的状态，算子可能会有零个、一个或多个 Operator State，这些状态会组成一个以算子任务为范围的列表。如果是算子应用在 KeyedStream，它还有零个、一个或者多个 Keyed State，它们的作用域范围是从每个已处理数据中提取 Key，可以将 Keyed State 看作是一个分布式的 Map。</p><p>State Processor API 现在提供了读取、新增和修改 Savepoint 数据的方法，比如从已加载的 Savepoint 中读取数据集，然后将数据集转换为状态并将其保存到 Savepoint。下面分别讲解下这三种方法该如何使用。</p><h4 id="读取现有的-Savepoint"><a href="#读取现有的-Savepoint" class="headerlink" title="读取现有的 Savepoint"></a>读取现有的 Savepoint</h4><p>读取状态首先需要指定一个 Savepoint（或者 Checkpoint） 的路径和状态后端存储的类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment bEnv   = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">ExistingSavepoint savepoint = Savepoint.load(bEnv, <span class="string">"hdfs://path/"</span>, <span class="keyword">new</span> RocksDBStateBackend());</span><br></pre></td></tr></table></figure><p>读取 Operator State 时，只需指定算子的 uid、状态名称和类型信息。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; listState  = savepoint.readListState(<span class="string">"zhisheng-uid"</span>, <span class="string">"list-state"</span>, Types.INT);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Integer&gt; unionState = savepoint.readUnionState(<span class="string">"zhisheng-uid"</span>, <span class="string">"union-state"</span>, Types.INT);</span><br><span class="line"> </span><br><span class="line">DataSet&lt;Tuple2&lt;Integer, Integer&gt;&gt; broadcastState = savepoint.readBroadcastState(<span class="string">"zhisheng-uid"</span>, <span class="string">"broadcast-state"</span>, Types.INT, Types.INT);</span><br></pre></td></tr></table></figure><p>如果在状态描述符（StateDescriptor）中使用了自定义类型序列化器 TypeSerializer，也可以指定它：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; listState = savepoint.readListState(</span><br><span class="line">    <span class="string">"zhisheng-uid"</span>, <span class="string">"list-state"</span>, </span><br><span class="line">    Types.INT, <span class="keyword">new</span> MyCustomIntSerializer());</span><br></pre></td></tr></table></figure><h4 id="写入新的-Savepoint"><a href="#写入新的-Savepoint" class="headerlink" title="写入新的 Savepoint"></a>写入新的 Savepoint</h4><h4 id="修改现有的-Savepoint"><a href="#修改现有的-Savepoint" class="headerlink" title="修改现有的 Savepoint"></a>修改现有的 Savepoint</h4><h3 id="6-3-5-为什么要使用-DataSet-API？"><a href="#6-3-5-为什么要使用-DataSet-API？" class="headerlink" title="6.3.5 为什么要使用 DataSet API？"></a>6.3.5 为什么要使用 DataSet API？</h3><h3 id="6-3-6-小结与反思"><a href="#6-3-6-小结与反思" class="headerlink" title="6.3.6 小结与反思"></a>6.3.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-3-Flink-扩展库——State-Processor-API&quot;&gt;&lt;a href=&quot;#6-3-Flink-扩展库——State-Processor-API&quot; class=&quot;headerlink&quot; title=&quot;6.3 Flink 扩展库——State Processor API&quot;&gt;&lt;/a&gt;6.3 Flink 扩展库——State Processor API&lt;/h2&gt;&lt;p&gt;State Processor API 功能是在 1.9 版本中新增加的一个功能，本节将带你了解一下其功能和如何使用？&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 使用 Flink CEP 处理复杂事件</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/29/flink-in-action-6.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/29/flink-in-action-6.2/</id>
    <published>2021-07-28T16:00:00.000Z</published>
    <updated>2021-12-26T11:32:24.812Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-2-使用-Flink-CEP-处理复杂事件"><a href="#6-2-使用-Flink-CEP-处理复杂事件" class="headerlink" title="6.2 使用 Flink CEP 处理复杂事件"></a>6.2 使用 Flink CEP 处理复杂事件</h2><p>6.1 节中介绍 Flink CEP 和其使用场景，本节将详细介绍 Flink CEP 的 API，教会大家如何去使用 Flink CEP。</p><a id="more"></a><h3 id="6-2-1-准备依赖"><a href="#6-2-1-准备依赖" class="headerlink" title="6.2.1 准备依赖"></a>6.2.1 准备依赖</h3><p>要开发 Flink CEP 应用程序，首先你得在项目的 <code>pom.xml</code> 中添加依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个依赖有两种，一个是 Java 版本的，一个是 Scala 版本，你可以根据项目的开发语言自行选择。</p><h3 id="6-2-2-Flink-CEP-入门应用程序"><a href="#6-2-2-Flink-CEP-入门应用程序" class="headerlink" title="6.2.2 Flink CEP 入门应用程序"></a>6.2.2 Flink CEP 入门应用程序</h3><p>准备好依赖后，我们开始第一个 Flink CEP 应用程序，这里我们只做一个简单的数据流匹配，当匹配成功后将匹配的两条数据打印出来。首先定义实体类 Event 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后构造读取 Socket 数据流将数据进行转换成 Event，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventDataStream = env.socketTextStream(<span class="string">"127.0.0.1"</span>, <span class="number">9200</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Event&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtil.isNotEmpty(s)) &#123;</span><br><span class="line">                String[] split = s.split(<span class="string">","</span>);</span><br><span class="line">                <span class="keyword">if</span> (split.length == <span class="number">2</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Event(Integer.valueOf(split[<span class="number">0</span>]), split[<span class="number">1</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>接着就是定义 CEP 中的匹配规则了，下面的规则表示第一个事件的 id 为 42，紧接着的第二个事件 id 要大于 10，满足这样的连续两个事件才会将这两条数据进行打印出来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; pattern = Pattern.&lt;Event&gt;begin(<span class="string">"start"</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">"start &#123;&#125;"</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() == <span class="number">42</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">).next(<span class="string">"middle"</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">"middle &#123;&#125;"</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() &gt;= <span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        log.info(<span class="string">"p = &#123;&#125;"</span>, p);</span><br><span class="line">        builder.append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getName()).append(<span class="string">"\n"</span>)</span><br><span class="line">                .append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();<span class="comment">//打印结果</span></span><br></pre></td></tr></table></figure><p>然后笔者在终端开启 Socket，输入的两条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">42,zhisheng</span><br><span class="line">20,zhisheng</span><br></pre></td></tr></table></figure><p>作业打印出来的日志如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072247.png" alt=""></p><p>整个作业 print 出来的结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072320.png" alt=""></p><p>好了，一个完整的 Flink CEP 应用程序如上，相信你也能大概理解上面的代码，接着来详细的讲解一下 Flink CEP 中的 Pattern API。</p><h3 id="6-2-3-Pattern-API"><a href="#6-2-3-Pattern-API" class="headerlink" title="6.2.3 Pattern API"></a>6.2.3 Pattern API</h3><p>你可以通过 Pattern API 去定义从流数据中匹配事件的 Pattern，每个复杂 Pattern 都是由多个简单的 Pattern 组成的，拿前面入门的应用来讲，它就是由 <code>start</code> 和 <code>middle</code> 两个简单的 Pattern 组成的，在其每个 Pattern 中都只是简单的处理了流数据。在处理的过程中需要标示该 Pattern 的名称，以便后续可以使用该名称来获取匹配到的数据，如 <code>p.get(&quot;start&quot;).get(0)</code> 它就可以获取到 Pattern 中匹配的第一个事件。接下来我们先来看下简单的 Pattern 。</p><h4 id="单个-Pattern"><a href="#单个-Pattern" class="headerlink" title="单个 Pattern"></a>单个 Pattern</h4><p><strong>数量</strong></p><p><strong>条件</strong></p><h4 id="组合-Pattern"><a href="#组合-Pattern" class="headerlink" title="组合 Pattern"></a>组合 Pattern</h4><h4 id="Group-Pattern"><a href="#Group-Pattern" class="headerlink" title="Group Pattern"></a>Group Pattern</h4><h4 id="事件匹配跳过策略"><a href="#事件匹配跳过策略" class="headerlink" title="事件匹配跳过策略"></a>事件匹配跳过策略</h4><h3 id="6-2-4-检测-Pattern"><a href="#6-2-4-检测-Pattern" class="headerlink" title="6.2.4 检测 Pattern"></a>6.2.4 检测 Pattern</h3><h4 id="选择-Pattern"><a href="#选择-Pattern" class="headerlink" title="选择 Pattern"></a>选择 Pattern</h4><h3 id="6-2-5-CEP-时间属性"><a href="#6-2-5-CEP-时间属性" class="headerlink" title="6.2.5 CEP 时间属性"></a>6.2.5 CEP 时间属性</h3><h4 id="根据事件时间处理延迟数据"><a href="#根据事件时间处理延迟数据" class="headerlink" title="根据事件时间处理延迟数据"></a>根据事件时间处理延迟数据</h4><h4 id="时间上下文"><a href="#时间上下文" class="headerlink" title="时间上下文"></a>时间上下文</h4><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="6-2-6-小结与反思"><a href="#6-2-6-小结与反思" class="headerlink" title="6.2.6 小结与反思"></a>6.2.6 小结与反思</h3><p>本节开始通过一个 Flink CEP 案例教大家上手，后面通过讲解 Flink CEP 的 Pattern API，更多详细的还是得去看官网文档，其实也建议大家好好的跟着官网的文档过一遍所有的 API，并跟着敲一些样例来实现，这样在开发需求的时候才能够及时的想到什么场景下该使用哪种 API，接着教了大家如何将 Pattern 与数据流结合起来匹配并获取匹配的数据，最后讲了下 CEP 中的时间概念。</p><p>你公司有使用 Flink CEP 吗？通常使用哪些 API 居多？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-2-使用-Flink-CEP-处理复杂事件&quot;&gt;&lt;a href=&quot;#6-2-使用-Flink-CEP-处理复杂事件&quot; class=&quot;headerlink&quot; title=&quot;6.2 使用 Flink CEP 处理复杂事件&quot;&gt;&lt;/a&gt;6.2 使用 Flink CEP 处理复杂事件&lt;/h2&gt;&lt;p&gt;6.1 节中介绍 Flink CEP 和其使用场景，本节将详细介绍 Flink CEP 的 API，教会大家如何去使用 Flink CEP。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink CEP 简介及其使用场景</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/28/flink-in-action-6.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/28/flink-in-action-6.1/</id>
    <published>2021-07-27T16:00:00.000Z</published>
    <updated>2021-12-26T11:28:30.835Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第六章-——-扩展库"><a href="#第六章-——-扩展库" class="headerlink" title="第六章 —— 扩展库"></a>第六章 —— 扩展库</h1><p>Flink 源码中有单独的 <code>flink-libraries</code> 模块用来存放一些扩展库，比如 CEP、Gelly、Machine Learning、State Processor API。本章将分别介绍这几种扩展库以及如何应用在我们的项目中。</p><h2 id="6-1-Flink-CEP-简介及其使用场景"><a href="#6-1-Flink-CEP-简介及其使用场景" class="headerlink" title="6.1 Flink CEP 简介及其使用场景"></a>6.1 Flink CEP 简介及其使用场景</h2><a id="more"></a><p>Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案，它可以在实时数据中匹配复杂事件，用处很广，本节将带大家了解其功能和应用场景。</p><h3 id="6-1-1-CEP-简介？"><a href="#6-1-1-CEP-简介？" class="headerlink" title="6.1.1 CEP 简介？"></a>6.1.1 CEP 简介？</h3><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。它可以用于处理实时数据并在事件流到达时从事件流中提取信息，并根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</p><p>因为这种事件匹配通常是根据提前制定好的规则去匹配的，而这些规则一般来说不仅多，而且复杂，所以就会引入一些规则引擎来处理这种复杂事件匹配。市面上常用的规则引擎有如下这些。</p><h3 id="6-1-2-规则引擎对比"><a href="#6-1-2-规则引擎对比" class="headerlink" title="6.1.2 规则引擎对比"></a>6.1.2 规则引擎对比</h3><p>目前开源的规则引擎有很多种，接下来将对比一下 Drools、Aviator、EasyRules、Esper、Flink CEP 之间的优势和劣势。</p><h4 id="Drools"><a href="#Drools" class="headerlink" title="Drools"></a>Drools</h4><p>Drools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。</p><p>优势:</p><ul><li>功能较为完善，具有如系统监控、操作平台等功能。</li><li>规则支持动态更新。</li></ul><p>劣势:</p><ul><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Aviator"><a href="#Aviator" class="headerlink" title="Aviator"></a>Aviator</h4><p>Aviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。</p><p>优势：</p><ul><li>支持大部分运算操作符。</li><li>支持函数调用和自定义函数。</li><li>支持正则表达式匹配。</li><li>支持传入变量并且性能优秀。</li></ul><p>劣势：</p><ul><li>没有 if else、do while 等语句，没有赋值语句，没有位运算符。</li></ul><h4 id="EasyRules"><a href="#EasyRules" class="headerlink" title="EasyRules"></a>EasyRules</h4><p>EasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。</p><p>优势：</p><ul><li>轻量级框架，学习成本低。</li><li>基于 POJO。</li><li>为定义业务引擎提供有用的抽象和简便的应用</li><li>支持从简单的规则组建成复杂规则</li></ul><h4 id="Esper"><a href="#Esper" class="headerlink" title="Esper"></a>Esper</h4><p>Esper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。</p><p>优势:</p><ul><li>轻量级可嵌入开发，常用的 CEP 功能简单好用。</li><li>EPL 语法与 SQL 类似，学习成本较低。</li></ul><p>劣势:</p><ul><li>单机全内存方案，需要整合其他分布式和存储。</li><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h4><p>Flink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。</p><p>优势:</p><ul><li>继承了 Flink 高吞吐的特点</li><li>事件支持存储到外部，可以支持较长跨度的时间窗。</li><li>可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现）</li></ul><p>劣势:</p><ul><li>无法动态更新规则（痛点）</li></ul><h3 id="6-1-3-Flink-CEP-简介"><a href="#6-1-3-Flink-CEP-简介" class="headerlink" title="6.1.3 Flink CEP 简介"></a>6.1.3 Flink CEP 简介</h3><h3 id="6-1-4-Flink-CEP-动态更新规则"><a href="#6-1-4-Flink-CEP-动态更新规则" class="headerlink" title="6.1.4 Flink CEP 动态更新规则"></a>6.1.4 Flink CEP 动态更新规则</h3><h3 id="6-1-5-Flink-CEP-使用场景分析"><a href="#6-1-5-Flink-CEP-使用场景分析" class="headerlink" title="6.1.5 Flink CEP 使用场景分析"></a>6.1.5 Flink CEP 使用场景分析</h3><h4 id="实时反作弊和风控"><a href="#实时反作弊和风控" class="headerlink" title="实时反作弊和风控"></a>实时反作弊和风控</h4><h4 id="实时营销"><a href="#实时营销" class="headerlink" title="实时营销"></a>实时营销</h4><h4 id="实时网络攻击检测"><a href="#实时网络攻击检测" class="headerlink" title="实时网络攻击检测"></a>实时网络攻击检测</h4><h3 id="6-1-6-小结与反思"><a href="#6-1-6-小结与反思" class="headerlink" title="6.1.6 小结与反思"></a>6.1.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第六章-——-扩展库&quot;&gt;&lt;a href=&quot;#第六章-——-扩展库&quot; class=&quot;headerlink&quot; title=&quot;第六章 —— 扩展库&quot;&gt;&lt;/a&gt;第六章 —— 扩展库&lt;/h1&gt;&lt;p&gt;Flink 源码中有单独的 &lt;code&gt;flink-libraries&lt;/code&gt; 模块用来存放一些扩展库，比如 CEP、Gelly、Machine Learning、State Processor API。本章将分别介绍这几种扩展库以及如何应用在我们的项目中。&lt;/p&gt;
&lt;h2 id=&quot;6-1-Flink-CEP-简介及其使用场景&quot;&gt;&lt;a href=&quot;#6-1-Flink-CEP-简介及其使用场景&quot; class=&quot;headerlink&quot; title=&quot;6.1 Flink CEP 简介及其使用场景&quot;&gt;&lt;/a&gt;6.1 Flink CEP 简介及其使用场景&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Table API &amp; SQL 功能</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.2/</id>
    <published>2021-07-26T16:00:00.000Z</published>
    <updated>2021-12-26T11:26:14.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="5-2-Flink-Table-API-amp-SQL-功能"><a href="#5-2-Flink-Table-API-amp-SQL-功能" class="headerlink" title="5.2 Flink Table API &amp; SQL 功能"></a>5.2 Flink Table API &amp; SQL 功能</h2><p>在 5.1 节中对 Flink Table API &amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。</p><a id="more"></a><h3 id="5-2-1-Flink-Table-和-SQL-与-DataStream-和-DataSet-集成"><a href="#5-2-1-Flink-Table-和-SQL-与-DataStream-和-DataSet-集成" class="headerlink" title="5.2.1 Flink Table 和 SQL 与 DataStream 和 DataSet 集成"></a>5.2.1 Flink Table 和 SQL 与 DataStream 和 DataSet 集成</h3><p>两个 planner 都可以与 DataStream API 集成，只有以前的 planner 才可以集成 DataSet API，所以下面讨论 DataSet API 都是和以前的 planner 有关。</p><p>Table API &amp; SQL 查询与 DataStream 和 DataSet 程序集成是非常简单的，比如可以通过 Table API 或者 SQL 查询外部表数据，进行一些预处理后，然后使用 DataStream 或 DataSet API 继续处理一些复杂的计算，另外也可以将 DataStream 或 DataSet 处理后的数据利用 Table API 或者 SQL 写入到外部表去。总而言之，它们之间互相转换或者集成比较容易。</p><h4 id="Scala-的隐式转换"><a href="#Scala-的隐式转换" class="headerlink" title="Scala 的隐式转换"></a>Scala 的隐式转换</h4><p>Scala Table API 提供了 DataSet、DataStream 和 Table 类的隐式转换，可以通过导入 <code>org.apache.flink.table.api.scala._</code> 或者 <code>org.apache.flink.api.scala._</code> 包来启用这些转换。</p><h4 id="将-DataStream-或-DataSet-注册为-Table"><a href="#将-DataStream-或-DataSet-注册为-Table" class="headerlink" title="将 DataStream 或 DataSet 注册为 Table"></a>将 DataStream 或 DataSet 注册为 Table</h4><p>DataStream 或者 DataSet 可以注册为 Table，结果表的 schema 取决于已经注册的 DataStream 和 DataSet 的数据类型。你可以像下面这种方式转换：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable 表</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable"</span>, stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable2 表（表中的字段为 myLong、myString）</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h4 id="将-DataStream-或-DataSet-转换为-Table"><a href="#将-DataStream-或-DataSet-转换为-Table" class="headerlink" title="将 DataStream 或 DataSet 转换为 Table"></a>将 DataStream 或 DataSet 转换为 Table</h4><p>除了可以将 DataStream 或 DataSet 注册为 Table，还可以将它们转换为 Table，代码如下所示，转换之后再去使用 Table API 查询就比较方便了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h4 id="将-Table-转换成-DataStream-或-DataSet"><a href="#将-Table-转换成-DataStream-或-DataSet" class="headerlink" title="将 Table 转换成 DataStream 或 DataSet"></a>将 Table 转换成 DataStream 或 DataSet</h4><p>Table 可以转换为 DataStream 或 DataSet，这样就可以在 Table API 或 SQL 查询的结果上运行自定义的 DataStream 或 DataSet 程序。当将一个 Table 转换成 DataStream 或 DataSet 时，需要指定结果 DataStream 或 DataSet 的数据类型，最方便的数据类型是 Row，下面几个数据类型表示不同的功能：</p><ul><li>Row：字段按位置映射，任意数量的字段，支持 null 值，没有类型安全访问。</li><li>POJO：字段按名称映射，POJO 属性必须按照 Table 中的属性来命名，任意数量的字段，支持 null 值，类型安全访问</li><li>Case Class：字段按位置映射，不支持 null 值，类型安全访问。</li><li>Tuple：按位置映射字段，限制为 22（Scala）或 25（Java）字段，不支持 null 值，类型安全访问。</li><li>原子类型：Table 必须具有单个字段，不支持 null 值，类型安全访问。</li></ul><h5 id="将-Table-转换成-DataStream"><a href="#将-Table-转换成-DataStream" class="headerlink" title="将 Table 转换成 DataStream"></a>将 Table 转换成 DataStream</h5><p>流查询的结果表会动态更新，即每个新的记录到达输入流时结果就会发生变化。所以在将 Table 转换成 DataStream 就需要对表的更新进行编码，有两种将 Table 转换为 DataStream 的模式：</p><ul><li>追加模式(Append Mode)：这种模式只能在动态表仅通过 INSERT 更改修改时才能使用，即仅追加，之前发出的结果不会更新。</li><li>撤回模式(Retract Mode)：任何时刻都可以使用此模式，它使用一个 boolean 标志来编码 INSERT 和 DELETE 的更改。</li></ul><p>两种模式的代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定类，将表转换为一个 append DataStream</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 append DataStream</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为一个 Retract DataStream Row</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h5 id="将-Table-转换成-DataSet"><a href="#将-Table-转换成-DataSet" class="headerlink" title="将 Table 转换成 DataSet"></a>将 Table 转换成 DataSet</h5><p>将 Table 转换成 DataSet 的样例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定一个类将表转换为一个 Row DataSet</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 DataSet</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure><h3 id="5-2-2-查询优化"><a href="#5-2-2-查询优化" class="headerlink" title="5.2.2 查询优化"></a>5.2.2 查询优化</h3><p>Flink 使用 Calcite 来优化和翻译查询，以前的 planner 不会去优化 join 的顺序，而是按照查询中定义的顺序去执行。通过提供一个 CalciteConfig 对象来调整在不同阶段应用的优化规则集，这个可以通过调用 <code>CalciteConfig.createBuilder()</code> 获得的 builder 来创建，并且可以通过调用<code>tableEnv.getConfig.setCalciteConfig(calciteConfig)</code> 来提供给 TableEnvironment。而在 Blink planner 中扩展了 Calcite 来执行复杂的查询优化，这包括一系列基于规则和成本的优化，比如：</p><ul><li>基于 Calcite 的子查询去相关性</li><li>Project pruning</li><li>Partition pruning</li><li>Filter push-down</li><li>删除子计划中的重复数据以避免重复计算</li><li>重写特殊的子查询，包括两部分：<ul><li>将 IN 和 EXISTS 转换为 left semi-joins</li><li>将 NOT IN 和 NOT EXISTS 转换为 left anti-join</li></ul></li><li>重排序可选的 join<ul><li>通过启用 table.optimizer.join-reorder-enabled</li></ul></li></ul><p>注意：IN/EXISTS/NOT IN/NOT EXISTS 目前只支持子查询重写中的连接条件。</p><h4 id="解释-Table"><a href="#解释-Table" class="headerlink" title="解释 Table"></a>解释 Table</h4><p>Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。你可以通过 <code>TableEnvironment.explain(table)</code> 或者 <code>TableEnvironment.explain()</code> 方法来完成。<code>explain(table)</code> 会返回给定计划的 Table，<code>explain()</code> 会返回多路 Sink 计划的结果（主要用于 Blink planner）。它返回一个描述三个计划的字符串：</p><ul><li>关系查询的抽象语法树，即未优化的逻辑查询计划</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ul><p>以下代码演示了一个 Table 示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line"></span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1.where(<span class="string">"LIKE(word, 'F%')"</span>).unionAll(table2);</span><br><span class="line"></span><br><span class="line">System.out.println(tEnv.explain(table));</span><br></pre></td></tr></table></figure><p>通过 <code>explain(table)</code> 方法返回的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">== Abstract Syntax Tree ==</span><br><span class="line">LogicalUnion(all=[true])</span><br><span class="line">  LogicalFilter(condition=[LIKE($1, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">DataStreamUnion(all=[true], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    DataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 2 : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 3 : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">Stage 4 : Operator</span><br><span class="line">content : where: (LIKE(word, _UTF-16LE&apos;F%&apos;)), select: (count, word)</span><br><span class="line">ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">Stage 5 : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h3 id="5-2-3-数据类型"><a href="#5-2-3-数据类型" class="headerlink" title="5.2.3 数据类型"></a>5.2.3 数据类型</h3><h3 id="5-2-4-时间属性"><a href="#5-2-4-时间属性" class="headerlink" title="5.2.4 时间属性"></a>5.2.4 时间属性</h3><h4 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h4><h4 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a>Event time</h4><h3 id="5-2-5-SQL-Connector"><a href="#5-2-5-SQL-Connector" class="headerlink" title="5.2.5 SQL Connector"></a>5.2.5 SQL Connector</h3><p><strong>使用代码</strong></p><p><strong>使用 YAML 文件</strong></p><p><strong>使用 DDL</strong></p><h3 id="5-2-6-SQL-Client"><a href="#5-2-6-SQL-Client" class="headerlink" title="5.2.6 SQL Client"></a>5.2.6 SQL Client</h3><h3 id="5-2-7-Hive"><a href="#5-2-7-Hive" class="headerlink" title="5.2.7 Hive"></a>5.2.7 Hive</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/MNBEYvf">https://t.zsxq.com/MNBEYvf</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="5-2-8-小结与反思"><a href="#5-2-8-小结与反思" class="headerlink" title="5.2.8 小结与反思"></a>5.2.8 小结与反思</h3><p>本章节继续介绍了 Flink Table API &amp; SQL 中的部分 API，然后讲解了 Flink 之前的 planner 和 Blink planner 在某些特性上面的区别，还讲解了 SQL Connector，最后介绍了 SQL Client 和 Hive。</p><p>本章讲解了 Flink Table API &amp; SQL 相关的概述，另外还介绍了它们的 API 使用方式，除此之外还对 Connectors、SQL Client、Hive 做了一定的讲解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;5-2-Flink-Table-API-amp-SQL-功能&quot;&gt;&lt;a href=&quot;#5-2-Flink-Table-API-amp-SQL-功能&quot; class=&quot;headerlink&quot; title=&quot;5.2 Flink Table API &amp;amp; SQL 功能&quot;&gt;&lt;/a&gt;5.2 Flink Table API &amp;amp; SQL 功能&lt;/h2&gt;&lt;p&gt;在 5.1 节中对 Flink Table API &amp;amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Table &amp; SQL 概念与通用 API</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.1/</id>
    <published>2021-07-26T16:00:00.000Z</published>
    <updated>2021-12-26T11:23:16.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第五章-——-Table-API-amp-SQL"><a href="#第五章-——-Table-API-amp-SQL" class="headerlink" title="第五章 —— Table API &amp; SQL"></a>第五章 —— Table API &amp; SQL</h1><p>Flink 中除了 DataStream 和 DataSet API，还有比较高级的 Table API &amp; SQL，它可以帮助我们简化开发的过程，能够快读的运用 Flink 去完成一些需求，本章将对 Flink Table API &amp; SQL 进行讲解，并将与其他的 API 结合对比分析。</p><h2 id="5-1-Flink-Table-amp-SQL-概念与通用-API"><a href="#5-1-Flink-Table-amp-SQL-概念与通用-API" class="headerlink" title="5.1 Flink Table &amp; SQL 概念与通用 API"></a>5.1 Flink Table &amp; SQL 概念与通用 API</h2><a id="more"></a><p>前面的内容都是讲解 DataStream 和 DataSet API 相关的，在 1.2.5 节中讲解 Flink API 时提及到 Flink 的高级 API —— Table API &amp; SQL，本节将开始 Table &amp; SQL 之旅。</p><h3 id="5-1-1-新增-Blink-SQL-查询处理器"><a href="#5-1-1-新增-Blink-SQL-查询处理器" class="headerlink" title="5.1.1 新增 Blink SQL 查询处理器"></a>5.1.1 新增 Blink SQL 查询处理器</h3><p>在 Flink 1.9 版本中，合进了阿里巴巴开源的 Blink 版本中的大量代码，其中最重要的贡献就是 Blink SQL 了。在 Blink 捐献给 Apache Flink 之后，社区就致力于为 Table API &amp; SQL 集成 Blink 的查询优化器和 runtime。先来看下 1.8 版本的 Flink Table 项目结构如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-30-130607.png" alt=""></p><p>1.9 版本的 Flink Table 项目结构图如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-30-130751.png" alt=""></p><p>可以发现新增了 <code>flink-sql-parser</code>、<code>flink-table-planner-blink</code>、<code>flink-table-runtime-blink</code>、<code>flink-table-uber-blink</code> 模块，对 Flink Table 模块的重构详细内容可以参考 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions">FLIP-32</a>。这样对于 Java 和 Scala API 模块、优化器以及 runtime 模块来说，分层更清楚，接口更明确。</p><p>另外 <code>flink-table-planner-blink</code> 模块中实现了新的优化器接口，所以现在有两个插件化的查询处理器来执行 Table API &amp; SQL：1.9 以前的 Flink 处理器和新的基于 Blink 的处理器。基于 Blink 的查询处理器提供了更好的 SQL 覆盖率、支持更广泛的查询优化、改进了代码生成机制、通过调优算子的实现来提升批处理查询的性能。除此之外，基于 Blink 的查询处理器还提供了更强大的流处理能力，包括了社区一些非常期待的新功能（如维表 Join、TopN、去重）和聚合场景缓解数据倾斜的优化，以及内置更多常用的函数，具体可以查看 <code>flink-table-runtime-blink</code> 代码。目前整个模块的结构如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-30-124512.png" alt=""></p><p>注意：两个查询处理器之间的语义和功能大部分是一致的，但未完全对齐，因为基于 Blink 的查询处理器还在优化中，所以在 1.9 版本中默认查询处理器还是 1.9 之前的版本。如果你想使用 Blink 处理器的话，可以在创建 TableEnvironment 时通过 EnvironmentSettings 配置启用。被选择的处理器必须要在正在执行的 Java 进程的类路径中。对于集群设置，默认两个查询处理器都会自动地加载到类路径中。如果要在 IDE 中运行一个查询，需要在项目中添加 planner 依赖。</p><h3 id="5-1-2-为什么选择-Table-API-amp-SQL？"><a href="#5-1-2-为什么选择-Table-API-amp-SQL？" class="headerlink" title="5.1.2 为什么选择 Table API &amp; SQL？"></a>5.1.2 为什么选择 Table API &amp; SQL？</h3><p>在 1.2 节中介绍了 Flink 的 API 是包含了 Table API &amp; SQL，在 1.3 节中也介绍了在 Flink 1.9 中阿里开源的 Blink 分支中的很强大的 SQL 功能合并进 Flink 主分支，另外通过阿里 Blink 相关的介绍，可以知道阿里在 SQL 功能这块是做了很多的工作。从前面章节的内容可以发现 Flink 的 DataStream／DataSet API 的功能已经很全并且很强大了，常见复杂的数据处理问题也都可以处理，那么社区为啥还在一直推广 Table API &amp; SQL 呢？</p><p>其实通过观察其它的大数据组件，就不会好奇了，比如 Spark、Storm、Beam、Hive 、KSQL（面向 Kafka 的 SQL 引擎）、Elasticsearch、Phoenix（使用 SQL 进行 HBase 数据的查询）等，可以发现 SQL 已经成为各个大数据组件必不可少的数据查询语言，那么 Flink 作为一个大数据实时处理引擎，笔者对其支持 SQL 查询流数据也不足为奇了，但是还是来稍微介绍一下 Table API &amp; SQL。</p><p>Table API &amp; SQL 是一种关系型 API，用户可以像操作数据库一样直接操作流数据，而不再需要通过 DataStream API 来写很多代码完成计算需求，更不用手动去调优你写的代码，另外 SQL 最大的优势在于它是一门学习成本很低的语言，普及率很高，用户基数大，和其他的编程语言相比，它的入门相对简单。</p><p>除了上面的原因，还有一个原因是：可以借助 Table API &amp; SQL 统一流处理和批处理，因为在 DataStream／DataSet API 中，用户开发流作业和批作业需要去了解两种不同的 API，这对于公司有些开发能力不高的数据分析师来说，学习成本有点高，他们其实更擅长写 SQL 来分析。Table API &amp; SQL 做到了批与流上的查询具有同样的语法语义，因此不用改代码就能同时在批和流上执行。</p><p>总结来说，为什么选择 Table API &amp; SQL：</p><ul><li>声明式语言表达业务逻辑</li><li>无需代码编程 —— 易于上手</li><li>查询能够被有效的优化</li><li>查询可以高效的执行</li></ul><h3 id="5-1-3-Flink-Table-项目模块"><a href="#5-1-3-Flink-Table-项目模块" class="headerlink" title="5.1.3 Flink Table 项目模块"></a>5.1.3 Flink Table 项目模块</h3><p>在上文中提及到 Flink Table 在 1.8 和 1.9 的区别，这里还是要再讲解一下这几个依赖，因为只有了解清楚了之后，我们在后面开发的时候才能够清楚挑选哪种依赖。它有如下几个模块：</p><ul><li>flink-table-common：table 中的公共模块，可以用于通过自定义 function，format 等来扩展 Table 生态系统</li><li>flink-table-api-java：支持使用 Java 语言，纯 Table＆SQL API</li><li>flink-table-api-scala：支持使用 Scala 语言，纯 Table＆SQL API</li><li>flink-table-api-java-bridge：支持使用 Java 语言，包含 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li><li>flink-table-api-scala-bridge：支持使用 Scala 语言，带有 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li><li>flink-sql-parser：SQL 语句解析层，主要依赖 calcite</li><li>flink-table-planner：Table 程序的 planner 和 runtime</li><li>flink-table-uber：将上诉模块打成一个 fat jar，在 lib 目录下</li><li>flink-table-planner-blink：Blink 的 Table 程序的 planner（阿里开源的版本）</li><li>flink-table-runtime-blink：Blink 的 Table 程序的 runtime（阿里开源的版本）</li><li>flink-table-uber-blink：将 Blink 版本的 planner 和 runtime 与前面模块（除 flink-table-planner 模块）打成一个 fat jar，在 lib 目录下，如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-02-164352.png" alt=""></p><ul><li>flink-sql-client：SQL 客户端</li></ul><h3 id="5-1-4-两种-planner-之间的区别"><a href="#5-1-4-两种-planner-之间的区别" class="headerlink" title="5.1.4 两种 planner 之间的区别"></a>5.1.4 两种 planner 之间的区别</h3><p>上面讲了两种不同的 planner 之间包含的模块有点区别，但是具体有什么区别如下所示：</p><ul><li>Blink planner 将批处理作业视为流的一种特殊情况。因此不支持 Table 和 DataSet 之间的转换，批处理作业会转换成 DataStream 程序，而不会转换成 DataSet 程序，流作业还是转换成 DataStream 程序。</li><li>Blink planner 不支持 BatchTableSource，而是使用有界的（bounded） StreamTableSource 代替它。</li><li>Blink planner 仅支持全新的 Catalog，不支持已经废弃的 ExternalCatalog。</li><li>以前的 planner 中 FilterableTableSource 的实现与现在的 Blink planner 有冲突，在以前的 planner 中是叠加 PlannerExpressions（在未来的版本中会移除），而在 Blink planner 中是 Expressions。</li><li>基于字符串的 KV 键值配置选项仅可以在 Blink planner 中使用。</li><li>PlannerConfig 的实现（CalciteConfig）在两种 planner 中不同。</li><li>Blink planner 会将多个 sink 优化在同一个 DAG 中（只在 TableEnvironment 中支持，StreamTableEnvironment 中不支持），而以前的 planner 是每个 sink 都有一个 DAG 中，相互独立的。</li><li>以前的 planner 不支持 catalog 统计，而 Blink planner 支持。</li></ul><p>在了解到了两种 planner 的区别后，接下来开始 Flink Table API &amp; SQL 之旅。</p><h3 id="5-1-5-添加项目依赖"><a href="#5-1-5-添加项目依赖" class="headerlink" title="5.1.5 添加项目依赖"></a>5.1.5 添加项目依赖</h3><p>因为在 Flink 1.9 版本中有两个 planner，所以得根据你使用的 planner 来选择对应的依赖，假设你选择的是最新的 Blink 版本，那么添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是以前的 planner，则使用下面这个依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果要自定义 format 格式或者自定义 function，则需要添加 <code>flink-table-common</code> 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5-1-6-创建一个-TableEnvironment"><a href="#5-1-6-创建一个-TableEnvironment" class="headerlink" title="5.1.6 创建一个 TableEnvironment"></a>5.1.6 创建一个 TableEnvironment</h3><p>TableEnvironment 是 Table API 和 SQL 的统称，它负责的内容有：</p><ul><li>在内部的 catalog 注册 Table</li><li>注册一个外部的 catalog</li><li>执行 SQL 查询</li><li>注册用户自定义的 function</li><li>将 DataStream 或者 DataSet 转换成 Table</li><li>保持对 ExecutionEnvironment 和 StreamExecutionEnvironment 的引用</li></ul><p>Table 总是会绑定在一个指定的 TableEnvironment，不能在同一个查询中组合不同 TableEnvironment 的 Table，比如 join 或 union 操作。你可以使用下面的几种静态方法创建 TableEnvironment。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 StreamTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, EnvironmentSettings.newInstance().build());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, EnvironmentSettings settings)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, settings, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** <span class="doctag">@deprecated</span> */</span></span><br><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, EnvironmentSettings.newInstance().build(), tableConfig);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 BatchTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你需要根据你的程序来使用对应的 TableEnvironment，是 BatchTableEnvironment 还是 StreamTableEnvironment。默认两个 planner 都是在 Flink 的安装目录下 lib 文件夹中存在的，所以应该在你的程序中指定使用哪种 planner。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Flink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure><p>如果在 lib 目录下只存在一个 planner，则可以使用 useAnyPlanner 来创建指定的 EnvironmentSettings。</p><h3 id="5-1-7-Table-API-amp-SQL-应用程序的结构"><a href="#5-1-7-Table-API-amp-SQL-应用程序的结构" class="headerlink" title="5.1.7 Table API &amp; SQL 应用程序的结构"></a>5.1.7 Table API &amp; SQL 应用程序的结构</h3><p>批处理和流处理的 Table API &amp; SQL 作业都有相同的模式，它们的代码结构如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据前面内容创建一个 TableEnvironment，指定是批作业还是流作业</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"></span><br><span class="line"><span class="comment">//用下面的其中一种方式注册一个 Table</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"table1"</span>, ...)          </span><br><span class="line">tableEnv.registerTableSource(<span class="string">"table2"</span>, ...); </span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">"extCat"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册一个 TableSink</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">"outputTable"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据一个 Table API 查询创建一个 Table</span></span><br><span class="line">Table tapiResult = tableEnv.scan(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">//根据一个 SQL 查询创建一个 Table</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table2 ... "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 Table API 或者 SQL 的结果发送给 TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//运行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><h3 id="5-1-8-Catalog-中注册-Table"><a href="#5-1-8-Catalog-中注册-Table" class="headerlink" title="5.1.8 Catalog 中注册 Table"></a>5.1.8 Catalog 中注册 Table</h3><p>Table 有两种类型，输入表和输出表，可以在 Table API &amp; SQL 查询中引用输入表并提供输入数据，输出表可以用于将 Table API &amp; SQL 的查询结果发送到外部系统。输出表可以通过 TableSink 来注册，输入表可以从各种数据源进行注册：</p><ul><li>已经存在的 Table 对象，通过是 Table API 或 SQL 查询的结果</li><li>连接了外部系统的 TableSource，比如文件、数据库、MQ</li><li>从 DataStream 或 DataSet 程序中返回的 DataStream 和 DataSet</li></ul><h4 id="注册-Table"><a href="#注册-Table" class="headerlink" title="注册 Table"></a>注册 Table</h4><p>在 TableEnvironment 中可以像下面这样注册一个 Table：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; <span class="comment">// see "Create a TableEnvironment" section</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//projTable 是一个简单查询的结果</span></span><br><span class="line">Table projTable = tableEnv.scan(<span class="string">"X"</span>).select(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 projTable 表注册为 projectedTable 表</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="注册-TableSource"><a href="#注册-TableSource" class="headerlink" title="注册 TableSource"></a>注册 TableSource</h4><h4 id="注册-TableSink"><a href="#注册-TableSink" class="headerlink" title="注册 TableSink"></a>注册 TableSink</h4><h3 id="5-1-9-注册外部的-Catalog"><a href="#5-1-9-注册外部的-Catalog" class="headerlink" title="5.1.9 注册外部的 Catalog"></a>5.1.9 注册外部的 Catalog</h3><h3 id="5-1-10-查询-Table"><a href="#5-1-10-查询-Table" class="headerlink" title="5.1.10 查询 Table"></a>5.1.10 查询 Table</h3><h4 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h4><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><h4 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h4><h3 id="5-1-11-提交-Table"><a href="#5-1-11-提交-Table" class="headerlink" title="5.1.11 提交 Table"></a>5.1.11 提交 Table</h3><h3 id="5-1-12-翻译并执行查询"><a href="#5-1-12-翻译并执行查询" class="headerlink" title="5.1.12 翻译并执行查询"></a>5.1.12 翻译并执行查询</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/MNBEYvf">https://t.zsxq.com/MNBEYvf</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="5-1-13-小结与反思"><a href="#5-1-13-小结与反思" class="headerlink" title="5.1.13 小结与反思"></a>5.1.13 小结与反思</h3><p>本节介绍了 Flink 新的 planner，然后详细的和之前的 planner 做了对比，然后对 Table API &amp; SQL 中的概念做了介绍，还通过样例去介绍了它们的通用 API。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第五章-——-Table-API-amp-SQL&quot;&gt;&lt;a href=&quot;#第五章-——-Table-API-amp-SQL&quot; class=&quot;headerlink&quot; title=&quot;第五章 —— Table API &amp;amp; SQL&quot;&gt;&lt;/a&gt;第五章 —— Table API &amp;amp; SQL&lt;/h1&gt;&lt;p&gt;Flink 中除了 DataStream 和 DataSet API，还有比较高级的 Table API &amp;amp; SQL，它可以帮助我们简化开发的过程，能够快读的运用 Flink 去完成一些需求，本章将对 Flink Table API &amp;amp; SQL 进行讲解，并将与其他的 API 结合对比分析。&lt;/p&gt;
&lt;h2 id=&quot;5-1-Flink-Table-amp-SQL-概念与通用-API&quot;&gt;&lt;a href=&quot;#5-1-Flink-Table-amp-SQL-概念与通用-API&quot; class=&quot;headerlink&quot; title=&quot;5.1 Flink Table &amp;amp; SQL 概念与通用 API&quot;&gt;&lt;/a&gt;5.1 Flink Table &amp;amp; SQL 概念与通用 API&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Checkpoint 和 Savepoint 的区别及其配置使用</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/26/flink-in-action-4.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/26/flink-in-action-4.3/</id>
    <published>2021-07-25T16:00:00.000Z</published>
    <updated>2021-12-26T11:22:27.264Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用"><a href="#4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用" class="headerlink" title="4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用"></a>4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用</h2><p>Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。</p><a id="more"></a><h3 id="4-3-1-Checkpoint-简介及使用"><a href="#4-3-1-Checkpoint-简介及使用" class="headerlink" title="4.3.1 Checkpoint 简介及使用"></a>4.3.1 Checkpoint 简介及使用</h3><p>在 Flink 任务运行过程中，为了保障故障容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。</p><p>Flink 的 Checkpoint 有以下先决条件：</p><ul><li>需要具有持久性且支持重放一定时间范围内数据的数据源。例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。</li><li>需要一个能保存状态的持久化存储介质，例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。</li></ul><p>Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。下面来介绍 Checkpoint 具体如何使用。</p><p>首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。Checkpoint 配置相关的 Java 代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutinEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开启 Checkpoint，每 1000毫秒进行一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 语义设置为 EXACTLY_ONCE</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckPoint 的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同一时间，只允许 有 1 个 Checkpoint 在发生</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="keyword">true</span>)</span><br></pre></td></tr></table></figure><p>以上 Checkpoint 相关的参数描述如下所示：</p><ul><li>Checkpoint 语义： EXACTLY_ONCE 或 AT_LEAST_ONCE，EXACTLY_ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT_LEAST_ONCE 表示要消费的数据至少处理一次，可能会重复消费。</li><li>Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。</li><li>同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。</li><li>两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。</li><li>当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为：<ul><li>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。</li><li>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。</li></ul></li><li><p>任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。</p></li><li><p>作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。</p></li></ul><p>关于 Checkpoint 时，状态后端相关的配置请参阅本书 4.2 节。</p><h3 id="4-3-2-Savepoint-简介及使用"><a href="#4-3-2-Savepoint-简介及使用" class="headerlink" title="4.3.2 Savepoint 简介及使用"></a>4.3.2 Savepoint 简介及使用</h3><p>Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 <code>uid(String)</code> 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.</span><br><span class="line">  <span class="comment">// Stateful source (e.g. Kafka) with ID</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">"source-id"</span>) <span class="comment">// ID for the source operator</span></span><br><span class="line">  .shuffle()</span><br><span class="line">  <span class="comment">// Stateful mapper with ID</span></span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">"mapper-id"</span>) <span class="comment">// ID for the mapper</span></span><br><span class="line">  <span class="comment">// Stateless printing sink</span></span><br><span class="line">  .print(); <span class="comment">// Auto-generated ID</span></span><br></pre></td></tr></table></figure><p>如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 <code>uid(String)</code> 手动指定了 Operator ID，就不会存在 上述问题了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-020528.jpg" alt=""></p><p>Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。</p><p>使用 YARN 触发 Savepoint 的方式如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 和 YARN 应用程序 ID <code>:yarnAppId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径。</p><p>使用 Savepoint 取消 Flink 任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure><p>这将自动触发 ID 为 <code>:jobid</code> 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。</p><h3 id="4-3-3-Savepoint-与-Checkpoint-的区别"><a href="#4-3-3-Savepoint-与-Checkpoint-的区别" class="headerlink" title="4.3.3 Savepoint 与 Checkpoint 的区别"></a>4.3.3 Savepoint 与 Checkpoint 的区别</h3><p>前面分别介绍了 Savepoint 和 Checkpoint，可以发现它们有不少相似之处，下面来看下它们之间的区别。</p><table><thead><tr><th style="text-align:center">Checkpoint</th><th style="text-align:center">Savepoint</th></tr></thead><tbody><tr><td style="text-align:center">由 Flink 的 JobManager 定时自动触发并管理</td><td style="text-align:center">由用户手动触发并管理</td></tr><tr><td style="text-align:center">主要用于任务发生故障时，为任务提供给自动恢复机制</td><td style="text-align:center">主要用户升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复</td></tr><tr><td style="text-align:center">当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照</td><td style="text-align:center">仅支持全量快照</td></tr><tr><td style="text-align:center">Flink 任务停止后，Checkpoint 的状态快照信息默认被清除</td><td style="text-align:center">一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除</td></tr><tr><td style="text-align:center">Checkpoint 设计目标：轻量级且尽可能快地恢复任务</td><td style="text-align:center">Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作</td></tr></tbody></table><h3 id="4-3-4-Checkpoint-流程"><a href="#4-3-4-Checkpoint-流程" class="headerlink" title="4.3.4 Checkpoint 流程"></a>4.3.4 Checkpoint 流程</h3><p>Flink 任务 Checkpoint 的详细流程如下面流程所示。</p><ol><li><p>JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021819.png" alt=""></p></li><li><p>当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中，如下图所示。</p></li></ol><ul><li>如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中</li><li>为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021846.png" alt=""></p><blockquote><p>注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分</p></blockquote><ol><li><p>当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除</p></li><li><p>如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021900.png" alt=""></p></li></ol><p>如果对上述 Checkpoint 过程不理解，在后续 9.5 节 Flink 如何保障 Exactly Once 中会详细介绍 Flink 的 Checkpoint 过程以及为什么这么做。</p><h4 id="基于-RocksDB-的增量-Checkpoint-实现原理"><a href="#基于-RocksDB-的增量-Checkpoint-实现原理" class="headerlink" title="基于 RocksDB 的增量 Checkpoint 实现原理"></a>基于 RocksDB 的增量 Checkpoint 实现原理</h4><p>当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021910.png" alt=""></p><p>第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。</p><p>基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。</p><p>基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。</p><h3 id="4-3-5-如何从-Checkpoint-中恢复状态"><a href="#4-3-5-如何从-Checkpoint-中恢复状态" class="headerlink" title="4.3.5 如何从 Checkpoint 中恢复状态"></a>4.3.5 如何从 Checkpoint 中恢复状态</h3><h3 id="4-3-6-如何从-Savepoint-中恢复状态"><a href="#4-3-6-如何从-Savepoint-中恢复状态" class="headerlink" title="4.3.6 如何从 Savepoint 中恢复状态"></a>4.3.6 如何从 Savepoint 中恢复状态</h3><h3 id="4-3-7-如何优雅地删除-Checkpoint-目录"><a href="#4-3-7-如何优雅地删除-Checkpoint-目录" class="headerlink" title="4.3.7 如何优雅地删除 Checkpoint 目录"></a>4.3.7 如何优雅地删除 Checkpoint 目录</h3><h3 id="4-3-8-小结与反思"><a href="#4-3-8-小结与反思" class="headerlink" title="4.3.8 小结与反思"></a>4.3.8 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/RNJeqFy">https://t.zsxq.com/RNJeqFy</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><p>本章属于属于本书的进阶篇，在第一节中讲解了 State 的各种类型，包括每种 State 的使用方式、实现原理和部分源码剖析，在第二节中介绍 Flink 中的状态后端存储的使用方式，对比分析了现有的几种方案的使用场景，在第三节中介绍了 Checkpoint 和 Savepoint 的区别，如果从状态中恢复作业，以及整个作业的 Checkpoint 流程。</p><p>本章的内容是 Flink 作业开发的重点，通常线上遇到 State、Checkpoint、Savepoint 的问题比较多，所以希望你能好好的弄清楚它们的原理，这样在出问题后才能够对症下药去解决问题。 </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用&quot;&gt;&lt;a href=&quot;#4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用&quot; class=&quot;headerlink&quot; title=&quot;4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用&quot;&gt;&lt;/a&gt;4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用&lt;/h2&gt;&lt;p&gt;Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 状态后端存储</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/25/flink-in-action-4.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/25/flink-in-action-4.2/</id>
    <published>2021-07-24T16:00:00.000Z</published>
    <updated>2021-12-26T11:19:07.537Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-2-Flink-状态后端存储"><a href="#4-2-Flink-状态后端存储" class="headerlink" title="4.2 Flink 状态后端存储"></a>4.2 Flink 状态后端存储</h2><p>在 4.1 节中介绍了 Flink 中的状态，那么在生产环境中，随着作业的运行时间变长，状态会变得越来越大，那么如何将这些状态存储也是 Flink 要解决的一大难点，本节来讲解下 Flink 中不同类型的状态后端存储。</p><a id="more"></a><h3 id="4-2-1-State-Backends"><a href="#4-2-1-State-Backends" class="headerlink" title="4.2.1 State Backends"></a>4.2.1 State Backends</h3><p>当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，为此在 Flink 源码中还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置：</p><ul><li>state.backend: 用于存储和进行状态 Checkpoint 的状态后端存储方式，无默认值</li><li>state.checkpoints.num-retained: 要保留的已完成 Checkpoint 的最大数量，默认值为 1</li><li>state.backend.async: 状态后端是否使用异步快照方法，默认值为 true</li><li>state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false</li><li>state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用</li><li>taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录</li><li>state.savepoints.dir: 存储 savepoints 的目录</li><li>state.checkpoints.dir: 存储 Checkpoint 的数据文件和元数据</li><li>state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024</li></ul><p>虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend());  <span class="comment">//设置堆内存存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));   //设置文件存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>StateBackend 接口的三种实现类如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-141800.png" alt=""></p><p>上面三种方式取一种就好了。但是有三种方式，我们该如何去挑选用哪种去存储状态呢？下面讲讲这三种的特点以及该如何选择。</p><h3 id="4-2-2-MemoryStateBackend-的用法及分析"><a href="#4-2-2-MemoryStateBackend-的用法及分析" class="headerlink" title="4.2.2 MemoryStateBackend 的用法及分析"></a>4.2.2 MemoryStateBackend 的用法及分析</h3><p>如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。如果你细心的话，可以从你的 Job 中看到类似日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask  - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: &apos;null&apos;, savepoints: &apos;null&apos;, asynchronous: TRUE, maxStateSize: 5242880)</span><br></pre></td></tr></table></figure><p>上面日志的意思就是说如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 TaskManagers 的内存（JVM 堆）中，当应用程序触发 Checkpoint 时，会将此时的状态进行快照然后存储在 JobManager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制，比如：</p><ul><li>不太适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 JobManager 中，如果状态较大的话，那么会使得 JobManager 的内存比较紧张，从而导致 JobManager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。</li><li>每个单独的状态大小不能超过最大的 DEFAULT_MAX_STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。</li><li>Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。</li></ul><p>另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有：</p><ul><li>TRUE（表示使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题）</li><li>FALSE（同步）</li><li>UNDEFINED（默认值）</li></ul><p>在构造 MemoryStateBackend 的默认函数时是使用的 UNDEFINED，而不是异步：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//使用的是 UNDEFINED</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>网上有人说默认是异步的，这里给大家解释清楚一下，从上面的那条日志打印的确实也是表示异步，但是前提是你对 State 无任何操作，笔者跟了下源码，当你没有配置任何的 state 时，它是会在 StateBackendLoader 类中通过 MemoryStateBackendFactory 来创建的 state 的，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-142223.png" alt=""></p><p>继续跟进 MemoryStateBackendFactory 可以发现他这里创建了一个 MemoryStateBackend 实例并通过 configure 方法进行配置，大概流程代码是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//MemoryStateBackendFactory 类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">createFromConfig</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend().configure(config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//MemoryStateBackend 类中的 config 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">configure</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend(<span class="keyword">this</span>, config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">MemoryStateBackend</span><span class="params">(MemoryStateBackend original, Configuration configuration, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.asynchronousSnapshots = original.asynchronousSnapshots.resolveUndefined(</span><br><span class="line">            configuration.getBoolean(CheckpointingOptions.ASYNC_SNAPSHOTS));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据 CheckpointingOptions 类中的 ASYNC_SNAPSHOTS 参数进行设置的</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ConfigOption&lt;Boolean&gt; ASYNC_SNAPSHOTS = ConfigOptions</span><br><span class="line">        .key(<span class="string">"state.backend.async"</span>)</span><br><span class="line">        .defaultValue(<span class="keyword">true</span>) <span class="comment">//默认值就是 true，代表异步</span></span><br><span class="line">        .withDescription(...)</span><br></pre></td></tr></table></figure><p>可以发现最终是通过读取 <code>state.backend.async</code> 参数的默认值（true）来配置是否要异步的进行快照，但是如果你手动配置 MemoryStateBackend 的话，利用无参数的构造方法，那么就不是默认异步，如果想使用异步的话，需要利用下面这个构造函数（需要传入一个 boolean 值，true 代表异步，false 代表同步）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(<span class="keyword">boolean</span> asynchronousSnapshots)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.fromBoolean(asynchronousSnapshots));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果你再细看了这个 MemoryStateBackend 类的话，那么你可能会发现这个构造函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(@Nullable String checkpointPath, @Nullable String savepointPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(checkpointPath, savepointPath, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//需要你传入 checkpointPath 和 savepointPath</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个也是用来创建一个 MemoryStateBackend 的，它需要传入的参数是两个路径（checkpointPath、savepointPath），其中 checkpointPath 是写入 Checkpoint 元数据的路径，savepointPath 是写入 savepoint 的路径。</p><p>这个来看看 MemoryStateBackend 的继承关系图可以更明确的知道它是继承自 AbstractFileStateBackend，然后 AbstractFileStateBackend 这个抽象类就是为了能够将状态存储中的数据或者元数据进行文件存储的，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-142403.png" alt=""></p><p>所以 FsStateBackend 和 MemoryStateBackend 都会继承该类。</p><h3 id="4-2-3-FsStateBackend-的用法及分析"><a href="#4-2-3-FsStateBackend-的用法及分析" class="headerlink" title="4.2.3 FsStateBackend 的用法及分析"></a>4.2.3 FsStateBackend 的用法及分析</h3><p>这种状态后端存储也是将工作状态存储在 TaskManager 中的内存（JVM 堆）中，但是 Checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置，比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 TaskManager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。</p><p>同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 Checkpoint 的，异步可以避免在状态 Checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX_FILE_STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 <code>state.backend.fs.memory-threshold</code> 来配置）。</p><p>那么我们该什么时候使用 FsStateBackend 呢？</p><ul><li>如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合</li><li>使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复</li></ul><p>使用 FsStateBackend 需要注意的地方有什么呢？</p><ul><li>工作状态仍然是存储在 TaskManager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 TaskManager 的内存</li></ul><h3 id="4-2-4-RocksDBStateBackend-的用法及分析"><a href="#4-2-4-RocksDBStateBackend-的用法及分析" class="headerlink" title="4.2.4 RocksDBStateBackend 的用法及分析"></a>4.2.4 RocksDBStateBackend 的用法及分析</h3><h3 id="4-2-5-如何选择状态后端存储？"><a href="#4-2-5-如何选择状态后端存储？" class="headerlink" title="4.2.5 如何选择状态后端存储？"></a>4.2.5 如何选择状态后端存储？</h3><h3 id="4-2-6-小结与反思"><a href="#4-2-6-小结与反思" class="headerlink" title="4.2.6 小结与反思"></a>4.2.6 小结与反思</h3><p>加入知识星球可以看到上面文章： <a href="https://t.zsxq.com/RNJeqFy">https://t.zsxq.com/RNJeqFy</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;4-2-Flink-状态后端存储&quot;&gt;&lt;a href=&quot;#4-2-Flink-状态后端存储&quot; class=&quot;headerlink&quot; title=&quot;4.2 Flink 状态后端存储&quot;&gt;&lt;/a&gt;4.2 Flink 状态后端存储&lt;/h2&gt;&lt;p&gt;在 4.1 节中介绍了 Flink 中的状态，那么在生产环境中，随着作业的运行时间变长，状态会变得越来越大，那么如何将这些状态存储也是 Flink 要解决的一大难点，本节来讲解下 Flink 中不同类型的状态后端存储。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 深度讲解 Flink 中的状态</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/24/flink-in-action-4.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/24/flink-in-action-4.1/</id>
    <published>2021-07-23T16:00:00.000Z</published>
    <updated>2021-12-26T11:11:05.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第四章-——-Flink-中的状态及容错机制"><a href="#第四章-——-Flink-中的状态及容错机制" class="headerlink" title="第四章 —— Flink 中的状态及容错机制"></a>第四章 —— Flink 中的状态及容错机制</h1><p>Flink 对比其他的流处理框架最大的特点是其支持状态，本章将深度的讲解 Flink 中的状态分类，如何在不同的场景使用不同的状态，接着会介绍 Flink 中的多种状态存储，最后会介绍 Checkpoint 和 Savepoint 的使用方式以及如何恢复状态。</p><h2 id="4-1-深度讲解-Flink-中的状态"><a href="#4-1-深度讲解-Flink-中的状态" class="headerlink" title="4.1 深度讲解 Flink 中的状态"></a>4.1 深度讲解 Flink 中的状态</h2><a id="more"></a><p>在基础篇中的 1.2 节中介绍了 Flink 是一款有状态的流处理框架。那么大家可能有点疑问，这个状态是什么意思？拿 Flink 最简单的 Word Count 程序来说，它需要不断的对 word 出现的个数进行结果统计，那么后一个结果就需要利用前一个的结果然后再做 +1 的操作，这样前一个计算就需要将 word 出现的次数 count 进行存着（这个 count 那么就是一个状态）然后后面才可以进行累加。</p><h3 id="4-1-1-为什么需要-State？"><a href="#4-1-1-为什么需要-State？" class="headerlink" title="4.1.1 为什么需要 State？"></a>4.1.1 为什么需要 State？</h3><p>对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：</p><ul><li>Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费</li><li>Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费</li></ul><p>上面两种情况用图片描述如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-030800.png" alt=""></p><p>为了解决上面两种情况（数据重复消费或者数据没有消费）的发生，那么是不是就得需要个什么东西做个记录将这种数据消费状态，Flink state 就这样诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态），当 Job 因为某种错误或者其他原因导致重启时，就能够从 Checkpoint（定时将 state 做一个全局快照，在 Flink 中，为了能够让 Job 在运行的过程中保证容错性，才会对这些 state 做一个快照，在 4.3 节中会详细讲） 中的 state 数据进行恢复。</p><h3 id="4-1-2-State-的种类"><a href="#4-1-2-State-的种类" class="headerlink" title="4.1.2 State 的种类"></a>4.1.2 State 的种类</h3><p>在 Flink 中有两个基本的 state：Keyed state 和 Operator state，下面来分别介绍一下这两种 State。</p><h3 id="4-1-3-Keyed-State"><a href="#4-1-3-Keyed-State" class="headerlink" title="4.1.3 Keyed State"></a>4.1.3 Keyed State</h3><p>Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。你可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。逻辑上，Keyed State 总是对应着一个 <parallel-operator-instance, key> 二元组，在某种程度上，因为每个具体的 key 总是属于唯一一个具体的 parallel-operator-instance（并行操作实例），这种情况下，那么就可以简化认为是 <operator, key>。Keyed State 可以进一步组织成 Key Group，Key Group 是 Flink 重新分配 Keyed State 的最小单元，所以有多少个并行，就会有多少个 Key Group。在执行过程中，每个 keyed operator 的并行实例会处理来自不同 key 的不同 Key Group。</p><h3 id="4-1-4-Operator-State"><a href="#4-1-4-Operator-State" class="headerlink" title="4.1.4 Operator State"></a>4.1.4 Operator State</h3><p>对 Operator State 而言，每个 operator state 都对应着一个并行实例。Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><p>在 Flink 源码中，在 flink-core module 下的 <code>org.apache.flink.api.common.state</code> 中可以看到 Flink 中所有和 State 相关的类，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143333.png" alt=""></p><h3 id="4-1-5-Raw-State-和-Managed-State"><a href="#4-1-5-Raw-State-和-Managed-State" class="headerlink" title="4.1.5 Raw State 和 Managed State"></a>4.1.5 Raw State 和 Managed State</h3><p>Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。</p><p>原始状态是 Operator（算子）保存它们自己的数据结构中的 state，当 Checkpoint 时，原始状态会以字节流的形式写入进 Checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。</p><p>托管状态可以使用 Flink runtime 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink runtime 会对这些状态进行编码然后将它们写入到 Checkpoint 中。</p><p>DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在实现 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。</p><p>注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。</p><h3 id="4-1-6-如何使用托管的-Keyed-State"><a href="#4-1-6-如何使用托管的-Keyed-State" class="headerlink" title="4.1.6 如何使用托管的 Keyed State"></a>4.1.6 如何使用托管的 Keyed State</h3><p>托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。</p><p>我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：</p><ul><li>ValueState<T>: 保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。</li><li>ListState<T>: 保存一个值的列表，用 add(T) 或者 addAll(List<T>) 来添加，用 Iterable<T> get() 来获取。</li><li>ReducingState<T>: 保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。</li><li>AggregatingState<IN, OUT>: 保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。</li><li>FoldingState<T, ACC>: 与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。</li><li>MapState<UK, UV>: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map<UK, UV>) 添加，或者使用 get(UK) 获取。</li></ul><p>所有类型的状态都有一个 clear() 方法来清除当前的状态。</p><p>注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。</p><p>需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。</p><p>要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。</p><p>状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：</p><ul><li>ValueState<T> getState(ValueStateDescriptor<T>)</li><li>ReducingState<T> getReducingState(ReducingStateDescriptor<T>)</li><li>ListState<T> getListState(ListStateDescriptor<T>)</li><li>AggregatingState<IN, OUT> getAggregatingState(AggregatingState<IN, OUT>)</li><li>FoldingState<T, ACC> getFoldingState(FoldingStateDescriptor<T, ACC>)</li><li>MapState<UK, UV> getMapState(MapStateDescriptor<UK, UV>)</li></ul><p>上面讲了这么多概念，那么来一个例子来看看如何使用状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//访问状态的 value 值</span></span><br><span class="line">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 count</span></span><br><span class="line">        currentSum.f0 += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 sum</span></span><br><span class="line">        currentSum.f1 += input.f1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        sum.update(currentSum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果 count 等于 2, 发出平均值并清除状态</span></span><br><span class="line">        <span class="keyword">if</span> (currentSum.f0 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class="line">            sum.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">                        <span class="string">"average"</span>, <span class="comment">//状态名称</span></span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class="comment">//类型信息</span></span><br><span class="line">                        Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>)); <span class="comment">//状态的默认值</span></span><br><span class="line">        sum = getRuntimeContext().getState(descriptor);<span class="comment">//获取状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">env.fromElements(Tuple2.of(<span class="number">1L</span>, <span class="number">3L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">5L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">7L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">4L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">2L</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果会打印出 (1,4) 和 (1,5)</span></span><br></pre></td></tr></table></figure><p>这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。</p><h3 id="4-1-7-State-TTL-存活时间"><a href="#4-1-7-State-TTL-存活时间" class="headerlink" title="4.1.7 State TTL(存活时间)"></a>4.1.7 State TTL(存活时间)</h3><p>随着作业的运行时间变长，作业的状态也会逐渐的变大，那么很有可能就会影响作业的稳定性，这时如果有状态的过期这种功能就可以将历史的一些状态清除，对应在 Flink 中的就是 State TTL，接下来将对其做详细介绍。</p><h4 id="State-TTL-介绍"><a href="#State-TTL-介绍" class="headerlink" title="State TTL 介绍"></a>State TTL 介绍</h4><p>TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line">    </span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"zhisheng"</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);    <span class="comment">//开启 ttl</span></span><br></pre></td></tr></table></figure><p>上面配置中有几个选项需要注意：</p><p>1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。</p><p>2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：</p><ul><li>StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新</li><li>StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新</li></ul><p>3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：</p><ul><li>StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值</li><li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回</li></ul><p>在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.</p><p>另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。</p><p>注意：</p><ul><li>状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。</li><li>目前仅支持参考 processing time 的 TTL</li><li>使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。</li><li>TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。</li><li>只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。</li></ul><h4 id="清除过期-State"><a href="#清除过期-State" class="headerlink" title="清除过期 State"></a>清除过期 State</h4><p>默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。</p><p>注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。</p><p>此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupFullSnapshot()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>此配置不适用于 RocksDB 状态后端中的增量 Checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。</p><p>除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupInBackground()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。</p><p>我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。</p><p>在该类中的属性如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143816.png" alt=""></p><p>这些属性的功能如下：</p><ul><li>DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig</li><li>UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选</li><li>StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired</li><li>TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选</li><li>Time：设置 TTL 的时间，这里有两个参数 unit 和 size</li><li>CleanupStrategies：TTL 清理策略，在该类中有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL_STATE_SCAN_SNAPSHOT、INCREMENTAL_CLEANUP 和 ROCKSDB_COMPACTION_FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器），该类和其实现类如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144111.png" alt=""></p><p>如果对 State TTL 还有不清楚的可以看看 Flink 源码 flink-runtime module 中的 state ttl 相关的实现类，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144324.png" alt=""></p><h3 id="4-1-8-如何使用托管的-Operator-State"><a href="#4-1-8-如何使用托管的-Operator-State" class="headerlink" title="4.1.8 如何使用托管的 Operator State"></a>4.1.8 如何使用托管的 Operator State</h3><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><h3 id="4-1-9-Stateful-Source-Functions"><a href="#4-1-9-Stateful-Source-Functions" class="headerlink" title="4.1.9 Stateful Source Functions"></a>4.1.9 Stateful Source Functions</h3><h3 id="4-1-10-Broadcast-State"><a href="#4-1-10-Broadcast-State" class="headerlink" title="4.1.10 Broadcast State"></a>4.1.10 Broadcast State</h3><p>Flink 中的 Broadcast State 在很多场景下也有使用，下面来讲解下其使用方式。</p><h4 id="Broadcast-State-如何使用"><a href="#Broadcast-State-如何使用" class="headerlink" title="Broadcast State 如何使用"></a>Broadcast State 如何使用</h4><h4 id="使用-Broadcast-state-需要注意"><a href="#使用-Broadcast-state-需要注意" class="headerlink" title="使用 Broadcast state 需要注意"></a>使用 Broadcast state 需要注意</h4><h3 id="4-1-11-Queryable-State"><a href="#4-1-11-Queryable-State" class="headerlink" title="4.1.11 Queryable State"></a>4.1.11 Queryable State</h3><p>加入知识星球可以看到上面文章： <a href="https://t.zsxq.com/ZVByvzN">https://t.zsxq.com/ZVByvzN</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="4-1-12-小结与反思"><a href="#4-1-12-小结与反思" class="headerlink" title="4.1.12 小结与反思"></a>4.1.12 小结与反思</h3><p>本节一开始讲解了 State 出现的原因，接着讲解了 Flink 中的 State 分类，然后对 Flink 中的每种 State 做了详细的讲解，希望可以好好消化这节的内容。你对本节的内容有什么不理解的地方吗？在使用 State 的过程中有遇到什么问题吗？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第四章-——-Flink-中的状态及容错机制&quot;&gt;&lt;a href=&quot;#第四章-——-Flink-中的状态及容错机制&quot; class=&quot;headerlink&quot; title=&quot;第四章 —— Flink 中的状态及容错机制&quot;&gt;&lt;/a&gt;第四章 —— Flink 中的状态及容错机制&lt;/h1&gt;&lt;p&gt;Flink 对比其他的流处理框架最大的特点是其支持状态，本章将深度的讲解 Flink 中的状态分类，如何在不同的场景使用不同的状态，接着会介绍 Flink 中的多种状态存储，最后会介绍 Checkpoint 和 Savepoint 的使用方式以及如何恢复状态。&lt;/p&gt;
&lt;h2 id=&quot;4-1-深度讲解-Flink-中的状态&quot;&gt;&lt;a href=&quot;#4-1-深度讲解-Flink-中的状态&quot; class=&quot;headerlink&quot; title=&quot;4.1 深度讲解 Flink 中的状态&quot;&gt;&lt;/a&gt;4.1 深度讲解 Flink 中的状态&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 使用 Side Output 分流</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/23/flink-in-action-3.12/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/23/flink-in-action-3.12/</id>
    <published>2021-07-22T16:00:00.000Z</published>
    <updated>2021-12-11T07:24:14.746Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-12-使用-Side-Output-分流"><a href="#3-12-使用-Side-Output-分流" class="headerlink" title="3.12 使用 Side Output 分流"></a>3.12 使用 Side Output 分流</h2><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。</p><a id="more"></a><h3 id="3-12-1-使用-Filter-分流"><a href="#3-12-1-使用-Filter-分流" class="headerlink" title="3.12.1 使用 Filter 分流"></a>3.12.1 使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">"machine"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">"docker"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">"application"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">"middleware"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure><h3 id="3-12-2-使用-Split-分流"><a href="#3-12-2-使用-Split-分流" class="headerlink" title="3.12.2 使用 Split 分流"></a>3.12.2 使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                tags.add(<span class="string">"machine"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                tags.add(<span class="string">"docker"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                tags.add(<span class="string">"application"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                tags.add(<span class="string">"middleware"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">"machine"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">"docker"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">"application"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">"middleware"</span>);</span><br></pre></td></tr></table></figure><p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p><ul><li><p><a href="https://issues.apache.org/jira/browse/FLINK-5031">FLINK-5031 Issue</a></p></li><li><p><a href="https://issues.apache.org/jira/browse/FLINK-11084">FLINK-11084 Issue</a></p></li></ul><p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p><h3 id="3-12-3-使用-Side-Output-分流"><a href="#3-12-3-使用-Side-Output-分流" class="headerlink" title="3.12.3 使用 Side Output 分流"></a>3.12.3 使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"machine"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"docker"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"application"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"middleware"</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure><p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p><h3 id="3-12-4-小结与反思"><a href="#3-12-4-小结与反思" class="headerlink" title="3.12.4 小结与反思"></a>3.12.4 小结与反思</h3><p>本节讲了下 Flink 中将数据分流的三种方式，完整代码的 GitHub 地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput">sideoutput</a></p><p>本章全部在介绍 Flink 的技术点，比如多种时间语义的对比分析和应用场景分析、多种灵活的窗口的使用方式及其原理实现、平时开发使用较多的一些算子、深入讲解了 DataStream 中的流类型及其对应的方法实现、如何将 Watermark 与 Window 结合来处理延迟数据、Flink Connector 的使用方式。</p><p>因为 Flink 的 Connector 比较多，所以本书只挑选了些平时工作用的比较多的 Connector，比如 Kafka、ElasticSearch、HBase、Redis 等，并教会了大家如何去自定义 Source 和 Sink，这样就算 Flink 中的 Connector 没有你需要的，那么也可以通过这种自定义的方法来实现读取和写入数据。</p><p>本章讲解的内容更侧重于在 Flink DataStream 和 Connector 的使用技巧和优化，在讲解这些时还提供了详细的代码实现，目的除了大家可以参考外，其实更期望大家能够自己跟着多动手去实现和优化，这样才可以提高自己的编程水平。另外本章还介绍了自己在使用这些 Connector 时遇到的一些问题，是怎么解决的，也希望大家在工作的时候遇到问题可以静下来自己独立的思考下出现问题的原因是啥，该如何解决，多独立思考后，相信遇到问题后你也能够从容的分析和解决问题。</p><p>加入知识星球可以看到更多文章</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-12-使用-Side-Output-分流&quot;&gt;&lt;a href=&quot;#3-12-使用-Side-Output-分流&quot; class=&quot;headerlink&quot; title=&quot;3.12 使用 Side Output 分流&quot;&gt;&lt;/a&gt;3.12 使用 Side Output 分流&lt;/h2&gt;&lt;p&gt;通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— Redis 的用法</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/22/flink-in-action-3.11/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/22/flink-in-action-3.11/</id>
    <published>2021-07-21T16:00:00.000Z</published>
    <updated>2021-12-11T07:19:40.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-11-Flink-Connector-——-Redis-的用法"><a href="#3-11-Flink-Connector-——-Redis-的用法" class="headerlink" title="3.11 Flink Connector —— Redis 的用法"></a>3.11 Flink Connector —— Redis 的用法</h2><p>在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。</p><a id="more"></a><h3 id="3-11-1-安装-Redis"><a href="#3-11-1-安装-Redis" class="headerlink" title="3.11.1 安装 Redis"></a>3.11.1 安装 Redis</h3><p>首先介绍下 Redis 的的安装和启动运行。</p><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><p>先从 <a href="https://redis.io/download">官网</a> 下载 Redis，然后解压。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br><span class="line">tar xzf redis-5.0.4.tar.gz</span><br><span class="line">cd redis-5.0.4</span><br><span class="line">make</span><br></pre></td></tr></table></figure><h4 id="通过-HomeBrew-安装"><a href="#通过-HomeBrew-安装" class="headerlink" title="通过 HomeBrew 安装"></a>通过 HomeBrew 安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br></pre></td></tr></table></figure><p>如果需要后台运行 Redis 服务，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew services start redis</span><br></pre></td></tr></table></figure><p>要运行命令，可以直接到 /usr/local/bin 目录下，有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br><span class="line">redis-cli</span><br></pre></td></tr></table></figure><p>两个命令，执行 <code>redis-server</code> 可以打开服务端，启动后结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080554.png" alt=""></p><p>然后另外开一个终端，运行 <code>redis-cli</code> 命令可以运行客户端，执行后效果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080617.png" alt=""></p><h3 id="3-11-2-将商品数据发送到-Kafka"><a href="#3-11-2-将商品数据发送到-Kafka" class="headerlink" title="3.11.2 将商品数据发送到 Kafka"></a>3.11.2 将商品数据发送到 Kafka</h3><p>这里我打算将从 Kafka 读取到所有到商品的信息，然后将商品信息中的 <strong>商品ID</strong> 和 <strong>商品价格</strong> 提取出来，然后写入到 Redis 中，供第三方服务根据商品 ID 查询到其对应的商品价格。</p><p>首先定义我们的商品类 （其中 id 和 price 字段是我们最后要提取的）为：</p><p>ProductEvent.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 商品</span></span><br><span class="line"><span class="comment"> * blog：http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> * 微信公众号：zhisheng</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Builder</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类目 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long categoryId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 编码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String code;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long shopId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String shopName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long brandId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String brandName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 图片地址</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String imageUrl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 状态（1(上架),-1(下架),-2(冻结),-3(删除)）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> status;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> type;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 标签</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;String&gt; tags;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 价格（以分为单位）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long price;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后写个工具类不断的模拟商品数据发往 Kafka，工具类 <code>ProductUtil.java</code> 的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"zhisheng"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10000</span>; i++) &#123;</span><br><span class="line">            ProductEvent product = ProductEvent.builder().id((<span class="keyword">long</span>) i)  <span class="comment">//商品的 id</span></span><br><span class="line">                    .name(<span class="string">"product"</span> + i)    <span class="comment">//商品 name</span></span><br><span class="line">                    .price(random.nextLong() / <span class="number">10000000000000L</span>) <span class="comment">//商品价格（以分为单位）</span></span><br><span class="line">                    .code(<span class="string">"code"</span> + i).build();  <span class="comment">//商品编码</span></span><br><span class="line"></span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(product));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(product));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-11-3-Flink-消费-Kafka-中的商品数据"><a href="#3-11-3-Flink-消费-Kafka-中的商品数据" class="headerlink" title="3.11.3 Flink 消费 Kafka 中的商品数据"></a>3.11.3 Flink 消费 Kafka 中的商品数据</h3><p>我们需要在 Flink 中消费 Kafka 数据，然后将商品中的两个数据（商品 id 和 price）取出来。先来看下这段 Flink Job 代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line">        Properties props = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; product = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props))</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, ProductEvent.class)) <span class="comment">//反序列化 JSON</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ProductEvent, Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(ProductEvent value, Collector&lt;Tuple2&lt;String, String&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//收集商品 id 和 price 两个属性</span></span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value.getId().toString(), value.getPrice().toString()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        product.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink redis connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后 IDEA 中启动运行 Job，再运行上面的 ProductUtil 发送 Kafka 数据的工具类（注意：也得提前启动 Kafka），运行结果如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-29-product-redult.png" alt=""></p><p>上图左半部分是工具类发送数据到 Kafka 打印的日志，右半部分是 Job 执行的结果，可以看到它已经将商品的 id 和 price 数据获取到了。</p><p>那么接下来我们需要的就是将这种 <code>Tuple2&lt;Long, Long&gt;</code> 格式的 KV 数据写入到 Redis 中去。要将数据写入到 Redis 的话是需要先添加依赖的。</p><h3 id="3-11-4-Redis-Connector-简介"><a href="#3-11-4-Redis-Connector-简介" class="headerlink" title="3.11.4 Redis Connector 简介"></a>3.11.4 Redis Connector 简介</h3><p>Redis Connector 提供用于向 Redis 发送数据的接口的类。接收器可以使用三种不同的方法与不同类型的 Redis 环境进行通信：</p><ul><li>单 Redis 服务器</li><li>Redis 集群</li><li>Redis Sentinel</li></ul><h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><p>需要添加 Flink Redis Sink 的 Connector，这个 Redis Connector 官方只有老的版本，后面也一直没有更新，所以可以看到网上有些文章都是添加老的版本的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>包括该部分的文档都是很早之前的啦，可以查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html">flink-docs-release-1.1 redis</a>。</p><p>另外在 <a href="https://bahir.apache.org/docs/flink/current/flink-streaming-redis/">flink-streaming-redis</a> 也看到一个 Flink Redis Connector 的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>两个依赖功能都是一样的，我们还是就用官方的那个 Maven 依赖来进行演示。</p><h3 id="3-11-5-Flink-写入数据到-Redis"><a href="#3-11-5-Flink-写入数据到-Redis" class="headerlink" title="3.11.5 Flink 写入数据到 Redis"></a>3.11.5 Flink 写入数据到 Redis</h3><h3 id="3-11-6-项目运行及验证"><a href="#3-11-6-项目运行及验证" class="headerlink" title="3.11.6 项目运行及验证"></a>3.11.6 项目运行及验证</h3><h3 id="3-11-7-小结与反思"><a href="#3-11-7-小结与反思" class="headerlink" title="3.11.7 小结与反思"></a>3.11.7 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/zr76I66">https://t.zsxq.com/zr76I66</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-11-Flink-Connector-——-Redis-的用法&quot;&gt;&lt;a href=&quot;#3-11-Flink-Connector-——-Redis-的用法&quot; class=&quot;headerlink&quot; title=&quot;3.11 Flink Connector —— Redis 的用法&quot;&gt;&lt;/a&gt;3.11 Flink Connector —— Redis 的用法&lt;/h2&gt;&lt;p&gt;在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— HBase 的用法</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/21/flink-in-action-3.10/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/21/flink-in-action-3.10/</id>
    <published>2021-07-20T16:00:00.000Z</published>
    <updated>2021-12-11T07:17:15.185Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-10-Flink-Connector-——-HBase-的用法"><a href="#3-10-Flink-Connector-——-HBase-的用法" class="headerlink" title="3.10 Flink Connector —— HBase 的用法"></a>3.10 Flink Connector —— HBase 的用法</h2><p>HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。</p><a id="more"></a><h3 id="3-10-1-准备环境和依赖"><a href="#3-10-1-准备环境和依赖" class="headerlink" title="3.10.1 准备环境和依赖"></a>3.10.1 准备环境和依赖</h3><p>下面分别讲解 HBase 的环境安装、配置、常用的命令操作以及添加项目需要的依赖。</p><h4 id="HBase-安装"><a href="#HBase-安装" class="headerlink" title="HBase 安装"></a>HBase 安装</h4><p>如果是苹果系统，可以使用 HomeBrew 命令安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install hbase</span><br></pre></td></tr></table></figure><p>HBase 最终会安装在路径 <code>/usr/local/Cellar/hbase/</code> 下面，安装版本不同，文件名也不同。</p><h4 id="配置-HBase"><a href="#配置-HBase" class="headerlink" title="配置 HBase"></a>配置 HBase</h4><p>打开 <code>libexec/conf/hbase-env.sh</code> 修改里面的 JAVA_HOME：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&quot;</span><br></pre></td></tr></table></figure><p>根据你自己的 JAVA_HOME 来配置这个变量。</p><p>打开 <code>libexec/conf/hbase-site.xml</code> 配置 HBase 文件存储目录:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/var/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储内建zookeeper文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/var/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="运行-HBase"><a href="#运行-HBase" class="headerlink" title="运行 HBase"></a>运行 HBase</h4><p>执行启动的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-hbase.sh</span><br></pre></td></tr></table></figure><p>执行后打印出来的日志如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/local/var/log/hbase/hbase-zhisheng-master-zhisheng.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><p>使用 jps 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/Cellar/hbase/1.2.9/libexec  jps</span><br><span class="line">91302 HMaster</span><br><span class="line">62535 RemoteMavenServer</span><br><span class="line">1100</span><br><span class="line">91471 Jps</span><br></pre></td></tr></table></figure><p>出现 HMaster 说明安装运行成功。</p><h4 id="启动-HBase-Shell"><a href="#启动-HBase-Shell" class="headerlink" title="启动 HBase Shell"></a>启动 HBase Shell</h4><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase shell</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-035328.jpg" alt=""></p><h4 id="停止-HBase"><a href="#停止-HBase" class="headerlink" title="停止 HBase"></a>停止 HBase</h4><p>执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/stop-hbase.sh</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-035513.jpg" alt=""></p><h4 id="HBase-常用命令"><a href="#HBase-常用命令" class="headerlink" title="HBase 常用命令"></a>HBase 常用命令</h4><p>HBase 中常用的命令有：list（列出已存在的表）、create（创建表）、put（写数据）、get（读数据）、scan（读数据，读全表）、describe（显示表详情），如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-040821.jpg" alt=""></p><p>简单使用上诉命令的结果如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-040230.jpg" alt=""></p><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 HBase 相关的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hbase_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink HBase Connector 中，HBase 不仅可以作为数据源，也还可以写入数据到 HBase 中去，我们先来看看如何从 HBase 中读取数据。</p><h3 id="3-10-2-Flink-使用-TableInputFormat-读取-HBase-批量数据"><a href="#3-10-2-Flink-使用-TableInputFormat-读取-HBase-批量数据" class="headerlink" title="3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据"></a>3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据</h3><p>这里我们使用 TableInputFormat 来读取 HBase 中的数据，首先准备数据。</p><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>先往 HBase 中插入五条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">put &apos;zhisheng&apos;, &apos;first&apos;, &apos;info:bar&apos;, &apos;hello&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;second&apos;, &apos;info:bar&apos;, &apos;zhisheng001&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;third&apos;, &apos;info:bar&apos;, &apos;zhisheng002&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;four&apos;, &apos;info:bar&apos;, &apos;zhisheng003&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;five&apos;, &apos;info:bar&apos;, &apos;zhisheng004&apos;</span><br></pre></td></tr></table></figure><p>scan 整个 <code>zhisheng</code> 表的话，有五条数据，运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-073344.jpg" alt=""></p><h4 id="Flink-Job-代码"><a href="#Flink-Job-代码" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><p>Flink 读取 HBase 数据的程序代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 读取 HBase 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReadMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.createInput(<span class="keyword">new</span> TableInputFormat&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> Tuple2&lt;String, String&gt; reuse = <span class="keyword">new</span> Tuple2&lt;String, String&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Scan <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">                scan.addColumn(INFO, BAR);</span><br><span class="line">                <span class="keyword">return</span> scan;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> String <span class="title">getTableName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> HBASE_TABLE_NAME;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Tuple2&lt;String, String&gt; <span class="title">mapResultToTuple</span><span class="params">(Result result)</span> </span>&#123;</span><br><span class="line">                String key = Bytes.toString(result.getRow());</span><br><span class="line">                String val = Bytes.toString(result.getValue(INFO, BAR));</span><br><span class="line">                reuse.setField(key, <span class="number">0</span>);</span><br><span class="line">                reuse.setField(val, <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">return</span> reuse;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple2&lt;String, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f1.startsWith(<span class="string">"zhisheng"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中将 HBase 中的读取全部读取出来后然后过滤以 <code>zhisheng</code> 开头的 value 数据。读取结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-161125.png" alt=""></p><p>可以看到输出的结果中已经将以 <code>zhisheng</code> 开头的四条数据都打印出来了。</p><h3 id="3-10-3-Flink-使用-TableOutputFormat-向-HBase-写入数据"><a href="#3-10-3-Flink-使用-TableOutputFormat-向-HBase-写入数据" class="headerlink" title="3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据"></a>3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据</h3><h4 id="添加依赖-1"><a href="#添加依赖-1" class="headerlink" title="添加依赖"></a>添加依赖</h4><h4 id="Flink-Job-代码-1"><a href="#Flink-Job-代码-1" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><h3 id="3-10-4-Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据"><a href="#3-10-4-Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据" class="headerlink" title="3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据"></a>3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据</h3><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><h4 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h4><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><h3 id="3-10-5-项目运行及验证"><a href="#3-10-5-项目运行及验证" class="headerlink" title="3.10.5 项目运行及验证"></a>3.10.5 项目运行及验证</h3><h3 id="3-10-6-小结与反思"><a href="#3-10-6-小结与反思" class="headerlink" title="3.10.6 小结与反思"></a>3.10.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/3bimqBM">https://t.zsxq.com/3bimqBM</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-10-Flink-Connector-——-HBase-的用法&quot;&gt;&lt;a href=&quot;#3-10-Flink-Connector-——-HBase-的用法&quot; class=&quot;headerlink&quot; title=&quot;3.10 Flink Connector —— HBase 的用法&quot;&gt;&lt;/a&gt;3.10 Flink Connector —— HBase 的用法&lt;/h2&gt;&lt;p&gt;HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— ElasticSearch 的用法和分析</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/20/flink-in-action-3.9/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/20/flink-in-action-3.9/</id>
    <published>2021-07-19T16:00:00.000Z</published>
    <updated>2021-12-11T07:13:50.133Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-9-Flink-Connector-——-ElasticSearch-的用法和分析"><a href="#3-9-Flink-Connector-——-ElasticSearch-的用法和分析" class="headerlink" title="3.9 Flink Connector —— ElasticSearch 的用法和分析"></a>3.9 Flink Connector —— ElasticSearch 的用法和分析</h2><p>ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。</p><a id="more"></a><h3 id="3-9-1-准备环境和依赖"><a href="#3-9-1-准备环境和依赖" class="headerlink" title="3.9.1 准备环境和依赖"></a>3.9.1 准备环境和依赖</h3><p>首先准备 ElasticSearch 的环境和项目的环境依赖。</p><p><strong>ElasticSearch 安装</strong></p><p>因为在 2.1 节中已经讲过 ElasticSearch 的安装，这里就不做过多的重复，需要注意的一点就是 Flink 的 ElasticSearch Connector 是区分版本号的，官方支持的版本如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-103746.png" alt=""></p><p>所以添加依赖的时候要区分一下，根据你安装的 ElasticSearch 来选择不一样的版本依赖，另外就是不同版本的 ElasticSearch 还会导致下面的数据写入到 ElasticSearch 中出现一些不同，我们这里使用的版本是 ElasticSearch6，如果你使用的是其他的版本可以参考官网的实现。</p><p><strong>添加依赖</strong></p><p>因为我们在 2.1 节中安装的 ElasticSearch 版本是 6.3.2 版本的，所有这里引入的依赖就选择 <code>flink-connector-elasticsearch6</code>，具体依赖如下所示。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面这个 <code>scala.binary.version</code> 和 <code>flink.version</code> 版本号需要自己在使用的时候根据使用的版本做相应的改变。</p><h3 id="3-9-2-使用-Flink-将数据写入到-ElasticSearch-应用程序"><a href="#3-9-2-使用-Flink-将数据写入到-ElasticSearch-应用程序" class="headerlink" title="3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序"></a>3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序</h3><p>准备好环境和相关的依赖后，接下来开始编写 Flink 程序。</p><p>ESSinkUtil 工具类，代码如下所示，这个工具类是笔者封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面章节还会再讲其他的配置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ESSinkUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flink 程序会读取到 ElasticSearch 的配置，然后将从 Kafka 读取到的数据写入进 ElasticSearch，具体的写入代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sink2ES6Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ESSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ESSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置文件中包含了 Kafka 和 ElasticSearch 的配置，如下所示，地址都支持集群模式填写，注意用 <code>,</code> 分隔。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><h3 id="3-9-3-验证数据是否写入-ElasticSearch？"><a href="#3-9-3-验证数据是否写入-ElasticSearch？" class="headerlink" title="3.9.3 验证数据是否写入 ElasticSearch？"></a>3.9.3 验证数据是否写入 ElasticSearch？</h3><h3 id="3-9-4-如何保证在海量数据实时写入下-ElasticSearch-的稳定性？"><a href="#3-9-4-如何保证在海量数据实时写入下-ElasticSearch-的稳定性？" class="headerlink" title="3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？"></a>3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？</h3><h3 id="3-9-5-使用-Flink-connector-elasticsearch-可能会遇到的问题"><a href="#3-9-5-使用-Flink-connector-elasticsearch-可能会遇到的问题" class="headerlink" title="3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题"></a>3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题</h3><h3 id="3-9-6-小结与反思"><a href="#3-9-6-小结与反思" class="headerlink" title="3.9.6 小结与反思"></a>3.9.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/Jeqzfem">https://t.zsxq.com/Jeqzfem</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-9-Flink-Connector-——-ElasticSearch-的用法和分析&quot;&gt;&lt;a href=&quot;#3-9-Flink-Connector-——-ElasticSearch-的用法和分析&quot; class=&quot;headerlink&quot; title=&quot;3.9 Flink Connector —— ElasticSearch 的用法和分析&quot;&gt;&lt;/a&gt;3.9 Flink Connector —— ElasticSearch 的用法和分析&lt;/h2&gt;&lt;p&gt;ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 自定义 Flink Connector</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/19/flink-in-action-3.8/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/19/flink-in-action-3.8/</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2021-12-11T07:14:43.418Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-8-自定义-Flink-Connector"><a href="#3-8-自定义-Flink-Connector" class="headerlink" title="3.8 自定义 Flink Connector"></a>3.8 自定义 Flink Connector</h2><p>在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。</p><a id="more"></a><h3 id="3-8-1-如何自定义-Source-Connector？"><a href="#3-8-1-如何自定义-Source-Connector？" class="headerlink" title="3.8.1 如何自定义 Source Connector？"></a>3.8.1 如何自定义 Source Connector？</h3><p>这里就演示一下如何自定义 Source 从 MySQL 中读取数据。</p><p><strong>添加依赖</strong></p><p>在 pom.xml 中添加 MySQL 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>数据库建表</strong></p><p>数据库建表的 SQL 语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`student`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`age`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_bin;</span><br></pre></td></tr></table></figure><p><strong>数据库插入数据</strong></p><p>往新建的数据库表中插入 4 条数据的 SQL 语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`student`</span> <span class="keyword">VALUES</span> (<span class="string">'1'</span>, <span class="string">'zhisheng01'</span>, <span class="string">'123456'</span>, <span class="string">'18'</span>), (<span class="string">'2'</span>, <span class="string">'zhisheng02'</span>, <span class="string">'123'</span>, <span class="string">'17'</span>), (<span class="string">'3'</span>, <span class="string">'zhisheng03'</span>, <span class="string">'1234'</span>, <span class="string">'18'</span>), (<span class="string">'4'</span>, <span class="string">'zhisheng04'</span>, <span class="string">'12345'</span>, <span class="string">'16'</span>);</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure><p><strong>新建实体类</strong></p><p>对应数据库字段的实体类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;  <span class="comment">//id</span></span><br><span class="line">    <span class="keyword">public</span> String name; <span class="comment">//姓名</span></span><br><span class="line">    <span class="keyword">public</span> String password; <span class="comment">//密码</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age; <span class="comment">//年龄</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自定义 Source 类</strong></p><p>SourceFromMySQL 是自定义的 Source 类，该类继承 RichSourceFunction ，实现里面的 open、close、run、cancel 方法，它的作用是读取 MySQL 中的数据，代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceFromMySQL</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"select * from Student;"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 程序执行完毕就可以进行，关闭连接和释放资源的动作了</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123; <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DataStream 调用一次 run() 方法用来获取数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Student&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(</span><br><span class="line">                    resultSet.getInt(<span class="string">"id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"name"</span>).trim(),</span><br><span class="line">                    resultSet.getString(<span class="string">"password"</span>).trim(),</span><br><span class="line">                    resultSet.getInt(<span class="string">"age"</span>));</span><br><span class="line">            ctx.collect(student);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">                con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                System.out.println(<span class="string">"mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Flink 应用程序代码</strong></p><p>读取 MySQL 数据的代码完成后，接下来 Flink 主程序的代码就可以直接在 <code>addSource()</code> 方法中构造一个 SourceFromMySQL 对象作为一个参数传入，具体代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> SourceFromMySQL()).print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data sourc"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 Flink 程序，控制台日志中可以看见打印的 student 信息，结果如下图所示。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cY9WwK.jpg" alt=""></p><h3 id="3-8-2-RichSourceFunction-的用法及源码分析"><a href="#3-8-2-RichSourceFunction-的用法及源码分析" class="headerlink" title="3.8.2 RichSourceFunction 的用法及源码分析"></a>3.8.2 RichSourceFunction 的用法及源码分析</h3><p>从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，其实也是可以使用 SourceFunction 函数来自定义 Source。 RichSourceFunction 函数比 SourceFunction 多了 open 方法（可以用来初始化）和获取应用上下文的方法，那么来了解一下该类，它的类结构如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-020426.png" alt=""></p><p>它是一个抽象类，继承自 AbstractRichFunction，实现了 SourceFunction 接口，其子类有三个，如下图所示，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-020702.png" alt=""></p><p>这三个子类的功能如下：</p><ul><li>MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。</li><li>MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。</li><li>ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。</li></ul><p>除了上面使用 RichSourceFunction 和 SourceFunction 来自定义 Source，还可以继承 RichParallelSourceFunction 抽象类或实现 ParallelSourceFunction 接口来实现自定义 Source 函数。</p><h3 id="3-8-3-自定义-Sink-Connector"><a href="#3-8-3-自定义-Sink-Connector" class="headerlink" title="3.8.3 自定义 Sink Connector"></a>3.8.3 自定义 Sink Connector</h3><p>下面将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去</p><h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>写了一个工具类往 Kafka 的 topic 中发送数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(student));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="SinkToMySQL"><a href="#SinkToMySQL" class="headerlink" title="SinkToMySQL"></a>SinkToMySQL</h4><p>该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法，在 invoke 方法中将数据插入到 MySQL 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Student value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//组装数据，执行插入操作</span></span><br><span class="line">        ps.setInt(<span class="number">1</span>, value.getId());</span><br><span class="line">        ps.setString(<span class="number">2</span>, value.getName());</span><br><span class="line">        ps.setString(<span class="number">3</span>, value.getPassword());</span><br><span class="line">        ps.setInt(<span class="number">4</span>, value.getAge());</span><br><span class="line">        ps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"root123456"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span>+ e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Flink-程序"><a href="#Flink-程序" class="headerlink" title="Flink 程序"></a>Flink 程序</h4><p>这里的 source 是从 Kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 Student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; JSON.parseObject(string, Student.class)); <span class="comment">//Fastjson 解析字符串成 student 对象</span></span><br><span class="line"></span><br><span class="line">        student.addSink(<span class="keyword">new</span> SinkToMySQL()); <span class="comment">//数据 sink 到 mysql</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add sink"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h3 id="3-8-4-RichSinkFunction-的用法及源码分析"><a href="#3-8-4-RichSinkFunction-的用法及源码分析" class="headerlink" title="3.8.4 RichSinkFunction 的用法及源码分析"></a>3.8.4 RichSinkFunction 的用法及源码分析</h3><h3 id="3-8-5-小结与反思"><a href="#3-8-5-小结与反思" class="headerlink" title="3.8.5 小结与反思"></a>3.8.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/Y3RBaaQ">https://t.zsxq.com/Y3RBaaQ</a></p><p>批量写 MySQL 可以参考 ：<a href="https://t.zsxq.com/FAmYFYJ">https://t.zsxq.com/FAmYFYJ</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-8-自定义-Flink-Connector&quot;&gt;&lt;a href=&quot;#3-8-自定义-Flink-Connector&quot; class=&quot;headerlink&quot; title=&quot;3.8 自定义 Flink Connector&quot;&gt;&lt;/a&gt;3.8 自定义 Flink Connector&lt;/h2&gt;&lt;p&gt;在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— Kafka 的使用和源码分析</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/17/flink-in-action-3.7/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/17/flink-in-action-3.7/</id>
    <published>2021-07-16T16:00:00.000Z</published>
    <updated>2021-12-11T07:10:06.981Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-7-Flink-Connector-——-Kafka-的使用和源码分析"><a href="#3-7-Flink-Connector-——-Kafka-的使用和源码分析" class="headerlink" title="3.7 Flink Connector —— Kafka 的使用和源码分析"></a>3.7 Flink Connector —— Kafka 的使用和源码分析</h2><p>在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去，整个案例的执行流程如下图所示。</p><a id="more"></a><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-101054.png" alt=""></p><h3 id="3-7-1-准备环境和依赖"><a href="#3-7-1-准备环境和依赖" class="headerlink" title="3.7.1 准备环境和依赖"></a>3.7.1 准备环境和依赖</h3><p>接下来准备 Kafka 环境的安装和添加相关的依赖。</p><h4 id="环境安装和启动"><a href="#环境安装和启动" class="headerlink" title="环境安装和启动"></a>环境安装和启动</h4><p>如果你已经安装好了 Flink 和 Kafka，那么接下来使用命令运行启动 Flink、Zookepeer、Kafka 就行了。</p><p>启动 Flink 的命令如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-042714.png" alt="启动 Flink"></p><p>启动 Kafka 的命令如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-142523.png" alt="启动 Kafka"></p><p>执行命令都启动好了后就可以添加依赖了。</p><h4 id="添加-Maven-依赖"><a href="#添加-Maven-依赖" class="headerlink" title="添加 Maven 依赖"></a>添加 Maven 依赖</h4><p>Flink 里面支持 Kafka 0.8.x 以上的版本，具体采用哪个版本的 Maven 依赖需要根据安装的 Kafka 版本来确定。因为之前我们安装的 Kafka 是 1.1.0 版本，所以这里我们选择的 Kafka Connector 为 <code>flink-connector-kafka-0.11_2.11</code> （支持 Kafka 0.11.x 版本及以上，该 Connector 支持 Kafka 事务消息传递，所以能保证 Exactly Once）。Flink Kafka Connector 支持的版本如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-043040.png" alt=""></p><p>添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink、Kafka、Flink Kafka Connector 三者对应的版本可以根据 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html">官网</a> 的对比来选择。需要注意的是 <code>flink-connector-kafka_2.11</code> 这个版本支持的 Kafka 版本要大于 1.0.0，从 Flink 1.9 版本开始，它使用的是 Kafka 2.2.0 版本的客户端，虽然这些客户端会做向后兼容，但是建议还是按照官网约定的来规范使用 Connector 版本。另外你还要添加的依赖有：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--flink java--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--log--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--alibaba fastjson--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-7-2-将测试数据发送到-Kafka-Topic"><a href="#3-7-2-将测试数据发送到-Kafka-Topic" class="headerlink" title="3.7.2 将测试数据发送到 Kafka Topic"></a>3.7.2 将测试数据发送到 Kafka Topic</h3><p>我们模拟一些测试数据，然后将这些测试数据发到 Kafka Topic 中去，数据的结构如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Metric</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name; <span class="comment">//指标名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;  <span class="comment">//时间戳</span></span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, Object&gt; fields;  <span class="comment">//指标含有的属性</span></span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, String&gt; tags;    <span class="comment">//指标的标识</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>往 kafka 中写数据工具类 <code>KafkaUtils.java</code>，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"metric"</span>;  <span class="comment">// kafka topic，Flink 程序中需要和这个统一 </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//key 序列化</span></span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//value 序列化</span></span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        Metric metric = <span class="keyword">new</span> Metric();</span><br><span class="line">        metric.setTimestamp(System.currentTimeMillis());</span><br><span class="line">        metric.setName(<span class="string">"mem"</span>);</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        tags.put(<span class="string">"cluster"</span>, <span class="string">"zhisheng"</span>);</span><br><span class="line">        tags.put(<span class="string">"host_ip"</span>, <span class="string">"101.147.022.106"</span>);</span><br><span class="line"></span><br><span class="line">        fields.put(<span class="string">"used_percent"</span>, <span class="number">90</span>d);</span><br><span class="line">        fields.put(<span class="string">"max"</span>, <span class="number">27244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"used"</span>, <span class="number">17244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"init"</span>, <span class="number">27244873</span>d);</span><br><span class="line"></span><br><span class="line">        metric.setTags(tags);</span><br><span class="line">        metric.setFields(fields);</span><br><span class="line"></span><br><span class="line">        ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(metric));</span><br><span class="line">        producer.send(record);</span><br><span class="line">        System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(metric));</span><br><span class="line"></span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">300</span>);</span><br><span class="line">            writeToKafka();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-101504.png" alt=""></p><p>如果出现如上图标记的，即代表能够不断往 kafka 发送数据的。</p><h3 id="3-7-3-Flink-如何消费-Kafka-数据？"><a href="#3-7-3-Flink-如何消费-Kafka-数据？" class="headerlink" title="3.7.3 Flink 如何消费 Kafka 数据？"></a>3.7.3 Flink 如何消费 Kafka 数据？</h3><p>Flink 消费 Kafka 数据的应用程序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);  <span class="comment">//key 反序列化</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>); <span class="comment">//value 反序列化</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"metric"</span>,  <span class="comment">//kafka topic</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),  <span class="comment">// String 序列化</span></span><br><span class="line">                props)).setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        dataStreamSource.print(); <span class="comment">//把从 kafka 读取到的数据打印在控制台</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data source"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下图所示（程序可以不断的消费到 Kafka Topic 中的数据）：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-101832.png" alt=""></p><p><strong>代码分析</strong></p><p>使用 FlinkKafkaConsumer011 时传入了三个参数：</p><ul><li>Kafka topic：这个代表了 Flink 要消费的是 Kafka 哪个 Topic，如果你要同时消费多个 Topic 的话，那么你可以传入一个 Topic List 进去，另外也支持正则表达式匹配 Topic，源码如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-102157.png" alt=""></p><ul><li><p>序列化：上面代码我们使用的是 SimpleStringSchema。</p></li><li><p>配置属性：将 Kafka 等的一些配置传入 。</p></li></ul><p>前面演示了 Flink 如何消费 Kafak 数据，接下来演示如何把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p><h3 id="3-7-4-Flink-如何将计算后的数据发送到-Kafka？"><a href="#3-7-4-Flink-如何将计算后的数据发送到-Kafka？" class="headerlink" title="3.7.4 Flink 如何将计算后的数据发送到 Kafka？"></a>3.7.4 Flink 如何将计算后的数据发送到 Kafka？</h3><p>将 Kafka 集群中 topic 数据写入本地 Kafka 的程序中要填写的配置有消费的 Kafka 集群地址、group.id、将数据写入 Kafka 的集群地址、topic 信息等，将所有的配置提取到配置文件中，如下所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//其他 Kafka 集群配置</span><br><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>执行上面命令后的结果如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6KFHKT.jpg" alt=""></p><p>可以看到本地的 Kafka 是没有任何 topic 的，如果等下程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。</p><p>整个 Flink 程序的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动程序，查看运行结果，不断执行查看 topic 列表的命令，观察是否有新的 topic 出来，结果如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nxqZmZ.jpg" alt=""></p><p>执行命令可以查看该 topic 的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure><p>该 topic 信息如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/y5vPRR.jpg" alt=""></p><p>前面代码使用的 FlinkKafkaProducer011 只传了三个参数：brokerList、topicId、serializationSchema（序列化），其实是支持传入多个参数的，Flink 中的源码如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-102620.png" alt=""></p><h3 id="3-7-5-FlinkKafkaConsumer-源码分析"><a href="#3-7-5-FlinkKafkaConsumer-源码分析" class="headerlink" title="3.7.5 FlinkKafkaConsumer 源码分析"></a>3.7.5 FlinkKafkaConsumer 源码分析</h3><h3 id="3-7-6-FlinkKafkaProducer-源码分析"><a href="#3-7-6-FlinkKafkaProducer-源码分析" class="headerlink" title="3.7.6 FlinkKafkaProducer 源码分析"></a>3.7.6 FlinkKafkaProducer 源码分析</h3><h3 id="3-7-7-使用-Flink-connector-kafka-可能会遇到的问题"><a href="#3-7-7-使用-Flink-connector-kafka-可能会遇到的问题" class="headerlink" title="3.7.7 使用 Flink-connector-kafka 可能会遇到的问题"></a>3.7.7 使用 Flink-connector-kafka 可能会遇到的问题</h3><h4 id="如何消费多个-Kafka-Topic"><a href="#如何消费多个-Kafka-Topic" class="headerlink" title="如何消费多个 Kafka Topic"></a>如何消费多个 Kafka Topic</h4><h4 id="想要获取数据的元数据信息"><a href="#想要获取数据的元数据信息" class="headerlink" title="想要获取数据的元数据信息"></a>想要获取数据的元数据信息</h4><h4 id="多种数据类型"><a href="#多种数据类型" class="headerlink" title="多种数据类型"></a>多种数据类型</h4><h4 id="序列化失败"><a href="#序列化失败" class="headerlink" title="序列化失败"></a>序列化失败</h4><h4 id="Kafka-消费-Offset-的选择"><a href="#Kafka-消费-Offset-的选择" class="headerlink" title="Kafka 消费 Offset 的选择"></a>Kafka 消费 Offset 的选择</h4><h4 id="如何自动发现-Topic-新增的分区并读取数据"><a href="#如何自动发现-Topic-新增的分区并读取数据" class="headerlink" title="如何自动发现 Topic 新增的分区并读取数据"></a>如何自动发现 Topic 新增的分区并读取数据</h4><h4 id="程序消费-Kafka-的-offset-是如何管理的"><a href="#程序消费-Kafka-的-offset-是如何管理的" class="headerlink" title="程序消费 Kafka 的 offset 是如何管理的"></a>程序消费 Kafka 的 offset 是如何管理的</h4><h3 id="3-7-8-小结与反思"><a href="#3-7-8-小结与反思" class="headerlink" title="3.7.8 小结与反思"></a>3.7.8 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/2bmurFy">https://t.zsxq.com/2bmurFy</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-7-Flink-Connector-——-Kafka-的使用和源码分析&quot;&gt;&lt;a href=&quot;#3-7-Flink-Connector-——-Kafka-的使用和源码分析&quot; class=&quot;headerlink&quot; title=&quot;3.7 Flink Connector —— Kafka 的使用和源码分析&quot;&gt;&lt;/a&gt;3.7 Flink Connector —— Kafka 的使用和源码分析&lt;/h2&gt;&lt;p&gt;在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去，整个案例的执行流程如下图所示。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 常用的 Source Connector 和 Sink Connector 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/16/flink-in-action-3.6/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/16/flink-in-action-3.6/</id>
    <published>2021-07-15T16:00:00.000Z</published>
    <updated>2021-12-11T07:06:00.727Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍"><a href="#3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍" class="headerlink" title="3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍"></a>3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍</h2><p>通过前面我们可以知道 Flink Job 的大致结构就是 <code>Source ——&gt; Transformation ——&gt; Sink</code>。</p><a id="more"></a><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-141653.png" alt=""></p><p>那么这个 Source 是什么意思呢？我们下面来看看。</p><h3 id="3-6-1-Data-Source-简介"><a href="#3-6-1-Data-Source-简介" class="headerlink" title="3.6.1 Data Source 简介"></a>3.6.1 Data Source 简介</h3><p>Data Source 是什么呢？就字面意思其实就可以知道：数据来源。</p><p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。</p><p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p><p>Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p><p>那么常用的 Data Source 有哪些呢？</p><h3 id="3-6-2-常用的-Data-Source"><a href="#3-6-2-常用的-Data-Source" class="headerlink" title="3.6.2 常用的 Data Source"></a>3.6.2 常用的 Data Source</h3><p>StreamExecutionEnvironment 中可以使用如下图所示的这些已实现的 Stream Source。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083744.png" alt=""></p><p>总的来说可以分为集合、文件、Socket、自定义四大类。</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><p>基于集合的有下面五种方法：</p><p>1、fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</p><p>2、fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</p><p>3、fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"barfoo"</span>, <span class="number">1.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"start"</span>, <span class="number">2.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"foobar"</span>, <span class="number">3.0</span>),</span><br><span class="line">...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>4、fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</p><p>5、generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</p><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>基于文件的有下面三种方法：</p><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">"file:///path/to/file"</span>);</span><br></pre></td></tr></table></figure><p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p><p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS_ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure><p><strong>实现:</strong></p><p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p><p><strong>重要注意：</strong></p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p><h4 id="基于-Socket"><a href="#基于-Socket" class="headerlink" title="基于 Socket"></a>基于 Socket</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h4><p>addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。</p><p><strong>说说上面几种的特点</strong></p><p>1、基于集合：有界数据集，更偏向于本地测试用</p><p>2、基于文件：适合监听文件修改并读取其内容</p><p>3、基于 Socket：监听主机的 host port，从 Socket 中获取数据</p><p>4、自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">.addSource(</span><br><span class="line"><span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">parameterTool.getRequired(<span class="string">"input-topic"</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line"><span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">parameterTool.getProperties())</span><br><span class="line">.assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure><p>Flink 目前支持的 Source 如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/UTfWCZ.jpg" alt=""></p><p>如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。</p><h3 id="3-6-3-Data-Sink-简介"><a href="#3-6-3-Data-Sink-简介" class="headerlink" title="3.6.3 Data Sink 简介"></a>3.6.3 Data Sink 简介</h3><h3 id="3-6-4-常用的-Data-Sink"><a href="#3-6-4-常用的-Data-Sink" class="headerlink" title="3.6.4 常用的 Data Sink"></a>3.6.4 常用的 Data Sink</h3><h3 id="3-6-5-小结与反思"><a href="#3-6-5-小结与反思" class="headerlink" title="3.6.5 小结与反思"></a>3.6.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/FAayJU3">https://t.zsxq.com/FAayJU3</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍&quot;&gt;&lt;a href=&quot;#3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍&quot; class=&quot;headerlink&quot; title=&quot;3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍&quot;&gt;&lt;/a&gt;3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍&lt;/h2&gt;&lt;p&gt;通过前面我们可以知道 Flink Job 的大致结构就是 &lt;code&gt;Source ——&amp;gt; Transformation ——&amp;gt; Sink&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Watermark 的用法和结合 Window 处理延迟数据</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/15/flink-in-action-3.5/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/15/flink-in-action-3.5/</id>
    <published>2021-07-14T16:00:00.000Z</published>
    <updated>2021-12-11T07:03:11.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-5-Watermark-的用法和结合-Window-处理延迟数据"><a href="#3-5-Watermark-的用法和结合-Window-处理延迟数据" class="headerlink" title="3.5 Watermark 的用法和结合 Window 处理延迟数据"></a>3.5 Watermark 的用法和结合 Window 处理延迟数据</h2><p>在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</p><a id="more"></a><p>选择 Event Time 与 Process Time 的实际效果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-142659.png" alt=""></p><p>在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实却仍然这么骨感，会因为各种各样的问题（网络的抖动、设备的故障、应用的异常等原因）从而导致如图中曲线一样，Process Time 总是会与 Event Time 有一些延迟。所谓乱序，其实是指 Flink 接收到的事件的先后顺序并不是严格的按照事件的 Event Time 顺序排列的。如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-142742.png" alt=""></p><p>然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：</p><ul><li>错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</li><li>设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</li></ul><p>这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。庆幸的是 Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</p><h3 id="3-5-1-Watermark-简介"><a href="#3-5-1-Watermark-简介" class="headerlink" title="3.5.1 Watermark 简介"></a>3.5.1 Watermark 简介</h3><p>举个例子：</p><p>统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</p><p>Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。</p><p>下面通过几个图来了解一下 Watermark 是如何工作的！如下图所示，数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，<code>W(4)</code> 和 <code>W(9)</code> 是水印。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154340.jpg" alt=""></p><p>经过 Flink 的消费，数据 <code>1</code>、<code>3</code>、<code>2</code> 进入了第一个窗口，然后 <code>7</code> 会进入第二个窗口，接着 <code>3</code> 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 <code>4</code> 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154747.jpg" alt=""></p><p>那么接着后面的数据 <code>5</code> 和 <code>6</code> 会进入到第二个窗口里面，数据 <code>9</code> 会进入在第三个窗口里面，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155309.jpg" alt=""></p><p>那么当遇到水印 <code>9</code> 时，发现水印比第二个窗口的结束时间 <code>8</code> 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155558.jpg" alt=""></p><p>相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。</p><h3 id="3-5-2-Flink-中的-Watermark-的设置"><a href="#3-5-2-Flink-中的-Watermark-的设置" class="headerlink" title="3.5.2 Flink 中的 Watermark 的设置"></a>3.5.2 Flink 中的 Watermark 的设置</h3><p>在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以设置 Watermark 是有如下两种方式：</p><ul><li>AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。</li></ul><p>在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。</p><ul><li>AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</li></ul><p>在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。</p><p>下面再分别详细讲下这两种的实现方式。</p><h3 id="3-5-3-Punctuated-Watermark"><a href="#3-5-3-Punctuated-Watermark" class="headerlink" title="3.5.3 Punctuated Watermark"></a>3.5.3 Punctuated Watermark</h3><p>AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。</p><p>那么该怎么利用这个来定义水印生成器呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPunctuatedWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(Word lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> extractedTimestamp % <span class="number">3</span> == <span class="number">0</span> ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。</p><h3 id="3-5-4-Periodic-Watermark"><a href="#3-5-4-Periodic-Watermark" class="headerlink" title="3.5.4 Periodic Watermark"></a>3.5.4 Periodic Watermark</h3><p>通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPeriodicWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word word, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> timestamp = word.getTimestamp();</span><br><span class="line">        currentTimestamp = Math.max(timestamp, currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> word.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> maxTimeLag = <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，然后将当前时间戳与事件时间进行比较，取最大值后赋值给当前时间戳 currentTimestamp，然后返回事件时间。getCurrentWatermark() 方法是获取当前的水位线，通过 <code>currentTimestamp - maxTimeLag</code> 得到水印的值，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 <code>long maxTimeLag = 5000;</code> 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。</p><p>通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。</p><p>AssignerWithPeriodicWatermarks 这个接口有四个实现类，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082804.png" alt=""></p><p>这四个实现类的功能和使用方式如下：</p><ul><li>BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下图所示：</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083043.png" alt=""></p><p>你可以像下面一样使用该类来分配时间和生成水印：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Time.seconds(10) 代表允许延迟的时间大小</span></span><br><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">    <span class="comment">//重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li><p>CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。</p></li><li><p>AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">    <span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">        <span class="keyword">this</span>.currentTimestamp = ne∏wTimestamp;</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 <code>System.currentTimeMillis()</code> 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。</li></ul><p><strong>注意</strong>：</p><p>1、使用这种方式周期性生成水印的话，你可以通过 <code>env.getConfig().setAutoWatermarkInterval(...);</code> 来设置生成水印的间隔（每隔 n 毫秒）。</p><p>2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;MyType&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">while</span> (<span class="comment">/* condition */</span>) &#123;</span><br><span class="line">MyType next = getNext();</span><br><span class="line">ctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class="line">ctx.emitWatermark(<span class="keyword">new</span> Watermark(next.getWatermarkTime()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-5-每个-Kafka-分区的时间戳"><a href="#3-5-5-每个-Kafka-分区的时间戳" class="headerlink" title="3.5.5 每个 Kafka 分区的时间戳"></a>3.5.5 每个 Kafka 分区的时间戳</h3><h3 id="3-5-6-将-Watermark-与-Window-结合起来处理延迟数据"><a href="#3-5-6-将-Watermark-与-Window-结合起来处理延迟数据" class="headerlink" title="3.5.6 将 Watermark 与 Window 结合起来处理延迟数据"></a>3.5.6 将 Watermark 与 Window 结合起来处理延迟数据</h3><h3 id="3-5-7-处理延迟数据的三种方法"><a href="#3-5-7-处理延迟数据的三种方法" class="headerlink" title="3.5.7 处理延迟数据的三种方法"></a>3.5.7 处理延迟数据的三种方法</h3><h4 id="丢弃（默认）"><a href="#丢弃（默认）" class="headerlink" title="丢弃（默认）"></a>丢弃（默认）</h4><h4 id="allowedLateness-再次指定允许数据延迟的时间"><a href="#allowedLateness-再次指定允许数据延迟的时间" class="headerlink" title="allowedLateness 再次指定允许数据延迟的时间"></a>allowedLateness 再次指定允许数据延迟的时间</h4><h4 id="sideOutputLateData-收集迟到的数据"><a href="#sideOutputLateData-收集迟到的数据" class="headerlink" title="sideOutputLateData 收集迟到的数据"></a>sideOutputLateData 收集迟到的数据</h4><h3 id="3-5-8-小结与反思"><a href="#3-5-8-小结与反思" class="headerlink" title="3.5.8 小结与反思"></a>3.5.8 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/RbufeIA">https://t.zsxq.com/RbufeIA</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-5-Watermark-的用法和结合-Window-处理延迟数据&quot;&gt;&lt;a href=&quot;#3-5-Watermark-的用法和结合-Window-处理延迟数据&quot; class=&quot;headerlink&quot; title=&quot;3.5 Watermark 的用法和结合 Window 处理延迟数据&quot;&gt;&lt;/a&gt;3.5 Watermark 的用法和结合 Window 处理延迟数据&lt;/h2&gt;&lt;p&gt;在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp;amp; 事件延迟。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
