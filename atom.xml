<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2021-11-11T15:33:45.652Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>宕机一台机器，结果一百多个 Flink 作业挂了</title>
    <link href="http://www.54tianzhisheng.cn/2021/11/11/flink-akka-framesize/"/>
    <id>http://www.54tianzhisheng.cn/2021/11/11/flink-akka-framesize/</id>
    <published>2021-11-10T16:00:00.000Z</published>
    <updated>2021-11-11T15:33:45.652Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下：</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2021-11-09 16:01:11</span><br><span class="line">java.util.concurrent.CompletionException: java.lang.reflect.UndeclaredThrowableException</span><br><span class="line"> at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)</span><br><span class="line"> at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)</span><br><span class="line"> at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592)</span><br><span class="line"> at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line"> at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)</span><br><span class="line"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)</span><br><span class="line"> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line"> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line"> at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.lang.reflect.UndeclaredThrowableException</span><br><span class="line"> at com.sun.proxy.$Proxy54.submitTask(Unknown Source)</span><br><span class="line"> at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72)</span><br><span class="line"> at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756)</span><br><span class="line"> at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)</span><br><span class="line"> ... 7 more</span><br><span class="line">Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize.</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.createRpcInvocationMessage(AkkaInvocationHandler.java:276)</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:205)</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:134)</span><br><span class="line"> ... 11 more</span><br></pre></td></tr></table></figure><h3 id="解决异常过程"><a href="#解决异常过程" class="headerlink" title="解决异常过程"></a>解决异常过程</h3><p>从上面的异常日志中我们提取到关键信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize.</span><br></pre></td></tr></table></figure><p>看起来是 RPC 的消息大小超过了默认的 akka framesize 的最大值了，所以我们来了解一下这个值的默认值，从 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.12/deployment/config.html#akka-framesize">官网</a> 我们可以看的到该值的默认大小为 “10485760b”，并且该参数的描述为：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwbm72hedkj31i806imya.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier.</span><br></pre></td></tr></table></figure><p>翻译过来的意思就是：这个参数是 JobManager 和 TaskManagers 之间通信允许的最大消息大小，如果 Flink 作业因为通信消息大小超过了该值，你可以通过增加该值的大小来解决，该参数需要指定一个单位。</p><h3 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h3><p>Flink 使用 Akka 作为组件（JobManager/TaskManager/ResourceManager）之间的 RPC 框架，在 JobManager 和 TaskManagers 之间发送的消息的最大大小默认为 10485760b，如果消息超过这个限制就会失败，报错。这个可以看下抛出异常处的源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">protected RpcInvocation createRpcInvocationMessage(String methodName, Class&lt;?&gt;[] parameterTypes, Object[] args) throws IOException &#123;</span><br><span class="line">    Object rpcInvocation;</span><br><span class="line">    if (this.isLocal) &#123;</span><br><span class="line">        rpcInvocation = new LocalRpcInvocation(methodName, parameterTypes, args);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            RemoteRpcInvocation remoteRpcInvocation = new RemoteRpcInvocation(methodName, parameterTypes, args);</span><br><span class="line">            if (remoteRpcInvocation.getSize() &gt; this.maximumFramesize) &#123;</span><br><span class="line">                // 异常所在位置</span><br><span class="line">                throw new IOException(&quot;The rpc invocation size exceeds the maximum akka framesize.&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            rpcInvocation = remoteRpcInvocation;</span><br><span class="line">        &#125; catch (IOException var6) &#123;</span><br><span class="line">            LOG.warn(&quot;Could not create remote rpc invocation message. Failing rpc invocation because...&quot;, var6);</span><br><span class="line">            throw var6;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return (RpcInvocation)rpcInvocation;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至于为什么 JobManager 和 TaskManager 之间的 RPC 消息大小会如此之大，初步的解释是在 task 出现异常之后，它需要调用 updateTaskExecutionState(TaskExecutionState，taskExecutionState) 这个 RPC 接口去通知 Flink Jobmanager 去改变对应 task 的状态并且重启 task。但是呢，taskExecutionState 这个参数里面有个 error 属性，当我的 task 打出来的错误栈太多的时候，在序列化的之后超过了 rpc 接口要求的最大数据大小（也就是 maximum akka framesize），导致调用 updateTaskExecutionState 这个 rpc 接口失败，Jobmanager 无法获知这个 task 已经处于 fail 的状态，也无法重启，然后就导致了一系列连锁反应。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>任务停止，在 <code>flink-conf.yaml</code> 中加入 <code>akka.framesize</code> 参数，调大该值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">akka.framesize: &quot;62914560b&quot;</span><br></pre></td></tr></table></figure><p>然后将任务重启，可以观察 Jobmanager Configration 看看参数是否生效。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下：&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>实时平台如何管理多个 Flink 版本？—— 为啥会出现多个版本？</title>
    <link href="http://www.54tianzhisheng.cn/2021/09/26/realtime-platform-flink-version/"/>
    <id>http://www.54tianzhisheng.cn/2021/09/26/realtime-platform-flink-version/</id>
    <published>2021-09-25T16:00:00.000Z</published>
    <updated>2021-11-14T03:12:49.099Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为啥会出现多个版本？"><a href="#为啥会出现多个版本？" class="headerlink" title="为啥会出现多个版本？"></a>为啥会出现多个版本？</h3><a id="more"></a><ul><li><p><strong>Flink 社区</strong>本身迭代速度非常快，目前阿里云有一大波的人专职做 Flink 开源，另外还拥有活跃的社区贡献者，所以功能开发较快，bug 修复速度较快，几乎每 4 个月一个大版本，每个大版本之间迭代的功能非常多，代码变动非常大，API 接口变动也大，动不动就干翻自己了。</p></li><li><p>社区迭代快就快呗，为什么<strong>公司</strong>也要要不断跟着社区鼻子走？社区迭代快意味着功能多，修复的 bug 多，相对于早期版本意味着稳定性也高些。除了国内一二线公司有特别多的专职人去负责这块，大多数中小公司最简单最快捷体验到稳定性最高、功能性最多、性能最好的 Flink 版本无非是直接使用最新的 Flink 版本。举个例子：Flink SQL 从最早期（1.9）的功能、性能到目前 1.14，差别真的大很多，优化了特别多的地方，增强了很多功能。原先使用 Flink SQL 完成一个流处理任务非常麻烦，还不如直接写几十行代码来的快，目前我情愿写 SQL 去处理一个流任务。那么自然会跟着升级到新版本。</p></li><li><p><strong>用户 A</strong> 问 Flink SQL 支持单独设置并行度吗？<strong>用户 B</strong> 问实时平台现在支持 Flink 1.13 版本的 Window TVF？这个要 Flink xxx 版本才能支持，要不你升级一下 Flink 版本到 xxx？这样就能支持了，类似的场景还有很多，对于<strong>中小公司的实时平台负责人</strong>来说，这无非最省事；对于<strong>大公司的负责实时开发的人</strong>来说，这无疑是一个噩梦，每次升级新版本都要将在老版本开发的各种功能都想尽办法移植到新版本上来，碰到 API 接口变动大的无非相当于重写了，或者将新版本的某些特别需要的功能通过打 patch 的方式打到老版本里面去。</p></li><li><p>新版本香是真的香，可是为啥有的人不用呢？问题就是，实时作业大多数是长期运行的，如果一个作业没啥错误，在生产运行的好好的，也不出啥故障，稳定性和性能也都能接受（并不是所有作业数据量都很大，会遇到性能问题），那么<strong>用户</strong>为啥要使用新版本？用户才不管你新版本功能多牛逼，性能多屌呢，老子升级还要改依赖版本、改接口代码、测试联调、性能测试（谁知道你说的性能提升是不是吹牛逼的）、稳定性测试（可能上线双跑一段时间验证），这些不需要时间呀，你叫我升级就升级，滚犊子吧，你知道我还有多少业务需求要做吗？</p></li></ul><p>那么就落下这个场地了，又要使用新版本的功能去解决问题，老作业的用户跟他各种扯皮也打动不了他升级作业的版本，那么自然就不断的出现了多个版本了。</p><p>这样，如果不对版本做好规划，那么摊子就逐渐越来越大，越来越难收拾了？</p><p>那么该如何管理公司的 Flink 版本？如果管理和兼容多个 Flink 版本的作业提交？如何兼容 Jar 包和 SQL 作业的提交</p><h3 id="怎么管理多个-Flink-版本的作业提交？"><a href="#怎么管理多个-Flink-版本的作业提交？" class="headerlink" title="怎么管理多个 Flink 版本的作业提交？"></a>怎么管理多个 Flink 版本的作业提交？</h3><p>尽请期待下篇文章</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;为啥会出现多个版本？&quot;&gt;&lt;a href=&quot;#为啥会出现多个版本？&quot; class=&quot;headerlink&quot; title=&quot;为啥会出现多个版本？&quot;&gt;&lt;/a&gt;为啥会出现多个版本？&lt;/h3&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 如何搭建一套 Flink 监控系统?</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/06/flink-in-action-8.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/06/flink-in-action-8.2/</id>
    <published>2021-08-05T16:00:00.000Z</published>
    <updated>2022-01-16T12:23:45.546Z</updated>
    
    <content type="html"><![CDATA[<h2 id="8-2-搭建一套-Flink-监控系统"><a href="#8-2-搭建一套-Flink-监控系统" class="headerlink" title="8.2 搭建一套 Flink 监控系统"></a>8.2 搭建一套 Flink 监控系统</h2><p>8.1 节中讲解了 JobManager、TaskManager 和 Flink Job 的监控，以及需要关注的监控指标有哪些。本节带大家讲解一下如何搭建一套完整的 Flink 监控系统，如果你所在的公司没有专门的监控平台，那么可以根据本节的内容来为公司搭建一套属于自己公司的 Flink 监控系统。</p><a id="more"></a><h3 id="8-2-1-利用-API-获取监控数据"><a href="#8-2-1-利用-API-获取监控数据" class="headerlink" title="8.2.1 利用 API 获取监控数据"></a>8.2.1 利用 API 获取监控数据</h3><p>熟悉 Flink 的朋友都知道 Flink 的 UI 上面已经详细地展示了很多监控指标的数据，并且这些指标还是比较重要的，所以如果不想搭建额外的监控系统，那么直接利用 Flink 自身的 UI 就可以获取到很多重要的监控信息。这里要讲的是这些监控信息其实也是通过 Flink 自身的 Rest API 来获取数据的，所以其实要搭建一个粗糙的监控平台，也是可以直接利用现有的接口定时去获取数据，然后将这些指标的数据存储在某种时序数据库中，最后用些可视化图表做个展示，这样一个完整的监控系统就做出来了。</p><p>这里通过 Chrome 浏览器的控制台来查看一下有哪些 REST API 是用来提供监控数据的。</p><p>1.在 Chrome 浏览器中打开 <code>http://localhost:8081/overview</code> 页面，可以获取到整个 Flink 集群的资源信息：TaskManager 个数（TaskManagers）、Slot 总个数（Total Task Slots）、可用 Slot 个数（Available Task Slots）、Job 运行个数（Running Jobs）、Job 运行状态（Finished 0 Canceled 0 Failed 0）等，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-03-161007.png" alt=""></p><p>2.通过 <code>http://localhost:8081/taskmanagers</code> 页面查看 TaskManager 列表，可以知道该集群下所有 TaskManager 的信息（数据端口号（Data Port）、上一次心跳时间（Last Heartbeat）、总共的 Slot 个数（All Slots）、空闲的 Slot 个数（Free Slots）、以及 CPU 和内存的分配使用情况，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-03-161422.png" alt=""></p><p>3.通过 <code>http://localhost:8081/taskmanagers/tm_id</code> 页面查看 TaskManager 的具体情况（这里的 tm_id 是个随机的 UUID 值）。在这个页面上，除了上一条的监控信息可以查看，还可以查看该 TaskManager 的 JVM（堆和非堆）、Direct 内存、网络、GC 次数和时间，如下图所示。内存和 GC 这些信息非常重要，很多时候 TaskManager 频繁重启的原因就是 JVM 内存设置得不合理，导致频繁的 GC，最后使得 OOM 崩溃，不得不重启。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-03-162532.png" alt=""></p><p>另外如果你在 <code>/taskmanagers/tm_id</code> 接口后面加个 <code>/log</code> 就可以查看该 TaskManager 的日志，注意，在 Flink 中的日志和平常自己写的应用中的日志是不一样的。在 Flink 中，日志是以 TaskManager 为概念打印出来的，而不是以单个 Job 打印出来的，如果你的 Job 在多个 TaskManager 上运行，那么日志就会在多个 TaskManager 中打印出来。如果一个 TaskManager 中运行了多个 Job，那么它里面的日志就会很混乱，查看日志时会发现它为什么既有这个 Job 打出来的日志，又有那个 Job 打出来的日志，如果你之前有这个疑问，那么相信你看完这里，就不会有疑问了。</p><p>对于这种设计是否真的好，不同的人有不同的看法，在 Flink 的 Issue 中就有人提出了该问题，Issue 中的描述是希望日志可以是 Job 与 Job 之间的隔离，这样日志更方便采集和查看，对于排查问题也会更快。对此国内有公司也对这一部分做了改进，不知道正在看本书的你是否有什么好的想法可以解决 Flink 的这一痛点。</p><p>4.通过 <code>http://localhost:8081/#/job-manager/config</code> 页面可以看到可 JobManager 的配置信息，另外通过 <code>http://localhost:8081/jobmanager/log</code> 页面可以查看 JobManager 的日志详情。</p><p>5.通过 <code>http://localhost:8081/jobs/job_id</code> 页面可以查看 Job 的监控数据，如下图所示，由于指标（包括了 Job 的 Task 数据、Operator 数据、Exception 数据、Checkpoint 数据等）过多，大家可以自己在本地测试查看。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-03-164158.png" alt=""></p><p>上面列举了几个 REST API（不是全部），主要是为了告诉大家，其实这些接口我们都知道，那么我们也可以利用这些接口去获取对应的监控数据，然后绘制出更酷炫的图表，用更直观的页面将这些数据展示出来，这样就能更好地控制。</p><p>除了利用 Flink UI 提供的接口去定时获取到监控数据，其实 Flink 还提供了很多的 reporter 去上报监控数据，比如 JMXReporter、PrometheusReporter、PrometheusPushGatewayReporter、InfluxDBReporter、StatsDReporter 等，这样就可以根据需求去定制获取到 Flink 的监控数据，下面教大家使用几个常用的 reporter。</p><p>相关 Rest API 可以查看官网链接：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#rest-api-integration">rest-api-integration</a></p><h3 id="8-2-2-Metrics-类型简介"><a href="#8-2-2-Metrics-类型简介" class="headerlink" title="8.2.2 Metrics 类型简介"></a>8.2.2 Metrics 类型简介</h3><p>可以在继承自 RichFunction 的函数中通过 <code>getRuntimeContext().getMetricGroup()</code> 获取 Metric 信息，常见的 Metrics 的类型有 Counter、Gauge、Histogram、Meter。</p><h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h4><h4 id="Gauge"><a href="#Gauge" class="headerlink" title="Gauge"></a>Gauge</h4><h4 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h4><h4 id="Meter"><a href="#Meter" class="headerlink" title="Meter"></a>Meter</h4><h3 id="8-2-3-利用-JMXReporter-获取监控数据"><a href="#8-2-3-利用-JMXReporter-获取监控数据" class="headerlink" title="8.2.3 利用 JMXReporter 获取监控数据"></a>8.2.3 利用 JMXReporter 获取监控数据</h3><h3 id="8-2-4-利用-PrometheusReporter-获取监控数据"><a href="#8-2-4-利用-PrometheusReporter-获取监控数据" class="headerlink" title="8.2.4 利用 PrometheusReporter 获取监控数据"></a>8.2.4 利用 PrometheusReporter 获取监控数据</h3><h3 id="8-2-5-利用-PrometheusPushGatewayReporter-获取监控数据"><a href="#8-2-5-利用-PrometheusPushGatewayReporter-获取监控数据" class="headerlink" title="8.2.5 利用 PrometheusPushGatewayReporter 获取监控数据"></a>8.2.5 利用 PrometheusPushGatewayReporter 获取监控数据</h3><h3 id="8-2-6-利用-InfluxDBReporter-获取监控数据"><a href="#8-2-6-利用-InfluxDBReporter-获取监控数据" class="headerlink" title="8.2.6 利用 InfluxDBReporter 获取监控数据"></a>8.2.6 利用 InfluxDBReporter 获取监控数据</h3><h3 id="8-2-7-安装-InfluxDB-和-Grafana"><a href="#8-2-7-安装-InfluxDB-和-Grafana" class="headerlink" title="8.2.7 安装 InfluxDB 和 Grafana"></a>8.2.7 安装 InfluxDB 和 Grafana</h3><h4 id="安装-InfluxDB"><a href="#安装-InfluxDB" class="headerlink" title="安装 InfluxDB"></a>安装 InfluxDB</h4><h4 id="安装-Grafana"><a href="#安装-Grafana" class="headerlink" title="安装 Grafana"></a>安装 Grafana</h4><h3 id="8-2-8-配置-Grafana-展示监控数据"><a href="#8-2-8-配置-Grafana-展示监控数据" class="headerlink" title="8.2.8 配置 Grafana 展示监控数据"></a>8.2.8 配置 Grafana 展示监控数据</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/f66iAMz">https://t.zsxq.com/f66iAMz</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="8-2-9-小结与反思"><a href="#8-2-9-小结与反思" class="headerlink" title="8.2.9 小结与反思"></a>8.2.9 小结与反思</h3><p>本节讲了如何利用 API 去获取监控数据，对 Metrics 的类型进行介绍，然后还介绍了怎么利用 Reporter 去将 Metrics 数据进行上报，并通过 InfluxDB + Grafana 搭建了一套 Flink 的监控系统。另外你还可以根据公司的需要使用其他的存储方案来存储监控数据，Grafana 也支持不同的数据源，你们公司的监控系统架构是怎么样的，是否可以直接接入这套监控系统？</p><p>作业部署上线后的监控尤其重要，虽说 Flink UI 自身提供了不少的监控信息，但是个人觉得还是比较弱，还是得去搭建一套完整的监控系统去监控 Flink 中的 JobManager、TaskManager 和作业。本章中讲解了 Flink UI 上获取监控数据的方式，还讲解了如何利用 Flink 自带的 Metrics Reporter 去采集各种监控数据，从而利用时序数据库存储这些监控数据，最后用 Grafana 这种可视化比较好的去展示这些监控数据，从而达到作业真正的监控运维效果。</p><p>整套监控系统也希望你可以运用在你们公司，当然你不一定非得选用相同的存储时序数据库，这样可以为你们节省不少作业出问题后的排查时间。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;8-2-搭建一套-Flink-监控系统&quot;&gt;&lt;a href=&quot;#8-2-搭建一套-Flink-监控系统&quot; class=&quot;headerlink&quot; title=&quot;8.2 搭建一套 Flink 监控系统&quot;&gt;&lt;/a&gt;8.2 搭建一套 Flink 监控系统&lt;/h2&gt;&lt;p&gt;8.1 节中讲解了 JobManager、TaskManager 和 Flink Job 的监控，以及需要关注的监控指标有哪些。本节带大家讲解一下如何搭建一套完整的 Flink 监控系统，如果你所在的公司没有专门的监控平台，那么可以根据本节的内容来为公司搭建一套属于自己公司的 Flink 监控系统。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 如何实时监控 Flink 及其作业？</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/05/flink-in-action-8.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/05/flink-in-action-8.1/</id>
    <published>2021-08-04T16:00:00.000Z</published>
    <updated>2022-01-16T12:20:27.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第八章-——-Flink-监控"><a href="#第八章-——-Flink-监控" class="headerlink" title="第八章 —— Flink 监控"></a>第八章 —— Flink 监控</h1><p>Flink 相关的组件和作业的稳定性通常是比较关键的，所以得需要对它们进行监控，如果有异常，则需要及时告警通知。本章先会教会教会大家如何利用现有 Flink UI 上面的信息去发现和排查问题，会指明一些比较重要和我们非常关心的指标，通过这些指标我们能够立马定位到问题的根本原因。接着笔者会教大家如何去利用现有的 Metrics Reporter 去构建一个 Flink 的监控系统，它可以收集到所有作业的监控指标，并会存储这些监控指标数据，最后还会有一个监控大盘做数据可视化，通过这个大盘可以方便排查问题。</p><h2 id="8-1-实时监控-Flink-及其作业"><a href="#8-1-实时监控-Flink-及其作业" class="headerlink" title="8.1 实时监控 Flink 及其作业"></a>8.1 实时监控 Flink 及其作业</h2><a id="more"></a><p>当将 Flink JobManager、TaskManager 都运行起来了，并且也部署了不少 Flink Job，那么它到底是否还在运行、运行的状态如何、资源 TaskManager 和 Slot 的个数是否足够、Job 内部是否出现异常、计算速度是否跟得上数据生产的速度 等这些问题其实对我们来说是比较关注的，所以就很迫切的需要一个监控系统帮我们把整个 Flink 集群的运行状态给展示出来。通过监控系统我们能够很好的知道 Flink 内部的整个运行状态，然后才能够根据项目生产环境遇到的问题 ‘对症下药’。下面分别来讲下 JobManager、TaskManager、Flink Job 的监控以及最关心的一些监控指标。</p><h3 id="8-1-1-监控-JobManager"><a href="#8-1-1-监控-JobManager" class="headerlink" title="8.1.1 监控 JobManager"></a>8.1.1 监控 JobManager</h3><p>我们知道 JobManager 是 Flink 集群的中控节点，类似于 Apache Storm 的 Nimbus 以及 Apache Spark 的 Driver 的角色。它负责作业的调度、作业 Jar 包的管理、Checkpoint 的协调和发起、与 TaskManager 之间的心跳检查等工作。如果 JobManager 出现问题的话，就会导致作业 UI 信息查看不了，TaskManager 和所有运行的作业都会受到一定的影响，所以这也是为啥在 7.1 节中强调 JobManager 的高可用问题。</p><p>在 Flink 自带的 UI 上 JobManager 那个 Tab 展示的其实并没有显示其对应的 Metrics，那么对于 JobManager 来说常见比较关心的监控指标有哪些呢？</p><h4 id="基础指标"><a href="#基础指标" class="headerlink" title="基础指标"></a>基础指标</h4><p>因为 Flink JobManager 其实也是一个 Java 的应用程序，那么它自然也会有 Java 应用程序的指标，比如内存、CPU、GC、类加载、线程信息等。</p><ul><li>内存：内存又分堆内存和非堆内存，在 Flink 中还有 Direct 内存，每种内存又有初始值、使用值、最大值等指标，因为在 JobManager 中的工作其实相当于 TaskManager 来说比较少，也不存储事件数据，所以通常 JobManager 占用的内存不会很多，在 Flink JobManager 中自带的内存 Metrics 指标有：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_Status_JVM_Memory_Direct_Count</span><br><span class="line">jobmanager_Status_JVM_Memory_Direct_MemoryUsed</span><br><span class="line">jobmanager_Status_JVM_Memory_Direct_TotalCapacity</span><br><span class="line">jobmanager_Status_JVM_Memory_Heap_Committed</span><br><span class="line">jobmanager_Status_JVM_Memory_Heap_Max</span><br><span class="line">jobmanager_Status_JVM_Memory_Heap_Used</span><br><span class="line">jobmanager_Status_JVM_Memory_Mapped_Count</span><br><span class="line">jobmanager_Status_JVM_Memory_Mapped_MemoryUsed</span><br><span class="line">jobmanager_Status_JVM_Memory_Mapped_TotalCapacity</span><br><span class="line">jobmanager_Status_JVM_Memory_NonHeap_Committed</span><br><span class="line">jobmanager_Status_JVM_Memory_NonHeap_Max</span><br><span class="line">jobmanager_Status_JVM_Memory_NonHeap_Used</span><br></pre></td></tr></table></figure><ul><li>CPU：JobManager 分配的 CPU 使用情况，如果使用类似 K8S 等资源调度系统，则需要对每个容器进行设置资源，比如 CPU 限制不能超过多少，在 Flink JobManager 中自带的 CPU 指标有：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_Status_JVM_CPU_Load</span><br><span class="line">jobmanager_Status_JVM_CPU_Time</span><br></pre></td></tr></table></figure><ul><li>GC：GC 信息对于 Java 应用来说是避免不了的，每种 GC 都有时间和次数的指标可以供参考，提供的指标有：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Count</span><br><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Time</span><br><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Count</span><br><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Time</span><br></pre></td></tr></table></figure><h4 id="Checkpoint-指标"><a href="#Checkpoint-指标" class="headerlink" title="Checkpoint 指标"></a>Checkpoint 指标</h4><p>因为 JobManager 负责了作业的 Checkpoint 的协调和发起功能，所以 Checkpoint 相关的指标就有表示 Checkpoint 执行的时间、Checkpoint 的时间长短、完成的 Checkpoint 的次数、Checkpoint 失败的次数、Checkpoint 正在执行 Checkpoint 的个数等，其对应的指标如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_job_lastCheckpointAlignmentBuffered</span><br><span class="line">jobmanager_job_lastCheckpointDuration</span><br><span class="line">jobmanager_job_lastCheckpointExternalPath</span><br><span class="line">jobmanager_job_lastCheckpointRestoreTimestamp</span><br><span class="line">jobmanager_job_lastCheckpointSize</span><br><span class="line">jobmanager_job_numberOfCompletedCheckpoints</span><br><span class="line">jobmanager_job_numberOfFailedCheckpoints</span><br><span class="line">jobmanager_job_numberOfInProgressCheckpoints</span><br><span class="line">jobmanager_job_totalNumberOfCheckpoints</span><br></pre></td></tr></table></figure><h4 id="重要的指标"><a href="#重要的指标" class="headerlink" title="重要的指标"></a>重要的指标</h4><p>另外还有比较重要的指标就是 Flink UI 上也提供的，类似于 Slot 总共个数、Slot 可使用的个数、TaskManager 的个数（通过查看该值可以知道是否有 TaskManager 发生异常重启）、正在运行的作业数量、作业运行的时间和完成的时间、作业的重启次数，对应的指标如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_job_uptime</span><br><span class="line">jobmanager_numRegisteredTaskManagers</span><br><span class="line">jobmanager_numRunningJobs</span><br><span class="line">jobmanager_taskSlotsAvailable</span><br><span class="line">jobmanager_taskSlotsTotal</span><br><span class="line">jobmanager_job_downtime</span><br><span class="line">jobmanager_job_fullRestarts</span><br><span class="line">jobmanager_job_restartingTime</span><br></pre></td></tr></table></figure><h3 id="8-1-2-监控-TaskManager"><a href="#8-1-2-监控-TaskManager" class="headerlink" title="8.1.2 监控 TaskManager"></a>8.1.2 监控 TaskManager</h3><p>….</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">taskmanager_Status_JVM_CPU_Load</span><br><span class="line">taskmanager_Status_JVM_CPU_Time</span><br><span class="line">taskmanager_Status_JVM_ClassLoader_ClassesLoaded</span><br><span class="line">taskmanager_Status_JVM_ClassLoader_ClassesUnloaded</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Count</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Time</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Count</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Time</span><br><span class="line">taskmanager_Status_JVM_Memory_Direct_Count</span><br><span class="line">taskmanager_Status_JVM_Memory_Direct_MemoryUsed</span><br><span class="line">taskmanager_Status_JVM_Memory_Direct_TotalCapacity</span><br><span class="line">taskmanager_Status_JVM_Memory_Heap_Committed</span><br><span class="line">taskmanager_Status_JVM_Memory_Heap_Max</span><br><span class="line">taskmanager_Status_JVM_Memory_Heap_Used</span><br><span class="line">taskmanager_Status_JVM_Memory_Mapped_Count</span><br><span class="line">taskmanager_Status_JVM_Memory_Mapped_MemoryUsed</span><br><span class="line">taskmanager_Status_JVM_Memory_Mapped_TotalCapacity</span><br><span class="line">taskmanager_Status_JVM_Memory_NonHeap_Committed</span><br><span class="line">taskmanager_Status_JVM_Memory_NonHeap_Max</span><br><span class="line">taskmanager_Status_JVM_Memory_NonHeap_Used</span><br><span class="line">taskmanager_Status_JVM_Threads_Count</span><br><span class="line">taskmanager_Status_Network_AvailableMemorySegments</span><br><span class="line">taskmanager_Status_Network_TotalMemorySegments</span><br><span class="line">taskmanager_Status_Shuffle_Netty_AvailableMemorySegments</span><br><span class="line">taskmanager_Status_Shuffle_Netty_TotalMemorySegments</span><br></pre></td></tr></table></figure><h3 id="8-1-3-监控-Flink-作业"><a href="#8-1-3-监控-Flink-作业" class="headerlink" title="8.1.3 监控 Flink 作业"></a>8.1.3 监控 Flink 作业</h3><p>…</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">taskmanager_job_task_Shuffle_Netty_Input_Buffers_outPoolUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Input_Buffers_outputQueueLength</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inPoolUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inputExclusiveBuffersUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inputFloatingBuffersUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inputQueueLength</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocal</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocalPerSecond</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemote</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemotePerSecond</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInLocal</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInLocalPerSecond</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInRemote</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInRemotePerSecond</span><br><span class="line">taskmanager_job_task_buffers_inPoolUsage</span><br><span class="line">taskmanager_job_task_buffers_inputExclusiveBuffersUsage</span><br><span class="line">taskmanager_job_task_buffers_inputFloatingBuffersUsage</span><br><span class="line">taskmanager_job_task_buffers_inputQueueLength</span><br><span class="line">taskmanager_job_task_buffers_outPoolUsage</span><br><span class="line">taskmanager_job_task_buffers_outputQueueLength</span><br><span class="line">taskmanager_job_task_checkpointAlignmentTime</span><br><span class="line">taskmanager_job_task_currentInputWatermark</span><br><span class="line">taskmanager_job_task_numBuffersInLocal</span><br><span class="line">taskmanager_job_task_numBuffersInLocalPerSecond</span><br><span class="line">taskmanager_job_task_numBuffersInRemote</span><br><span class="line">taskmanager_job_task_numBuffersInRemotePerSecond</span><br><span class="line">taskmanager_job_task_numBuffersOut</span><br><span class="line">taskmanager_job_task_numBuffersOutPerSecond</span><br><span class="line">taskmanager_job_task_numBytesIn</span><br><span class="line">taskmanager_job_task_numBytesInLocal</span><br><span class="line">taskmanager_job_task_numBytesInLocalPerSecond</span><br><span class="line">taskmanager_job_task_numBytesInPerSecond</span><br><span class="line">taskmanager_job_task_numBytesInRemote</span><br><span class="line">taskmanager_job_task_numBytesInRemotePerSecond</span><br><span class="line">taskmanager_job_task_numBytesOut</span><br><span class="line">taskmanager_job_task_numBytesOutPerSecond</span><br><span class="line">taskmanager_job_task_numRecordsIn</span><br><span class="line">taskmanager_job_task_numRecordsInPerSecond</span><br><span class="line">taskmanager_job_task_numRecordsOut</span><br><span class="line">taskmanager_job_task_numRecordsOutPerSecond</span><br><span class="line">taskmanager_job_task_operator_currentInputWatermark</span><br><span class="line">taskmanager_job_task_operator_currentOutputWatermark</span><br><span class="line">taskmanager_job_task_operator_numLateRecordsDropped</span><br><span class="line">taskmanager_job_task_operator_numRecordsIn</span><br><span class="line">taskmanager_job_task_operator_numRecordsInPerSecond</span><br><span class="line">taskmanager_job_task_operator_numRecordsOut</span><br><span class="line">taskmanager_job_task_operator_numRecordsOutPerSecond</span><br></pre></td></tr></table></figure><h3 id="8-1-4-最关心的性能指标"><a href="#8-1-4-最关心的性能指标" class="headerlink" title="8.1.4 最关心的性能指标"></a>8.1.4 最关心的性能指标</h3><h4 id="JobManager"><a href="#JobManager" class="headerlink" title="JobManager"></a>JobManager</h4><h4 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h4><h4 id="Flink-Job"><a href="#Flink-Job" class="headerlink" title="Flink Job"></a>Flink Job</h4><h3 id="8-1-5-小结与反思"><a href="#8-1-5-小结与反思" class="headerlink" title="8.1.5 小结与反思"></a>8.1.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/f66iAMz">https://t.zsxq.com/f66iAMz</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第八章-——-Flink-监控&quot;&gt;&lt;a href=&quot;#第八章-——-Flink-监控&quot; class=&quot;headerlink&quot; title=&quot;第八章 —— Flink 监控&quot;&gt;&lt;/a&gt;第八章 —— Flink 监控&lt;/h1&gt;&lt;p&gt;Flink 相关的组件和作业的稳定性通常是比较关键的，所以得需要对它们进行监控，如果有异常，则需要及时告警通知。本章先会教会教会大家如何利用现有 Flink UI 上面的信息去发现和排查问题，会指明一些比较重要和我们非常关心的指标，通过这些指标我们能够立马定位到问题的根本原因。接着笔者会教大家如何去利用现有的 Metrics Reporter 去构建一个 Flink 的监控系统，它可以收集到所有作业的监控指标，并会存储这些监控指标数据，最后还会有一个监控大盘做数据可视化，通过这个大盘可以方便排查问题。&lt;/p&gt;
&lt;h2 id=&quot;8-1-实时监控-Flink-及其作业&quot;&gt;&lt;a href=&quot;#8-1-实时监控-Flink-及其作业&quot; class=&quot;headerlink&quot; title=&quot;8.1 实时监控 Flink 及其作业&quot;&gt;&lt;/a&gt;8.1 实时监控 Flink 及其作业&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/04/flink-in-action-7.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/04/flink-in-action-7.2/</id>
    <published>2021-08-03T16:00:00.000Z</published>
    <updated>2022-01-16T12:17:22.975Z</updated>
    
    <content type="html"><![CDATA[<h2 id="7-2-Flink-作业如何在-Standalone、YARN、Mesos、K8S-上部署运行？"><a href="#7-2-Flink-作业如何在-Standalone、YARN、Mesos、K8S-上部署运行？" class="headerlink" title="7.2 Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？"></a>7.2 Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？</h2><p>前面章节已经有很多学习案列带大家使用 Flink，不仅有讲将 Flink 应用程序在 IDEA 中运行，也有讲将 Flink Job 编译打包上传到 Flink UI 上运行，在这 UI 背后可能是通过 Standalone、YARN、Mesos、Kubernetes 等运行启动的 Flink。那么这节就系统讲下如何部署和运行我们的 Flink Job，大家可以根据自己公司的场景进行选择使用哪种方式进行部署 Flink 作业！</p><a id="more"></a><h3 id="7-2-1-Standalone"><a href="#7-2-1-Standalone" class="headerlink" title="7.2.1 Standalone"></a>7.2.1 Standalone</h3><p>第一种方式就是 Standalone 模式，这种模式笔者在前面 2.2 节里面演示的就是这种，我们通过执行命令：<code>./bin/start-cluster.sh</code> 启动一个 Flink Standalone 集群。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ./bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><p>默认的话是启动一个 JobManager 和一个 TaskManager，我们可以通过 <code>jps</code> 查看进程有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">65425 Jps</span><br><span class="line">51572 TaskManagerRunner</span><br><span class="line">51142 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><p>其中上面的 TaskManagerRunner 代表的是 TaskManager 进程，StandaloneSessionClusterEntrypoint 代表的是 JobManager 进程。上面运行产生的只有一个 JobManager 和一个 TaskManager，如果是生产环境的话，这样的配置肯定是不够运行多个 Job 的，那么我们该如何在生产环境中配置 standalone 模式的集群呢？我们就需要修改 Flink 安装目录下面的 conf 文件夹里面配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink-conf.yaml</span><br><span class="line">masters</span><br><span class="line">slaves</span><br></pre></td></tr></table></figure><p>将 slaves 中再添加一个 <code>localhost</code>，这样就可以启动两个 TaskManager 了。接着启动脚本 <code>start-cluster.sh</code>，启动日志显示如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-161333.png" alt=""></p><p>可以看见有两个 TaskManager 启动了，再看下 UI 显示的也是有两个 TaskManager，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-161431.png" alt=""></p><p>那么如果还想要添加一个 JobManager 或者 TaskManager 怎么办？总不能再次重启修改配置文件后然后再重启吧！这里你可以这样操作。</p><p><strong>增加一个 JobManager</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/jobmanager.sh ((start|start-foreground) [host] [webui-port])|stop|stop-all</span><br></pre></td></tr></table></figure><p>但是注意 Standalone 下一台机器最多只能运行一个 JobManager。</p><p><strong>增加一个 TaskManager</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/taskmanager.sh start|start-foreground|stop|stop-all</span><br></pre></td></tr></table></figure><p>比如我执行了 <code>./bin/taskmanager.sh start</code> 命令后，运行结果如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-161657.png" alt=""></p><p>Standalone 模式下可以先对 Flink Job 通过 <code>mvn clean package</code> 编译打包，得到 Jar 包后，可以在 UI 上直接上传 Jar 包，然后点击 Submit 就可以运行了。</p><h3 id="7-2-2-YARN"><a href="#7-2-2-YARN" class="headerlink" title="7.2.2 YARN"></a>7.2.2 YARN</h3><h3 id="7-2-3-Mesos"><a href="#7-2-3-Mesos" class="headerlink" title="7.2.3 Mesos"></a>7.2.3 Mesos</h3><h4 id="Session-集群"><a href="#Session-集群" class="headerlink" title="Session 集群"></a>Session 集群</h4><h4 id="Per-Job-集群"><a href="#Per-Job-集群" class="headerlink" title="Per Job 集群"></a>Per Job 集群</h4><h3 id="7-3-4-Kubernetes"><a href="#7-3-4-Kubernetes" class="headerlink" title="7.3.4 Kubernetes"></a>7.3.4 Kubernetes</h3><h3 id="7-2-5-小结与反思"><a href="#7-2-5-小结与反思" class="headerlink" title="7.2.5 小结与反思"></a>7.2.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/f66iAMz">https://t.zsxq.com/f66iAMz</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><p>本章讲解了 Flink 中所有的配置文件，每个配置文件中的配置有啥作用，并且如果在不同环境下配置 JobManager 的高可用，另外还介绍了 Flink 的部署问题，因为 Flink 本身是支持在不同的环境下部署的，比如 Standalone、K8S、YARN、Mesos 等，其中在调度平台上又有 Session 模式和 Per Job 模式，每种模式都有自己的特点，所以你可能需要根据公司的情况来做一定的选型，每种的部署也可能会有点不一样，遇到问题的化还得根据特殊情况进行特殊处理，希望你可以在公司灵活的处理这种问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;7-2-Flink-作业如何在-Standalone、YARN、Mesos、K8S-上部署运行？&quot;&gt;&lt;a href=&quot;#7-2-Flink-作业如何在-Standalone、YARN、Mesos、K8S-上部署运行？&quot; class=&quot;headerlink&quot; title=&quot;7.2 Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？&quot;&gt;&lt;/a&gt;7.2 Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？&lt;/h2&gt;&lt;p&gt;前面章节已经有很多学习案列带大家使用 Flink，不仅有讲将 Flink 应用程序在 IDEA 中运行，也有讲将 Flink Job 编译打包上传到 Flink UI 上运行，在这 UI 背后可能是通过 Standalone、YARN、Mesos、Kubernetes 等运行启动的 Flink。那么这节就系统讲下如何部署和运行我们的 Flink Job，大家可以根据自己公司的场景进行选择使用哪种方式进行部署 Flink 作业！&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 配置详解及如何配置高可用？</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/03/flink-in-action-7.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/03/flink-in-action-7.1/</id>
    <published>2021-08-02T16:00:00.000Z</published>
    <updated>2022-01-16T12:14:17.703Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第七章-——-Flink-作业环境部署"><a href="#第七章-——-Flink-作业环境部署" class="headerlink" title="第七章 —— Flink 作业环境部署"></a>第七章 —— Flink 作业环境部署</h1><p>在第一章中介绍过 Flink 是可以以多种方式部署的，比如 Standalone、YARN、Mesos、K8S。本章将先对 Flink 中的所有配置文件做一个详细的讲解，接下来将讲解 JobManager 高可用部署相关的配置，最后会分别讲解如何在不同的平台上部署运行 Flink 作业。虽然在你们公司可能只会用到其中的一种，但是仍然建议你将每种方式都熟悉一下。</p><h2 id="7-1-Flink-配置详解及如何配置高可用？"><a href="#7-1-Flink-配置详解及如何配置高可用？" class="headerlink" title="7.1 Flink 配置详解及如何配置高可用？"></a>7.1 Flink 配置详解及如何配置高可用？</h2><a id="more"></a><p>在讲解如何部署 Flink 作业（在 7.2 节中会讲）之前，先来详细的看一下 Flink 中的所有配置文件以及文件中的各种配置代表的内容，这样对于后面部署和调优 Flink 作业有一定的帮助。</p><h3 id="7-1-1-Flink-配置详解"><a href="#7-1-1-Flink-配置详解" class="headerlink" title="7.1.1 Flink 配置详解"></a>7.1.1 Flink 配置详解</h3><p>先来看下 Flink 配置文件目录中最重要的配置文件 <code>flink-conf.yaml</code> 的配置。</p><h4 id="flink-conf-yaml"><a href="#flink-conf-yaml" class="headerlink" title="flink-conf.yaml"></a>flink-conf.yaml</h4><p>基础配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># jobManager 的IP地址</span><br><span class="line">jobmanager.rpc.address: localhost</span><br><span class="line"></span><br><span class="line"># JobManager 的端口号</span><br><span class="line">jobmanager.rpc.port: 6123</span><br><span class="line"></span><br><span class="line"># JobManager JVM heap 内存大小</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"></span><br><span class="line"># TaskManager JVM heap 内存大小</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line"></span><br><span class="line"># 每个 TaskManager 提供的任务 slots 数量大小</span><br><span class="line">taskmanager.numberOfTaskSlots: 1</span><br><span class="line"></span><br><span class="line"># 程序默认并行计算的个数</span><br><span class="line">parallelism.default: 1</span><br><span class="line"></span><br><span class="line"># 文件系统来源</span><br><span class="line"># fs.default-scheme</span><br></pre></td></tr></table></figure><p>高可用性相关的配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 可以选择 &apos;NONE&apos; 或者 &apos;zookeeper&apos;.</span><br><span class="line"># high-availability: zookeeper</span><br><span class="line"></span><br><span class="line"># 文件系统路径，让 Flink 在高可用性设置中持久保存元数据</span><br><span class="line"># high-availability.storageDir: hdfs:///flink/ha/</span><br><span class="line"></span><br><span class="line"># zookeeper 集群中仲裁者的机器 ip 和 port 端口号</span><br><span class="line"># high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line"></span><br><span class="line"># 默认是 open，如果 zookeeper security 启用了该值会更改成 creator</span><br><span class="line"># high-availability.zookeeper.client.acl: open</span><br></pre></td></tr></table></figure><p>容错和 Checkpoint 相关的配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 用于存储和检查点状态</span><br><span class="line"># state.backend: filesystem</span><br><span class="line"></span><br><span class="line"># 存储检查点的数据文件和元数据的默认目录</span><br><span class="line"># state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints</span><br><span class="line"></span><br><span class="line"># savepoints 的默认目标目录(可选)</span><br><span class="line"># state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints</span><br><span class="line"></span><br><span class="line"># 用于启用/禁用增量 checkpoints 的标志</span><br><span class="line"># state.backend.incremental: false</span><br></pre></td></tr></table></figure><p>Web 前端相关的配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 基于 Web 的运行时监视器侦听的地址.</span><br><span class="line">#jobmanager.web.address: 0.0.0.0</span><br><span class="line"></span><br><span class="line">#  Web 的运行时监视器端口</span><br><span class="line">rest.port: 8081</span><br><span class="line"></span><br><span class="line"># 是否从基于 Web 的 jobmanager 启用作业提交</span><br><span class="line"># jobmanager.web.submit.enable: false</span><br></pre></td></tr></table></figure><p>高级配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># io.tmp.dirs: /tmp</span><br><span class="line"></span><br><span class="line"># 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存</span><br><span class="line"># taskmanager.memory.preallocate: false</span><br><span class="line"></span><br><span class="line"># 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类</span><br><span class="line"># classloader.resolve-order: child-first</span><br><span class="line"></span><br><span class="line"># 用于网络缓冲区的 JVM 内存的分数。 这决定了 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度。 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值。 另请注意，“taskmanager.network.memory.min”和“taskmanager.network.memory.max”可能会覆盖此分数</span><br><span class="line"># taskmanager.network.memory.fraction: 0.1</span><br><span class="line"># taskmanager.network.memory.min: 67108864</span><br><span class="line"># taskmanager.network.memory.max: 1073741824</span><br></pre></td></tr></table></figure><p>Flink 集群安全配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 指示是否从 Kerberos ticket 缓存中读取</span><br><span class="line"># security.kerberos.login.use-ticket-cache: true</span><br><span class="line"></span><br><span class="line"># 包含用户凭据的 Kerberos 密钥表文件的绝对路径</span><br><span class="line"># security.kerberos.login.keytab: /path/to/kerberos/keytab</span><br><span class="line"></span><br><span class="line"># 与 keytab 关联的 Kerberos 主体名称</span><br><span class="line"># security.kerberos.login.principal: flink-user</span><br><span class="line"></span><br><span class="line"># 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）</span><br><span class="line"># security.kerberos.login.contexts: Client,KafkaClient</span><br></pre></td></tr></table></figure><p>Zookeeper 安全配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 覆盖以下配置以提供自定义 ZK 服务名称</span><br><span class="line"># zookeeper.sasl.service-name: zookeeper</span><br><span class="line"></span><br><span class="line"># 该配置必须匹配 &quot;security.kerberos.login.contexts&quot; 中的列表（含有一个）</span><br><span class="line"># zookeeper.sasl.login-context-name: Client</span><br></pre></td></tr></table></figure><p>HistoryServer 相关的配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer</span><br><span class="line"></span><br><span class="line"># 将已完成的作业上传到的目录</span><br><span class="line"># jobmanager.archive.fs.dir: hdfs:///completed-jobs/</span><br><span class="line"></span><br><span class="line"># 基于 Web 的 HistoryServer 的地址</span><br><span class="line"># historyserver.web.address: 0.0.0.0</span><br><span class="line"></span><br><span class="line"># 基于 Web 的 HistoryServer 的端口号</span><br><span class="line"># historyserver.web.port: 8082</span><br><span class="line"></span><br><span class="line"># 以逗号分隔的目录列表，用于监视已完成的作业</span><br><span class="line"># historyserver.archive.fs.dir: hdfs:///completed-jobs/</span><br><span class="line"></span><br><span class="line"># 刷新受监控目录的时间间隔（以毫秒为单位）</span><br><span class="line"># historyserver.archive.fs.refresh-interval: 10000</span><br></pre></td></tr></table></figure><h4 id="masters"><a href="#masters" class="headerlink" title="masters"></a>masters</h4><p>masters 配置文件中以 host:port 构成就行，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br></pre></td></tr></table></figure><h4 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h4><p>slaves 文件里面是每个 worker 节点的 IP/Hostname，每一个 worker 结点之后都会运行一个 TaskManager，一个一行，如下所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure><h3 id="7-1-2-Log-的配置"><a href="#7-1-2-Log-的配置" class="headerlink" title="7.1.2 Log 的配置"></a>7.1.2 Log 的配置</h3><p>在 Flink 的日志配置文件（<code>logback.xml</code> 或 <code>log4j.properties</code>）中有配置日志存储的地方，<code>logback.xml</code> 配置日志存储的路径是：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"file"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.FileAppender"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;log.file&#125;<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>false<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;60&#125; %X&#123;sourceThread&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>log4j.properties</code> 和 <code>log4j-cli.properties</code> 的配置日志存储的路径是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log4j.appender.file.file=$&#123;log.file&#125;</span><br></pre></td></tr></table></figure><p>从上面两个配置可以看到日志的路径都是由 <code>log.file</code> 变量控制的，如果系统变量没有配置的话，则会使用 <code>bin／flink</code> 脚本里配置的值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-client-$HOSTNAME.log</span><br><span class="line">log_setting=(-Dlog.file=&quot;$log&quot; -Dlog4j.configuration=file:&quot;$FLINK_CONF_DIR&quot;/log4j-cli.properties -Dlogback.configurationFile=file:&quot;$FLINK_CONF_DIR&quot;/logback.xml)</span><br></pre></td></tr></table></figure><p>从上面可以看到 log 里配置的 FLINK_LOG_DIR 变量是在 bin 目录下的 config.sh 里初始化的。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_FLINK_LOG_DIR=<span class="variable">$FLINK_HOME_DIR_MANGLED</span>/<span class="built_in">log</span></span><br><span class="line">KEY_ENV_LOG_DIR=<span class="string">"env.log.dir"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;FLINK_LOG_DIR&#125;</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    FLINK_LOG_DIR=$(readFromConfig <span class="variable">$&#123;KEY_ENV_LOG_DIR&#125;</span> <span class="string">"<span class="variable">$&#123;DEFAULT_FLINK_LOG_DIR&#125;</span>"</span> <span class="string">"<span class="variable">$&#123;YAML_CONF&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>从上面可以知道日志默认就是在 Flink 的 log 目录下，你可以通过在 <code>flink-conf.yaml</code> 配置文件中配置 <code>env.log.dir</code> 参数来更改保存日志的目录。另外通过源码可以发现，如果找不到 <code>log.file</code> 环境变量，则会去找 <code>web.log.path</code> 的配置，但是该配置在 Standalone 下是不起作用的，日志依旧是会在 <code>log</code> 目录，在 YARN 下是会起作用的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LogFileLocation <span class="title">find</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String logEnv = <span class="string">"log.file"</span>;</span><br><span class="line">    String logFilePath = System.getProperty(logEnv);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (logFilePath == <span class="keyword">null</span>) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Log file environment variable '&#123;&#125;' is not set."</span>, logEnv);</span><br><span class="line">        logFilePath = config.getString(WebOptions.LOG_PATH); <span class="comment">//该值为 web.log.path</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// not configured, cannot serve log files</span></span><br><span class="line">    <span class="keyword">if</span> (logFilePath == <span class="keyword">null</span> || logFilePath.length() &lt; <span class="number">4</span>) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"JobManager log files are unavailable in the web dashboard. "</span> +</span><br><span class="line">            <span class="string">"Log file location not found in environment variable '&#123;&#125;' or configuration key '&#123;&#125;'."</span>,</span><br><span class="line">            logEnv, WebOptions.LOG_PATH);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogFileLocation(<span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    String outFilePath = logFilePath.substring(<span class="number">0</span>, logFilePath.length() - <span class="number">3</span>).concat(<span class="string">"out"</span>);</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"Determined location of main cluster component log file: &#123;&#125;"</span>, logFilePath);</span><br><span class="line">    LOG.info(<span class="string">"Determined location of main cluster component stdout file: &#123;&#125;"</span>, outFilePath);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> LogFileLocation(resolveFileLocation(logFilePath), resolveFileLocation(outFilePath));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The log file location (may be in /log for standalone but under log directory when using YARN).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ConfigOption&lt;String&gt; LOG_PATH =</span><br><span class="line">    key(<span class="string">"web.log.path"</span>)</span><br><span class="line">        .noDefaultValue()</span><br><span class="line">        .withDeprecatedKeys(<span class="string">"jobmanager.web.log.path"</span>)</span><br><span class="line">        .withDescription(<span class="string">"Path to the log file (may be in /log for standalone but under log directory when using YARN)."</span>);</span><br></pre></td></tr></table></figure><p>另外可能会在本地 IDE 中运行作业出不来日志的情况，这时请检查是否有添加日志的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.25<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-simple<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.25<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="7-1-3-如何配置-JobManager-高可用？"><a href="#7-1-3-如何配置-JobManager-高可用？" class="headerlink" title="7.1.3 如何配置 JobManager 高可用？"></a>7.1.3 如何配置 JobManager 高可用？</h3><p>JobManager 协调每个 Flink 作业的部署，它负责调度和资源管理。默认情况下，每个 Flink 集群只有一个 JobManager 实例，这样就可能会产生单点故障，如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。如果保证 JobManager 的高可用，则可以避免这个问题。下面分别下如何搭建 Standalone 集群和 YARN 集群高可用的 JobManager。</p><h4 id="搭建-Standalone-集群高可用-JobManager"><a href="#搭建-Standalone-集群高可用-JobManager" class="headerlink" title="搭建 Standalone 集群高可用 JobManager"></a>搭建 Standalone 集群高可用 JobManager</h4><p>Standalone 集群的 JobManager 高可用性的概念是：任何时候只有一个主 JobManager 和多个备 JobManager，以便在主节点失败时有新的 JobManager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别，每个 JobManager 都可以充当主备节点。例如，请考虑以下三个 JobManager 实例的设置。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lsiuRC.jpg" alt=""></p><p><strong>如何配置</strong></p><p>要启用 JobManager 高可用性功能，首先必须在配置文件 flink-conf.yaml 中将高可用性模式设置为 ZooKeeper，配置 ZooKeeper quorum，将所有 JobManager 主机及其 Web UI 端口写入配置文件。每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 ZooKeeper。另外就是高可用存储目录，JobManager 元数据保存在 <code>high-availability.storageDir</code> 指定的文件系统中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS、S3、Ceph、NFS 等，该文件系统中保存了 JobManager 恢复状态需要的所有元数据。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181</span><br><span class="line">high-availability.storageDir: hdfs:///flink/ha/</span><br></pre></td></tr></table></figure><p>Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。<br>它在我们的 Flink 安装路径下面 /conf/zoo.cfg 。</p><h4 id="搭建-YARN-集群高可用-JobManager"><a href="#搭建-YARN-集群高可用-JobManager" class="headerlink" title="搭建 YARN 集群高可用 JobManager"></a>搭建 YARN 集群高可用 JobManager</h4><h3 id="7-1-4-小结与反思"><a href="#7-1-4-小结与反思" class="headerlink" title="7.1.4 小结与反思"></a>7.1.4 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/f66iAMz">https://t.zsxq.com/f66iAMz</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第七章-——-Flink-作业环境部署&quot;&gt;&lt;a href=&quot;#第七章-——-Flink-作业环境部署&quot; class=&quot;headerlink&quot; title=&quot;第七章 —— Flink 作业环境部署&quot;&gt;&lt;/a&gt;第七章 —— Flink 作业环境部署&lt;/h1&gt;&lt;p&gt;在第一章中介绍过 Flink 是可以以多种方式部署的，比如 Standalone、YARN、Mesos、K8S。本章将先对 Flink 中的所有配置文件做一个详细的讲解，接下来将讲解 JobManager 高可用部署相关的配置，最后会分别讲解如何在不同的平台上部署运行 Flink 作业。虽然在你们公司可能只会用到其中的一种，但是仍然建议你将每种方式都熟悉一下。&lt;/p&gt;
&lt;h2 id=&quot;7-1-Flink-配置详解及如何配置高可用？&quot;&gt;&lt;a href=&quot;#7-1-Flink-配置详解及如何配置高可用？&quot; class=&quot;headerlink&quot; title=&quot;7.1 Flink 配置详解及如何配置高可用？&quot;&gt;&lt;/a&gt;7.1 Flink 配置详解及如何配置高可用？&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 扩展库——Gelly</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/02/flink-in-action-6.5/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/02/flink-in-action-6.5/</id>
    <published>2021-08-01T16:00:00.000Z</published>
    <updated>2021-12-26T11:40:16.522Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-5-Flink-扩展库——Gelly"><a href="#6-5-Flink-扩展库——Gelly" class="headerlink" title="6.5 Flink 扩展库——Gelly"></a>6.5 Flink 扩展库——Gelly</h2><p>在 1.9 版本中还剩最后一个扩展库就是 Gelly，本节将带你了解一下 Gelly 的功能以及如何使用。</p><a id="more"></a><h3 id="6-5-1-Gelly-简介"><a href="#6-5-1-Gelly-简介" class="headerlink" title="6.5.1 Gelly 简介"></a>6.5.1 Gelly 简介</h3><p>Gelly 是 Flink 的图 API 库，它包含了一组旨在简化 Flink 中图形分析应用程序开发的方法和实用程序。在 Gelly 中，可以使用类似于批处理 API 提供的高级函数来转换和修改图。Gelly 提供了创建、转换和修改图的方法以及图算法库。</p><h3 id="6-5-2-使用-Gelly"><a href="#6-5-2-使用-Gelly" class="headerlink" title="6.5.2 使用 Gelly"></a>6.5.2 使用 Gelly</h3><p>因为 Gelly 是 Flink 项目中库的一部分，它本身不在 Flink 的二进制包中，所以运行 Gelly 项目（Java 应用程序）是需要将 <code>opt/flink-gelly_2.11-1.9.0.jar</code> 移动到 <code>lib</code> 目录中，如果是 Scala 应用程序则需要将 <code>opt/flink-gelly-scala_2.11-1.9.0.jar</code> 移动到 <code>lib</code> 中，接着运行下面的命令就可以运行一个 flink-gelly-examples 项目。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run examples/gelly/flink-gelly-examples_2.11-1.9.0.jar \</span><br><span class="line">    --algorithm GraphMetrics --order directed \</span><br><span class="line">    --input RMatGraph --type integer --scale 20 --simplify directed \</span><br><span class="line">    --output print</span><br></pre></td></tr></table></figure><p>接下来可以在 UI 上看到运行的结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-155600.png" alt=""></p><p>如果是自己创建的 Gelly Java 应用程序，则需要添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-gelly_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是 Gelly Scala 应用程序，添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-gelly-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="6-5-3-Gelly-API"><a href="#6-5-3-Gelly-API" class="headerlink" title="6.5.3 Gelly API"></a>6.5.3 Gelly API</h3><p>引入好依赖后，接着将介绍一下 Gelly 该如何使用。</p><h4 id="Graph-介绍"><a href="#Graph-介绍" class="headerlink" title="Graph 介绍"></a>Graph 介绍</h4><p>在 Gelly 中，一个图（Graph）由顶点的数据集（DataSet）和边的数据集（DataSet）组成。图中的顶点由 Vertex 类型来表示，一个 Vertex 由唯一的 ID 和一个值来表示。其中 Vertex 的 ID 必须是全局唯一的值，且实现了 Comparable 接口。如果节点不需要由任何值，则该值类型可以声明成 NullValue 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 Vertex&lt;Long，String&gt;</span></span><br><span class="line">Vertex&lt;Long, String&gt; v = <span class="keyword">new</span> Vertex&lt;Long, String&gt;(<span class="number">1L</span>, <span class="string">"foo"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个 Vertex&lt;Long，NullValue&gt;</span></span><br><span class="line">Vertex&lt;Long, NullValue&gt; v = <span class="keyword">new</span> Vertex&lt;Long, NullValue&gt;(<span class="number">1L</span>, NullValue.getInstance());</span><br></pre></td></tr></table></figure><p>Graph 中的边由 Edge 类型来表示，一个 Edge 通常由源顶点的 ID，目标顶点的 ID 以及一个可选的值来表示。其中源顶点和目标顶点的类型必须与 Vertex 的 ID 类型相同。同样的，如果边不需要由任何值，则该值类型可以声明成 NullValue 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Edge&lt;Long, Double&gt; e = <span class="keyword">new</span> Edge&lt;Long, Double&gt;(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">0.5</span>);</span><br><span class="line"><span class="comment">//反转此 edge 的源和目标</span></span><br><span class="line">Edge&lt;Long, Double&gt; reversed = e.reverse();</span><br><span class="line">Double weight = e.getValue(); <span class="comment">// weight = 0.5</span></span><br></pre></td></tr></table></figure><p>在 Gelly 中，一个 Edge 总是从源顶点指向目标顶点。如果图中每条边都能匹配一个从目标顶点到源顶点的 Edge，那么这个图可能是个无向图。同样地，无向图可以用这个方式来表示。</p><h4 id="创建-Graph"><a href="#创建-Graph" class="headerlink" title="创建 Graph"></a>创建 Graph</h4><p>可以通过以下几种方式创建一个 Graph：</p><ul><li>从一个 Edge 数据集合和一个 Vertex 数据集合中创建图。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Vertex&lt;String, Long&gt;&gt; vertices = ...</span><br><span class="line">DataSet&lt;Edge&lt;String, Double&gt;&gt; edges = ...</span><br><span class="line"></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromDataSet(vertices, edges, env);</span><br></pre></td></tr></table></figure><ul><li>从一个表示边的 Tuple2 数据集合中创建图。Gelly 会将每个 Tuple2 转换成一个 Edge，其中第一个元素表示源顶点的 ID，第二个元素表示目标顶点的 ID，图中的顶点和边的 value 值均被设置为 NullValue。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; edges = ...</span><br><span class="line"></span><br><span class="line">Graph&lt;String, NullValue, NullValue&gt; graph = Graph.fromTuple2DataSet(edges, env);</span><br></pre></td></tr></table></figure><ul><li>从一个 Tuple3 数据集和一个可选的 Tuple2 数据集中生成图。在这种情况下，Gelly 会将每个 Tuple3 转换成 Edge，其中第一个元素域是源顶点 ID，第二个域是目标顶点 ID，第三个域是边的值。同样的，每个 Tuple2 会转换成一个顶点 Vertex，其中第一个域是顶点的 ID，第二个域是顶点的 value。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Long&gt;&gt; vertexTuples = env.readCsvFile(<span class="string">"path/to/vertex/input"</span>).types(String.class, Long.class);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; edgeTuples = env.readCsvFile(<span class="string">"path/to/edge/input"</span>).types(String.class, String.class, Double.class);</span><br><span class="line"></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env);</span><br></pre></td></tr></table></figure><ul><li>从一个表示边数据的CSV文件和一个可选的表示节点的CSV文件中生成图。在这种情况下，Gelly会将表示边的CSV文件中的每一行转换成一个Edge，其中第一个域表示源顶点ID，第二个域表示目标顶点ID，第三个域表示边的值。同样的，表示节点的CSV中的每一行都被转换成一个Vertex，其中第一个域表示顶点的ID，第二个域表示顶点的值。为了通过GraphCsvReader生成图，需要指定每个域的类型，可以使用 types、edgeTypes、vertexTypes、keyType 中的方法。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个具有字符串 Vertex id、Long Vertex 和双边缘的图</span></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromCsvReader(<span class="string">"path/to/vertex/input"</span>, <span class="string">"path/to/edge/input"</span>, env)</span><br><span class="line">                    .types(String.class, Long.class, Double.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个既没有顶点值也没有边值的图</span></span><br><span class="line">Graph&lt;Long, NullValue, NullValue&gt; simpleGraph = Graph.fromCsvReader(<span class="string">"path/to/edge/input"</span>, env).keyType(Long.class);</span><br></pre></td></tr></table></figure><ul><li>从一个边的集合和一个可选的顶点的集合中生成图。如果在图创建的时候顶点的集合没有传入，Gelly 会依据数据的边数据集合自动地生成一个 Vertex 集合。这种情况下，创建的节点是没有值的。或者也可以像下面一样，在创建图的时候提供一个 MapFunction 方法来初始化节点的值。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Vertex&lt;Long, Long&gt;&gt; vertexList = <span class="keyword">new</span> ArrayList...</span><br><span class="line"></span><br><span class="line">List&lt;Edge&lt;Long, String&gt;&gt; edgeList = <span class="keyword">new</span> ArrayList...</span><br><span class="line"></span><br><span class="line">Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(vertexList, edgeList, env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将顶点值初始化为顶点ID</span></span><br><span class="line">Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(edgeList,</span><br><span class="line">                <span class="keyword">new</span> MapFunction&lt;Long, Long&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(Long value)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;, env);</span><br></pre></td></tr></table></figure><h4 id="Graph-属性"><a href="#Graph-属性" class="headerlink" title="Graph 属性"></a>Graph 属性</h4><p>Gelly 提供了下列方法来查询图的属性和指标：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Vertex&lt;K, VV&gt;&gt; getVertices()</span><br><span class="line"><span class="comment">//获取边缘数据集</span></span><br><span class="line">DataSet&lt;Edge&lt;K, EV&gt;&gt; getEdges()</span><br><span class="line"><span class="comment">//获取顶点的 id 数据集</span></span><br><span class="line"><span class="function">DataSet&lt;K&gt; <span class="title">getVertexIds</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, K&gt;&gt; <span class="title">getEdgeIds</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">inDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">outDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">getDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">numberOfVertices</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">numberOfEdges</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Triplet&lt;K, VV, EV&gt;&gt; <span class="title">getTriplets</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><h4 id="Graph-转换"><a href="#Graph-转换" class="headerlink" title="Graph 转换"></a>Graph 转换</h4><p>Graph 转换方式有下面几种方式：</p><ul><li>Map：Gelly 提供了专门的用于转换顶点值和边值的方法。mapVertices 和 mapEdges 会返回一个新图，图中的每个顶点和边的 ID 不会改变，但是顶点和边的值会根据用户自定义的映射方法进行修改。这些映射方法同时也可以修改顶点和边的值的类型。</li><li>Translate：Gelly 还提供了专门用于根据用户定义的函数转换顶点和边的 ID 和值的值及类型的方法（translateGraphIDs/translateVertexValues/translateEdgeValues），是Map 功能的升级版，因为 Map 操作不支持修订顶点和边的 ID。</li><li>Filter：Gelly 支持在图中的顶点上或边上执行一个用户指定的 filter 转换。filterOnEdges 会根据提供的在边上的断言在原图的基础上生成一个新的子图，注意，顶点的数据不会被修改。同样的 filterOnVertices 在原图的顶点上进行 filter 转换，不满足断言条件的源节点或目标节点会在新的子图中移除。该子图方法支持同时对顶点和边应用 filter 函数，如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-165050.jpg" alt=""></p><ul><li>Reverse：Gelly中得reverse()方法用于在原图的基础上，生成一个所有边方向与原图相反的新图。</li><li>Undirected：在前面的内容中，我们提到过，Gelly中的图通常都是有向的，而无向图可以通过对所有边添加反向的边来实现，出于这个目的，Gelly提供了getUndirected()方法，用于获取原图的无向图。</li><li>Union：Gelly的union()操作用于联合当前图和指定的输入图，并生成一个新图，在输出的新图中，相同的节点只保留一份，但是重复的边会保留。如下图所示：</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-165224.jpg" alt=""></p><ul><li>Difference：Gelly提供了difference()方法用于发现当前图与指定的输入图之间的差异。</li><li>Intersect：Gelly提供了intersect()方法用于发现两个图中共同存在的边，并将相同的边以新图的方式返回。相同的边指的是具有相同的源顶点，相同的目标顶点和相同的边值。返回的新图中，所有的节点没有任何值，如果需要节点值，可以使用joinWithVertices()方法去任何一个输入图中检索。</li></ul><h4 id="Graph-变化"><a href="#Graph-变化" class="headerlink" title="Graph 变化"></a>Graph 变化</h4><p>Gelly 内置下列方法以支持对一个图进行节点和边的增加/移除操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addVertex</span><span class="params">(<span class="keyword">final</span> Vertex&lt;K, VV&gt; vertex)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addVertices</span><span class="params">(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToAdd)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addEdge</span><span class="params">(Vertex&lt;K, VV&gt; source, Vertex&lt;K, VV&gt; target, EV edgeValue)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addEdges</span><span class="params">(List&lt;Edge&lt;K, EV&gt;&gt; newEdges)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeVertex</span><span class="params">(Vertex&lt;K, VV&gt; vertex)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeVertices</span><span class="params">(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToBeRemoved)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeEdge</span><span class="params">(Edge&lt;K, EV&gt; edge)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeEdges</span><span class="params">(List&lt;Edge&lt;K, EV&gt;&gt; edgesToBeRemoved)</span></span></span><br></pre></td></tr></table></figure><h4 id="Neighborhood-Methods"><a href="#Neighborhood-Methods" class="headerlink" title="Neighborhood Methods"></a>Neighborhood Methods</h4><h4 id="Graph-验证"><a href="#Graph-验证" class="headerlink" title="Graph 验证"></a>Graph 验证</h4><h3 id="6-5-4-小结与反思"><a href="#6-5-4-小结与反思" class="headerlink" title="6.5.4 小结与反思"></a>6.5.4 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><p>本章所讲的内容属于 Flink 的扩展库，包含了 CEP 复杂事件处理、State Processor API、Machine Learning 和 Gelly，各种都有讲解一些样例，但是没有过多深入的讲，但还是希望你可以在书本外自己去扩充这些内容的知识点。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-5-Flink-扩展库——Gelly&quot;&gt;&lt;a href=&quot;#6-5-Flink-扩展库——Gelly&quot; class=&quot;headerlink&quot; title=&quot;6.5 Flink 扩展库——Gelly&quot;&gt;&lt;/a&gt;6.5 Flink 扩展库——Gelly&lt;/h2&gt;&lt;p&gt;在 1.9 版本中还剩最后一个扩展库就是 Gelly，本节将带你了解一下 Gelly 的功能以及如何使用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 扩展库——Machine Learning</title>
    <link href="http://www.54tianzhisheng.cn/2021/08/01/flink-in-action-6.4/"/>
    <id>http://www.54tianzhisheng.cn/2021/08/01/flink-in-action-6.4/</id>
    <published>2021-07-31T16:00:00.000Z</published>
    <updated>2021-12-26T11:38:23.421Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-4-Flink-扩展库——Machine-Learning"><a href="#6-4-Flink-扩展库——Machine-Learning" class="headerlink" title="6.4 Flink 扩展库——Machine Learning"></a>6.4 Flink 扩展库——Machine Learning</h2><p>随着人工智能的火热，机器学习这门技术也变得异常重要，Flink 作为一个数据处理的引擎，虽然目前在该方面还较弱，但是在 Flink Forward Asia 2019 北京站后，阿里开源了 <a href="https://github.com/alibaba/Alink">Alink</a> 平台的核心代码，并上传了一系列的算法库，该项目是基于 Flink 的通用算法平台，开发者和数据分析师可以利用 Alink 提供的一系列算法来构建软件功能，例如统计分析、机器学习、实时预测、个性化推荐和异常检测。相信未来 Flink 的机器学习库将会应用到更多的场景去，本节将带你了解一下 Flink 中的机器学习库。</p><a id="more"></a><h3 id="6-4-1-Flink-ML-简介"><a href="#6-4-1-Flink-ML-简介" class="headerlink" title="6.4.1 Flink-ML 简介"></a>6.4.1 Flink-ML 简介</h3><p>ML 是 Machine Learning 的简称，Flink-ML 是 Flink 的机器学习类库。在 Flink 1.9 之前该类库是存在 <code>flink-libraries</code> 模块下的，但是在 Flink 1.9 版本中，为了支持 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-39+Flink+ML+pipeline+and+ML+libs">FLIP-39</a> ，所以该类库被移除了。</p><p>建立 FLIP-39 的目的主要是增强 Flink-ML 的可伸缩性和易用性。通常使用机器学习的有两类人，一类是机器学习算法库的开发者，他们需要一套标准的 API 来实现算法，每个机器学习算法会在这些 API 的基础上实现；另一类用户是直接利用这些现有的机器学习算法库去训练数据模型，整个训练是要通过很多转换或者算法才能完成的，所以如果能够提供 ML Pipeline，那么对于后一类用户来说绝对是一种福音。虽然在 1.9 中移除了之前的 Flink-ML 模块，但是在 Flink 项目下出现了一个 <code>flink-ml-parent</code> 的模块，该模块有两个子模块 <code>flink-ml-api</code> 和 <code>flink-ml-lib</code>。</p><p><code>flink-ml-api</code> 模块增加了 ML Pipeline 和 MLLib 的接口，它的类结构图如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-22-124512.png" alt=""></p><p>主要接口如下所示：</p><ul><li>Transformer: Transformer 是一种可以将一个表转换成另一个表的算法</li><li>Model: Model 是一种特别的 Transformer，它继承自 Transformer。它通常是由 Estimator 生成，Model 用于推断，输入一个数据表会生成结果表。</li><li>Estimator: Estimator 是一个可以根据一个数据表生成一个模型的算法。</li><li>Pipeline: Pipeline 描述的是机器学习的工作流，它将很多 Transformer 和 Estimator 连接在一起成一个工作流。</li><li>PipelineStage: PipelineStage 是 Pipeline 的基础节点，Transformer 和 Estimator 两个都继承自 PipelineStage 接口。</li><li>Params: Params 是一个参数容器。</li><li>WithParams: WithParams 有一个保存参数的 Params 容器。通常会使用在 PipelineStage 里面，因为几乎所有的算法都需要参数。</li></ul><p>Flink-ML 的 pipeline 流程如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-22-135555.png" alt=""></p><p><code>flink-ml-lib</code> 模块包括了 DenseMatrix、DenseVector、SparseVector 等类的基本操作。这两个模块是 Flink-ML 的基础模块，相信社区在后面的稳定版本一定会带来更加完善的 Flink-ML 库。</p><h3 id="6-4-2-使用-Flink-ML"><a href="#6-4-2-使用-Flink-ML" class="headerlink" title="6.4.2 使用 Flink-ML"></a>6.4.2 使用 Flink-ML</h3><p>虽然在 Flink 1.9 中已经移除了 Flink-ML 模块，但是在之前的版本还是支持的，如果你们公司使用的是低于 1.9 的版本，那么还是可以使用的，在使用之前引入依赖（假设使用的是 Flink 1.8 版本）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-ml_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>另外如果是要运行的话还是要将 opt 目录下的 flink-ml_2.11-1.8.0.jar 移到 lib 目录下。下面演示下如何训练多元线性回归模型：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//带标签的特征向量</span></span><br><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataSet: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="comment">//使用 Splitter 将数据集拆分成训练数据和测试数据</span></span><br><span class="line"><span class="keyword">val</span> trainTestData: <span class="type">DataSet</span>[<span class="type">TrainTestDataSet</span>] = <span class="type">Splitter</span>.trainTestSplit(dataSet)</span><br><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = trainTestData.training</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = trainTestData.testing.map(lv =&gt; lv.vector)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mlr = <span class="type">MultipleLinearRegression</span>()</span><br><span class="line">  .setStepsize(<span class="number">1.0</span>)</span><br><span class="line">  .setIterations(<span class="number">100</span>)</span><br><span class="line">  .setConvergenceThreshold(<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">mlr.fit(trainingData)</span><br><span class="line"></span><br><span class="line"><span class="comment">//已经形成的模型可以用来预测数据了</span></span><br><span class="line"><span class="keyword">val</span> predictions: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = mlr.predict(testingData)</span><br></pre></td></tr></table></figure><h3 id="6-4-3-使用-Flink-ML-Pipeline"><a href="#6-4-3-使用-Flink-ML-Pipeline" class="headerlink" title="6.4.3 使用 Flink-ML Pipeline"></a>6.4.3 使用 Flink-ML Pipeline</h3><h3 id="6-4-4-Alink-介绍"><a href="#6-4-4-Alink-介绍" class="headerlink" title="6.4.4 Alink 介绍"></a>6.4.4 Alink 介绍</h3><h3 id="6-4-5-小结与反思"><a href="#6-4-5-小结与反思" class="headerlink" title="6.4.5 小结与反思"></a>6.4.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-4-Flink-扩展库——Machine-Learning&quot;&gt;&lt;a href=&quot;#6-4-Flink-扩展库——Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;6.4 Flink 扩展库——Machine Learning&quot;&gt;&lt;/a&gt;6.4 Flink 扩展库——Machine Learning&lt;/h2&gt;&lt;p&gt;随着人工智能的火热，机器学习这门技术也变得异常重要，Flink 作为一个数据处理的引擎，虽然目前在该方面还较弱，但是在 Flink Forward Asia 2019 北京站后，阿里开源了 &lt;a href=&quot;https://github.com/alibaba/Alink&quot;&gt;Alink&lt;/a&gt; 平台的核心代码，并上传了一系列的算法库，该项目是基于 Flink 的通用算法平台，开发者和数据分析师可以利用 Alink 提供的一系列算法来构建软件功能，例如统计分析、机器学习、实时预测、个性化推荐和异常检测。相信未来 Flink 的机器学习库将会应用到更多的场景去，本节将带你了解一下 Flink 中的机器学习库。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 扩展库——State Processor API</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/30/flink-in-action-6.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/30/flink-in-action-6.3/</id>
    <published>2021-07-29T16:00:00.000Z</published>
    <updated>2021-12-26T11:37:33.546Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-3-Flink-扩展库——State-Processor-API"><a href="#6-3-Flink-扩展库——State-Processor-API" class="headerlink" title="6.3 Flink 扩展库——State Processor API"></a>6.3 Flink 扩展库——State Processor API</h2><p>State Processor API 功能是在 1.9 版本中新增加的一个功能，本节将带你了解一下其功能和如何使用？</p><a id="more"></a><h3 id="6-3-1-State-Processor-API-简介"><a href="#6-3-1-State-Processor-API-简介" class="headerlink" title="6.3.1 State Processor API 简介"></a>6.3.1 State Processor API 简介</h3><p>能够从外部访问 Flink 作业的状态一直用户迫切需要的功能之一，在 Apache Flink 1.9.0 中新引入了 State Processor API，该 API 让用户可以通过  Flink DataSet 作业来灵活读取、写入和修改 Flink 的 Savepoint 和 Checkpoint。</p><h3 id="6-3-2-在-Flink-1-9-之前是如何处理状态的？"><a href="#6-3-2-在-Flink-1-9-之前是如何处理状态的？" class="headerlink" title="6.3.2 在 Flink 1.9 之前是如何处理状态的？"></a>6.3.2 在 Flink 1.9 之前是如何处理状态的？</h3><p>一般来说，大多数的 Flink 作业都是有状态的，并且随着作业运行的时间越来越久，就会累积越多越多的状态，如果因为故障导致作业崩溃可能会导致作业的状态都丢失，那么对于比较重要的状态来说，损失就会很大。为了保证作业状态的一致性和持久性，Flink 从一开始使用的就是 Checkpoint 和 Savepoint 来保存状态，并且可以从 Savepoint 中恢复状态。在 Flink 的每个新 Release 版本中，Flink 社区添加了越来越多与状态相关的功能以提高 Checkpoint 的速度和恢复速度。</p><p>有的时候，用户可能会有这些需求场景，比如从第三方外部系统访问作业的状态、将作业的状态信息迁移到另一个应用程序等，目前现有支持查询作业状态的功能 Queryable State，但是在 Flink 中目前该功能只支持根据 Key 查找，并且不能保证返回值的一致性。另外该功能不支持添加和修改作业的状态，所以适用的场景还是比较有限。</p><h3 id="6-3-3-使用-State-Processor-API-读写作业状态"><a href="#6-3-3-使用-State-Processor-API-读写作业状态" class="headerlink" title="6.3.3 使用 State Processor API 读写作业状态"></a>6.3.3 使用 State Processor API 读写作业状态</h3><p>在 1.9 版本中的 State Processor API，它完全和之前不一致，该功能使用 InputFormat 和 OutputFormat 扩展了 DataSet API 以读取和写入 Checkpoint 和 Savepoint 数据。由于 DataSet 和 Table API 的互通性，所以也可以使用 Table API 或者 SQL 查询和分析状态的数据。例如，再获取到正在运行的流作业状态的 Checkpoint 后，可以使用 DataSet 批处理程序对其进行分析，以验证该流作业的运行是否正确。另外 State Processor API 还可以修复不一致的状态信息，它提供了很多方法来开发有状态的应用程序，这些方法在以前的版本中因为设计的问题导致作业在启动后不能再修改，否则状态可能会丢失。现在，你可以任意修改状态的数据类型、调整算子的最大并行度、拆分或合并算子的状态、重新分配算子的 uid 等。</p><p>如果要使用 State Processor API 去读写作业的状态，你需要添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-state-processor-api_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="6-3-4-使用-DataSet-读取作业状态"><a href="#6-3-4-使用-DataSet-读取作业状态" class="headerlink" title="6.3.4 使用 DataSet 读取作业状态"></a>6.3.4 使用 DataSet 读取作业状态</h3><p>State Processor API 将作业的状态映射到一个或多个可以单独处理的数据集，为了能够使用该 API，需要先了解这个映射的工作方式，首先来看下有状态的 Flink 作业是什么样子的。Flink 作业是由很多算子组成，通常是一个或多个数据源（Source）、一些实际处理数据的算子（比如 Map／Filter／FlatMap 等）和一个或者多个 Sink。每个算子会在一个或者多个任务中并行运行（取决于并行度），并且可以使用不同类型的状态，算子可能会有零个、一个或多个 Operator State，这些状态会组成一个以算子任务为范围的列表。如果是算子应用在 KeyedStream，它还有零个、一个或者多个 Keyed State，它们的作用域范围是从每个已处理数据中提取 Key，可以将 Keyed State 看作是一个分布式的 Map。</p><p>State Processor API 现在提供了读取、新增和修改 Savepoint 数据的方法，比如从已加载的 Savepoint 中读取数据集，然后将数据集转换为状态并将其保存到 Savepoint。下面分别讲解下这三种方法该如何使用。</p><h4 id="读取现有的-Savepoint"><a href="#读取现有的-Savepoint" class="headerlink" title="读取现有的 Savepoint"></a>读取现有的 Savepoint</h4><p>读取状态首先需要指定一个 Savepoint（或者 Checkpoint） 的路径和状态后端存储的类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment bEnv   = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">ExistingSavepoint savepoint = Savepoint.load(bEnv, <span class="string">"hdfs://path/"</span>, <span class="keyword">new</span> RocksDBStateBackend());</span><br></pre></td></tr></table></figure><p>读取 Operator State 时，只需指定算子的 uid、状态名称和类型信息。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; listState  = savepoint.readListState(<span class="string">"zhisheng-uid"</span>, <span class="string">"list-state"</span>, Types.INT);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Integer&gt; unionState = savepoint.readUnionState(<span class="string">"zhisheng-uid"</span>, <span class="string">"union-state"</span>, Types.INT);</span><br><span class="line"> </span><br><span class="line">DataSet&lt;Tuple2&lt;Integer, Integer&gt;&gt; broadcastState = savepoint.readBroadcastState(<span class="string">"zhisheng-uid"</span>, <span class="string">"broadcast-state"</span>, Types.INT, Types.INT);</span><br></pre></td></tr></table></figure><p>如果在状态描述符（StateDescriptor）中使用了自定义类型序列化器 TypeSerializer，也可以指定它：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; listState = savepoint.readListState(</span><br><span class="line">    <span class="string">"zhisheng-uid"</span>, <span class="string">"list-state"</span>, </span><br><span class="line">    Types.INT, <span class="keyword">new</span> MyCustomIntSerializer());</span><br></pre></td></tr></table></figure><h4 id="写入新的-Savepoint"><a href="#写入新的-Savepoint" class="headerlink" title="写入新的 Savepoint"></a>写入新的 Savepoint</h4><h4 id="修改现有的-Savepoint"><a href="#修改现有的-Savepoint" class="headerlink" title="修改现有的 Savepoint"></a>修改现有的 Savepoint</h4><h3 id="6-3-5-为什么要使用-DataSet-API？"><a href="#6-3-5-为什么要使用-DataSet-API？" class="headerlink" title="6.3.5 为什么要使用 DataSet API？"></a>6.3.5 为什么要使用 DataSet API？</h3><h3 id="6-3-6-小结与反思"><a href="#6-3-6-小结与反思" class="headerlink" title="6.3.6 小结与反思"></a>6.3.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-3-Flink-扩展库——State-Processor-API&quot;&gt;&lt;a href=&quot;#6-3-Flink-扩展库——State-Processor-API&quot; class=&quot;headerlink&quot; title=&quot;6.3 Flink 扩展库——State Processor API&quot;&gt;&lt;/a&gt;6.3 Flink 扩展库——State Processor API&lt;/h2&gt;&lt;p&gt;State Processor API 功能是在 1.9 版本中新增加的一个功能，本节将带你了解一下其功能和如何使用？&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 使用 Flink CEP 处理复杂事件</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/29/flink-in-action-6.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/29/flink-in-action-6.2/</id>
    <published>2021-07-28T16:00:00.000Z</published>
    <updated>2021-12-26T11:32:24.812Z</updated>
    
    <content type="html"><![CDATA[<h2 id="6-2-使用-Flink-CEP-处理复杂事件"><a href="#6-2-使用-Flink-CEP-处理复杂事件" class="headerlink" title="6.2 使用 Flink CEP 处理复杂事件"></a>6.2 使用 Flink CEP 处理复杂事件</h2><p>6.1 节中介绍 Flink CEP 和其使用场景，本节将详细介绍 Flink CEP 的 API，教会大家如何去使用 Flink CEP。</p><a id="more"></a><h3 id="6-2-1-准备依赖"><a href="#6-2-1-准备依赖" class="headerlink" title="6.2.1 准备依赖"></a>6.2.1 准备依赖</h3><p>要开发 Flink CEP 应用程序，首先你得在项目的 <code>pom.xml</code> 中添加依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个依赖有两种，一个是 Java 版本的，一个是 Scala 版本，你可以根据项目的开发语言自行选择。</p><h3 id="6-2-2-Flink-CEP-入门应用程序"><a href="#6-2-2-Flink-CEP-入门应用程序" class="headerlink" title="6.2.2 Flink CEP 入门应用程序"></a>6.2.2 Flink CEP 入门应用程序</h3><p>准备好依赖后，我们开始第一个 Flink CEP 应用程序，这里我们只做一个简单的数据流匹配，当匹配成功后将匹配的两条数据打印出来。首先定义实体类 Event 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后构造读取 Socket 数据流将数据进行转换成 Event，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventDataStream = env.socketTextStream(<span class="string">"127.0.0.1"</span>, <span class="number">9200</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Event&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtil.isNotEmpty(s)) &#123;</span><br><span class="line">                String[] split = s.split(<span class="string">","</span>);</span><br><span class="line">                <span class="keyword">if</span> (split.length == <span class="number">2</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Event(Integer.valueOf(split[<span class="number">0</span>]), split[<span class="number">1</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>接着就是定义 CEP 中的匹配规则了，下面的规则表示第一个事件的 id 为 42，紧接着的第二个事件 id 要大于 10，满足这样的连续两个事件才会将这两条数据进行打印出来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; pattern = Pattern.&lt;Event&gt;begin(<span class="string">"start"</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">"start &#123;&#125;"</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() == <span class="number">42</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">).next(<span class="string">"middle"</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">"middle &#123;&#125;"</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() &gt;= <span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        log.info(<span class="string">"p = &#123;&#125;"</span>, p);</span><br><span class="line">        builder.append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getName()).append(<span class="string">"\n"</span>)</span><br><span class="line">                .append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();<span class="comment">//打印结果</span></span><br></pre></td></tr></table></figure><p>然后笔者在终端开启 Socket，输入的两条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">42,zhisheng</span><br><span class="line">20,zhisheng</span><br></pre></td></tr></table></figure><p>作业打印出来的日志如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072247.png" alt=""></p><p>整个作业 print 出来的结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072320.png" alt=""></p><p>好了，一个完整的 Flink CEP 应用程序如上，相信你也能大概理解上面的代码，接着来详细的讲解一下 Flink CEP 中的 Pattern API。</p><h3 id="6-2-3-Pattern-API"><a href="#6-2-3-Pattern-API" class="headerlink" title="6.2.3 Pattern API"></a>6.2.3 Pattern API</h3><p>你可以通过 Pattern API 去定义从流数据中匹配事件的 Pattern，每个复杂 Pattern 都是由多个简单的 Pattern 组成的，拿前面入门的应用来讲，它就是由 <code>start</code> 和 <code>middle</code> 两个简单的 Pattern 组成的，在其每个 Pattern 中都只是简单的处理了流数据。在处理的过程中需要标示该 Pattern 的名称，以便后续可以使用该名称来获取匹配到的数据，如 <code>p.get(&quot;start&quot;).get(0)</code> 它就可以获取到 Pattern 中匹配的第一个事件。接下来我们先来看下简单的 Pattern 。</p><h4 id="单个-Pattern"><a href="#单个-Pattern" class="headerlink" title="单个 Pattern"></a>单个 Pattern</h4><p><strong>数量</strong></p><p><strong>条件</strong></p><h4 id="组合-Pattern"><a href="#组合-Pattern" class="headerlink" title="组合 Pattern"></a>组合 Pattern</h4><h4 id="Group-Pattern"><a href="#Group-Pattern" class="headerlink" title="Group Pattern"></a>Group Pattern</h4><h4 id="事件匹配跳过策略"><a href="#事件匹配跳过策略" class="headerlink" title="事件匹配跳过策略"></a>事件匹配跳过策略</h4><h3 id="6-2-4-检测-Pattern"><a href="#6-2-4-检测-Pattern" class="headerlink" title="6.2.4 检测 Pattern"></a>6.2.4 检测 Pattern</h3><h4 id="选择-Pattern"><a href="#选择-Pattern" class="headerlink" title="选择 Pattern"></a>选择 Pattern</h4><h3 id="6-2-5-CEP-时间属性"><a href="#6-2-5-CEP-时间属性" class="headerlink" title="6.2.5 CEP 时间属性"></a>6.2.5 CEP 时间属性</h3><h4 id="根据事件时间处理延迟数据"><a href="#根据事件时间处理延迟数据" class="headerlink" title="根据事件时间处理延迟数据"></a>根据事件时间处理延迟数据</h4><h4 id="时间上下文"><a href="#时间上下文" class="headerlink" title="时间上下文"></a>时间上下文</h4><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="6-2-6-小结与反思"><a href="#6-2-6-小结与反思" class="headerlink" title="6.2.6 小结与反思"></a>6.2.6 小结与反思</h3><p>本节开始通过一个 Flink CEP 案例教大家上手，后面通过讲解 Flink CEP 的 Pattern API，更多详细的还是得去看官网文档，其实也建议大家好好的跟着官网的文档过一遍所有的 API，并跟着敲一些样例来实现，这样在开发需求的时候才能够及时的想到什么场景下该使用哪种 API，接着教了大家如何将 Pattern 与数据流结合起来匹配并获取匹配的数据，最后讲了下 CEP 中的时间概念。</p><p>你公司有使用 Flink CEP 吗？通常使用哪些 API 居多？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;6-2-使用-Flink-CEP-处理复杂事件&quot;&gt;&lt;a href=&quot;#6-2-使用-Flink-CEP-处理复杂事件&quot; class=&quot;headerlink&quot; title=&quot;6.2 使用 Flink CEP 处理复杂事件&quot;&gt;&lt;/a&gt;6.2 使用 Flink CEP 处理复杂事件&lt;/h2&gt;&lt;p&gt;6.1 节中介绍 Flink CEP 和其使用场景，本节将详细介绍 Flink CEP 的 API，教会大家如何去使用 Flink CEP。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink CEP 简介及其使用场景</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/28/flink-in-action-6.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/28/flink-in-action-6.1/</id>
    <published>2021-07-27T16:00:00.000Z</published>
    <updated>2021-12-26T11:28:30.835Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第六章-——-扩展库"><a href="#第六章-——-扩展库" class="headerlink" title="第六章 —— 扩展库"></a>第六章 —— 扩展库</h1><p>Flink 源码中有单独的 <code>flink-libraries</code> 模块用来存放一些扩展库，比如 CEP、Gelly、Machine Learning、State Processor API。本章将分别介绍这几种扩展库以及如何应用在我们的项目中。</p><h2 id="6-1-Flink-CEP-简介及其使用场景"><a href="#6-1-Flink-CEP-简介及其使用场景" class="headerlink" title="6.1 Flink CEP 简介及其使用场景"></a>6.1 Flink CEP 简介及其使用场景</h2><a id="more"></a><p>Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案，它可以在实时数据中匹配复杂事件，用处很广，本节将带大家了解其功能和应用场景。</p><h3 id="6-1-1-CEP-简介？"><a href="#6-1-1-CEP-简介？" class="headerlink" title="6.1.1 CEP 简介？"></a>6.1.1 CEP 简介？</h3><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。它可以用于处理实时数据并在事件流到达时从事件流中提取信息，并根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</p><p>因为这种事件匹配通常是根据提前制定好的规则去匹配的，而这些规则一般来说不仅多，而且复杂，所以就会引入一些规则引擎来处理这种复杂事件匹配。市面上常用的规则引擎有如下这些。</p><h3 id="6-1-2-规则引擎对比"><a href="#6-1-2-规则引擎对比" class="headerlink" title="6.1.2 规则引擎对比"></a>6.1.2 规则引擎对比</h3><p>目前开源的规则引擎有很多种，接下来将对比一下 Drools、Aviator、EasyRules、Esper、Flink CEP 之间的优势和劣势。</p><h4 id="Drools"><a href="#Drools" class="headerlink" title="Drools"></a>Drools</h4><p>Drools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。</p><p>优势:</p><ul><li>功能较为完善，具有如系统监控、操作平台等功能。</li><li>规则支持动态更新。</li></ul><p>劣势:</p><ul><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Aviator"><a href="#Aviator" class="headerlink" title="Aviator"></a>Aviator</h4><p>Aviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。</p><p>优势：</p><ul><li>支持大部分运算操作符。</li><li>支持函数调用和自定义函数。</li><li>支持正则表达式匹配。</li><li>支持传入变量并且性能优秀。</li></ul><p>劣势：</p><ul><li>没有 if else、do while 等语句，没有赋值语句，没有位运算符。</li></ul><h4 id="EasyRules"><a href="#EasyRules" class="headerlink" title="EasyRules"></a>EasyRules</h4><p>EasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。</p><p>优势：</p><ul><li>轻量级框架，学习成本低。</li><li>基于 POJO。</li><li>为定义业务引擎提供有用的抽象和简便的应用</li><li>支持从简单的规则组建成复杂规则</li></ul><h4 id="Esper"><a href="#Esper" class="headerlink" title="Esper"></a>Esper</h4><p>Esper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。</p><p>优势:</p><ul><li>轻量级可嵌入开发，常用的 CEP 功能简单好用。</li><li>EPL 语法与 SQL 类似，学习成本较低。</li></ul><p>劣势:</p><ul><li>单机全内存方案，需要整合其他分布式和存储。</li><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h4><p>Flink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。</p><p>优势:</p><ul><li>继承了 Flink 高吞吐的特点</li><li>事件支持存储到外部，可以支持较长跨度的时间窗。</li><li>可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现）</li></ul><p>劣势:</p><ul><li>无法动态更新规则（痛点）</li></ul><h3 id="6-1-3-Flink-CEP-简介"><a href="#6-1-3-Flink-CEP-简介" class="headerlink" title="6.1.3 Flink CEP 简介"></a>6.1.3 Flink CEP 简介</h3><h3 id="6-1-4-Flink-CEP-动态更新规则"><a href="#6-1-4-Flink-CEP-动态更新规则" class="headerlink" title="6.1.4 Flink CEP 动态更新规则"></a>6.1.4 Flink CEP 动态更新规则</h3><h3 id="6-1-5-Flink-CEP-使用场景分析"><a href="#6-1-5-Flink-CEP-使用场景分析" class="headerlink" title="6.1.5 Flink CEP 使用场景分析"></a>6.1.5 Flink CEP 使用场景分析</h3><h4 id="实时反作弊和风控"><a href="#实时反作弊和风控" class="headerlink" title="实时反作弊和风控"></a>实时反作弊和风控</h4><h4 id="实时营销"><a href="#实时营销" class="headerlink" title="实时营销"></a>实时营销</h4><h4 id="实时网络攻击检测"><a href="#实时网络攻击检测" class="headerlink" title="实时网络攻击检测"></a>实时网络攻击检测</h4><h3 id="6-1-6-小结与反思"><a href="#6-1-6-小结与反思" class="headerlink" title="6.1.6 小结与反思"></a>6.1.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/nMR7ufq">https://t.zsxq.com/nMR7ufq</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第六章-——-扩展库&quot;&gt;&lt;a href=&quot;#第六章-——-扩展库&quot; class=&quot;headerlink&quot; title=&quot;第六章 —— 扩展库&quot;&gt;&lt;/a&gt;第六章 —— 扩展库&lt;/h1&gt;&lt;p&gt;Flink 源码中有单独的 &lt;code&gt;flink-libraries&lt;/code&gt; 模块用来存放一些扩展库，比如 CEP、Gelly、Machine Learning、State Processor API。本章将分别介绍这几种扩展库以及如何应用在我们的项目中。&lt;/p&gt;
&lt;h2 id=&quot;6-1-Flink-CEP-简介及其使用场景&quot;&gt;&lt;a href=&quot;#6-1-Flink-CEP-简介及其使用场景&quot; class=&quot;headerlink&quot; title=&quot;6.1 Flink CEP 简介及其使用场景&quot;&gt;&lt;/a&gt;6.1 Flink CEP 简介及其使用场景&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Table &amp; SQL 概念与通用 API</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.1/</id>
    <published>2021-07-26T16:00:00.000Z</published>
    <updated>2021-12-26T11:23:16.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第五章-——-Table-API-amp-SQL"><a href="#第五章-——-Table-API-amp-SQL" class="headerlink" title="第五章 —— Table API &amp; SQL"></a>第五章 —— Table API &amp; SQL</h1><p>Flink 中除了 DataStream 和 DataSet API，还有比较高级的 Table API &amp; SQL，它可以帮助我们简化开发的过程，能够快读的运用 Flink 去完成一些需求，本章将对 Flink Table API &amp; SQL 进行讲解，并将与其他的 API 结合对比分析。</p><h2 id="5-1-Flink-Table-amp-SQL-概念与通用-API"><a href="#5-1-Flink-Table-amp-SQL-概念与通用-API" class="headerlink" title="5.1 Flink Table &amp; SQL 概念与通用 API"></a>5.1 Flink Table &amp; SQL 概念与通用 API</h2><a id="more"></a><p>前面的内容都是讲解 DataStream 和 DataSet API 相关的，在 1.2.5 节中讲解 Flink API 时提及到 Flink 的高级 API —— Table API &amp; SQL，本节将开始 Table &amp; SQL 之旅。</p><h3 id="5-1-1-新增-Blink-SQL-查询处理器"><a href="#5-1-1-新增-Blink-SQL-查询处理器" class="headerlink" title="5.1.1 新增 Blink SQL 查询处理器"></a>5.1.1 新增 Blink SQL 查询处理器</h3><p>在 Flink 1.9 版本中，合进了阿里巴巴开源的 Blink 版本中的大量代码，其中最重要的贡献就是 Blink SQL 了。在 Blink 捐献给 Apache Flink 之后，社区就致力于为 Table API &amp; SQL 集成 Blink 的查询优化器和 runtime。先来看下 1.8 版本的 Flink Table 项目结构如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-30-130607.png" alt=""></p><p>1.9 版本的 Flink Table 项目结构图如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-30-130751.png" alt=""></p><p>可以发现新增了 <code>flink-sql-parser</code>、<code>flink-table-planner-blink</code>、<code>flink-table-runtime-blink</code>、<code>flink-table-uber-blink</code> 模块，对 Flink Table 模块的重构详细内容可以参考 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions">FLIP-32</a>。这样对于 Java 和 Scala API 模块、优化器以及 runtime 模块来说，分层更清楚，接口更明确。</p><p>另外 <code>flink-table-planner-blink</code> 模块中实现了新的优化器接口，所以现在有两个插件化的查询处理器来执行 Table API &amp; SQL：1.9 以前的 Flink 处理器和新的基于 Blink 的处理器。基于 Blink 的查询处理器提供了更好的 SQL 覆盖率、支持更广泛的查询优化、改进了代码生成机制、通过调优算子的实现来提升批处理查询的性能。除此之外，基于 Blink 的查询处理器还提供了更强大的流处理能力，包括了社区一些非常期待的新功能（如维表 Join、TopN、去重）和聚合场景缓解数据倾斜的优化，以及内置更多常用的函数，具体可以查看 <code>flink-table-runtime-blink</code> 代码。目前整个模块的结构如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-30-124512.png" alt=""></p><p>注意：两个查询处理器之间的语义和功能大部分是一致的，但未完全对齐，因为基于 Blink 的查询处理器还在优化中，所以在 1.9 版本中默认查询处理器还是 1.9 之前的版本。如果你想使用 Blink 处理器的话，可以在创建 TableEnvironment 时通过 EnvironmentSettings 配置启用。被选择的处理器必须要在正在执行的 Java 进程的类路径中。对于集群设置，默认两个查询处理器都会自动地加载到类路径中。如果要在 IDE 中运行一个查询，需要在项目中添加 planner 依赖。</p><h3 id="5-1-2-为什么选择-Table-API-amp-SQL？"><a href="#5-1-2-为什么选择-Table-API-amp-SQL？" class="headerlink" title="5.1.2 为什么选择 Table API &amp; SQL？"></a>5.1.2 为什么选择 Table API &amp; SQL？</h3><p>在 1.2 节中介绍了 Flink 的 API 是包含了 Table API &amp; SQL，在 1.3 节中也介绍了在 Flink 1.9 中阿里开源的 Blink 分支中的很强大的 SQL 功能合并进 Flink 主分支，另外通过阿里 Blink 相关的介绍，可以知道阿里在 SQL 功能这块是做了很多的工作。从前面章节的内容可以发现 Flink 的 DataStream／DataSet API 的功能已经很全并且很强大了，常见复杂的数据处理问题也都可以处理，那么社区为啥还在一直推广 Table API &amp; SQL 呢？</p><p>其实通过观察其它的大数据组件，就不会好奇了，比如 Spark、Storm、Beam、Hive 、KSQL（面向 Kafka 的 SQL 引擎）、Elasticsearch、Phoenix（使用 SQL 进行 HBase 数据的查询）等，可以发现 SQL 已经成为各个大数据组件必不可少的数据查询语言，那么 Flink 作为一个大数据实时处理引擎，笔者对其支持 SQL 查询流数据也不足为奇了，但是还是来稍微介绍一下 Table API &amp; SQL。</p><p>Table API &amp; SQL 是一种关系型 API，用户可以像操作数据库一样直接操作流数据，而不再需要通过 DataStream API 来写很多代码完成计算需求，更不用手动去调优你写的代码，另外 SQL 最大的优势在于它是一门学习成本很低的语言，普及率很高，用户基数大，和其他的编程语言相比，它的入门相对简单。</p><p>除了上面的原因，还有一个原因是：可以借助 Table API &amp; SQL 统一流处理和批处理，因为在 DataStream／DataSet API 中，用户开发流作业和批作业需要去了解两种不同的 API，这对于公司有些开发能力不高的数据分析师来说，学习成本有点高，他们其实更擅长写 SQL 来分析。Table API &amp; SQL 做到了批与流上的查询具有同样的语法语义，因此不用改代码就能同时在批和流上执行。</p><p>总结来说，为什么选择 Table API &amp; SQL：</p><ul><li>声明式语言表达业务逻辑</li><li>无需代码编程 —— 易于上手</li><li>查询能够被有效的优化</li><li>查询可以高效的执行</li></ul><h3 id="5-1-3-Flink-Table-项目模块"><a href="#5-1-3-Flink-Table-项目模块" class="headerlink" title="5.1.3 Flink Table 项目模块"></a>5.1.3 Flink Table 项目模块</h3><p>在上文中提及到 Flink Table 在 1.8 和 1.9 的区别，这里还是要再讲解一下这几个依赖，因为只有了解清楚了之后，我们在后面开发的时候才能够清楚挑选哪种依赖。它有如下几个模块：</p><ul><li>flink-table-common：table 中的公共模块，可以用于通过自定义 function，format 等来扩展 Table 生态系统</li><li>flink-table-api-java：支持使用 Java 语言，纯 Table＆SQL API</li><li>flink-table-api-scala：支持使用 Scala 语言，纯 Table＆SQL API</li><li>flink-table-api-java-bridge：支持使用 Java 语言，包含 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li><li>flink-table-api-scala-bridge：支持使用 Scala 语言，带有 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li><li>flink-sql-parser：SQL 语句解析层，主要依赖 calcite</li><li>flink-table-planner：Table 程序的 planner 和 runtime</li><li>flink-table-uber：将上诉模块打成一个 fat jar，在 lib 目录下</li><li>flink-table-planner-blink：Blink 的 Table 程序的 planner（阿里开源的版本）</li><li>flink-table-runtime-blink：Blink 的 Table 程序的 runtime（阿里开源的版本）</li><li>flink-table-uber-blink：将 Blink 版本的 planner 和 runtime 与前面模块（除 flink-table-planner 模块）打成一个 fat jar，在 lib 目录下，如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-02-164352.png" alt=""></p><ul><li>flink-sql-client：SQL 客户端</li></ul><h3 id="5-1-4-两种-planner-之间的区别"><a href="#5-1-4-两种-planner-之间的区别" class="headerlink" title="5.1.4 两种 planner 之间的区别"></a>5.1.4 两种 planner 之间的区别</h3><p>上面讲了两种不同的 planner 之间包含的模块有点区别，但是具体有什么区别如下所示：</p><ul><li>Blink planner 将批处理作业视为流的一种特殊情况。因此不支持 Table 和 DataSet 之间的转换，批处理作业会转换成 DataStream 程序，而不会转换成 DataSet 程序，流作业还是转换成 DataStream 程序。</li><li>Blink planner 不支持 BatchTableSource，而是使用有界的（bounded） StreamTableSource 代替它。</li><li>Blink planner 仅支持全新的 Catalog，不支持已经废弃的 ExternalCatalog。</li><li>以前的 planner 中 FilterableTableSource 的实现与现在的 Blink planner 有冲突，在以前的 planner 中是叠加 PlannerExpressions（在未来的版本中会移除），而在 Blink planner 中是 Expressions。</li><li>基于字符串的 KV 键值配置选项仅可以在 Blink planner 中使用。</li><li>PlannerConfig 的实现（CalciteConfig）在两种 planner 中不同。</li><li>Blink planner 会将多个 sink 优化在同一个 DAG 中（只在 TableEnvironment 中支持，StreamTableEnvironment 中不支持），而以前的 planner 是每个 sink 都有一个 DAG 中，相互独立的。</li><li>以前的 planner 不支持 catalog 统计，而 Blink planner 支持。</li></ul><p>在了解到了两种 planner 的区别后，接下来开始 Flink Table API &amp; SQL 之旅。</p><h3 id="5-1-5-添加项目依赖"><a href="#5-1-5-添加项目依赖" class="headerlink" title="5.1.5 添加项目依赖"></a>5.1.5 添加项目依赖</h3><p>因为在 Flink 1.9 版本中有两个 planner，所以得根据你使用的 planner 来选择对应的依赖，假设你选择的是最新的 Blink 版本，那么添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是以前的 planner，则使用下面这个依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果要自定义 format 格式或者自定义 function，则需要添加 <code>flink-table-common</code> 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5-1-6-创建一个-TableEnvironment"><a href="#5-1-6-创建一个-TableEnvironment" class="headerlink" title="5.1.6 创建一个 TableEnvironment"></a>5.1.6 创建一个 TableEnvironment</h3><p>TableEnvironment 是 Table API 和 SQL 的统称，它负责的内容有：</p><ul><li>在内部的 catalog 注册 Table</li><li>注册一个外部的 catalog</li><li>执行 SQL 查询</li><li>注册用户自定义的 function</li><li>将 DataStream 或者 DataSet 转换成 Table</li><li>保持对 ExecutionEnvironment 和 StreamExecutionEnvironment 的引用</li></ul><p>Table 总是会绑定在一个指定的 TableEnvironment，不能在同一个查询中组合不同 TableEnvironment 的 Table，比如 join 或 union 操作。你可以使用下面的几种静态方法创建 TableEnvironment。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 StreamTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, EnvironmentSettings.newInstance().build());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, EnvironmentSettings settings)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, settings, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** <span class="doctag">@deprecated</span> */</span></span><br><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, EnvironmentSettings.newInstance().build(), tableConfig);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 BatchTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你需要根据你的程序来使用对应的 TableEnvironment，是 BatchTableEnvironment 还是 StreamTableEnvironment。默认两个 planner 都是在 Flink 的安装目录下 lib 文件夹中存在的，所以应该在你的程序中指定使用哪种 planner。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Flink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure><p>如果在 lib 目录下只存在一个 planner，则可以使用 useAnyPlanner 来创建指定的 EnvironmentSettings。</p><h3 id="5-1-7-Table-API-amp-SQL-应用程序的结构"><a href="#5-1-7-Table-API-amp-SQL-应用程序的结构" class="headerlink" title="5.1.7 Table API &amp; SQL 应用程序的结构"></a>5.1.7 Table API &amp; SQL 应用程序的结构</h3><p>批处理和流处理的 Table API &amp; SQL 作业都有相同的模式，它们的代码结构如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据前面内容创建一个 TableEnvironment，指定是批作业还是流作业</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"></span><br><span class="line"><span class="comment">//用下面的其中一种方式注册一个 Table</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"table1"</span>, ...)          </span><br><span class="line">tableEnv.registerTableSource(<span class="string">"table2"</span>, ...); </span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">"extCat"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册一个 TableSink</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">"outputTable"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据一个 Table API 查询创建一个 Table</span></span><br><span class="line">Table tapiResult = tableEnv.scan(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">//根据一个 SQL 查询创建一个 Table</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table2 ... "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 Table API 或者 SQL 的结果发送给 TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//运行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><h3 id="5-1-8-Catalog-中注册-Table"><a href="#5-1-8-Catalog-中注册-Table" class="headerlink" title="5.1.8 Catalog 中注册 Table"></a>5.1.8 Catalog 中注册 Table</h3><p>Table 有两种类型，输入表和输出表，可以在 Table API &amp; SQL 查询中引用输入表并提供输入数据，输出表可以用于将 Table API &amp; SQL 的查询结果发送到外部系统。输出表可以通过 TableSink 来注册，输入表可以从各种数据源进行注册：</p><ul><li>已经存在的 Table 对象，通过是 Table API 或 SQL 查询的结果</li><li>连接了外部系统的 TableSource，比如文件、数据库、MQ</li><li>从 DataStream 或 DataSet 程序中返回的 DataStream 和 DataSet</li></ul><h4 id="注册-Table"><a href="#注册-Table" class="headerlink" title="注册 Table"></a>注册 Table</h4><p>在 TableEnvironment 中可以像下面这样注册一个 Table：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; <span class="comment">// see "Create a TableEnvironment" section</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//projTable 是一个简单查询的结果</span></span><br><span class="line">Table projTable = tableEnv.scan(<span class="string">"X"</span>).select(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 projTable 表注册为 projectedTable 表</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="注册-TableSource"><a href="#注册-TableSource" class="headerlink" title="注册 TableSource"></a>注册 TableSource</h4><h4 id="注册-TableSink"><a href="#注册-TableSink" class="headerlink" title="注册 TableSink"></a>注册 TableSink</h4><h3 id="5-1-9-注册外部的-Catalog"><a href="#5-1-9-注册外部的-Catalog" class="headerlink" title="5.1.9 注册外部的 Catalog"></a>5.1.9 注册外部的 Catalog</h3><h3 id="5-1-10-查询-Table"><a href="#5-1-10-查询-Table" class="headerlink" title="5.1.10 查询 Table"></a>5.1.10 查询 Table</h3><h4 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h4><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><h4 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h4><h3 id="5-1-11-提交-Table"><a href="#5-1-11-提交-Table" class="headerlink" title="5.1.11 提交 Table"></a>5.1.11 提交 Table</h3><h3 id="5-1-12-翻译并执行查询"><a href="#5-1-12-翻译并执行查询" class="headerlink" title="5.1.12 翻译并执行查询"></a>5.1.12 翻译并执行查询</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/MNBEYvf">https://t.zsxq.com/MNBEYvf</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="5-1-13-小结与反思"><a href="#5-1-13-小结与反思" class="headerlink" title="5.1.13 小结与反思"></a>5.1.13 小结与反思</h3><p>本节介绍了 Flink 新的 planner，然后详细的和之前的 planner 做了对比，然后对 Table API &amp; SQL 中的概念做了介绍，还通过样例去介绍了它们的通用 API。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第五章-——-Table-API-amp-SQL&quot;&gt;&lt;a href=&quot;#第五章-——-Table-API-amp-SQL&quot; class=&quot;headerlink&quot; title=&quot;第五章 —— Table API &amp;amp; SQL&quot;&gt;&lt;/a&gt;第五章 —— Table API &amp;amp; SQL&lt;/h1&gt;&lt;p&gt;Flink 中除了 DataStream 和 DataSet API，还有比较高级的 Table API &amp;amp; SQL，它可以帮助我们简化开发的过程，能够快读的运用 Flink 去完成一些需求，本章将对 Flink Table API &amp;amp; SQL 进行讲解，并将与其他的 API 结合对比分析。&lt;/p&gt;
&lt;h2 id=&quot;5-1-Flink-Table-amp-SQL-概念与通用-API&quot;&gt;&lt;a href=&quot;#5-1-Flink-Table-amp-SQL-概念与通用-API&quot; class=&quot;headerlink&quot; title=&quot;5.1 Flink Table &amp;amp; SQL 概念与通用 API&quot;&gt;&lt;/a&gt;5.1 Flink Table &amp;amp; SQL 概念与通用 API&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Table API &amp; SQL 功能</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/27/flink-in-action-5.2/</id>
    <published>2021-07-26T16:00:00.000Z</published>
    <updated>2021-12-26T11:26:14.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="5-2-Flink-Table-API-amp-SQL-功能"><a href="#5-2-Flink-Table-API-amp-SQL-功能" class="headerlink" title="5.2 Flink Table API &amp; SQL 功能"></a>5.2 Flink Table API &amp; SQL 功能</h2><p>在 5.1 节中对 Flink Table API &amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。</p><a id="more"></a><h3 id="5-2-1-Flink-Table-和-SQL-与-DataStream-和-DataSet-集成"><a href="#5-2-1-Flink-Table-和-SQL-与-DataStream-和-DataSet-集成" class="headerlink" title="5.2.1 Flink Table 和 SQL 与 DataStream 和 DataSet 集成"></a>5.2.1 Flink Table 和 SQL 与 DataStream 和 DataSet 集成</h3><p>两个 planner 都可以与 DataStream API 集成，只有以前的 planner 才可以集成 DataSet API，所以下面讨论 DataSet API 都是和以前的 planner 有关。</p><p>Table API &amp; SQL 查询与 DataStream 和 DataSet 程序集成是非常简单的，比如可以通过 Table API 或者 SQL 查询外部表数据，进行一些预处理后，然后使用 DataStream 或 DataSet API 继续处理一些复杂的计算，另外也可以将 DataStream 或 DataSet 处理后的数据利用 Table API 或者 SQL 写入到外部表去。总而言之，它们之间互相转换或者集成比较容易。</p><h4 id="Scala-的隐式转换"><a href="#Scala-的隐式转换" class="headerlink" title="Scala 的隐式转换"></a>Scala 的隐式转换</h4><p>Scala Table API 提供了 DataSet、DataStream 和 Table 类的隐式转换，可以通过导入 <code>org.apache.flink.table.api.scala._</code> 或者 <code>org.apache.flink.api.scala._</code> 包来启用这些转换。</p><h4 id="将-DataStream-或-DataSet-注册为-Table"><a href="#将-DataStream-或-DataSet-注册为-Table" class="headerlink" title="将 DataStream 或 DataSet 注册为 Table"></a>将 DataStream 或 DataSet 注册为 Table</h4><p>DataStream 或者 DataSet 可以注册为 Table，结果表的 schema 取决于已经注册的 DataStream 和 DataSet 的数据类型。你可以像下面这种方式转换：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable 表</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable"</span>, stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable2 表（表中的字段为 myLong、myString）</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h4 id="将-DataStream-或-DataSet-转换为-Table"><a href="#将-DataStream-或-DataSet-转换为-Table" class="headerlink" title="将 DataStream 或 DataSet 转换为 Table"></a>将 DataStream 或 DataSet 转换为 Table</h4><p>除了可以将 DataStream 或 DataSet 注册为 Table，还可以将它们转换为 Table，代码如下所示，转换之后再去使用 Table API 查询就比较方便了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h4 id="将-Table-转换成-DataStream-或-DataSet"><a href="#将-Table-转换成-DataStream-或-DataSet" class="headerlink" title="将 Table 转换成 DataStream 或 DataSet"></a>将 Table 转换成 DataStream 或 DataSet</h4><p>Table 可以转换为 DataStream 或 DataSet，这样就可以在 Table API 或 SQL 查询的结果上运行自定义的 DataStream 或 DataSet 程序。当将一个 Table 转换成 DataStream 或 DataSet 时，需要指定结果 DataStream 或 DataSet 的数据类型，最方便的数据类型是 Row，下面几个数据类型表示不同的功能：</p><ul><li>Row：字段按位置映射，任意数量的字段，支持 null 值，没有类型安全访问。</li><li>POJO：字段按名称映射，POJO 属性必须按照 Table 中的属性来命名，任意数量的字段，支持 null 值，类型安全访问</li><li>Case Class：字段按位置映射，不支持 null 值，类型安全访问。</li><li>Tuple：按位置映射字段，限制为 22（Scala）或 25（Java）字段，不支持 null 值，类型安全访问。</li><li>原子类型：Table 必须具有单个字段，不支持 null 值，类型安全访问。</li></ul><h5 id="将-Table-转换成-DataStream"><a href="#将-Table-转换成-DataStream" class="headerlink" title="将 Table 转换成 DataStream"></a>将 Table 转换成 DataStream</h5><p>流查询的结果表会动态更新，即每个新的记录到达输入流时结果就会发生变化。所以在将 Table 转换成 DataStream 就需要对表的更新进行编码，有两种将 Table 转换为 DataStream 的模式：</p><ul><li>追加模式(Append Mode)：这种模式只能在动态表仅通过 INSERT 更改修改时才能使用，即仅追加，之前发出的结果不会更新。</li><li>撤回模式(Retract Mode)：任何时刻都可以使用此模式，它使用一个 boolean 标志来编码 INSERT 和 DELETE 的更改。</li></ul><p>两种模式的代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定类，将表转换为一个 append DataStream</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 append DataStream</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为一个 Retract DataStream Row</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h5 id="将-Table-转换成-DataSet"><a href="#将-Table-转换成-DataSet" class="headerlink" title="将 Table 转换成 DataSet"></a>将 Table 转换成 DataSet</h5><p>将 Table 转换成 DataSet 的样例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定一个类将表转换为一个 Row DataSet</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 DataSet</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure><h3 id="5-2-2-查询优化"><a href="#5-2-2-查询优化" class="headerlink" title="5.2.2 查询优化"></a>5.2.2 查询优化</h3><p>Flink 使用 Calcite 来优化和翻译查询，以前的 planner 不会去优化 join 的顺序，而是按照查询中定义的顺序去执行。通过提供一个 CalciteConfig 对象来调整在不同阶段应用的优化规则集，这个可以通过调用 <code>CalciteConfig.createBuilder()</code> 获得的 builder 来创建，并且可以通过调用<code>tableEnv.getConfig.setCalciteConfig(calciteConfig)</code> 来提供给 TableEnvironment。而在 Blink planner 中扩展了 Calcite 来执行复杂的查询优化，这包括一系列基于规则和成本的优化，比如：</p><ul><li>基于 Calcite 的子查询去相关性</li><li>Project pruning</li><li>Partition pruning</li><li>Filter push-down</li><li>删除子计划中的重复数据以避免重复计算</li><li>重写特殊的子查询，包括两部分：<ul><li>将 IN 和 EXISTS 转换为 left semi-joins</li><li>将 NOT IN 和 NOT EXISTS 转换为 left anti-join</li></ul></li><li>重排序可选的 join<ul><li>通过启用 table.optimizer.join-reorder-enabled</li></ul></li></ul><p>注意：IN/EXISTS/NOT IN/NOT EXISTS 目前只支持子查询重写中的连接条件。</p><h4 id="解释-Table"><a href="#解释-Table" class="headerlink" title="解释 Table"></a>解释 Table</h4><p>Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。你可以通过 <code>TableEnvironment.explain(table)</code> 或者 <code>TableEnvironment.explain()</code> 方法来完成。<code>explain(table)</code> 会返回给定计划的 Table，<code>explain()</code> 会返回多路 Sink 计划的结果（主要用于 Blink planner）。它返回一个描述三个计划的字符串：</p><ul><li>关系查询的抽象语法树，即未优化的逻辑查询计划</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ul><p>以下代码演示了一个 Table 示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line"></span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1.where(<span class="string">"LIKE(word, 'F%')"</span>).unionAll(table2);</span><br><span class="line"></span><br><span class="line">System.out.println(tEnv.explain(table));</span><br></pre></td></tr></table></figure><p>通过 <code>explain(table)</code> 方法返回的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">== Abstract Syntax Tree ==</span><br><span class="line">LogicalUnion(all=[true])</span><br><span class="line">  LogicalFilter(condition=[LIKE($1, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">DataStreamUnion(all=[true], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    DataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 2 : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 3 : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">Stage 4 : Operator</span><br><span class="line">content : where: (LIKE(word, _UTF-16LE&apos;F%&apos;)), select: (count, word)</span><br><span class="line">ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">Stage 5 : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h3 id="5-2-3-数据类型"><a href="#5-2-3-数据类型" class="headerlink" title="5.2.3 数据类型"></a>5.2.3 数据类型</h3><h3 id="5-2-4-时间属性"><a href="#5-2-4-时间属性" class="headerlink" title="5.2.4 时间属性"></a>5.2.4 时间属性</h3><h4 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h4><h4 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a>Event time</h4><h3 id="5-2-5-SQL-Connector"><a href="#5-2-5-SQL-Connector" class="headerlink" title="5.2.5 SQL Connector"></a>5.2.5 SQL Connector</h3><p><strong>使用代码</strong></p><p><strong>使用 YAML 文件</strong></p><p><strong>使用 DDL</strong></p><h3 id="5-2-6-SQL-Client"><a href="#5-2-6-SQL-Client" class="headerlink" title="5.2.6 SQL Client"></a>5.2.6 SQL Client</h3><h3 id="5-2-7-Hive"><a href="#5-2-7-Hive" class="headerlink" title="5.2.7 Hive"></a>5.2.7 Hive</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/MNBEYvf">https://t.zsxq.com/MNBEYvf</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="5-2-8-小结与反思"><a href="#5-2-8-小结与反思" class="headerlink" title="5.2.8 小结与反思"></a>5.2.8 小结与反思</h3><p>本章节继续介绍了 Flink Table API &amp; SQL 中的部分 API，然后讲解了 Flink 之前的 planner 和 Blink planner 在某些特性上面的区别，还讲解了 SQL Connector，最后介绍了 SQL Client 和 Hive。</p><p>本章讲解了 Flink Table API &amp; SQL 相关的概述，另外还介绍了它们的 API 使用方式，除此之外还对 Connectors、SQL Client、Hive 做了一定的讲解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;5-2-Flink-Table-API-amp-SQL-功能&quot;&gt;&lt;a href=&quot;#5-2-Flink-Table-API-amp-SQL-功能&quot; class=&quot;headerlink&quot; title=&quot;5.2 Flink Table API &amp;amp; SQL 功能&quot;&gt;&lt;/a&gt;5.2 Flink Table API &amp;amp; SQL 功能&lt;/h2&gt;&lt;p&gt;在 5.1 节中对 Flink Table API &amp;amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Checkpoint 和 Savepoint 的区别及其配置使用</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/26/flink-in-action-4.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/26/flink-in-action-4.3/</id>
    <published>2021-07-25T16:00:00.000Z</published>
    <updated>2021-12-26T11:22:27.264Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用"><a href="#4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用" class="headerlink" title="4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用"></a>4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用</h2><p>Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。</p><a id="more"></a><h3 id="4-3-1-Checkpoint-简介及使用"><a href="#4-3-1-Checkpoint-简介及使用" class="headerlink" title="4.3.1 Checkpoint 简介及使用"></a>4.3.1 Checkpoint 简介及使用</h3><p>在 Flink 任务运行过程中，为了保障故障容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。</p><p>Flink 的 Checkpoint 有以下先决条件：</p><ul><li>需要具有持久性且支持重放一定时间范围内数据的数据源。例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。</li><li>需要一个能保存状态的持久化存储介质，例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。</li></ul><p>Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。下面来介绍 Checkpoint 具体如何使用。</p><p>首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。Checkpoint 配置相关的 Java 代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutinEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开启 Checkpoint，每 1000毫秒进行一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 语义设置为 EXACTLY_ONCE</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckPoint 的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同一时间，只允许 有 1 个 Checkpoint 在发生</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="keyword">true</span>)</span><br></pre></td></tr></table></figure><p>以上 Checkpoint 相关的参数描述如下所示：</p><ul><li>Checkpoint 语义： EXACTLY_ONCE 或 AT_LEAST_ONCE，EXACTLY_ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT_LEAST_ONCE 表示要消费的数据至少处理一次，可能会重复消费。</li><li>Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。</li><li>同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。</li><li>两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。</li><li>当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为：<ul><li>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。</li><li>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。</li></ul></li><li><p>任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。</p></li><li><p>作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。</p></li></ul><p>关于 Checkpoint 时，状态后端相关的配置请参阅本书 4.2 节。</p><h3 id="4-3-2-Savepoint-简介及使用"><a href="#4-3-2-Savepoint-简介及使用" class="headerlink" title="4.3.2 Savepoint 简介及使用"></a>4.3.2 Savepoint 简介及使用</h3><p>Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 <code>uid(String)</code> 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.</span><br><span class="line">  <span class="comment">// Stateful source (e.g. Kafka) with ID</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">"source-id"</span>) <span class="comment">// ID for the source operator</span></span><br><span class="line">  .shuffle()</span><br><span class="line">  <span class="comment">// Stateful mapper with ID</span></span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">"mapper-id"</span>) <span class="comment">// ID for the mapper</span></span><br><span class="line">  <span class="comment">// Stateless printing sink</span></span><br><span class="line">  .print(); <span class="comment">// Auto-generated ID</span></span><br></pre></td></tr></table></figure><p>如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 <code>uid(String)</code> 手动指定了 Operator ID，就不会存在 上述问题了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-020528.jpg" alt=""></p><p>Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。</p><p>使用 YARN 触发 Savepoint 的方式如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 和 YARN 应用程序 ID <code>:yarnAppId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径。</p><p>使用 Savepoint 取消 Flink 任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure><p>这将自动触发 ID 为 <code>:jobid</code> 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。</p><h3 id="4-3-3-Savepoint-与-Checkpoint-的区别"><a href="#4-3-3-Savepoint-与-Checkpoint-的区别" class="headerlink" title="4.3.3 Savepoint 与 Checkpoint 的区别"></a>4.3.3 Savepoint 与 Checkpoint 的区别</h3><p>前面分别介绍了 Savepoint 和 Checkpoint，可以发现它们有不少相似之处，下面来看下它们之间的区别。</p><table><thead><tr><th style="text-align:center">Checkpoint</th><th style="text-align:center">Savepoint</th></tr></thead><tbody><tr><td style="text-align:center">由 Flink 的 JobManager 定时自动触发并管理</td><td style="text-align:center">由用户手动触发并管理</td></tr><tr><td style="text-align:center">主要用于任务发生故障时，为任务提供给自动恢复机制</td><td style="text-align:center">主要用户升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复</td></tr><tr><td style="text-align:center">当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照</td><td style="text-align:center">仅支持全量快照</td></tr><tr><td style="text-align:center">Flink 任务停止后，Checkpoint 的状态快照信息默认被清除</td><td style="text-align:center">一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除</td></tr><tr><td style="text-align:center">Checkpoint 设计目标：轻量级且尽可能快地恢复任务</td><td style="text-align:center">Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作</td></tr></tbody></table><h3 id="4-3-4-Checkpoint-流程"><a href="#4-3-4-Checkpoint-流程" class="headerlink" title="4.3.4 Checkpoint 流程"></a>4.3.4 Checkpoint 流程</h3><p>Flink 任务 Checkpoint 的详细流程如下面流程所示。</p><ol><li><p>JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021819.png" alt=""></p></li><li><p>当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中，如下图所示。</p></li></ol><ul><li>如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中</li><li>为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021846.png" alt=""></p><blockquote><p>注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分</p></blockquote><ol><li><p>当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除</p></li><li><p>如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021900.png" alt=""></p></li></ol><p>如果对上述 Checkpoint 过程不理解，在后续 9.5 节 Flink 如何保障 Exactly Once 中会详细介绍 Flink 的 Checkpoint 过程以及为什么这么做。</p><h4 id="基于-RocksDB-的增量-Checkpoint-实现原理"><a href="#基于-RocksDB-的增量-Checkpoint-实现原理" class="headerlink" title="基于 RocksDB 的增量 Checkpoint 实现原理"></a>基于 RocksDB 的增量 Checkpoint 实现原理</h4><p>当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021910.png" alt=""></p><p>第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。</p><p>基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。</p><p>基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。</p><h3 id="4-3-5-如何从-Checkpoint-中恢复状态"><a href="#4-3-5-如何从-Checkpoint-中恢复状态" class="headerlink" title="4.3.5 如何从 Checkpoint 中恢复状态"></a>4.3.5 如何从 Checkpoint 中恢复状态</h3><h3 id="4-3-6-如何从-Savepoint-中恢复状态"><a href="#4-3-6-如何从-Savepoint-中恢复状态" class="headerlink" title="4.3.6 如何从 Savepoint 中恢复状态"></a>4.3.6 如何从 Savepoint 中恢复状态</h3><h3 id="4-3-7-如何优雅地删除-Checkpoint-目录"><a href="#4-3-7-如何优雅地删除-Checkpoint-目录" class="headerlink" title="4.3.7 如何优雅地删除 Checkpoint 目录"></a>4.3.7 如何优雅地删除 Checkpoint 目录</h3><h3 id="4-3-8-小结与反思"><a href="#4-3-8-小结与反思" class="headerlink" title="4.3.8 小结与反思"></a>4.3.8 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/RNJeqFy">https://t.zsxq.com/RNJeqFy</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><p>本章属于属于本书的进阶篇，在第一节中讲解了 State 的各种类型，包括每种 State 的使用方式、实现原理和部分源码剖析，在第二节中介绍 Flink 中的状态后端存储的使用方式，对比分析了现有的几种方案的使用场景，在第三节中介绍了 Checkpoint 和 Savepoint 的区别，如果从状态中恢复作业，以及整个作业的 Checkpoint 流程。</p><p>本章的内容是 Flink 作业开发的重点，通常线上遇到 State、Checkpoint、Savepoint 的问题比较多，所以希望你能好好的弄清楚它们的原理，这样在出问题后才能够对症下药去解决问题。 </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用&quot;&gt;&lt;a href=&quot;#4-3-Flink-Checkpoint-和-Savepoint-的区别及其配置使用&quot; class=&quot;headerlink&quot; title=&quot;4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用&quot;&gt;&lt;/a&gt;4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用&lt;/h2&gt;&lt;p&gt;Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 状态后端存储</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/25/flink-in-action-4.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/25/flink-in-action-4.2/</id>
    <published>2021-07-24T16:00:00.000Z</published>
    <updated>2021-12-26T11:19:07.537Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-2-Flink-状态后端存储"><a href="#4-2-Flink-状态后端存储" class="headerlink" title="4.2 Flink 状态后端存储"></a>4.2 Flink 状态后端存储</h2><p>在 4.1 节中介绍了 Flink 中的状态，那么在生产环境中，随着作业的运行时间变长，状态会变得越来越大，那么如何将这些状态存储也是 Flink 要解决的一大难点，本节来讲解下 Flink 中不同类型的状态后端存储。</p><a id="more"></a><h3 id="4-2-1-State-Backends"><a href="#4-2-1-State-Backends" class="headerlink" title="4.2.1 State Backends"></a>4.2.1 State Backends</h3><p>当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，为此在 Flink 源码中还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置：</p><ul><li>state.backend: 用于存储和进行状态 Checkpoint 的状态后端存储方式，无默认值</li><li>state.checkpoints.num-retained: 要保留的已完成 Checkpoint 的最大数量，默认值为 1</li><li>state.backend.async: 状态后端是否使用异步快照方法，默认值为 true</li><li>state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false</li><li>state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用</li><li>taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录</li><li>state.savepoints.dir: 存储 savepoints 的目录</li><li>state.checkpoints.dir: 存储 Checkpoint 的数据文件和元数据</li><li>state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024</li></ul><p>虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend());  <span class="comment">//设置堆内存存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));   //设置文件存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>StateBackend 接口的三种实现类如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-141800.png" alt=""></p><p>上面三种方式取一种就好了。但是有三种方式，我们该如何去挑选用哪种去存储状态呢？下面讲讲这三种的特点以及该如何选择。</p><h3 id="4-2-2-MemoryStateBackend-的用法及分析"><a href="#4-2-2-MemoryStateBackend-的用法及分析" class="headerlink" title="4.2.2 MemoryStateBackend 的用法及分析"></a>4.2.2 MemoryStateBackend 的用法及分析</h3><p>如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。如果你细心的话，可以从你的 Job 中看到类似日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask  - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: &apos;null&apos;, savepoints: &apos;null&apos;, asynchronous: TRUE, maxStateSize: 5242880)</span><br></pre></td></tr></table></figure><p>上面日志的意思就是说如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 TaskManagers 的内存（JVM 堆）中，当应用程序触发 Checkpoint 时，会将此时的状态进行快照然后存储在 JobManager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制，比如：</p><ul><li>不太适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 JobManager 中，如果状态较大的话，那么会使得 JobManager 的内存比较紧张，从而导致 JobManager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。</li><li>每个单独的状态大小不能超过最大的 DEFAULT_MAX_STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。</li><li>Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。</li></ul><p>另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有：</p><ul><li>TRUE（表示使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题）</li><li>FALSE（同步）</li><li>UNDEFINED（默认值）</li></ul><p>在构造 MemoryStateBackend 的默认函数时是使用的 UNDEFINED，而不是异步：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//使用的是 UNDEFINED</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>网上有人说默认是异步的，这里给大家解释清楚一下，从上面的那条日志打印的确实也是表示异步，但是前提是你对 State 无任何操作，笔者跟了下源码，当你没有配置任何的 state 时，它是会在 StateBackendLoader 类中通过 MemoryStateBackendFactory 来创建的 state 的，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-142223.png" alt=""></p><p>继续跟进 MemoryStateBackendFactory 可以发现他这里创建了一个 MemoryStateBackend 实例并通过 configure 方法进行配置，大概流程代码是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//MemoryStateBackendFactory 类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">createFromConfig</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend().configure(config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//MemoryStateBackend 类中的 config 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">configure</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend(<span class="keyword">this</span>, config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">MemoryStateBackend</span><span class="params">(MemoryStateBackend original, Configuration configuration, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.asynchronousSnapshots = original.asynchronousSnapshots.resolveUndefined(</span><br><span class="line">            configuration.getBoolean(CheckpointingOptions.ASYNC_SNAPSHOTS));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据 CheckpointingOptions 类中的 ASYNC_SNAPSHOTS 参数进行设置的</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ConfigOption&lt;Boolean&gt; ASYNC_SNAPSHOTS = ConfigOptions</span><br><span class="line">        .key(<span class="string">"state.backend.async"</span>)</span><br><span class="line">        .defaultValue(<span class="keyword">true</span>) <span class="comment">//默认值就是 true，代表异步</span></span><br><span class="line">        .withDescription(...)</span><br></pre></td></tr></table></figure><p>可以发现最终是通过读取 <code>state.backend.async</code> 参数的默认值（true）来配置是否要异步的进行快照，但是如果你手动配置 MemoryStateBackend 的话，利用无参数的构造方法，那么就不是默认异步，如果想使用异步的话，需要利用下面这个构造函数（需要传入一个 boolean 值，true 代表异步，false 代表同步）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(<span class="keyword">boolean</span> asynchronousSnapshots)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.fromBoolean(asynchronousSnapshots));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果你再细看了这个 MemoryStateBackend 类的话，那么你可能会发现这个构造函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(@Nullable String checkpointPath, @Nullable String savepointPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(checkpointPath, savepointPath, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//需要你传入 checkpointPath 和 savepointPath</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个也是用来创建一个 MemoryStateBackend 的，它需要传入的参数是两个路径（checkpointPath、savepointPath），其中 checkpointPath 是写入 Checkpoint 元数据的路径，savepointPath 是写入 savepoint 的路径。</p><p>这个来看看 MemoryStateBackend 的继承关系图可以更明确的知道它是继承自 AbstractFileStateBackend，然后 AbstractFileStateBackend 这个抽象类就是为了能够将状态存储中的数据或者元数据进行文件存储的，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-142403.png" alt=""></p><p>所以 FsStateBackend 和 MemoryStateBackend 都会继承该类。</p><h3 id="4-2-3-FsStateBackend-的用法及分析"><a href="#4-2-3-FsStateBackend-的用法及分析" class="headerlink" title="4.2.3 FsStateBackend 的用法及分析"></a>4.2.3 FsStateBackend 的用法及分析</h3><p>这种状态后端存储也是将工作状态存储在 TaskManager 中的内存（JVM 堆）中，但是 Checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置，比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 TaskManager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。</p><p>同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 Checkpoint 的，异步可以避免在状态 Checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX_FILE_STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 <code>state.backend.fs.memory-threshold</code> 来配置）。</p><p>那么我们该什么时候使用 FsStateBackend 呢？</p><ul><li>如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合</li><li>使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复</li></ul><p>使用 FsStateBackend 需要注意的地方有什么呢？</p><ul><li>工作状态仍然是存储在 TaskManager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 TaskManager 的内存</li></ul><h3 id="4-2-4-RocksDBStateBackend-的用法及分析"><a href="#4-2-4-RocksDBStateBackend-的用法及分析" class="headerlink" title="4.2.4 RocksDBStateBackend 的用法及分析"></a>4.2.4 RocksDBStateBackend 的用法及分析</h3><h3 id="4-2-5-如何选择状态后端存储？"><a href="#4-2-5-如何选择状态后端存储？" class="headerlink" title="4.2.5 如何选择状态后端存储？"></a>4.2.5 如何选择状态后端存储？</h3><h3 id="4-2-6-小结与反思"><a href="#4-2-6-小结与反思" class="headerlink" title="4.2.6 小结与反思"></a>4.2.6 小结与反思</h3><p>加入知识星球可以看到上面文章： <a href="https://t.zsxq.com/RNJeqFy">https://t.zsxq.com/RNJeqFy</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;4-2-Flink-状态后端存储&quot;&gt;&lt;a href=&quot;#4-2-Flink-状态后端存储&quot; class=&quot;headerlink&quot; title=&quot;4.2 Flink 状态后端存储&quot;&gt;&lt;/a&gt;4.2 Flink 状态后端存储&lt;/h2&gt;&lt;p&gt;在 4.1 节中介绍了 Flink 中的状态，那么在生产环境中，随着作业的运行时间变长，状态会变得越来越大，那么如何将这些状态存储也是 Flink 要解决的一大难点，本节来讲解下 Flink 中不同类型的状态后端存储。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 深度讲解 Flink 中的状态</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/24/flink-in-action-4.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/24/flink-in-action-4.1/</id>
    <published>2021-07-23T16:00:00.000Z</published>
    <updated>2021-12-26T11:11:05.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第四章-——-Flink-中的状态及容错机制"><a href="#第四章-——-Flink-中的状态及容错机制" class="headerlink" title="第四章 —— Flink 中的状态及容错机制"></a>第四章 —— Flink 中的状态及容错机制</h1><p>Flink 对比其他的流处理框架最大的特点是其支持状态，本章将深度的讲解 Flink 中的状态分类，如何在不同的场景使用不同的状态，接着会介绍 Flink 中的多种状态存储，最后会介绍 Checkpoint 和 Savepoint 的使用方式以及如何恢复状态。</p><h2 id="4-1-深度讲解-Flink-中的状态"><a href="#4-1-深度讲解-Flink-中的状态" class="headerlink" title="4.1 深度讲解 Flink 中的状态"></a>4.1 深度讲解 Flink 中的状态</h2><a id="more"></a><p>在基础篇中的 1.2 节中介绍了 Flink 是一款有状态的流处理框架。那么大家可能有点疑问，这个状态是什么意思？拿 Flink 最简单的 Word Count 程序来说，它需要不断的对 word 出现的个数进行结果统计，那么后一个结果就需要利用前一个的结果然后再做 +1 的操作，这样前一个计算就需要将 word 出现的次数 count 进行存着（这个 count 那么就是一个状态）然后后面才可以进行累加。</p><h3 id="4-1-1-为什么需要-State？"><a href="#4-1-1-为什么需要-State？" class="headerlink" title="4.1.1 为什么需要 State？"></a>4.1.1 为什么需要 State？</h3><p>对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：</p><ul><li>Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费</li><li>Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费</li></ul><p>上面两种情况用图片描述如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-030800.png" alt=""></p><p>为了解决上面两种情况（数据重复消费或者数据没有消费）的发生，那么是不是就得需要个什么东西做个记录将这种数据消费状态，Flink state 就这样诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态），当 Job 因为某种错误或者其他原因导致重启时，就能够从 Checkpoint（定时将 state 做一个全局快照，在 Flink 中，为了能够让 Job 在运行的过程中保证容错性，才会对这些 state 做一个快照，在 4.3 节中会详细讲） 中的 state 数据进行恢复。</p><h3 id="4-1-2-State-的种类"><a href="#4-1-2-State-的种类" class="headerlink" title="4.1.2 State 的种类"></a>4.1.2 State 的种类</h3><p>在 Flink 中有两个基本的 state：Keyed state 和 Operator state，下面来分别介绍一下这两种 State。</p><h3 id="4-1-3-Keyed-State"><a href="#4-1-3-Keyed-State" class="headerlink" title="4.1.3 Keyed State"></a>4.1.3 Keyed State</h3><p>Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。你可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。逻辑上，Keyed State 总是对应着一个 <parallel-operator-instance, key> 二元组，在某种程度上，因为每个具体的 key 总是属于唯一一个具体的 parallel-operator-instance（并行操作实例），这种情况下，那么就可以简化认为是 <operator, key>。Keyed State 可以进一步组织成 Key Group，Key Group 是 Flink 重新分配 Keyed State 的最小单元，所以有多少个并行，就会有多少个 Key Group。在执行过程中，每个 keyed operator 的并行实例会处理来自不同 key 的不同 Key Group。</p><h3 id="4-1-4-Operator-State"><a href="#4-1-4-Operator-State" class="headerlink" title="4.1.4 Operator State"></a>4.1.4 Operator State</h3><p>对 Operator State 而言，每个 operator state 都对应着一个并行实例。Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><p>在 Flink 源码中，在 flink-core module 下的 <code>org.apache.flink.api.common.state</code> 中可以看到 Flink 中所有和 State 相关的类，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143333.png" alt=""></p><h3 id="4-1-5-Raw-State-和-Managed-State"><a href="#4-1-5-Raw-State-和-Managed-State" class="headerlink" title="4.1.5 Raw State 和 Managed State"></a>4.1.5 Raw State 和 Managed State</h3><p>Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。</p><p>原始状态是 Operator（算子）保存它们自己的数据结构中的 state，当 Checkpoint 时，原始状态会以字节流的形式写入进 Checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。</p><p>托管状态可以使用 Flink runtime 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink runtime 会对这些状态进行编码然后将它们写入到 Checkpoint 中。</p><p>DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在实现 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。</p><p>注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。</p><h3 id="4-1-6-如何使用托管的-Keyed-State"><a href="#4-1-6-如何使用托管的-Keyed-State" class="headerlink" title="4.1.6 如何使用托管的 Keyed State"></a>4.1.6 如何使用托管的 Keyed State</h3><p>托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。</p><p>我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：</p><ul><li>ValueState<T>: 保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。</li><li>ListState<T>: 保存一个值的列表，用 add(T) 或者 addAll(List<T>) 来添加，用 Iterable<T> get() 来获取。</li><li>ReducingState<T>: 保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。</li><li>AggregatingState<IN, OUT>: 保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。</li><li>FoldingState<T, ACC>: 与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。</li><li>MapState<UK, UV>: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map<UK, UV>) 添加，或者使用 get(UK) 获取。</li></ul><p>所有类型的状态都有一个 clear() 方法来清除当前的状态。</p><p>注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。</p><p>需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。</p><p>要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。</p><p>状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：</p><ul><li>ValueState<T> getState(ValueStateDescriptor<T>)</li><li>ReducingState<T> getReducingState(ReducingStateDescriptor<T>)</li><li>ListState<T> getListState(ListStateDescriptor<T>)</li><li>AggregatingState<IN, OUT> getAggregatingState(AggregatingState<IN, OUT>)</li><li>FoldingState<T, ACC> getFoldingState(FoldingStateDescriptor<T, ACC>)</li><li>MapState<UK, UV> getMapState(MapStateDescriptor<UK, UV>)</li></ul><p>上面讲了这么多概念，那么来一个例子来看看如何使用状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//访问状态的 value 值</span></span><br><span class="line">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 count</span></span><br><span class="line">        currentSum.f0 += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 sum</span></span><br><span class="line">        currentSum.f1 += input.f1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        sum.update(currentSum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果 count 等于 2, 发出平均值并清除状态</span></span><br><span class="line">        <span class="keyword">if</span> (currentSum.f0 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class="line">            sum.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">                        <span class="string">"average"</span>, <span class="comment">//状态名称</span></span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class="comment">//类型信息</span></span><br><span class="line">                        Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>)); <span class="comment">//状态的默认值</span></span><br><span class="line">        sum = getRuntimeContext().getState(descriptor);<span class="comment">//获取状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">env.fromElements(Tuple2.of(<span class="number">1L</span>, <span class="number">3L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">5L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">7L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">4L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">2L</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果会打印出 (1,4) 和 (1,5)</span></span><br></pre></td></tr></table></figure><p>这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。</p><h3 id="4-1-7-State-TTL-存活时间"><a href="#4-1-7-State-TTL-存活时间" class="headerlink" title="4.1.7 State TTL(存活时间)"></a>4.1.7 State TTL(存活时间)</h3><p>随着作业的运行时间变长，作业的状态也会逐渐的变大，那么很有可能就会影响作业的稳定性，这时如果有状态的过期这种功能就可以将历史的一些状态清除，对应在 Flink 中的就是 State TTL，接下来将对其做详细介绍。</p><h4 id="State-TTL-介绍"><a href="#State-TTL-介绍" class="headerlink" title="State TTL 介绍"></a>State TTL 介绍</h4><p>TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line">    </span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"zhisheng"</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);    <span class="comment">//开启 ttl</span></span><br></pre></td></tr></table></figure><p>上面配置中有几个选项需要注意：</p><p>1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。</p><p>2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：</p><ul><li>StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新</li><li>StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新</li></ul><p>3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：</p><ul><li>StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值</li><li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回</li></ul><p>在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.</p><p>另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。</p><p>注意：</p><ul><li>状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。</li><li>目前仅支持参考 processing time 的 TTL</li><li>使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。</li><li>TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。</li><li>只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。</li></ul><h4 id="清除过期-State"><a href="#清除过期-State" class="headerlink" title="清除过期 State"></a>清除过期 State</h4><p>默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。</p><p>注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。</p><p>此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupFullSnapshot()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>此配置不适用于 RocksDB 状态后端中的增量 Checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。</p><p>除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupInBackground()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。</p><p>我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。</p><p>在该类中的属性如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143816.png" alt=""></p><p>这些属性的功能如下：</p><ul><li>DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig</li><li>UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选</li><li>StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired</li><li>TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选</li><li>Time：设置 TTL 的时间，这里有两个参数 unit 和 size</li><li>CleanupStrategies：TTL 清理策略，在该类中有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL_STATE_SCAN_SNAPSHOT、INCREMENTAL_CLEANUP 和 ROCKSDB_COMPACTION_FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器），该类和其实现类如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144111.png" alt=""></p><p>如果对 State TTL 还有不清楚的可以看看 Flink 源码 flink-runtime module 中的 state ttl 相关的实现类，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144324.png" alt=""></p><h3 id="4-1-8-如何使用托管的-Operator-State"><a href="#4-1-8-如何使用托管的-Operator-State" class="headerlink" title="4.1.8 如何使用托管的 Operator State"></a>4.1.8 如何使用托管的 Operator State</h3><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><h3 id="4-1-9-Stateful-Source-Functions"><a href="#4-1-9-Stateful-Source-Functions" class="headerlink" title="4.1.9 Stateful Source Functions"></a>4.1.9 Stateful Source Functions</h3><h3 id="4-1-10-Broadcast-State"><a href="#4-1-10-Broadcast-State" class="headerlink" title="4.1.10 Broadcast State"></a>4.1.10 Broadcast State</h3><p>Flink 中的 Broadcast State 在很多场景下也有使用，下面来讲解下其使用方式。</p><h4 id="Broadcast-State-如何使用"><a href="#Broadcast-State-如何使用" class="headerlink" title="Broadcast State 如何使用"></a>Broadcast State 如何使用</h4><h4 id="使用-Broadcast-state-需要注意"><a href="#使用-Broadcast-state-需要注意" class="headerlink" title="使用 Broadcast state 需要注意"></a>使用 Broadcast state 需要注意</h4><h3 id="4-1-11-Queryable-State"><a href="#4-1-11-Queryable-State" class="headerlink" title="4.1.11 Queryable State"></a>4.1.11 Queryable State</h3><p>加入知识星球可以看到上面文章： <a href="https://t.zsxq.com/ZVByvzN">https://t.zsxq.com/ZVByvzN</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="4-1-12-小结与反思"><a href="#4-1-12-小结与反思" class="headerlink" title="4.1.12 小结与反思"></a>4.1.12 小结与反思</h3><p>本节一开始讲解了 State 出现的原因，接着讲解了 Flink 中的 State 分类，然后对 Flink 中的每种 State 做了详细的讲解，希望可以好好消化这节的内容。你对本节的内容有什么不理解的地方吗？在使用 State 的过程中有遇到什么问题吗？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第四章-——-Flink-中的状态及容错机制&quot;&gt;&lt;a href=&quot;#第四章-——-Flink-中的状态及容错机制&quot; class=&quot;headerlink&quot; title=&quot;第四章 —— Flink 中的状态及容错机制&quot;&gt;&lt;/a&gt;第四章 —— Flink 中的状态及容错机制&lt;/h1&gt;&lt;p&gt;Flink 对比其他的流处理框架最大的特点是其支持状态，本章将深度的讲解 Flink 中的状态分类，如何在不同的场景使用不同的状态，接着会介绍 Flink 中的多种状态存储，最后会介绍 Checkpoint 和 Savepoint 的使用方式以及如何恢复状态。&lt;/p&gt;
&lt;h2 id=&quot;4-1-深度讲解-Flink-中的状态&quot;&gt;&lt;a href=&quot;#4-1-深度讲解-Flink-中的状态&quot; class=&quot;headerlink&quot; title=&quot;4.1 深度讲解 Flink 中的状态&quot;&gt;&lt;/a&gt;4.1 深度讲解 Flink 中的状态&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 使用 Side Output 分流</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/23/flink-in-action-3.12/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/23/flink-in-action-3.12/</id>
    <published>2021-07-22T16:00:00.000Z</published>
    <updated>2021-12-11T07:24:14.746Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-12-使用-Side-Output-分流"><a href="#3-12-使用-Side-Output-分流" class="headerlink" title="3.12 使用 Side Output 分流"></a>3.12 使用 Side Output 分流</h2><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。</p><a id="more"></a><h3 id="3-12-1-使用-Filter-分流"><a href="#3-12-1-使用-Filter-分流" class="headerlink" title="3.12.1 使用 Filter 分流"></a>3.12.1 使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">"machine"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">"docker"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">"application"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">"middleware"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure><h3 id="3-12-2-使用-Split-分流"><a href="#3-12-2-使用-Split-分流" class="headerlink" title="3.12.2 使用 Split 分流"></a>3.12.2 使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                tags.add(<span class="string">"machine"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                tags.add(<span class="string">"docker"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                tags.add(<span class="string">"application"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                tags.add(<span class="string">"middleware"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">"machine"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">"docker"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">"application"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">"middleware"</span>);</span><br></pre></td></tr></table></figure><p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p><ul><li><p><a href="https://issues.apache.org/jira/browse/FLINK-5031">FLINK-5031 Issue</a></p></li><li><p><a href="https://issues.apache.org/jira/browse/FLINK-11084">FLINK-11084 Issue</a></p></li></ul><p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p><h3 id="3-12-3-使用-Side-Output-分流"><a href="#3-12-3-使用-Side-Output-分流" class="headerlink" title="3.12.3 使用 Side Output 分流"></a>3.12.3 使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"machine"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"docker"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"application"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"middleware"</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure><p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p><h3 id="3-12-4-小结与反思"><a href="#3-12-4-小结与反思" class="headerlink" title="3.12.4 小结与反思"></a>3.12.4 小结与反思</h3><p>本节讲了下 Flink 中将数据分流的三种方式，完整代码的 GitHub 地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput">sideoutput</a></p><p>本章全部在介绍 Flink 的技术点，比如多种时间语义的对比分析和应用场景分析、多种灵活的窗口的使用方式及其原理实现、平时开发使用较多的一些算子、深入讲解了 DataStream 中的流类型及其对应的方法实现、如何将 Watermark 与 Window 结合来处理延迟数据、Flink Connector 的使用方式。</p><p>因为 Flink 的 Connector 比较多，所以本书只挑选了些平时工作用的比较多的 Connector，比如 Kafka、ElasticSearch、HBase、Redis 等，并教会了大家如何去自定义 Source 和 Sink，这样就算 Flink 中的 Connector 没有你需要的，那么也可以通过这种自定义的方法来实现读取和写入数据。</p><p>本章讲解的内容更侧重于在 Flink DataStream 和 Connector 的使用技巧和优化，在讲解这些时还提供了详细的代码实现，目的除了大家可以参考外，其实更期望大家能够自己跟着多动手去实现和优化，这样才可以提高自己的编程水平。另外本章还介绍了自己在使用这些 Connector 时遇到的一些问题，是怎么解决的，也希望大家在工作的时候遇到问题可以静下来自己独立的思考下出现问题的原因是啥，该如何解决，多独立思考后，相信遇到问题后你也能够从容的分析和解决问题。</p><p>加入知识星球可以看到更多文章</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-12-使用-Side-Output-分流&quot;&gt;&lt;a href=&quot;#3-12-使用-Side-Output-分流&quot; class=&quot;headerlink&quot; title=&quot;3.12 使用 Side Output 分流&quot;&gt;&lt;/a&gt;3.12 使用 Side Output 分流&lt;/h2&gt;&lt;p&gt;通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— Redis 的用法</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/22/flink-in-action-3.11/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/22/flink-in-action-3.11/</id>
    <published>2021-07-21T16:00:00.000Z</published>
    <updated>2021-12-11T07:19:40.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-11-Flink-Connector-——-Redis-的用法"><a href="#3-11-Flink-Connector-——-Redis-的用法" class="headerlink" title="3.11 Flink Connector —— Redis 的用法"></a>3.11 Flink Connector —— Redis 的用法</h2><p>在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。</p><a id="more"></a><h3 id="3-11-1-安装-Redis"><a href="#3-11-1-安装-Redis" class="headerlink" title="3.11.1 安装 Redis"></a>3.11.1 安装 Redis</h3><p>首先介绍下 Redis 的的安装和启动运行。</p><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><p>先从 <a href="https://redis.io/download">官网</a> 下载 Redis，然后解压。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br><span class="line">tar xzf redis-5.0.4.tar.gz</span><br><span class="line">cd redis-5.0.4</span><br><span class="line">make</span><br></pre></td></tr></table></figure><h4 id="通过-HomeBrew-安装"><a href="#通过-HomeBrew-安装" class="headerlink" title="通过 HomeBrew 安装"></a>通过 HomeBrew 安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br></pre></td></tr></table></figure><p>如果需要后台运行 Redis 服务，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew services start redis</span><br></pre></td></tr></table></figure><p>要运行命令，可以直接到 /usr/local/bin 目录下，有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br><span class="line">redis-cli</span><br></pre></td></tr></table></figure><p>两个命令，执行 <code>redis-server</code> 可以打开服务端，启动后结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080554.png" alt=""></p><p>然后另外开一个终端，运行 <code>redis-cli</code> 命令可以运行客户端，执行后效果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080617.png" alt=""></p><h3 id="3-11-2-将商品数据发送到-Kafka"><a href="#3-11-2-将商品数据发送到-Kafka" class="headerlink" title="3.11.2 将商品数据发送到 Kafka"></a>3.11.2 将商品数据发送到 Kafka</h3><p>这里我打算将从 Kafka 读取到所有到商品的信息，然后将商品信息中的 <strong>商品ID</strong> 和 <strong>商品价格</strong> 提取出来，然后写入到 Redis 中，供第三方服务根据商品 ID 查询到其对应的商品价格。</p><p>首先定义我们的商品类 （其中 id 和 price 字段是我们最后要提取的）为：</p><p>ProductEvent.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 商品</span></span><br><span class="line"><span class="comment"> * blog：http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> * 微信公众号：zhisheng</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Builder</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类目 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long categoryId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 编码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String code;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long shopId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String shopName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long brandId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String brandName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 图片地址</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String imageUrl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 状态（1(上架),-1(下架),-2(冻结),-3(删除)）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> status;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> type;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 标签</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;String&gt; tags;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 价格（以分为单位）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long price;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后写个工具类不断的模拟商品数据发往 Kafka，工具类 <code>ProductUtil.java</code> 的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"zhisheng"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10000</span>; i++) &#123;</span><br><span class="line">            ProductEvent product = ProductEvent.builder().id((<span class="keyword">long</span>) i)  <span class="comment">//商品的 id</span></span><br><span class="line">                    .name(<span class="string">"product"</span> + i)    <span class="comment">//商品 name</span></span><br><span class="line">                    .price(random.nextLong() / <span class="number">10000000000000L</span>) <span class="comment">//商品价格（以分为单位）</span></span><br><span class="line">                    .code(<span class="string">"code"</span> + i).build();  <span class="comment">//商品编码</span></span><br><span class="line"></span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(product));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(product));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-11-3-Flink-消费-Kafka-中的商品数据"><a href="#3-11-3-Flink-消费-Kafka-中的商品数据" class="headerlink" title="3.11.3 Flink 消费 Kafka 中的商品数据"></a>3.11.3 Flink 消费 Kafka 中的商品数据</h3><p>我们需要在 Flink 中消费 Kafka 数据，然后将商品中的两个数据（商品 id 和 price）取出来。先来看下这段 Flink Job 代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line">        Properties props = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; product = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props))</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, ProductEvent.class)) <span class="comment">//反序列化 JSON</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ProductEvent, Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(ProductEvent value, Collector&lt;Tuple2&lt;String, String&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//收集商品 id 和 price 两个属性</span></span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value.getId().toString(), value.getPrice().toString()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        product.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink redis connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后 IDEA 中启动运行 Job，再运行上面的 ProductUtil 发送 Kafka 数据的工具类（注意：也得提前启动 Kafka），运行结果如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-29-product-redult.png" alt=""></p><p>上图左半部分是工具类发送数据到 Kafka 打印的日志，右半部分是 Job 执行的结果，可以看到它已经将商品的 id 和 price 数据获取到了。</p><p>那么接下来我们需要的就是将这种 <code>Tuple2&lt;Long, Long&gt;</code> 格式的 KV 数据写入到 Redis 中去。要将数据写入到 Redis 的话是需要先添加依赖的。</p><h3 id="3-11-4-Redis-Connector-简介"><a href="#3-11-4-Redis-Connector-简介" class="headerlink" title="3.11.4 Redis Connector 简介"></a>3.11.4 Redis Connector 简介</h3><p>Redis Connector 提供用于向 Redis 发送数据的接口的类。接收器可以使用三种不同的方法与不同类型的 Redis 环境进行通信：</p><ul><li>单 Redis 服务器</li><li>Redis 集群</li><li>Redis Sentinel</li></ul><h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><p>需要添加 Flink Redis Sink 的 Connector，这个 Redis Connector 官方只有老的版本，后面也一直没有更新，所以可以看到网上有些文章都是添加老的版本的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>包括该部分的文档都是很早之前的啦，可以查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html">flink-docs-release-1.1 redis</a>。</p><p>另外在 <a href="https://bahir.apache.org/docs/flink/current/flink-streaming-redis/">flink-streaming-redis</a> 也看到一个 Flink Redis Connector 的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>两个依赖功能都是一样的，我们还是就用官方的那个 Maven 依赖来进行演示。</p><h3 id="3-11-5-Flink-写入数据到-Redis"><a href="#3-11-5-Flink-写入数据到-Redis" class="headerlink" title="3.11.5 Flink 写入数据到 Redis"></a>3.11.5 Flink 写入数据到 Redis</h3><h3 id="3-11-6-项目运行及验证"><a href="#3-11-6-项目运行及验证" class="headerlink" title="3.11.6 项目运行及验证"></a>3.11.6 项目运行及验证</h3><h3 id="3-11-7-小结与反思"><a href="#3-11-7-小结与反思" class="headerlink" title="3.11.7 小结与反思"></a>3.11.7 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/zr76I66">https://t.zsxq.com/zr76I66</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-11-Flink-Connector-——-Redis-的用法&quot;&gt;&lt;a href=&quot;#3-11-Flink-Connector-——-Redis-的用法&quot; class=&quot;headerlink&quot; title=&quot;3.11 Flink Connector —— Redis 的用法&quot;&gt;&lt;/a&gt;3.11 Flink Connector —— Redis 的用法&lt;/h2&gt;&lt;p&gt;在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— HBase 的用法</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/21/flink-in-action-3.10/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/21/flink-in-action-3.10/</id>
    <published>2021-07-20T16:00:00.000Z</published>
    <updated>2021-12-11T07:17:15.185Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-10-Flink-Connector-——-HBase-的用法"><a href="#3-10-Flink-Connector-——-HBase-的用法" class="headerlink" title="3.10 Flink Connector —— HBase 的用法"></a>3.10 Flink Connector —— HBase 的用法</h2><p>HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。</p><a id="more"></a><h3 id="3-10-1-准备环境和依赖"><a href="#3-10-1-准备环境和依赖" class="headerlink" title="3.10.1 准备环境和依赖"></a>3.10.1 准备环境和依赖</h3><p>下面分别讲解 HBase 的环境安装、配置、常用的命令操作以及添加项目需要的依赖。</p><h4 id="HBase-安装"><a href="#HBase-安装" class="headerlink" title="HBase 安装"></a>HBase 安装</h4><p>如果是苹果系统，可以使用 HomeBrew 命令安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install hbase</span><br></pre></td></tr></table></figure><p>HBase 最终会安装在路径 <code>/usr/local/Cellar/hbase/</code> 下面，安装版本不同，文件名也不同。</p><h4 id="配置-HBase"><a href="#配置-HBase" class="headerlink" title="配置 HBase"></a>配置 HBase</h4><p>打开 <code>libexec/conf/hbase-env.sh</code> 修改里面的 JAVA_HOME：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&quot;</span><br></pre></td></tr></table></figure><p>根据你自己的 JAVA_HOME 来配置这个变量。</p><p>打开 <code>libexec/conf/hbase-site.xml</code> 配置 HBase 文件存储目录:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/var/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储内建zookeeper文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/var/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="运行-HBase"><a href="#运行-HBase" class="headerlink" title="运行 HBase"></a>运行 HBase</h4><p>执行启动的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-hbase.sh</span><br></pre></td></tr></table></figure><p>执行后打印出来的日志如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/local/var/log/hbase/hbase-zhisheng-master-zhisheng.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><p>使用 jps 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/Cellar/hbase/1.2.9/libexec  jps</span><br><span class="line">91302 HMaster</span><br><span class="line">62535 RemoteMavenServer</span><br><span class="line">1100</span><br><span class="line">91471 Jps</span><br></pre></td></tr></table></figure><p>出现 HMaster 说明安装运行成功。</p><h4 id="启动-HBase-Shell"><a href="#启动-HBase-Shell" class="headerlink" title="启动 HBase Shell"></a>启动 HBase Shell</h4><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase shell</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-035328.jpg" alt=""></p><h4 id="停止-HBase"><a href="#停止-HBase" class="headerlink" title="停止 HBase"></a>停止 HBase</h4><p>执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/stop-hbase.sh</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-035513.jpg" alt=""></p><h4 id="HBase-常用命令"><a href="#HBase-常用命令" class="headerlink" title="HBase 常用命令"></a>HBase 常用命令</h4><p>HBase 中常用的命令有：list（列出已存在的表）、create（创建表）、put（写数据）、get（读数据）、scan（读数据，读全表）、describe（显示表详情），如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-040821.jpg" alt=""></p><p>简单使用上诉命令的结果如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-040230.jpg" alt=""></p><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 HBase 相关的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hbase_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink HBase Connector 中，HBase 不仅可以作为数据源，也还可以写入数据到 HBase 中去，我们先来看看如何从 HBase 中读取数据。</p><h3 id="3-10-2-Flink-使用-TableInputFormat-读取-HBase-批量数据"><a href="#3-10-2-Flink-使用-TableInputFormat-读取-HBase-批量数据" class="headerlink" title="3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据"></a>3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据</h3><p>这里我们使用 TableInputFormat 来读取 HBase 中的数据，首先准备数据。</p><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>先往 HBase 中插入五条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">put &apos;zhisheng&apos;, &apos;first&apos;, &apos;info:bar&apos;, &apos;hello&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;second&apos;, &apos;info:bar&apos;, &apos;zhisheng001&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;third&apos;, &apos;info:bar&apos;, &apos;zhisheng002&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;four&apos;, &apos;info:bar&apos;, &apos;zhisheng003&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;five&apos;, &apos;info:bar&apos;, &apos;zhisheng004&apos;</span><br></pre></td></tr></table></figure><p>scan 整个 <code>zhisheng</code> 表的话，有五条数据，运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-073344.jpg" alt=""></p><h4 id="Flink-Job-代码"><a href="#Flink-Job-代码" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><p>Flink 读取 HBase 数据的程序代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 读取 HBase 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReadMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.createInput(<span class="keyword">new</span> TableInputFormat&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> Tuple2&lt;String, String&gt; reuse = <span class="keyword">new</span> Tuple2&lt;String, String&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Scan <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">                scan.addColumn(INFO, BAR);</span><br><span class="line">                <span class="keyword">return</span> scan;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> String <span class="title">getTableName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> HBASE_TABLE_NAME;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Tuple2&lt;String, String&gt; <span class="title">mapResultToTuple</span><span class="params">(Result result)</span> </span>&#123;</span><br><span class="line">                String key = Bytes.toString(result.getRow());</span><br><span class="line">                String val = Bytes.toString(result.getValue(INFO, BAR));</span><br><span class="line">                reuse.setField(key, <span class="number">0</span>);</span><br><span class="line">                reuse.setField(val, <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">return</span> reuse;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple2&lt;String, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f1.startsWith(<span class="string">"zhisheng"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中将 HBase 中的读取全部读取出来后然后过滤以 <code>zhisheng</code> 开头的 value 数据。读取结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-161125.png" alt=""></p><p>可以看到输出的结果中已经将以 <code>zhisheng</code> 开头的四条数据都打印出来了。</p><h3 id="3-10-3-Flink-使用-TableOutputFormat-向-HBase-写入数据"><a href="#3-10-3-Flink-使用-TableOutputFormat-向-HBase-写入数据" class="headerlink" title="3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据"></a>3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据</h3><h4 id="添加依赖-1"><a href="#添加依赖-1" class="headerlink" title="添加依赖"></a>添加依赖</h4><h4 id="Flink-Job-代码-1"><a href="#Flink-Job-代码-1" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><h3 id="3-10-4-Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据"><a href="#3-10-4-Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据" class="headerlink" title="3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据"></a>3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据</h3><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><h4 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h4><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><h3 id="3-10-5-项目运行及验证"><a href="#3-10-5-项目运行及验证" class="headerlink" title="3.10.5 项目运行及验证"></a>3.10.5 项目运行及验证</h3><h3 id="3-10-6-小结与反思"><a href="#3-10-6-小结与反思" class="headerlink" title="3.10.6 小结与反思"></a>3.10.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/3bimqBM">https://t.zsxq.com/3bimqBM</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-10-Flink-Connector-——-HBase-的用法&quot;&gt;&lt;a href=&quot;#3-10-Flink-Connector-——-HBase-的用法&quot; class=&quot;headerlink&quot; title=&quot;3.10 Flink Connector —— HBase 的用法&quot;&gt;&lt;/a&gt;3.10 Flink Connector —— HBase 的用法&lt;/h2&gt;&lt;p&gt;HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— ElasticSearch 的用法和分析</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/20/flink-in-action-3.9/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/20/flink-in-action-3.9/</id>
    <published>2021-07-19T16:00:00.000Z</published>
    <updated>2021-12-11T07:13:50.133Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-9-Flink-Connector-——-ElasticSearch-的用法和分析"><a href="#3-9-Flink-Connector-——-ElasticSearch-的用法和分析" class="headerlink" title="3.9 Flink Connector —— ElasticSearch 的用法和分析"></a>3.9 Flink Connector —— ElasticSearch 的用法和分析</h2><p>ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。</p><a id="more"></a><h3 id="3-9-1-准备环境和依赖"><a href="#3-9-1-准备环境和依赖" class="headerlink" title="3.9.1 准备环境和依赖"></a>3.9.1 准备环境和依赖</h3><p>首先准备 ElasticSearch 的环境和项目的环境依赖。</p><p><strong>ElasticSearch 安装</strong></p><p>因为在 2.1 节中已经讲过 ElasticSearch 的安装，这里就不做过多的重复，需要注意的一点就是 Flink 的 ElasticSearch Connector 是区分版本号的，官方支持的版本如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-103746.png" alt=""></p><p>所以添加依赖的时候要区分一下，根据你安装的 ElasticSearch 来选择不一样的版本依赖，另外就是不同版本的 ElasticSearch 还会导致下面的数据写入到 ElasticSearch 中出现一些不同，我们这里使用的版本是 ElasticSearch6，如果你使用的是其他的版本可以参考官网的实现。</p><p><strong>添加依赖</strong></p><p>因为我们在 2.1 节中安装的 ElasticSearch 版本是 6.3.2 版本的，所有这里引入的依赖就选择 <code>flink-connector-elasticsearch6</code>，具体依赖如下所示。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面这个 <code>scala.binary.version</code> 和 <code>flink.version</code> 版本号需要自己在使用的时候根据使用的版本做相应的改变。</p><h3 id="3-9-2-使用-Flink-将数据写入到-ElasticSearch-应用程序"><a href="#3-9-2-使用-Flink-将数据写入到-ElasticSearch-应用程序" class="headerlink" title="3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序"></a>3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序</h3><p>准备好环境和相关的依赖后，接下来开始编写 Flink 程序。</p><p>ESSinkUtil 工具类，代码如下所示，这个工具类是笔者封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面章节还会再讲其他的配置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ESSinkUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flink 程序会读取到 ElasticSearch 的配置，然后将从 Kafka 读取到的数据写入进 ElasticSearch，具体的写入代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sink2ES6Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ESSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ESSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置文件中包含了 Kafka 和 ElasticSearch 的配置，如下所示，地址都支持集群模式填写，注意用 <code>,</code> 分隔。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><h3 id="3-9-3-验证数据是否写入-ElasticSearch？"><a href="#3-9-3-验证数据是否写入-ElasticSearch？" class="headerlink" title="3.9.3 验证数据是否写入 ElasticSearch？"></a>3.9.3 验证数据是否写入 ElasticSearch？</h3><h3 id="3-9-4-如何保证在海量数据实时写入下-ElasticSearch-的稳定性？"><a href="#3-9-4-如何保证在海量数据实时写入下-ElasticSearch-的稳定性？" class="headerlink" title="3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？"></a>3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？</h3><h3 id="3-9-5-使用-Flink-connector-elasticsearch-可能会遇到的问题"><a href="#3-9-5-使用-Flink-connector-elasticsearch-可能会遇到的问题" class="headerlink" title="3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题"></a>3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题</h3><h3 id="3-9-6-小结与反思"><a href="#3-9-6-小结与反思" class="headerlink" title="3.9.6 小结与反思"></a>3.9.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/Jeqzfem">https://t.zsxq.com/Jeqzfem</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-9-Flink-Connector-——-ElasticSearch-的用法和分析&quot;&gt;&lt;a href=&quot;#3-9-Flink-Connector-——-ElasticSearch-的用法和分析&quot; class=&quot;headerlink&quot; title=&quot;3.9 Flink Connector —— ElasticSearch 的用法和分析&quot;&gt;&lt;/a&gt;3.9 Flink Connector —— ElasticSearch 的用法和分析&lt;/h2&gt;&lt;p&gt;ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
