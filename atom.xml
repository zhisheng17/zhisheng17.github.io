<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2021-11-11T15:33:45.652Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>宕机一台机器，结果一百多个 Flink 作业挂了</title>
    <link href="http://www.54tianzhisheng.cn/2021/11/11/flink-akka-framesize/"/>
    <id>http://www.54tianzhisheng.cn/2021/11/11/flink-akka-framesize/</id>
    <published>2021-11-10T16:00:00.000Z</published>
    <updated>2021-11-11T15:33:45.652Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下：</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2021-11-09 16:01:11</span><br><span class="line">java.util.concurrent.CompletionException: java.lang.reflect.UndeclaredThrowableException</span><br><span class="line"> at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)</span><br><span class="line"> at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)</span><br><span class="line"> at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592)</span><br><span class="line"> at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line"> at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)</span><br><span class="line"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)</span><br><span class="line"> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line"> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line"> at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.lang.reflect.UndeclaredThrowableException</span><br><span class="line"> at com.sun.proxy.$Proxy54.submitTask(Unknown Source)</span><br><span class="line"> at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72)</span><br><span class="line"> at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756)</span><br><span class="line"> at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)</span><br><span class="line"> ... 7 more</span><br><span class="line">Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize.</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.createRpcInvocationMessage(AkkaInvocationHandler.java:276)</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:205)</span><br><span class="line"> at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:134)</span><br><span class="line"> ... 11 more</span><br></pre></td></tr></table></figure><h3 id="解决异常过程"><a href="#解决异常过程" class="headerlink" title="解决异常过程"></a>解决异常过程</h3><p>从上面的异常日志中我们提取到关键信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize.</span><br></pre></td></tr></table></figure><p>看起来是 RPC 的消息大小超过了默认的 akka framesize 的最大值了，所以我们来了解一下这个值的默认值，从 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.12/deployment/config.html#akka-framesize">官网</a> 我们可以看的到该值的默认大小为 “10485760b”，并且该参数的描述为：</p><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwbm72hedkj31i806imya.jpg" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier.</span><br></pre></td></tr></table></figure><p>翻译过来的意思就是：这个参数是 JobManager 和 TaskManagers 之间通信允许的最大消息大小，如果 Flink 作业因为通信消息大小超过了该值，你可以通过增加该值的大小来解决，该参数需要指定一个单位。</p><h3 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h3><p>Flink 使用 Akka 作为组件（JobManager/TaskManager/ResourceManager）之间的 RPC 框架，在 JobManager 和 TaskManagers 之间发送的消息的最大大小默认为 10485760b，如果消息超过这个限制就会失败，报错。这个可以看下抛出异常处的源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">protected RpcInvocation createRpcInvocationMessage(String methodName, Class&lt;?&gt;[] parameterTypes, Object[] args) throws IOException &#123;</span><br><span class="line">    Object rpcInvocation;</span><br><span class="line">    if (this.isLocal) &#123;</span><br><span class="line">        rpcInvocation = new LocalRpcInvocation(methodName, parameterTypes, args);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            RemoteRpcInvocation remoteRpcInvocation = new RemoteRpcInvocation(methodName, parameterTypes, args);</span><br><span class="line">            if (remoteRpcInvocation.getSize() &gt; this.maximumFramesize) &#123;</span><br><span class="line">                // 异常所在位置</span><br><span class="line">                throw new IOException(&quot;The rpc invocation size exceeds the maximum akka framesize.&quot;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            rpcInvocation = remoteRpcInvocation;</span><br><span class="line">        &#125; catch (IOException var6) &#123;</span><br><span class="line">            LOG.warn(&quot;Could not create remote rpc invocation message. Failing rpc invocation because...&quot;, var6);</span><br><span class="line">            throw var6;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return (RpcInvocation)rpcInvocation;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至于为什么 JobManager 和 TaskManager 之间的 RPC 消息大小会如此之大，初步的解释是在 task 出现异常之后，它需要调用 updateTaskExecutionState(TaskExecutionState，taskExecutionState) 这个 RPC 接口去通知 Flink Jobmanager 去改变对应 task 的状态并且重启 task。但是呢，taskExecutionState 这个参数里面有个 error 属性，当我的 task 打出来的错误栈太多的时候，在序列化的之后超过了 rpc 接口要求的最大数据大小（也就是 maximum akka framesize），导致调用 updateTaskExecutionState 这个 rpc 接口失败，Jobmanager 无法获知这个 task 已经处于 fail 的状态，也无法重启，然后就导致了一系列连锁反应。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>任务停止，在 <code>flink-conf.yaml</code> 中加入 <code>akka.framesize</code> 参数，调大该值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">akka.framesize: &quot;62914560b&quot;</span><br></pre></td></tr></table></figure><p>然后将任务重启，可以观察 Jobmanager Configration 看看参数是否生效。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下：&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>实时平台如何管理多个 Flink 版本？—— 为啥会出现多个版本？</title>
    <link href="http://www.54tianzhisheng.cn/2021/09/26/realtime-platform-flink-version/"/>
    <id>http://www.54tianzhisheng.cn/2021/09/26/realtime-platform-flink-version/</id>
    <published>2021-09-25T16:00:00.000Z</published>
    <updated>2021-11-14T03:12:49.099Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为啥会出现多个版本？"><a href="#为啥会出现多个版本？" class="headerlink" title="为啥会出现多个版本？"></a>为啥会出现多个版本？</h3><a id="more"></a><ul><li><p><strong>Flink 社区</strong>本身迭代速度非常快，目前阿里云有一大波的人专职做 Flink 开源，另外还拥有活跃的社区贡献者，所以功能开发较快，bug 修复速度较快，几乎每 4 个月一个大版本，每个大版本之间迭代的功能非常多，代码变动非常大，API 接口变动也大，动不动就干翻自己了。</p></li><li><p>社区迭代快就快呗，为什么<strong>公司</strong>也要要不断跟着社区鼻子走？社区迭代快意味着功能多，修复的 bug 多，相对于早期版本意味着稳定性也高些。除了国内一二线公司有特别多的专职人去负责这块，大多数中小公司最简单最快捷体验到稳定性最高、功能性最多、性能最好的 Flink 版本无非是直接使用最新的 Flink 版本。举个例子：Flink SQL 从最早期（1.9）的功能、性能到目前 1.14，差别真的大很多，优化了特别多的地方，增强了很多功能。原先使用 Flink SQL 完成一个流处理任务非常麻烦，还不如直接写几十行代码来的快，目前我情愿写 SQL 去处理一个流任务。那么自然会跟着升级到新版本。</p></li><li><p><strong>用户 A</strong> 问 Flink SQL 支持单独设置并行度吗？<strong>用户 B</strong> 问实时平台现在支持 Flink 1.13 版本的 Window TVF？这个要 Flink xxx 版本才能支持，要不你升级一下 Flink 版本到 xxx？这样就能支持了，类似的场景还有很多，对于<strong>中小公司的实时平台负责人</strong>来说，这无非最省事；对于<strong>大公司的负责实时开发的人</strong>来说，这无疑是一个噩梦，每次升级新版本都要将在老版本开发的各种功能都想尽办法移植到新版本上来，碰到 API 接口变动大的无非相当于重写了，或者将新版本的某些特别需要的功能通过打 patch 的方式打到老版本里面去。</p></li><li><p>新版本香是真的香，可是为啥有的人不用呢？问题就是，实时作业大多数是长期运行的，如果一个作业没啥错误，在生产运行的好好的，也不出啥故障，稳定性和性能也都能接受（并不是所有作业数据量都很大，会遇到性能问题），那么<strong>用户</strong>为啥要使用新版本？用户才不管你新版本功能多牛逼，性能多屌呢，老子升级还要改依赖版本、改接口代码、测试联调、性能测试（谁知道你说的性能提升是不是吹牛逼的）、稳定性测试（可能上线双跑一段时间验证），这些不需要时间呀，你叫我升级就升级，滚犊子吧，你知道我还有多少业务需求要做吗？</p></li></ul><p>那么就落下这个场地了，又要使用新版本的功能去解决问题，老作业的用户跟他各种扯皮也打动不了他升级作业的版本，那么自然就不断的出现了多个版本了。</p><p>这样，如果不对版本做好规划，那么摊子就逐渐越来越大，越来越难收拾了？</p><p>那么该如何管理公司的 Flink 版本？如果管理和兼容多个 Flink 版本的作业提交？如何兼容 Jar 包和 SQL 作业的提交</p><h3 id="怎么管理多个-Flink-版本的作业提交？"><a href="#怎么管理多个-Flink-版本的作业提交？" class="headerlink" title="怎么管理多个 Flink 版本的作业提交？"></a>怎么管理多个 Flink 版本的作业提交？</h3><p>尽请期待下篇文章</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;为啥会出现多个版本？&quot;&gt;&lt;a href=&quot;#为啥会出现多个版本？&quot; class=&quot;headerlink&quot; title=&quot;为啥会出现多个版本？&quot;&gt;&lt;/a&gt;为啥会出现多个版本？&lt;/h3&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 使用 Side Output 分流</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/23/flink-in-action-3.12/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/23/flink-in-action-3.12/</id>
    <published>2021-07-22T16:00:00.000Z</published>
    <updated>2021-12-11T07:24:14.746Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-12-使用-Side-Output-分流"><a href="#3-12-使用-Side-Output-分流" class="headerlink" title="3.12 使用 Side Output 分流"></a>3.12 使用 Side Output 分流</h2><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。</p><a id="more"></a><h3 id="3-12-1-使用-Filter-分流"><a href="#3-12-1-使用-Filter-分流" class="headerlink" title="3.12.1 使用 Filter 分流"></a>3.12.1 使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">"machine"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">"docker"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">"application"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">"middleware"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure><h3 id="3-12-2-使用-Split-分流"><a href="#3-12-2-使用-Split-分流" class="headerlink" title="3.12.2 使用 Split 分流"></a>3.12.2 使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                tags.add(<span class="string">"machine"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                tags.add(<span class="string">"docker"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                tags.add(<span class="string">"application"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                tags.add(<span class="string">"middleware"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">"machine"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">"docker"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">"application"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">"middleware"</span>);</span><br></pre></td></tr></table></figure><p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p><ul><li><p><a href="https://issues.apache.org/jira/browse/FLINK-5031">FLINK-5031 Issue</a></p></li><li><p><a href="https://issues.apache.org/jira/browse/FLINK-11084">FLINK-11084 Issue</a></p></li></ul><p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p><h3 id="3-12-3-使用-Side-Output-分流"><a href="#3-12-3-使用-Side-Output-分流" class="headerlink" title="3.12.3 使用 Side Output 分流"></a>3.12.3 使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"machine"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"docker"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"application"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"middleware"</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure><p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p><h3 id="3-12-4-小结与反思"><a href="#3-12-4-小结与反思" class="headerlink" title="3.12.4 小结与反思"></a>3.12.4 小结与反思</h3><p>本节讲了下 Flink 中将数据分流的三种方式，完整代码的 GitHub 地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput">sideoutput</a></p><p>本章全部在介绍 Flink 的技术点，比如多种时间语义的对比分析和应用场景分析、多种灵活的窗口的使用方式及其原理实现、平时开发使用较多的一些算子、深入讲解了 DataStream 中的流类型及其对应的方法实现、如何将 Watermark 与 Window 结合来处理延迟数据、Flink Connector 的使用方式。</p><p>因为 Flink 的 Connector 比较多，所以本书只挑选了些平时工作用的比较多的 Connector，比如 Kafka、ElasticSearch、HBase、Redis 等，并教会了大家如何去自定义 Source 和 Sink，这样就算 Flink 中的 Connector 没有你需要的，那么也可以通过这种自定义的方法来实现读取和写入数据。</p><p>本章讲解的内容更侧重于在 Flink DataStream 和 Connector 的使用技巧和优化，在讲解这些时还提供了详细的代码实现，目的除了大家可以参考外，其实更期望大家能够自己跟着多动手去实现和优化，这样才可以提高自己的编程水平。另外本章还介绍了自己在使用这些 Connector 时遇到的一些问题，是怎么解决的，也希望大家在工作的时候遇到问题可以静下来自己独立的思考下出现问题的原因是啥，该如何解决，多独立思考后，相信遇到问题后你也能够从容的分析和解决问题。</p><p>加入知识星球可以看到更多文章</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-12-使用-Side-Output-分流&quot;&gt;&lt;a href=&quot;#3-12-使用-Side-Output-分流&quot; class=&quot;headerlink&quot; title=&quot;3.12 使用 Side Output 分流&quot;&gt;&lt;/a&gt;3.12 使用 Side Output 分流&lt;/h2&gt;&lt;p&gt;通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— Redis 的用法</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/22/flink-in-action-3.11/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/22/flink-in-action-3.11/</id>
    <published>2021-07-21T16:00:00.000Z</published>
    <updated>2021-12-11T07:19:40.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-11-Flink-Connector-——-Redis-的用法"><a href="#3-11-Flink-Connector-——-Redis-的用法" class="headerlink" title="3.11 Flink Connector —— Redis 的用法"></a>3.11 Flink Connector —— Redis 的用法</h2><p>在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。</p><a id="more"></a><h3 id="3-11-1-安装-Redis"><a href="#3-11-1-安装-Redis" class="headerlink" title="3.11.1 安装 Redis"></a>3.11.1 安装 Redis</h3><p>首先介绍下 Redis 的的安装和启动运行。</p><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><p>先从 <a href="https://redis.io/download">官网</a> 下载 Redis，然后解压。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br><span class="line">tar xzf redis-5.0.4.tar.gz</span><br><span class="line">cd redis-5.0.4</span><br><span class="line">make</span><br></pre></td></tr></table></figure><h4 id="通过-HomeBrew-安装"><a href="#通过-HomeBrew-安装" class="headerlink" title="通过 HomeBrew 安装"></a>通过 HomeBrew 安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br></pre></td></tr></table></figure><p>如果需要后台运行 Redis 服务，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew services start redis</span><br></pre></td></tr></table></figure><p>要运行命令，可以直接到 /usr/local/bin 目录下，有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br><span class="line">redis-cli</span><br></pre></td></tr></table></figure><p>两个命令，执行 <code>redis-server</code> 可以打开服务端，启动后结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080554.png" alt=""></p><p>然后另外开一个终端，运行 <code>redis-cli</code> 命令可以运行客户端，执行后效果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080617.png" alt=""></p><h3 id="3-11-2-将商品数据发送到-Kafka"><a href="#3-11-2-将商品数据发送到-Kafka" class="headerlink" title="3.11.2 将商品数据发送到 Kafka"></a>3.11.2 将商品数据发送到 Kafka</h3><p>这里我打算将从 Kafka 读取到所有到商品的信息，然后将商品信息中的 <strong>商品ID</strong> 和 <strong>商品价格</strong> 提取出来，然后写入到 Redis 中，供第三方服务根据商品 ID 查询到其对应的商品价格。</p><p>首先定义我们的商品类 （其中 id 和 price 字段是我们最后要提取的）为：</p><p>ProductEvent.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 商品</span></span><br><span class="line"><span class="comment"> * blog：http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> * 微信公众号：zhisheng</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Builder</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类目 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long categoryId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 编码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String code;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long shopId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String shopName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long brandId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String brandName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 图片地址</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String imageUrl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 状态（1(上架),-1(下架),-2(冻结),-3(删除)）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> status;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> type;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 标签</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;String&gt; tags;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 价格（以分为单位）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long price;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后写个工具类不断的模拟商品数据发往 Kafka，工具类 <code>ProductUtil.java</code> 的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"zhisheng"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10000</span>; i++) &#123;</span><br><span class="line">            ProductEvent product = ProductEvent.builder().id((<span class="keyword">long</span>) i)  <span class="comment">//商品的 id</span></span><br><span class="line">                    .name(<span class="string">"product"</span> + i)    <span class="comment">//商品 name</span></span><br><span class="line">                    .price(random.nextLong() / <span class="number">10000000000000L</span>) <span class="comment">//商品价格（以分为单位）</span></span><br><span class="line">                    .code(<span class="string">"code"</span> + i).build();  <span class="comment">//商品编码</span></span><br><span class="line"></span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(product));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(product));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-11-3-Flink-消费-Kafka-中的商品数据"><a href="#3-11-3-Flink-消费-Kafka-中的商品数据" class="headerlink" title="3.11.3 Flink 消费 Kafka 中的商品数据"></a>3.11.3 Flink 消费 Kafka 中的商品数据</h3><p>我们需要在 Flink 中消费 Kafka 数据，然后将商品中的两个数据（商品 id 和 price）取出来。先来看下这段 Flink Job 代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line">        Properties props = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; product = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props))</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, ProductEvent.class)) <span class="comment">//反序列化 JSON</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ProductEvent, Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(ProductEvent value, Collector&lt;Tuple2&lt;String, String&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//收集商品 id 和 price 两个属性</span></span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value.getId().toString(), value.getPrice().toString()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        product.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink redis connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后 IDEA 中启动运行 Job，再运行上面的 ProductUtil 发送 Kafka 数据的工具类（注意：也得提前启动 Kafka），运行结果如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-29-product-redult.png" alt=""></p><p>上图左半部分是工具类发送数据到 Kafka 打印的日志，右半部分是 Job 执行的结果，可以看到它已经将商品的 id 和 price 数据获取到了。</p><p>那么接下来我们需要的就是将这种 <code>Tuple2&lt;Long, Long&gt;</code> 格式的 KV 数据写入到 Redis 中去。要将数据写入到 Redis 的话是需要先添加依赖的。</p><h3 id="3-11-4-Redis-Connector-简介"><a href="#3-11-4-Redis-Connector-简介" class="headerlink" title="3.11.4 Redis Connector 简介"></a>3.11.4 Redis Connector 简介</h3><p>Redis Connector 提供用于向 Redis 发送数据的接口的类。接收器可以使用三种不同的方法与不同类型的 Redis 环境进行通信：</p><ul><li>单 Redis 服务器</li><li>Redis 集群</li><li>Redis Sentinel</li></ul><h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><p>需要添加 Flink Redis Sink 的 Connector，这个 Redis Connector 官方只有老的版本，后面也一直没有更新，所以可以看到网上有些文章都是添加老的版本的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>包括该部分的文档都是很早之前的啦，可以查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html">flink-docs-release-1.1 redis</a>。</p><p>另外在 <a href="https://bahir.apache.org/docs/flink/current/flink-streaming-redis/">flink-streaming-redis</a> 也看到一个 Flink Redis Connector 的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>两个依赖功能都是一样的，我们还是就用官方的那个 Maven 依赖来进行演示。</p><h3 id="3-11-5-Flink-写入数据到-Redis"><a href="#3-11-5-Flink-写入数据到-Redis" class="headerlink" title="3.11.5 Flink 写入数据到 Redis"></a>3.11.5 Flink 写入数据到 Redis</h3><h3 id="3-11-6-项目运行及验证"><a href="#3-11-6-项目运行及验证" class="headerlink" title="3.11.6 项目运行及验证"></a>3.11.6 项目运行及验证</h3><h3 id="3-11-7-小结与反思"><a href="#3-11-7-小结与反思" class="headerlink" title="3.11.7 小结与反思"></a>3.11.7 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/zr76I66">https://t.zsxq.com/zr76I66</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-11-Flink-Connector-——-Redis-的用法&quot;&gt;&lt;a href=&quot;#3-11-Flink-Connector-——-Redis-的用法&quot; class=&quot;headerlink&quot; title=&quot;3.11 Flink Connector —— Redis 的用法&quot;&gt;&lt;/a&gt;3.11 Flink Connector —— Redis 的用法&lt;/h2&gt;&lt;p&gt;在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— HBase 的用法</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/21/flink-in-action-3.10/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/21/flink-in-action-3.10/</id>
    <published>2021-07-20T16:00:00.000Z</published>
    <updated>2021-12-11T07:17:15.185Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-10-Flink-Connector-——-HBase-的用法"><a href="#3-10-Flink-Connector-——-HBase-的用法" class="headerlink" title="3.10 Flink Connector —— HBase 的用法"></a>3.10 Flink Connector —— HBase 的用法</h2><p>HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。</p><a id="more"></a><h3 id="3-10-1-准备环境和依赖"><a href="#3-10-1-准备环境和依赖" class="headerlink" title="3.10.1 准备环境和依赖"></a>3.10.1 准备环境和依赖</h3><p>下面分别讲解 HBase 的环境安装、配置、常用的命令操作以及添加项目需要的依赖。</p><h4 id="HBase-安装"><a href="#HBase-安装" class="headerlink" title="HBase 安装"></a>HBase 安装</h4><p>如果是苹果系统，可以使用 HomeBrew 命令安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install hbase</span><br></pre></td></tr></table></figure><p>HBase 最终会安装在路径 <code>/usr/local/Cellar/hbase/</code> 下面，安装版本不同，文件名也不同。</p><h4 id="配置-HBase"><a href="#配置-HBase" class="headerlink" title="配置 HBase"></a>配置 HBase</h4><p>打开 <code>libexec/conf/hbase-env.sh</code> 修改里面的 JAVA_HOME：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&quot;</span><br></pre></td></tr></table></figure><p>根据你自己的 JAVA_HOME 来配置这个变量。</p><p>打开 <code>libexec/conf/hbase-site.xml</code> 配置 HBase 文件存储目录:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/var/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储内建zookeeper文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/var/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="运行-HBase"><a href="#运行-HBase" class="headerlink" title="运行 HBase"></a>运行 HBase</h4><p>执行启动的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-hbase.sh</span><br></pre></td></tr></table></figure><p>执行后打印出来的日志如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/local/var/log/hbase/hbase-zhisheng-master-zhisheng.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><p>使用 jps 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/Cellar/hbase/1.2.9/libexec  jps</span><br><span class="line">91302 HMaster</span><br><span class="line">62535 RemoteMavenServer</span><br><span class="line">1100</span><br><span class="line">91471 Jps</span><br></pre></td></tr></table></figure><p>出现 HMaster 说明安装运行成功。</p><h4 id="启动-HBase-Shell"><a href="#启动-HBase-Shell" class="headerlink" title="启动 HBase Shell"></a>启动 HBase Shell</h4><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase shell</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-035328.jpg" alt=""></p><h4 id="停止-HBase"><a href="#停止-HBase" class="headerlink" title="停止 HBase"></a>停止 HBase</h4><p>执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/stop-hbase.sh</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-035513.jpg" alt=""></p><h4 id="HBase-常用命令"><a href="#HBase-常用命令" class="headerlink" title="HBase 常用命令"></a>HBase 常用命令</h4><p>HBase 中常用的命令有：list（列出已存在的表）、create（创建表）、put（写数据）、get（读数据）、scan（读数据，读全表）、describe（显示表详情），如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-040821.jpg" alt=""></p><p>简单使用上诉命令的结果如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-040230.jpg" alt=""></p><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 HBase 相关的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hbase_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink HBase Connector 中，HBase 不仅可以作为数据源，也还可以写入数据到 HBase 中去，我们先来看看如何从 HBase 中读取数据。</p><h3 id="3-10-2-Flink-使用-TableInputFormat-读取-HBase-批量数据"><a href="#3-10-2-Flink-使用-TableInputFormat-读取-HBase-批量数据" class="headerlink" title="3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据"></a>3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据</h3><p>这里我们使用 TableInputFormat 来读取 HBase 中的数据，首先准备数据。</p><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>先往 HBase 中插入五条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">put &apos;zhisheng&apos;, &apos;first&apos;, &apos;info:bar&apos;, &apos;hello&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;second&apos;, &apos;info:bar&apos;, &apos;zhisheng001&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;third&apos;, &apos;info:bar&apos;, &apos;zhisheng002&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;four&apos;, &apos;info:bar&apos;, &apos;zhisheng003&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;five&apos;, &apos;info:bar&apos;, &apos;zhisheng004&apos;</span><br></pre></td></tr></table></figure><p>scan 整个 <code>zhisheng</code> 表的话，有五条数据，运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-04-073344.jpg" alt=""></p><h4 id="Flink-Job-代码"><a href="#Flink-Job-代码" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><p>Flink 读取 HBase 数据的程序代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 读取 HBase 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReadMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.createInput(<span class="keyword">new</span> TableInputFormat&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> Tuple2&lt;String, String&gt; reuse = <span class="keyword">new</span> Tuple2&lt;String, String&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Scan <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">                scan.addColumn(INFO, BAR);</span><br><span class="line">                <span class="keyword">return</span> scan;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> String <span class="title">getTableName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> HBASE_TABLE_NAME;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Tuple2&lt;String, String&gt; <span class="title">mapResultToTuple</span><span class="params">(Result result)</span> </span>&#123;</span><br><span class="line">                String key = Bytes.toString(result.getRow());</span><br><span class="line">                String val = Bytes.toString(result.getValue(INFO, BAR));</span><br><span class="line">                reuse.setField(key, <span class="number">0</span>);</span><br><span class="line">                reuse.setField(val, <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">return</span> reuse;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple2&lt;String, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f1.startsWith(<span class="string">"zhisheng"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中将 HBase 中的读取全部读取出来后然后过滤以 <code>zhisheng</code> 开头的 value 数据。读取结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-161125.png" alt=""></p><p>可以看到输出的结果中已经将以 <code>zhisheng</code> 开头的四条数据都打印出来了。</p><h3 id="3-10-3-Flink-使用-TableOutputFormat-向-HBase-写入数据"><a href="#3-10-3-Flink-使用-TableOutputFormat-向-HBase-写入数据" class="headerlink" title="3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据"></a>3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据</h3><h4 id="添加依赖-1"><a href="#添加依赖-1" class="headerlink" title="添加依赖"></a>添加依赖</h4><h4 id="Flink-Job-代码-1"><a href="#Flink-Job-代码-1" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><h3 id="3-10-4-Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据"><a href="#3-10-4-Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据" class="headerlink" title="3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据"></a>3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据</h3><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><h4 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h4><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><h3 id="3-10-5-项目运行及验证"><a href="#3-10-5-项目运行及验证" class="headerlink" title="3.10.5 项目运行及验证"></a>3.10.5 项目运行及验证</h3><h3 id="3-10-6-小结与反思"><a href="#3-10-6-小结与反思" class="headerlink" title="3.10.6 小结与反思"></a>3.10.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/3bimqBM">https://t.zsxq.com/3bimqBM</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-10-Flink-Connector-——-HBase-的用法&quot;&gt;&lt;a href=&quot;#3-10-Flink-Connector-——-HBase-的用法&quot; class=&quot;headerlink&quot; title=&quot;3.10 Flink Connector —— HBase 的用法&quot;&gt;&lt;/a&gt;3.10 Flink Connector —— HBase 的用法&lt;/h2&gt;&lt;p&gt;HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— ElasticSearch 的用法和分析</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/20/flink-in-action-3.9/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/20/flink-in-action-3.9/</id>
    <published>2021-07-19T16:00:00.000Z</published>
    <updated>2021-12-11T07:13:50.133Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-9-Flink-Connector-——-ElasticSearch-的用法和分析"><a href="#3-9-Flink-Connector-——-ElasticSearch-的用法和分析" class="headerlink" title="3.9 Flink Connector —— ElasticSearch 的用法和分析"></a>3.9 Flink Connector —— ElasticSearch 的用法和分析</h2><p>ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。</p><a id="more"></a><h3 id="3-9-1-准备环境和依赖"><a href="#3-9-1-准备环境和依赖" class="headerlink" title="3.9.1 准备环境和依赖"></a>3.9.1 准备环境和依赖</h3><p>首先准备 ElasticSearch 的环境和项目的环境依赖。</p><p><strong>ElasticSearch 安装</strong></p><p>因为在 2.1 节中已经讲过 ElasticSearch 的安装，这里就不做过多的重复，需要注意的一点就是 Flink 的 ElasticSearch Connector 是区分版本号的，官方支持的版本如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-103746.png" alt=""></p><p>所以添加依赖的时候要区分一下，根据你安装的 ElasticSearch 来选择不一样的版本依赖，另外就是不同版本的 ElasticSearch 还会导致下面的数据写入到 ElasticSearch 中出现一些不同，我们这里使用的版本是 ElasticSearch6，如果你使用的是其他的版本可以参考官网的实现。</p><p><strong>添加依赖</strong></p><p>因为我们在 2.1 节中安装的 ElasticSearch 版本是 6.3.2 版本的，所有这里引入的依赖就选择 <code>flink-connector-elasticsearch6</code>，具体依赖如下所示。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面这个 <code>scala.binary.version</code> 和 <code>flink.version</code> 版本号需要自己在使用的时候根据使用的版本做相应的改变。</p><h3 id="3-9-2-使用-Flink-将数据写入到-ElasticSearch-应用程序"><a href="#3-9-2-使用-Flink-将数据写入到-ElasticSearch-应用程序" class="headerlink" title="3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序"></a>3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序</h3><p>准备好环境和相关的依赖后，接下来开始编写 Flink 程序。</p><p>ESSinkUtil 工具类，代码如下所示，这个工具类是笔者封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面章节还会再讲其他的配置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ESSinkUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flink 程序会读取到 ElasticSearch 的配置，然后将从 Kafka 读取到的数据写入进 ElasticSearch，具体的写入代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sink2ES6Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ESSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ESSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置文件中包含了 Kafka 和 ElasticSearch 的配置，如下所示，地址都支持集群模式填写，注意用 <code>,</code> 分隔。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><h3 id="3-9-3-验证数据是否写入-ElasticSearch？"><a href="#3-9-3-验证数据是否写入-ElasticSearch？" class="headerlink" title="3.9.3 验证数据是否写入 ElasticSearch？"></a>3.9.3 验证数据是否写入 ElasticSearch？</h3><h3 id="3-9-4-如何保证在海量数据实时写入下-ElasticSearch-的稳定性？"><a href="#3-9-4-如何保证在海量数据实时写入下-ElasticSearch-的稳定性？" class="headerlink" title="3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？"></a>3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？</h3><h3 id="3-9-5-使用-Flink-connector-elasticsearch-可能会遇到的问题"><a href="#3-9-5-使用-Flink-connector-elasticsearch-可能会遇到的问题" class="headerlink" title="3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题"></a>3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题</h3><h3 id="3-9-6-小结与反思"><a href="#3-9-6-小结与反思" class="headerlink" title="3.9.6 小结与反思"></a>3.9.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/Jeqzfem">https://t.zsxq.com/Jeqzfem</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-9-Flink-Connector-——-ElasticSearch-的用法和分析&quot;&gt;&lt;a href=&quot;#3-9-Flink-Connector-——-ElasticSearch-的用法和分析&quot; class=&quot;headerlink&quot; title=&quot;3.9 Flink Connector —— ElasticSearch 的用法和分析&quot;&gt;&lt;/a&gt;3.9 Flink Connector —— ElasticSearch 的用法和分析&lt;/h2&gt;&lt;p&gt;ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 自定义 Flink Connector</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/19/flink-in-action-3.8/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/19/flink-in-action-3.8/</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2021-12-11T07:14:43.418Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-8-自定义-Flink-Connector"><a href="#3-8-自定义-Flink-Connector" class="headerlink" title="3.8 自定义 Flink Connector"></a>3.8 自定义 Flink Connector</h2><p>在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。</p><a id="more"></a><h3 id="3-8-1-如何自定义-Source-Connector？"><a href="#3-8-1-如何自定义-Source-Connector？" class="headerlink" title="3.8.1 如何自定义 Source Connector？"></a>3.8.1 如何自定义 Source Connector？</h3><p>这里就演示一下如何自定义 Source 从 MySQL 中读取数据。</p><p><strong>添加依赖</strong></p><p>在 pom.xml 中添加 MySQL 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>数据库建表</strong></p><p>数据库建表的 SQL 语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`student`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`age`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_bin;</span><br></pre></td></tr></table></figure><p><strong>数据库插入数据</strong></p><p>往新建的数据库表中插入 4 条数据的 SQL 语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`student`</span> <span class="keyword">VALUES</span> (<span class="string">'1'</span>, <span class="string">'zhisheng01'</span>, <span class="string">'123456'</span>, <span class="string">'18'</span>), (<span class="string">'2'</span>, <span class="string">'zhisheng02'</span>, <span class="string">'123'</span>, <span class="string">'17'</span>), (<span class="string">'3'</span>, <span class="string">'zhisheng03'</span>, <span class="string">'1234'</span>, <span class="string">'18'</span>), (<span class="string">'4'</span>, <span class="string">'zhisheng04'</span>, <span class="string">'12345'</span>, <span class="string">'16'</span>);</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure><p><strong>新建实体类</strong></p><p>对应数据库字段的实体类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;  <span class="comment">//id</span></span><br><span class="line">    <span class="keyword">public</span> String name; <span class="comment">//姓名</span></span><br><span class="line">    <span class="keyword">public</span> String password; <span class="comment">//密码</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age; <span class="comment">//年龄</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自定义 Source 类</strong></p><p>SourceFromMySQL 是自定义的 Source 类，该类继承 RichSourceFunction ，实现里面的 open、close、run、cancel 方法，它的作用是读取 MySQL 中的数据，代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceFromMySQL</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"select * from Student;"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 程序执行完毕就可以进行，关闭连接和释放资源的动作了</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123; <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DataStream 调用一次 run() 方法用来获取数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Student&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(</span><br><span class="line">                    resultSet.getInt(<span class="string">"id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"name"</span>).trim(),</span><br><span class="line">                    resultSet.getString(<span class="string">"password"</span>).trim(),</span><br><span class="line">                    resultSet.getInt(<span class="string">"age"</span>));</span><br><span class="line">            ctx.collect(student);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">                con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                System.out.println(<span class="string">"mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Flink 应用程序代码</strong></p><p>读取 MySQL 数据的代码完成后，接下来 Flink 主程序的代码就可以直接在 <code>addSource()</code> 方法中构造一个 SourceFromMySQL 对象作为一个参数传入，具体代码如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> SourceFromMySQL()).print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data sourc"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 Flink 程序，控制台日志中可以看见打印的 student 信息，结果如下图所示。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cY9WwK.jpg" alt=""></p><h3 id="3-8-2-RichSourceFunction-的用法及源码分析"><a href="#3-8-2-RichSourceFunction-的用法及源码分析" class="headerlink" title="3.8.2 RichSourceFunction 的用法及源码分析"></a>3.8.2 RichSourceFunction 的用法及源码分析</h3><p>从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，其实也是可以使用 SourceFunction 函数来自定义 Source。 RichSourceFunction 函数比 SourceFunction 多了 open 方法（可以用来初始化）和获取应用上下文的方法，那么来了解一下该类，它的类结构如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-020426.png" alt=""></p><p>它是一个抽象类，继承自 AbstractRichFunction，实现了 SourceFunction 接口，其子类有三个，如下图所示，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-020702.png" alt=""></p><p>这三个子类的功能如下：</p><ul><li>MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。</li><li>MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。</li><li>ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。</li></ul><p>除了上面使用 RichSourceFunction 和 SourceFunction 来自定义 Source，还可以继承 RichParallelSourceFunction 抽象类或实现 ParallelSourceFunction 接口来实现自定义 Source 函数。</p><h3 id="3-8-3-自定义-Sink-Connector"><a href="#3-8-3-自定义-Sink-Connector" class="headerlink" title="3.8.3 自定义 Sink Connector"></a>3.8.3 自定义 Sink Connector</h3><p>下面将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去</p><h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>写了一个工具类往 Kafka 的 topic 中发送数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(student));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="SinkToMySQL"><a href="#SinkToMySQL" class="headerlink" title="SinkToMySQL"></a>SinkToMySQL</h4><p>该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法，在 invoke 方法中将数据插入到 MySQL 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Student value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//组装数据，执行插入操作</span></span><br><span class="line">        ps.setInt(<span class="number">1</span>, value.getId());</span><br><span class="line">        ps.setString(<span class="number">2</span>, value.getName());</span><br><span class="line">        ps.setString(<span class="number">3</span>, value.getPassword());</span><br><span class="line">        ps.setInt(<span class="number">4</span>, value.getAge());</span><br><span class="line">        ps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"root123456"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span>+ e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Flink-程序"><a href="#Flink-程序" class="headerlink" title="Flink 程序"></a>Flink 程序</h4><p>这里的 source 是从 Kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 Student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; JSON.parseObject(string, Student.class)); <span class="comment">//Fastjson 解析字符串成 student 对象</span></span><br><span class="line"></span><br><span class="line">        student.addSink(<span class="keyword">new</span> SinkToMySQL()); <span class="comment">//数据 sink 到 mysql</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add sink"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h3 id="3-8-4-RichSinkFunction-的用法及源码分析"><a href="#3-8-4-RichSinkFunction-的用法及源码分析" class="headerlink" title="3.8.4 RichSinkFunction 的用法及源码分析"></a>3.8.4 RichSinkFunction 的用法及源码分析</h3><h3 id="3-8-5-小结与反思"><a href="#3-8-5-小结与反思" class="headerlink" title="3.8.5 小结与反思"></a>3.8.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/Y3RBaaQ">https://t.zsxq.com/Y3RBaaQ</a></p><p>批量写 MySQL 可以参考 ：<a href="https://t.zsxq.com/FAmYFYJ">https://t.zsxq.com/FAmYFYJ</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-8-自定义-Flink-Connector&quot;&gt;&lt;a href=&quot;#3-8-自定义-Flink-Connector&quot; class=&quot;headerlink&quot; title=&quot;3.8 自定义 Flink Connector&quot;&gt;&lt;/a&gt;3.8 自定义 Flink Connector&lt;/h2&gt;&lt;p&gt;在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink Connector —— Kafka 的使用和源码分析</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/17/flink-in-action-3.7/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/17/flink-in-action-3.7/</id>
    <published>2021-07-16T16:00:00.000Z</published>
    <updated>2021-12-11T07:10:06.981Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-7-Flink-Connector-——-Kafka-的使用和源码分析"><a href="#3-7-Flink-Connector-——-Kafka-的使用和源码分析" class="headerlink" title="3.7 Flink Connector —— Kafka 的使用和源码分析"></a>3.7 Flink Connector —— Kafka 的使用和源码分析</h2><p>在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去，整个案例的执行流程如下图所示。</p><a id="more"></a><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-101054.png" alt=""></p><h3 id="3-7-1-准备环境和依赖"><a href="#3-7-1-准备环境和依赖" class="headerlink" title="3.7.1 准备环境和依赖"></a>3.7.1 准备环境和依赖</h3><p>接下来准备 Kafka 环境的安装和添加相关的依赖。</p><h4 id="环境安装和启动"><a href="#环境安装和启动" class="headerlink" title="环境安装和启动"></a>环境安装和启动</h4><p>如果你已经安装好了 Flink 和 Kafka，那么接下来使用命令运行启动 Flink、Zookepeer、Kafka 就行了。</p><p>启动 Flink 的命令如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-042714.png" alt="启动 Flink"></p><p>启动 Kafka 的命令如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-142523.png" alt="启动 Kafka"></p><p>执行命令都启动好了后就可以添加依赖了。</p><h4 id="添加-Maven-依赖"><a href="#添加-Maven-依赖" class="headerlink" title="添加 Maven 依赖"></a>添加 Maven 依赖</h4><p>Flink 里面支持 Kafka 0.8.x 以上的版本，具体采用哪个版本的 Maven 依赖需要根据安装的 Kafka 版本来确定。因为之前我们安装的 Kafka 是 1.1.0 版本，所以这里我们选择的 Kafka Connector 为 <code>flink-connector-kafka-0.11_2.11</code> （支持 Kafka 0.11.x 版本及以上，该 Connector 支持 Kafka 事务消息传递，所以能保证 Exactly Once）。Flink Kafka Connector 支持的版本如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-11-043040.png" alt=""></p><p>添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink、Kafka、Flink Kafka Connector 三者对应的版本可以根据 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html">官网</a> 的对比来选择。需要注意的是 <code>flink-connector-kafka_2.11</code> 这个版本支持的 Kafka 版本要大于 1.0.0，从 Flink 1.9 版本开始，它使用的是 Kafka 2.2.0 版本的客户端，虽然这些客户端会做向后兼容，但是建议还是按照官网约定的来规范使用 Connector 版本。另外你还要添加的依赖有：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--flink java--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--log--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--alibaba fastjson--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-7-2-将测试数据发送到-Kafka-Topic"><a href="#3-7-2-将测试数据发送到-Kafka-Topic" class="headerlink" title="3.7.2 将测试数据发送到 Kafka Topic"></a>3.7.2 将测试数据发送到 Kafka Topic</h3><p>我们模拟一些测试数据，然后将这些测试数据发到 Kafka Topic 中去，数据的结构如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Metric</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name; <span class="comment">//指标名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;  <span class="comment">//时间戳</span></span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, Object&gt; fields;  <span class="comment">//指标含有的属性</span></span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, String&gt; tags;    <span class="comment">//指标的标识</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>往 kafka 中写数据工具类 <code>KafkaUtils.java</code>，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"metric"</span>;  <span class="comment">// kafka topic，Flink 程序中需要和这个统一 </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//key 序列化</span></span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//value 序列化</span></span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        Metric metric = <span class="keyword">new</span> Metric();</span><br><span class="line">        metric.setTimestamp(System.currentTimeMillis());</span><br><span class="line">        metric.setName(<span class="string">"mem"</span>);</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        tags.put(<span class="string">"cluster"</span>, <span class="string">"zhisheng"</span>);</span><br><span class="line">        tags.put(<span class="string">"host_ip"</span>, <span class="string">"101.147.022.106"</span>);</span><br><span class="line"></span><br><span class="line">        fields.put(<span class="string">"used_percent"</span>, <span class="number">90</span>d);</span><br><span class="line">        fields.put(<span class="string">"max"</span>, <span class="number">27244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"used"</span>, <span class="number">17244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"init"</span>, <span class="number">27244873</span>d);</span><br><span class="line"></span><br><span class="line">        metric.setTags(tags);</span><br><span class="line">        metric.setFields(fields);</span><br><span class="line"></span><br><span class="line">        ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(metric));</span><br><span class="line">        producer.send(record);</span><br><span class="line">        System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(metric));</span><br><span class="line"></span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">300</span>);</span><br><span class="line">            writeToKafka();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-101504.png" alt=""></p><p>如果出现如上图标记的，即代表能够不断往 kafka 发送数据的。</p><h3 id="3-7-3-Flink-如何消费-Kafka-数据？"><a href="#3-7-3-Flink-如何消费-Kafka-数据？" class="headerlink" title="3.7.3 Flink 如何消费 Kafka 数据？"></a>3.7.3 Flink 如何消费 Kafka 数据？</h3><p>Flink 消费 Kafka 数据的应用程序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);  <span class="comment">//key 反序列化</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>); <span class="comment">//value 反序列化</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"metric"</span>,  <span class="comment">//kafka topic</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),  <span class="comment">// String 序列化</span></span><br><span class="line">                props)).setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        dataStreamSource.print(); <span class="comment">//把从 kafka 读取到的数据打印在控制台</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data source"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下图所示（程序可以不断的消费到 Kafka Topic 中的数据）：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-101832.png" alt=""></p><p><strong>代码分析</strong></p><p>使用 FlinkKafkaConsumer011 时传入了三个参数：</p><ul><li>Kafka topic：这个代表了 Flink 要消费的是 Kafka 哪个 Topic，如果你要同时消费多个 Topic 的话，那么你可以传入一个 Topic List 进去，另外也支持正则表达式匹配 Topic，源码如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-102157.png" alt=""></p><ul><li><p>序列化：上面代码我们使用的是 SimpleStringSchema。</p></li><li><p>配置属性：将 Kafka 等的一些配置传入 。</p></li></ul><p>前面演示了 Flink 如何消费 Kafak 数据，接下来演示如何把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p><h3 id="3-7-4-Flink-如何将计算后的数据发送到-Kafka？"><a href="#3-7-4-Flink-如何将计算后的数据发送到-Kafka？" class="headerlink" title="3.7.4 Flink 如何将计算后的数据发送到 Kafka？"></a>3.7.4 Flink 如何将计算后的数据发送到 Kafka？</h3><p>将 Kafka 集群中 topic 数据写入本地 Kafka 的程序中要填写的配置有消费的 Kafka 集群地址、group.id、将数据写入 Kafka 的集群地址、topic 信息等，将所有的配置提取到配置文件中，如下所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//其他 Kafka 集群配置</span><br><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>执行上面命令后的结果如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6KFHKT.jpg" alt=""></p><p>可以看到本地的 Kafka 是没有任何 topic 的，如果等下程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。</p><p>整个 Flink 程序的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动程序，查看运行结果，不断执行查看 topic 列表的命令，观察是否有新的 topic 出来，结果如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nxqZmZ.jpg" alt=""></p><p>执行命令可以查看该 topic 的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure><p>该 topic 信息如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/y5vPRR.jpg" alt=""></p><p>前面代码使用的 FlinkKafkaProducer011 只传了三个参数：brokerList、topicId、serializationSchema（序列化），其实是支持传入多个参数的，Flink 中的源码如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-102620.png" alt=""></p><h3 id="3-7-5-FlinkKafkaConsumer-源码分析"><a href="#3-7-5-FlinkKafkaConsumer-源码分析" class="headerlink" title="3.7.5 FlinkKafkaConsumer 源码分析"></a>3.7.5 FlinkKafkaConsumer 源码分析</h3><h3 id="3-7-6-FlinkKafkaProducer-源码分析"><a href="#3-7-6-FlinkKafkaProducer-源码分析" class="headerlink" title="3.7.6 FlinkKafkaProducer 源码分析"></a>3.7.6 FlinkKafkaProducer 源码分析</h3><h3 id="3-7-7-使用-Flink-connector-kafka-可能会遇到的问题"><a href="#3-7-7-使用-Flink-connector-kafka-可能会遇到的问题" class="headerlink" title="3.7.7 使用 Flink-connector-kafka 可能会遇到的问题"></a>3.7.7 使用 Flink-connector-kafka 可能会遇到的问题</h3><h4 id="如何消费多个-Kafka-Topic"><a href="#如何消费多个-Kafka-Topic" class="headerlink" title="如何消费多个 Kafka Topic"></a>如何消费多个 Kafka Topic</h4><h4 id="想要获取数据的元数据信息"><a href="#想要获取数据的元数据信息" class="headerlink" title="想要获取数据的元数据信息"></a>想要获取数据的元数据信息</h4><h4 id="多种数据类型"><a href="#多种数据类型" class="headerlink" title="多种数据类型"></a>多种数据类型</h4><h4 id="序列化失败"><a href="#序列化失败" class="headerlink" title="序列化失败"></a>序列化失败</h4><h4 id="Kafka-消费-Offset-的选择"><a href="#Kafka-消费-Offset-的选择" class="headerlink" title="Kafka 消费 Offset 的选择"></a>Kafka 消费 Offset 的选择</h4><h4 id="如何自动发现-Topic-新增的分区并读取数据"><a href="#如何自动发现-Topic-新增的分区并读取数据" class="headerlink" title="如何自动发现 Topic 新增的分区并读取数据"></a>如何自动发现 Topic 新增的分区并读取数据</h4><h4 id="程序消费-Kafka-的-offset-是如何管理的"><a href="#程序消费-Kafka-的-offset-是如何管理的" class="headerlink" title="程序消费 Kafka 的 offset 是如何管理的"></a>程序消费 Kafka 的 offset 是如何管理的</h4><h3 id="3-7-8-小结与反思"><a href="#3-7-8-小结与反思" class="headerlink" title="3.7.8 小结与反思"></a>3.7.8 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/2bmurFy">https://t.zsxq.com/2bmurFy</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-7-Flink-Connector-——-Kafka-的使用和源码分析&quot;&gt;&lt;a href=&quot;#3-7-Flink-Connector-——-Kafka-的使用和源码分析&quot; class=&quot;headerlink&quot; title=&quot;3.7 Flink Connector —— Kafka 的使用和源码分析&quot;&gt;&lt;/a&gt;3.7 Flink Connector —— Kafka 的使用和源码分析&lt;/h2&gt;&lt;p&gt;在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去，整个案例的执行流程如下图所示。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 常用的 Source Connector 和 Sink Connector 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/16/flink-in-action-3.6/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/16/flink-in-action-3.6/</id>
    <published>2021-07-15T16:00:00.000Z</published>
    <updated>2021-12-11T07:06:00.727Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍"><a href="#3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍" class="headerlink" title="3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍"></a>3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍</h2><p>通过前面我们可以知道 Flink Job 的大致结构就是 <code>Source ——&gt; Transformation ——&gt; Sink</code>。</p><a id="more"></a><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-141653.png" alt=""></p><p>那么这个 Source 是什么意思呢？我们下面来看看。</p><h3 id="3-6-1-Data-Source-简介"><a href="#3-6-1-Data-Source-简介" class="headerlink" title="3.6.1 Data Source 简介"></a>3.6.1 Data Source 简介</h3><p>Data Source 是什么呢？就字面意思其实就可以知道：数据来源。</p><p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。</p><p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p><p>Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p><p>那么常用的 Data Source 有哪些呢？</p><h3 id="3-6-2-常用的-Data-Source"><a href="#3-6-2-常用的-Data-Source" class="headerlink" title="3.6.2 常用的 Data Source"></a>3.6.2 常用的 Data Source</h3><p>StreamExecutionEnvironment 中可以使用如下图所示的这些已实现的 Stream Source。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083744.png" alt=""></p><p>总的来说可以分为集合、文件、Socket、自定义四大类。</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><p>基于集合的有下面五种方法：</p><p>1、fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</p><p>2、fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</p><p>3、fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"barfoo"</span>, <span class="number">1.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"start"</span>, <span class="number">2.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"foobar"</span>, <span class="number">3.0</span>),</span><br><span class="line">...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>4、fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</p><p>5、generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</p><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>基于文件的有下面三种方法：</p><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">"file:///path/to/file"</span>);</span><br></pre></td></tr></table></figure><p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p><p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS_ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure><p><strong>实现:</strong></p><p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p><p><strong>重要注意：</strong></p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p><h4 id="基于-Socket"><a href="#基于-Socket" class="headerlink" title="基于 Socket"></a>基于 Socket</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h4><p>addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。</p><p><strong>说说上面几种的特点</strong></p><p>1、基于集合：有界数据集，更偏向于本地测试用</p><p>2、基于文件：适合监听文件修改并读取其内容</p><p>3、基于 Socket：监听主机的 host port，从 Socket 中获取数据</p><p>4、自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">.addSource(</span><br><span class="line"><span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">parameterTool.getRequired(<span class="string">"input-topic"</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line"><span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">parameterTool.getProperties())</span><br><span class="line">.assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure><p>Flink 目前支持的 Source 如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/UTfWCZ.jpg" alt=""></p><p>如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。</p><h3 id="3-6-3-Data-Sink-简介"><a href="#3-6-3-Data-Sink-简介" class="headerlink" title="3.6.3 Data Sink 简介"></a>3.6.3 Data Sink 简介</h3><h3 id="3-6-4-常用的-Data-Sink"><a href="#3-6-4-常用的-Data-Sink" class="headerlink" title="3.6.4 常用的 Data Sink"></a>3.6.4 常用的 Data Sink</h3><h3 id="3-6-5-小结与反思"><a href="#3-6-5-小结与反思" class="headerlink" title="3.6.5 小结与反思"></a>3.6.5 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/FAayJU3">https://t.zsxq.com/FAayJU3</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍&quot;&gt;&lt;a href=&quot;#3-6-Flink-常用的-Source-Connector-和-Sink-Connector-介绍&quot; class=&quot;headerlink&quot; title=&quot;3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍&quot;&gt;&lt;/a&gt;3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍&lt;/h2&gt;&lt;p&gt;通过前面我们可以知道 Flink Job 的大致结构就是 &lt;code&gt;Source ——&amp;gt; Transformation ——&amp;gt; Sink&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Watermark 的用法和结合 Window 处理延迟数据</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/15/flink-in-action-3.5/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/15/flink-in-action-3.5/</id>
    <published>2021-07-14T16:00:00.000Z</published>
    <updated>2021-12-11T07:03:11.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-5-Watermark-的用法和结合-Window-处理延迟数据"><a href="#3-5-Watermark-的用法和结合-Window-处理延迟数据" class="headerlink" title="3.5 Watermark 的用法和结合 Window 处理延迟数据"></a>3.5 Watermark 的用法和结合 Window 处理延迟数据</h2><p>在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</p><a id="more"></a><p>选择 Event Time 与 Process Time 的实际效果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-142659.png" alt=""></p><p>在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实却仍然这么骨感，会因为各种各样的问题（网络的抖动、设备的故障、应用的异常等原因）从而导致如图中曲线一样，Process Time 总是会与 Event Time 有一些延迟。所谓乱序，其实是指 Flink 接收到的事件的先后顺序并不是严格的按照事件的 Event Time 顺序排列的。如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-142742.png" alt=""></p><p>然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：</p><ul><li>错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</li><li>设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</li></ul><p>这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。庆幸的是 Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</p><h3 id="3-5-1-Watermark-简介"><a href="#3-5-1-Watermark-简介" class="headerlink" title="3.5.1 Watermark 简介"></a>3.5.1 Watermark 简介</h3><p>举个例子：</p><p>统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</p><p>Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。</p><p>下面通过几个图来了解一下 Watermark 是如何工作的！如下图所示，数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，<code>W(4)</code> 和 <code>W(9)</code> 是水印。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154340.jpg" alt=""></p><p>经过 Flink 的消费，数据 <code>1</code>、<code>3</code>、<code>2</code> 进入了第一个窗口，然后 <code>7</code> 会进入第二个窗口，接着 <code>3</code> 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 <code>4</code> 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154747.jpg" alt=""></p><p>那么接着后面的数据 <code>5</code> 和 <code>6</code> 会进入到第二个窗口里面，数据 <code>9</code> 会进入在第三个窗口里面，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155309.jpg" alt=""></p><p>那么当遇到水印 <code>9</code> 时，发现水印比第二个窗口的结束时间 <code>8</code> 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155558.jpg" alt=""></p><p>相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。</p><h3 id="3-5-2-Flink-中的-Watermark-的设置"><a href="#3-5-2-Flink-中的-Watermark-的设置" class="headerlink" title="3.5.2 Flink 中的 Watermark 的设置"></a>3.5.2 Flink 中的 Watermark 的设置</h3><p>在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以设置 Watermark 是有如下两种方式：</p><ul><li>AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。</li></ul><p>在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。</p><ul><li>AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</li></ul><p>在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。</p><p>下面再分别详细讲下这两种的实现方式。</p><h3 id="3-5-3-Punctuated-Watermark"><a href="#3-5-3-Punctuated-Watermark" class="headerlink" title="3.5.3 Punctuated Watermark"></a>3.5.3 Punctuated Watermark</h3><p>AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。</p><p>那么该怎么利用这个来定义水印生成器呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPunctuatedWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(Word lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> extractedTimestamp % <span class="number">3</span> == <span class="number">0</span> ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。</p><h3 id="3-5-4-Periodic-Watermark"><a href="#3-5-4-Periodic-Watermark" class="headerlink" title="3.5.4 Periodic Watermark"></a>3.5.4 Periodic Watermark</h3><p>通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPeriodicWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word word, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> timestamp = word.getTimestamp();</span><br><span class="line">        currentTimestamp = Math.max(timestamp, currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> word.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> maxTimeLag = <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，然后将当前时间戳与事件时间进行比较，取最大值后赋值给当前时间戳 currentTimestamp，然后返回事件时间。getCurrentWatermark() 方法是获取当前的水位线，通过 <code>currentTimestamp - maxTimeLag</code> 得到水印的值，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 <code>long maxTimeLag = 5000;</code> 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。</p><p>通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。</p><p>AssignerWithPeriodicWatermarks 这个接口有四个实现类，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082804.png" alt=""></p><p>这四个实现类的功能和使用方式如下：</p><ul><li>BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下图所示：</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083043.png" alt=""></p><p>你可以像下面一样使用该类来分配时间和生成水印：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Time.seconds(10) 代表允许延迟的时间大小</span></span><br><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">    <span class="comment">//重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li><p>CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。</p></li><li><p>AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">    <span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">        <span class="keyword">this</span>.currentTimestamp = ne∏wTimestamp;</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 <code>System.currentTimeMillis()</code> 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。</li></ul><p><strong>注意</strong>：</p><p>1、使用这种方式周期性生成水印的话，你可以通过 <code>env.getConfig().setAutoWatermarkInterval(...);</code> 来设置生成水印的间隔（每隔 n 毫秒）。</p><p>2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;MyType&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">while</span> (<span class="comment">/* condition */</span>) &#123;</span><br><span class="line">MyType next = getNext();</span><br><span class="line">ctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class="line">ctx.emitWatermark(<span class="keyword">new</span> Watermark(next.getWatermarkTime()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-5-每个-Kafka-分区的时间戳"><a href="#3-5-5-每个-Kafka-分区的时间戳" class="headerlink" title="3.5.5 每个 Kafka 分区的时间戳"></a>3.5.5 每个 Kafka 分区的时间戳</h3><h3 id="3-5-6-将-Watermark-与-Window-结合起来处理延迟数据"><a href="#3-5-6-将-Watermark-与-Window-结合起来处理延迟数据" class="headerlink" title="3.5.6 将 Watermark 与 Window 结合起来处理延迟数据"></a>3.5.6 将 Watermark 与 Window 结合起来处理延迟数据</h3><h3 id="3-5-7-处理延迟数据的三种方法"><a href="#3-5-7-处理延迟数据的三种方法" class="headerlink" title="3.5.7 处理延迟数据的三种方法"></a>3.5.7 处理延迟数据的三种方法</h3><h4 id="丢弃（默认）"><a href="#丢弃（默认）" class="headerlink" title="丢弃（默认）"></a>丢弃（默认）</h4><h4 id="allowedLateness-再次指定允许数据延迟的时间"><a href="#allowedLateness-再次指定允许数据延迟的时间" class="headerlink" title="allowedLateness 再次指定允许数据延迟的时间"></a>allowedLateness 再次指定允许数据延迟的时间</h4><h4 id="sideOutputLateData-收集迟到的数据"><a href="#sideOutputLateData-收集迟到的数据" class="headerlink" title="sideOutputLateData 收集迟到的数据"></a>sideOutputLateData 收集迟到的数据</h4><h3 id="3-5-8-小结与反思"><a href="#3-5-8-小结与反思" class="headerlink" title="3.5.8 小结与反思"></a>3.5.8 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/RbufeIA">https://t.zsxq.com/RbufeIA</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-5-Watermark-的用法和结合-Window-处理延迟数据&quot;&gt;&lt;a href=&quot;#3-5-Watermark-的用法和结合-Window-处理延迟数据&quot; class=&quot;headerlink&quot; title=&quot;3.5 Watermark 的用法和结合 Window 处理延迟数据&quot;&gt;&lt;/a&gt;3.5 Watermark 的用法和结合 Window 处理延迟数据&lt;/h2&gt;&lt;p&gt;在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp;amp; 事件延迟。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 使用 DataStream API 来处理数据</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/14/flink-in-action-3.4/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/14/flink-in-action-3.4/</id>
    <published>2021-07-13T16:00:00.000Z</published>
    <updated>2021-12-11T06:59:41.976Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-4-使用-DataStream-API-来处理数据"><a href="#3-4-使用-DataStream-API-来处理数据" class="headerlink" title="3.4 使用 DataStream API 来处理数据"></a>3.4 使用 DataStream API 来处理数据</h2><p>在 3.3 节中讲了数据转换常用的 Operators（算子），然后在 3.2 节中也讲了 Flink 中窗口的概念和原理，那么我们这篇文章再来细讲一下 Flink 中的各种 DataStream API。</p><a id="more"></a><p>我们先来看下源码里面的 DataStream 大概有哪些类呢？如下图所示，展示了 1.9 版本中的 DataStream 类。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-080701.png" alt=""></p><p>可以发现其实还是有很多的类，只有熟练掌握了这些 API，我们才能在做数据转换和计算的时候足够灵活的运用开来（知道何时该选用哪种 DataStream？选用哪个 Function？）。那么我们先从 DataStream 开始吧！</p><h3 id="3-4-1-DataStream-的用法及分析"><a href="#3-4-1-DataStream-的用法及分析" class="headerlink" title="3.4.1 DataStream 的用法及分析"></a>3.4.1 DataStream 的用法及分析</h3><p>首先我们来看下 DataStream 这个类的定义吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A DataStream represents a stream of elements of the same type. A DataStreamcan be transformed into another DataStream by applying a transformation as</span><br><span class="line"> DataStream#map or DataStream#filter&#125;</span><br></pre></td></tr></table></figure><p>大概意思是：DataStream 表示相同类型的元素组成的数据流，一个数据流可以通过 map/filter 等算子转换成另一个数据流。</p><p>然后 DataStream 的类结构图如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082134.png" alt=""></p><p>它的继承类有 KeyedStream、SingleOutputStreamOperator 和 SplitStream。这几个类本文后面都会一一给大家讲清楚。下面我们来看看 DataStream 这个类中的属性和方法吧。</p><p>它的属性就只有两个：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> StreamExecutionEnvironment environment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> StreamTransformation&lt;T&gt; transformation;</span><br></pre></td></tr></table></figure><p>但是它的方法却有很多，并且我们平时写的 Flink Job 几乎离不开这些方法，这也注定了这个类的重要性，所以得好好看下这些方法该如何使用，以及是如何实现的。</p><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>通过合并相同数据类型的数据流，然后创建一个新的数据流，union 方法代码实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> DataStream&lt;T&gt; <span class="title">union</span><span class="params">(DataStream&lt;T&gt;... streams)</span> </span>&#123;</span><br><span class="line">List&lt;StreamTransformation&lt;T&gt;&gt; unionedTransforms = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">unionedTransforms.add(<span class="keyword">this</span>.transformation);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (DataStream&lt;T&gt; newStream : streams) &#123;</span><br><span class="line"><span class="keyword">if</span> (!getType().equals(newStream.getType())) &#123;<span class="comment">//判断数据类型是否一致</span></span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot union streams of different types: "</span> + getType() + <span class="string">" and "</span> + newStream.getType());</span><br><span class="line">&#125;</span><br><span class="line">unionedTransforms.add(newStream.getTransformation());</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//构建新的数据流</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> DataStream&lt;&gt;(<span class="keyword">this</span>.environment, <span class="keyword">new</span> UnionTransformation&lt;&gt;(unionedTransforms));<span class="comment">//通过使用 UnionTransformation 将多个 StreamTransformation 合并起来</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么我们该如何去使用 union 呢（不止连接一个数据流，也可以连接多个数据流）？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//数据流 1 和 2</span></span><br><span class="line"><span class="keyword">final</span> DataStream&lt;Integer&gt; stream1 = env.addSource(...);</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;Integer&gt; stream2 = env.addSource(...);</span><br><span class="line"><span class="comment">//union</span></span><br><span class="line">stream1.union(stream2)</span><br></pre></td></tr></table></figure><h4 id="split"><a href="#split" class="headerlink" title="split"></a>split</h4><p>该方法可以将两个数据流进行拆分，拆分后的数据流变成了 SplitStream（在下文会详细介绍这个类的内部实现），该 split 方法通过传入一个 OutputSelector 参数进行数据选择，方法内部实现就是构造一个 SplitStream 对象然后返回：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SplitStream&lt;T&gt; <span class="title">split</span><span class="params">(OutputSelector&lt;T&gt; outputSelector)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> SplitStream&lt;&gt;(<span class="keyword">this</span>, clean(outputSelector));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后我们该如何使用这个方法呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dataStream.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">8354166915727490130L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">List&lt;String&gt; s = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"><span class="keyword">if</span> (value &gt; <span class="number">4</span>) &#123;<span class="comment">//大于 4 的数据放到 &gt; 这个 tag 里面去</span></span><br><span class="line">s.add(<span class="string">"&gt;"</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;<span class="comment">//小于等于 4 的数据放到 &lt; 这个 tag 里面去</span></span><br><span class="line">s.add(<span class="string">"&lt;"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>注意：该方法已经不推荐使用了！在 1.7 版本以后建议使用 Side Output 来实现分流操作。</p><h4 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h4><p>通过连接不同或相同数据类型的数据流，然后创建一个新的连接数据流，如果连接的数据流也是一个 DataStream 的话，那么连接后的数据流为 ConnectedStreams（会在下文介绍这个类的具体实现），它的具体实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">ConnectedStreams&lt;T, R&gt; <span class="title">connect</span><span class="params">(DataStream&lt;R&gt; dataStream)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> ConnectedStreams&lt;&gt;(environment, <span class="keyword">this</span>, dataStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果连接的数据流是一个 BroadcastStream（广播数据流），那么连接后的数据流是一个 BroadcastConnectedStream（会在下文详细介绍该类的内部实现），它的具体实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">BroadcastConnectedStream&lt;T, R&gt; <span class="title">connect</span><span class="params">(BroadcastStream&lt;R&gt; broadcastStream)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> BroadcastConnectedStream&lt;&gt;(</span><br><span class="line">environment, <span class="keyword">this</span>, Preconditions.checkNotNull(broadcastStream), </span><br><span class="line">broadcastStream.getBroadcastStateDescriptor());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、连接 DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、连接 BroadcastStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line"><span class="keyword">final</span> BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);</span><br><span class="line">BroadcastConnectedStream&lt;Tuple2&lt;Long, Long&gt;, String&gt; connect = src1.connect(broadcast);</span><br></pre></td></tr></table></figure><h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h4><p>keyBy 方法是用来将数据进行分组的，通过该方法可以将具有相同 key 的数据划分在一起组成新的数据流，该方法有四种（它们的参数各不一样）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、参数是 KeySelector 对象</span></span><br><span class="line"><span class="keyword">public</span> &lt;K&gt; <span class="function">KeyedStream&lt;T, K&gt; <span class="title">keyBy</span><span class="params">(KeySelector&lt;T, K&gt; key)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(key));<span class="comment">//构造 KeyedStream 对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、参数是 KeySelector 对象和 TypeInformation 对象</span></span><br><span class="line"><span class="keyword">public</span> &lt;K&gt; <span class="function">KeyedStream&lt;T, K&gt; <span class="title">keyBy</span><span class="params">(KeySelector&lt;T, K&gt; key, TypeInformation&lt;K&gt; keyType)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(key), keyType);<span class="comment">//构造 KeyedStream 对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、参数是 1 至多个字段（用 0、1、2... 表示）</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(<span class="keyword">int</span>... fields)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (getType() <span class="keyword">instanceof</span> BasicArrayTypeInfo || getType() <span class="keyword">instanceof</span> PrimitiveArrayTypeInfo) &#123;</span><br><span class="line"><span class="keyword">return</span> keyBy(KeySelectorUtil.getSelectorForArray(fields, getType()));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> keyBy(<span class="keyword">new</span> Keys.ExpressionKeys&lt;&gt;(fields, getType()));<span class="comment">//调用 private 的 keyBy 方法</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4、参数是 1 至多个字符串</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(String... fields)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> keyBy(<span class="keyword">new</span> Keys.ExpressionKeys&lt;&gt;(fields, getType()));<span class="comment">//调用 private 的 keyBy 方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//真正调用的方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(Keys&lt;T&gt; keys)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(KeySelectorUtil.getSelectorForKeys(keys,</span><br><span class="line">getType(), getExecutionConfig())));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如何使用呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Event&gt; dataStream = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"zhisheng01"</span>, <span class="number">1.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"zhisheng02"</span>, <span class="number">2.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"zhisheng03"</span>, <span class="number">2.1</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"zhisheng04"</span>, <span class="number">3.0</span>),</span><br><span class="line"><span class="keyword">new</span> SubEvent(<span class="number">4</span>, <span class="string">"zhisheng05"</span>, <span class="number">4.0</span>, <span class="number">1.0</span>),</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第1种</span></span><br><span class="line">dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value.getId();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第2种</span></span><br><span class="line">dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value.getId();</span><br><span class="line">&#125;</span><br><span class="line">&#125;, Types.STRING);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第3种</span></span><br><span class="line">dataStream.keyBy(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第4种</span></span><br><span class="line">dataStream.keyBy(<span class="string">"zhisheng01"</span>, <span class="string">"zhisheng02"</span>);</span><br></pre></td></tr></table></figure><h4 id="partitionCustom"><a href="#partitionCustom" class="headerlink" title="partitionCustom"></a>partitionCustom</h4><p>使用自定义分区器在指定的 key 字段上将 DataStream 分区，这个 partitionCustom 有 3 个不同参数的方法，分别要传入的参数有自定义分区 Partitioner 对象、位置、字符和 KeySelector。它们内部也都是调用了私有的 partitionCustom 方法。</p><h4 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h4><p>broadcast 是将数据流进行广播，然后让下游的每个并行 Task 中都可以获取到这份数据流，通常这些数据是一些配置，一般这些配置数据的数据量不能太大，否则资源消耗会比较大。这个 broadcast 方法也有两个，一个是无参数，它返回的数据是 DataStream；另一种的参数是 MapStateDescriptor，它返回的参数是 BroadcastStream（这个也会在下文详细介绍）。</p><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、第一种</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; source = env.addSource(...).broadcast();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、第二种</span></span><br><span class="line"><span class="keyword">final</span> MapStateDescriptor&lt;Long, String&gt; utterDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line"><span class="string">"broadcast-state"</span>, BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO</span><br><span class="line">);</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; srcTwo = env.fromCollection(expected.values());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);</span><br></pre></td></tr></table></figure><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>map 方法需要传入的参数是一个 MapFunction，当然传入 RichMapFunction 也是可以的，它返回的是 SingleOutputStreamOperator（这个类在会在下文详细介绍），该 map 方法里面的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span><span class="params">(MapFunction&lt;T, R&gt; mapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),</span><br><span class="line">Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//调用 transform 方法</span></span><br><span class="line"><span class="keyword">return</span> transform(<span class="string">"Map"</span>, outType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法平时使用的非常频繁，然后我们该如何使用这个方法呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map(<span class="keyword">new</span> MapFunction&lt;Integer, String&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value.toString();</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>flatMap 方法需要传入一个 FlatMapFunction 参数，当然传入 RichFlatMapFunction 也是可以的，如果你的 Flink Job 里面有连续的 filter 和 map 算子在一起，可以考虑使用 flatMap 一个算子来完成两个算子的工作，它返回的是 SingleOutputStreamOperator，该 flatMap 方法里面的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">flatMap</span><span class="params">(FlatMapFunction&lt;T, R&gt; flatMapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper),</span><br><span class="line">getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//调用 transform 方法</span></span><br><span class="line"><span class="keyword">return</span> transform(<span class="string">"Flat Map"</span>, outType, <span class="keyword">new</span> StreamFlatMap&lt;&gt;(clean(flatMapper)));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法平时使用的非常频繁，使用方式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Integer value, Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">out.collect(value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="process"><a href="#process" class="headerlink" title="process"></a>process</h4><p>在输入流上应用给定的 ProcessFunction，从而创建转换后的输出流，通过该方法返回的是 SingleOutputStreamOperator，具体代码实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">process</span><span class="params">(ProcessFunction&lt;T, R&gt; processFunction)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">TypeInformation&lt;R&gt; outType = TypeExtractor.getUnaryOperatorReturnType(</span><br><span class="line">processFunction, ProcessFunction.class, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">TypeExtractor.NO_INDEX, getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//调用下面的 process 方法</span></span><br><span class="line"><span class="keyword">return</span> process(processFunction, outType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">ProcessFunction&lt;T, R&gt; processFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">TypeInformation&lt;R&gt; outputType)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">ProcessOperator&lt;T, R&gt; operator = <span class="keyword">new</span> ProcessOperator&lt;&gt;(clean(processFunction));</span><br><span class="line"><span class="comment">//调用 transform 方法</span></span><br><span class="line"><span class="keyword">return</span> transform(<span class="string">"Process"</span>, outputType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Long&gt; data = env.generateSequence(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义的 ProcessFunction</span></span><br><span class="line">ProcessFunction&lt;Long, Integer&gt; processFunction = <span class="keyword">new</span> ProcessFunction&lt;Long, Integer&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Long value, Context ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//具体逻辑</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//具体逻辑</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; processed = data.keyBy(<span class="keyword">new</span> IdentityKeySelector&lt;Long&gt;()).process(processFunction);</span><br></pre></td></tr></table></figure><h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>filter 用来过滤数据的，它需要传入一个 FilterFunction，然后返回的数据也是 SingleOutputStreamOperator，该方法的实现是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">filter</span><span class="params">(FilterFunction&lt;T&gt; filter)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> transform(<span class="string">"Filter"</span>, getType(), <span class="keyword">new</span> StreamFilter&lt;&gt;(clean(filter)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法平时使用非常多：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; filter1 = src</span><br><span class="line">.filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"zhisheng"</span>.equals(value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>上面这些方法是平时写代码时用的非常多的方法，我们这里讲解了它们的实现原理和使用方式，当然还有其他方法，比如 assignTimestampsAndWatermarks、join、shuffle、forward、addSink、rebalance、iterate、coGroup、project、timeWindowAll、countWindowAll、windowAll、print 等，这里由于篇幅的问题就不一一展开来讲了。</p><h3 id="3-4-2-SingleOutputStreamOperator-的用法及分析"><a href="#3-4-2-SingleOutputStreamOperator-的用法及分析" class="headerlink" title="3.4.2 SingleOutputStreamOperator 的用法及分析"></a>3.4.2 SingleOutputStreamOperator 的用法及分析</h3><p>SingleOutputStreamOperator 这个类继承自 DataStream，所以 DataStream 中有的方法在这里也都有，那么这里就讲解下额外的方法的作用，如下。</p><ul><li>name()：该方法可以设置当前数据流的名称，如果设置了该值，则可以在 Flink UI 上看到该值；uid() 方法可以为算子设置一个指定的 ID，该 ID 有个作用就是如果想从 savepoint 恢复 Job 时是可以根据这个算子的 ID 来恢复到它之前的运行状态；</li><li>setParallelism() ：该方法是为每个算子单独设置并行度的，这个设置优先于你通过 env 设置的全局并行度；</li><li>setMaxParallelism() ：该为算子设置最大的并行度；</li><li>setResources()：该方法有两个（参数不同），设置算子的资源，但是这两个方法对外还没开放（是私有的，暂时功能性还不全）；</li><li>forceNonParallel()：该方法强行将并行度和最大并行度都设置为 1；</li><li>setChainingStrategy()：该方法对给定的算子设置 ChainingStrategy；</li><li>disableChaining()：该这个方法设置后将禁止该算子与其他的算子 chain 在一起；</li><li>getSideOutput()：该方法通过给定的 OutputTag 参数从 side output 中来筛选出对应的数据流。</li></ul><h3 id="3-4-3-KeyedStream-的用法及分析"><a href="#3-4-3-KeyedStream-的用法及分析" class="headerlink" title="3.4.3 KeyedStream 的用法及分析"></a>3.4.3 KeyedStream 的用法及分析</h3><p>KeyedStream 是 DataStream 在根据 KeySelector 分区后的数据流，DataStream 中常用的方法在 KeyedStream 后也可以用（除了 shuffle、forward 和 keyBy 等分区方法），在该类中的属性分别是 KeySelector 和 TypeInformation。</p><p>DataStream 中的窗口方法只有 timeWindowAll、countWindowAll 和 windowAll 这三种全局窗口方法，但是在 KeyedStream 类中的种类就稍微多了些，新增了 timeWindow、countWindow 方法，并且是还支持滑动窗口。</p><p>除了窗口方法的新增外，还支持大量的聚合操作方法，比如 reduce、fold、sum、min、max、minBy、maxBy、aggregate 等方法（列举的这几个方法都支持多种参数的）。</p><p>最后就是它还有 asQueryableState() 方法，能够将 KeyedStream 发布为可查询的 ValueState 实例。</p><h3 id="3-4-4-SplitStream-的用法及分析"><a href="#3-4-4-SplitStream-的用法及分析" class="headerlink" title="3.4.4 SplitStream 的用法及分析"></a>3.4.4 SplitStream 的用法及分析</h3><p>SplitStream 这个类比较简单，它代表着数据分流后的数据流了，它有一个 select 方法可以选择分流后的哪种数据流了，通常它是结合 split 使用的，对于单次分流来说还挺方便的。但是它是一个被废弃的类（Flink 1.7 后被废弃的，可以看下笔者之前写的一篇文章 <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ），其实可以用 side output 来代替这种 split，后面文章中我们也会讲通过简单的案例来讲解一下该如何使用 side output 做数据分流操作。</p><p>因为这个类的源码比较少，我们可以看下这个类的实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStream</span>&lt;<span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">DataStream</span>&lt;<span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">SplitStream</span><span class="params">(DataStream&lt;OUT&gt; dataStream, OutputSelector&lt;OUT&gt; outputSelector)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>(dataStream.getExecutionEnvironment(), <span class="keyword">new</span> SplitTransformation&lt;OUT&gt;(dataStream.getTransformation(), outputSelector));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//选择要输出哪种数据流</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;OUT&gt; <span class="title">select</span><span class="params">(String... outputNames)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> selectOutput(outputNames);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//上面那个 public 方法内部调用的就是这个方法，该方法是个 private 方法，对外隐藏了它是如何去找到特定的数据流。</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> DataStream&lt;OUT&gt; <span class="title">selectOutput</span><span class="params">(String[] outputNames)</span> </span>&#123;</span><br><span class="line"><span class="keyword">for</span> (String outName : outputNames) &#123;</span><br><span class="line"><span class="keyword">if</span> (outName == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Selected names must not be null"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">//构造了一个 SelectTransformation 对象</span></span><br><span class="line">SelectTransformation&lt;OUT&gt; selectTransform = <span class="keyword">new</span> SelectTransformation&lt;OUT&gt;(<span class="keyword">this</span>.getTransformation(), Lists.newArrayList(outputNames));</span><br><span class="line"><span class="comment">//构造了一个 DataStream 对象</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> DataStream&lt;OUT&gt;(<span class="keyword">this</span>.getExecutionEnvironment(), selectTransform);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-5-WindowedStream-的用法及分析"><a href="#3-4-5-WindowedStream-的用法及分析" class="headerlink" title="3.4.5 WindowedStream 的用法及分析"></a>3.4.5 WindowedStream 的用法及分析</h3><p>虽然 WindowedStream 不是继承自 DataStream，并且我们在 3.1 节中也做了一定的讲解，但是当时没讲里面的 Function，所以在这里刚好一起做一个补充。</p><p>在 WindowedStream 类中定义的属性有 KeyedStream、WindowAssigner、Trigger、Evictor、allowedLateness 和 lateDataOutputTag。</p><ul><li>KeyedStream：代表着数据流，数据分组后再开 Window</li><li>WindowAssigner：Window 的组件之一</li><li>Trigger：Window 的组件之一</li><li>Evictor：Window 的组件之一（可选）</li><li>allowedLateness：用户指定的允许迟到时间长</li><li>lateDataOutputTag：数据延迟到达的 Side output，如果延迟数据没有设置任何标记，则会被丢弃</li></ul><p>在 3.1 节中我们讲了上面的三个窗口组件 WindowAssigner、Trigger、Evictor，并教大家该如何使用，那么在这篇文章我就不再重复，那么接下来就来分析下其他几个的使用方式和其实现原理。</p><p>先来看下 allowedLateness 这个它可以在窗口后指定允许迟到的时间长，使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>)</span><br><span class="line">.timeWindow(Time.milliseconds(<span class="number">20</span>))</span><br><span class="line">.allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>lateDataOutputTag 这个它将延迟到达的数据发送到由给定 OutputTag 标识的 side output（侧输出），当水印经过窗口末尾（并加上了允许的延迟后），数据就被认为是延迟了。</p><p>对于 keyed windows 有五个不同参数的 reduce 方法可以使用，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、参数为 ReduceFunction</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; function)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> reduce(function, <span class="keyword">new</span> PassThroughWindowFunction&lt;K, W, T&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、参数为 ReduceFunction 和 WindowFunction</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> reduce(reduceFunction, function, resultType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、参数为 ReduceFunction、WindowFunction 和 TypeInformation</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> input.transform(opName, resultType, operator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4、参数为 ReduceFunction 和 ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> reduce(reduceFunction, function, resultType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//5、参数为 ReduceFunction、ProcessWindowFunction 和 TypeInformation</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> input.transform(opName, resultType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除了 reduce 方法，还有六个不同参数的 fold 方法、aggregate 方法；两个不同参数的 apply 方法、process 方法（其中你会发现这两个 apply 方法和 process 方法内部其实都隐式的调用了一个私有的 apply 方法）；其实除了前面说的两个不同参数的 apply 方法外，还有四个其他的 apply 方法，这四个方法也是参数不同，但是其实最终的是利用了 transform 方法；还有的就是一些预定义的聚合方法比如 sum、min、minBy、max、maxBy，它们的方法参数的个数不一致，这些预聚合的方法内部调用的其实都是私有的 aggregate 方法，该方法允许你传入一个 AggregationFunction 参数。我们来看一个具体的实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//max</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">max</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line"><span class="comment">//内部调用私有的的 aggregate 方法</span></span><br><span class="line"><span class="keyword">return</span> aggregate(<span class="keyword">new</span> ComparableAggregator&lt;&gt;(field, input.getType(), AggregationFunction.AggregationType.MAX, <span class="keyword">false</span>, input.getExecutionConfig()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的 aggregate 方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">aggregate</span><span class="params">(AggregationFunction&lt;T&gt; aggregator)</span> </span>&#123;</span><br><span class="line"><span class="comment">//继续调用的是 reduce 方法</span></span><br><span class="line"><span class="keyword">return</span> reduce(aggregator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//该 reduce 方法内部其实又是调用了其他多个参数的 reduce 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; function)</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line">function = input.getExecutionEnvironment().clean(function);</span><br><span class="line"><span class="keyword">return</span> reduce(function, <span class="keyword">new</span> PassThroughWindowFunction&lt;K, W, T&gt;());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的方法调用过程，你会发现代码封装的很深，得需要你自己好好跟一下源码才可以了解更深些。</p><p>上面讲了这么多方法，你会发现 reduce 方法其实是用的蛮多的之一，那么就来看看该如何使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">.reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span>  </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value1;</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print();</span><br></pre></td></tr></table></figure><h3 id="3-4-6-AllWindowedStream-的用法及分析"><a href="#3-4-6-AllWindowedStream-的用法及分析" class="headerlink" title="3.4.6 AllWindowedStream 的用法及分析"></a>3.4.6 AllWindowedStream 的用法及分析</h3><p>前面讲完了 WindowedStream，再来看看这个 AllWindowedStream 你会发现它的实现其实无太大区别，该类中的属性和方法都和前面 WindowedStream 是一样的，然后我们就不再做过多的介绍，直接来看看该如何使用呢？</p><p>AllWindowedStream 这种场景下是不需要让数据流做 keyBy 分组操作，直接就进行 windowAll 操作，然后在 windowAll 方法中传入 WindowAssigner 参数对象即可，然后返回的数据结果就是 AllWindowedStream 了，下面使用方式继续执行了 AllWindowedStream 中的 reduce 方法来返回数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dataStream.windowAll(SlidingEventTimeWindows.of(Time.of(<span class="number">1</span>, TimeUnit.SECONDS), Time.of(<span class="number">100</span>, TimeUnit.MILLISECONDS)))</span><br><span class="line">.reduce(<span class="keyword">new</span> RichReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">6448847205314995812L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1,</span></span></span><br><span class="line"><span class="function"><span class="params">Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value1;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="3-4-7-ConnectedStreams-的用法及分析"><a href="#3-4-7-ConnectedStreams-的用法及分析" class="headerlink" title="3.4.7 ConnectedStreams 的用法及分析"></a>3.4.7 ConnectedStreams 的用法及分析</h3><p>ConnectedStreams 这个类定义是表示（可能）两个不同数据类型的数据连接流，该场景如果对一个数据流进行操作会直接影响另一个数据流，因此可以通过流连接来共享状态。比较常见的一个例子就是一个数据流（随时间变化的规则数据流）通过连接其他的数据流，这样另一个数据流就可以利用这些连接的规则数据流。</p><p>ConnectedStreams 在概念上可以认为和 Union 数据流是一样的。</p><p>在 ConnectedStreams 类中有三个属性：environment、inputStream1 和 inputStream2，该类中的方法如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082354.png" alt=""></p><p>在 ConnectedStreams 中可以通过 getFirstInput 获取连接的第一个流、通过 getSecondInput 获取连接的第二个流，同时它还含有六个 keyBy 方法来将连接后的数据流进行分组，这六个 keyBy 方法的参数各有不同。另外它还含有 map、flatMap、process 方法来处理数据（其中 map 和 flatMap 方法的参数分别使用的是 CoMapFunction 和 CoFlatMapFunction），其实如果你细看其方法里面的实现就会发现都是调用的 transform 方法。</p><p>上面讲完了 ConnectedStreams 类的基础定义，接下来我们来看下该类如何使用呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));<span class="comment">//流 1</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));<span class="comment">//流 2</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);<span class="comment">//连接流 1 和流 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的六种 keyBy 方法</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup1 = connected.keyBy(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup2 = connected.keyBy(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>&#125;, <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>&#125;);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup3 = connected.keyBy(<span class="string">"f0"</span>, <span class="string">"f0"</span>);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup4 = connected.keyBy(<span class="keyword">new</span> String[]&#123;<span class="string">"f0"</span>&#125;, <span class="keyword">new</span> String[]&#123;<span class="string">"f0"</span>&#125;);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(<span class="keyword">new</span> FirstSelector(), <span class="keyword">new</span> FirstSelector());</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(<span class="keyword">new</span> FirstSelector(), <span class="keyword">new</span> FirstSelector(), Types.STRING);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 map 方法</span></span><br><span class="line">connected.map(<span class="keyword">new</span> CoMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Object&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">map1</span><span class="params">(Tuple2&lt;Long, Long&gt; value)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">map2</span><span class="params">(Tuple2&lt;Long, Long&gt; value)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 flatMap 方法</span></span><br><span class="line">connected.flatMap(<span class="keyword">new</span> CoFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap1</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap2</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;).name(<span class="string">"testCoFlatMap"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 process 方法</span></span><br><span class="line">connected.process(<span class="keyword">new</span> CoProcessFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (value.f0 &lt; <span class="number">3</span>) &#123;</span><br><span class="line">out.collect(value);</span><br><span class="line">ctx.output(sideOutputTag, <span class="string">"sideout1-"</span> + String.valueOf(value));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (value.f0 &gt;= <span class="number">3</span>) &#123;</span><br><span class="line">out.collect(value);</span><br><span class="line">ctx.output(sideOutputTag, <span class="string">"sideout2-"</span> + String.valueOf(value));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="3-4-8-BroadcastStream-的用法及分析"><a href="#3-4-8-BroadcastStream-的用法及分析" class="headerlink" title="3.4.8 BroadcastStream 的用法及分析"></a>3.4.8 BroadcastStream 的用法及分析</h3><h3 id="3-4-9-BroadcastConnectedStream-的用法及分析"><a href="#3-4-9-BroadcastConnectedStream-的用法及分析" class="headerlink" title="3.4.9 BroadcastConnectedStream 的用法及分析"></a>3.4.9 BroadcastConnectedStream 的用法及分析</h3><h3 id="3-4-10-QueryableStateStream-的用法及分析"><a href="#3-4-10-QueryableStateStream-的用法及分析" class="headerlink" title="3.4.10 QueryableStateStream 的用法及分析"></a>3.4.10 QueryableStateStream 的用法及分析</h3><h3 id="3-4-11-小结与反思"><a href="#3-4-11-小结与反思" class="headerlink" title="3.4.11 小结与反思"></a>3.4.11 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/fy3RnMv">https://t.zsxq.com/fy3RnMv</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-4-使用-DataStream-API-来处理数据&quot;&gt;&lt;a href=&quot;#3-4-使用-DataStream-API-来处理数据&quot; class=&quot;headerlink&quot; title=&quot;3.4 使用 DataStream API 来处理数据&quot;&gt;&lt;/a&gt;3.4 使用 DataStream API 来处理数据&lt;/h2&gt;&lt;p&gt;在 3.3 节中讲了数据转换常用的 Operators（算子），然后在 3.2 节中也讲了 Flink 中窗口的概念和原理，那么我们这篇文章再来细讲一下 Flink 中的各种 DataStream API。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 数据转换必须熟悉的算子（Operator)</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/13/flink-in-action-3.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/13/flink-in-action-3.3/</id>
    <published>2021-07-12T16:00:00.000Z</published>
    <updated>2021-11-14T04:33:27.153Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-3-必须熟悉的数据转换-Operator-算子"><a href="#3-3-必须熟悉的数据转换-Operator-算子" class="headerlink" title="3.3 必须熟悉的数据转换 Operator(算子)"></a>3.3 必须熟悉的数据转换 Operator(算子)</h2><p>在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是上图这种模型，有数据源（source），有数据下游（sink），我们写的应用程序多是对数据源过来的数据做一系列操作，总结如下。</p><a id="more"></a><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-141653.png" alt=""></p><p>1、<strong>Source</strong>: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</p><p>2、<strong>Transformation</strong>: 数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p><p>3、<strong>Sink</strong>: 接收器，Sink 是指 Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来。Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 Socket 、自定义的 Sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。</p><p>那么本文将给大家介绍的就是 Flink 中的批和流程序常用的算子（Operator）。</p><h3 id="3-3-1-DataStream-Operator"><a href="#3-3-1-DataStream-Operator" class="headerlink" title="3.3.1 DataStream Operator"></a>3.3.1 DataStream Operator</h3><p>我们先来看看流程序中常用的算子。</p><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><p>Map 算子的输入流是 DataStream，经过 Map 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成一个元素，举个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; map = employeeStream.map(<span class="keyword">new</span> MapFunction&lt;Employee, Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Employee <span class="title">map</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        employee.salary = employee.salary + <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> employee;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">map.print();</span><br></pre></td></tr></table></figure><p>新的一年给每个员工的工资加 5000。</p><h4 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h4><p>FlatMap 算子的输入流是 DataStream，经过 FlatMap 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成零个、一个或多个元素，举个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; flatMap = employeeStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Employee, Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Employee employee, Collector&lt;Employee&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (employee.salary &gt;= <span class="number">40000</span>) &#123;</span><br><span class="line">            out.collect(employee);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">flatMap.print();</span><br></pre></td></tr></table></figure><p>将工资大于 40000 的找出来。</p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><h4 id="KeyBy"><a href="#KeyBy" class="headerlink" title="KeyBy"></a>KeyBy</h4><h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><h4 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h4><h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><h4 id="WindowAll"><a href="#WindowAll" class="headerlink" title="WindowAll"></a>WindowAll</h4><h4 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h4><h4 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a>Window Join</h4><h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><h4 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h4><h3 id="3-3-2-DataSet-Operator"><a href="#3-3-2-DataSet-Operator" class="headerlink" title="3.3.2 DataSet Operator"></a>3.3.2 DataSet Operator</h3><h4 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h4><h3 id="3-3-3-流计算与批计算统一的思路"><a href="#3-3-3-流计算与批计算统一的思路" class="headerlink" title="3.3.3 流计算与批计算统一的思路"></a>3.3.3 流计算与批计算统一的思路</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/iYFMZFA">https://t.zsxq.com/iYFMZFA</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="3-3-4-小结与反思"><a href="#3-3-4-小结与反思" class="headerlink" title="3.3.4 小结与反思"></a>3.3.4 小结与反思</h3><p>本节介绍了在开发 Flink 作业中数据转换常使用的算子（包含流作业和批作业），DataStream API 和 DataSet API 中部分算子名字是一致的，也有不同的地方，最后讲解了下 Flink 社区后面流批统一的思路。</p><p>你们公司使用 Flink 是流作业居多还是批作业居多？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-3-必须熟悉的数据转换-Operator-算子&quot;&gt;&lt;a href=&quot;#3-3-必须熟悉的数据转换-Operator-算子&quot; class=&quot;headerlink&quot; title=&quot;3.3 必须熟悉的数据转换 Operator(算子)&quot;&gt;&lt;/a&gt;3.3 必须熟悉的数据转换 Operator(算子)&lt;/h2&gt;&lt;p&gt;在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是上图这种模型，有数据源（source），有数据下游（sink），我们写的应用程序多是对数据源过来的数据做一系列操作，总结如下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 如何使用 Flink Window 及 Window 基本概念与实现原理?</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/12/flink-in-action-3.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/12/flink-in-action-3.2/</id>
    <published>2021-07-11T16:00:00.000Z</published>
    <updated>2021-11-14T04:29:54.825Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-2-Flink-Window-基础概念与实现原理"><a href="#3-2-Flink-Window-基础概念与实现原理" class="headerlink" title="3.2 Flink Window 基础概念与实现原理"></a>3.2 Flink Window 基础概念与实现原理</h2><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。</p><a id="more"></a><p>对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p><p>在本节将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。</p><h3 id="3-2-1-Window-简介"><a href="#3-2-1-Window-简介" class="headerlink" title="3.2.1 Window 简介"></a>3.2.1 Window 简介</h3><p>下面我们结合一个现实的例子来说明。</p><p>就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？</p><p>假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-064257.png" alt=""></p><p>可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-064320.png" alt=""></p><p>这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？</p><p>这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-065851.png" alt=""></p><p>第一分钟的数量为 18，第二分钟是 28，第三分钟是 24……这样，1 个小时内会有 60 个 Window。</p><p>再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-071008.png" alt=""></p><p>此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。</p><h3 id="3-2-2-Window-有什么作用？"><a href="#3-2-2-Window-有什么作用？" class="headerlink" title="3.2.2 Window 有什么作用？"></a>3.2.2 Window 有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。</p><h3 id="3-2-3-Flink-自带的-Window"><a href="#3-2-3-Flink-自带的-Window" class="headerlink" title="3.2.3 Flink 自带的 Window"></a>3.2.3 Flink 自带的 Window</h3><p>Flink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window：</p><ul><li>以时间驱动的 Time Window</li><li>以事件数量驱动的 Count Window</li><li>以会话间隔驱动的 Session Window</li></ul><p>提供上面三种 Window 机制后，由于某些特殊的需要，DataStream API 也提供了定制化的 Window 操作，供用户自定义 Window。</p><p>下面将先围绕上面说的三种 Window 来进行分析并教大家如何使用，然后对其原理分析，最后在解析其源码实现。</p><h3 id="3-2-4-Time-Window-的用法及源码分析"><a href="#3-2-4-Time-Window-的用法及源码分析" class="headerlink" title="3.2.4 Time Window 的用法及源码分析"></a>3.2.4 Time Window 的用法及源码分析</h3><h3 id="3-2-5-Count-Window-的用法及源码分析"><a href="#3-2-5-Count-Window-的用法及源码分析" class="headerlink" title="3.2.5 Count Window 的用法及源码分析"></a>3.2.5 Count Window 的用法及源码分析</h3><h3 id="3-2-6-Session-Window-的用法及源码分析"><a href="#3-2-6-Session-Window-的用法及源码分析" class="headerlink" title="3.2.6 Session Window 的用法及源码分析"></a>3.2.6 Session Window 的用法及源码分析</h3><h3 id="3-2-7-如何自定义-Window？"><a href="#3-2-7-如何自定义-Window？" class="headerlink" title="3.2.7 如何自定义 Window？"></a>3.2.7 如何自定义 Window？</h3><h3 id="3-2-8-Window-源码分析"><a href="#3-2-8-Window-源码分析" class="headerlink" title="3.2.8 Window 源码分析"></a>3.2.8 Window 源码分析</h3><h3 id="3-2-9-Window-组件之-WindowAssigner-的用法及源码分析"><a href="#3-2-9-Window-组件之-WindowAssigner-的用法及源码分析" class="headerlink" title="3.2.9 Window 组件之 WindowAssigner 的用法及源码分析"></a>3.2.9 Window 组件之 WindowAssigner 的用法及源码分析</h3><h3 id="3-2-10-Window-组件之-Trigger-的用法及源码分析"><a href="#3-2-10-Window-组件之-Trigger-的用法及源码分析" class="headerlink" title="3.2.10 Window 组件之 Trigger 的用法及源码分析"></a>3.2.10 Window 组件之 Trigger 的用法及源码分析</h3><h3 id="3-2-11-Window-组件之-Evictor-的用法及源码分析"><a href="#3-2-11-Window-组件之-Evictor-的用法及源码分析" class="headerlink" title="3.2.11 Window 组件之 Evictor 的用法及源码分析"></a>3.2.11 Window 组件之 Evictor 的用法及源码分析</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/qnQRvrf">https://t.zsxq.com/qnQRvrf</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="3-2-12-小结与反思"><a href="#3-2-12-小结与反思" class="headerlink" title="3.2.12 小结与反思"></a>3.2.12 小结与反思</h3><p>本节从生活案例来分享关于 Window 方面的需求，进而开始介绍 Window 相关的知识，并把 Flink 中常使用的三种窗口都一一做了介绍，并告诉大家如何使用，还分析了其实现原理。最后还对 Window 的内部组件做了详细的分析，为自定义 Window 提供了方法。</p><p>不知道你看完本节后对 Window 还有什么疑问吗？你们是根据什么条件来选择使用哪种 Window 的？在使用的过程中有遇到什么问题吗？ </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-2-Flink-Window-基础概念与实现原理&quot;&gt;&lt;a href=&quot;#3-2-Flink-Window-基础概念与实现原理&quot; class=&quot;headerlink&quot; title=&quot;3.2 Flink Window 基础概念与实现原理&quot;&gt;&lt;/a&gt;3.2 Flink Window 基础概念与实现原理&lt;/h2&gt;&lt;p&gt;目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/11/flink-in-action-3.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/11/flink-in-action-3.1/</id>
    <published>2021-07-10T16:00:00.000Z</published>
    <updated>2021-11-14T04:34:33.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第三章-——-Flink-中的流计算处理"><a href="#第三章-——-Flink-中的流计算处理" class="headerlink" title="第三章 —— Flink 中的流计算处理"></a>第三章 —— Flink 中的流计算处理</h1><p>通过第二章的入门案例讲解，相信你已经知道了 Flink 程序的开发过程，本章将带你熟悉 Flink 中的各种特性，比如多种时间语义、丰富的窗口机制、流计算中常见的运算操作符、Watermark 机制、丰富的 Connectors（Kafka、ElasticSearch、Redis、HBase 等） 的使用方式。除了介绍这些知识点的原理之外，笔者还将通过案例来教会大家如何去实战使用，最后还会讲解这些原理的源码实现，希望你可以更深刻的理解这些特性。</p><h2 id="3-1-Flink-多种时间语义对比"><a href="#3-1-Flink-多种时间语义对比" class="headerlink" title="3.1 Flink 多种时间语义对比"></a>3.1 Flink 多种时间语义对比</h2><a id="more"></a><p>Flink 在流应用程序中支持不同的 <strong>Time</strong> 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。下面我们一起来看看这三个 Time。</p><h3 id="3-1-1-Processing-Time"><a href="#3-1-1-Processing-Time" class="headerlink" title="3.1.1 Processing Time"></a>3.1.1 Processing Time</h3><p>Processing Time 是指事件被处理时机器的系统时间。</p><p>如果我们 Flink Job 设置的时间策略是 Processing Time 的话，那么后面所有基于时间的操作（如时间窗口）都将会使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p><p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p><p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p><h3 id="3-1-2-Event-Time"><a href="#3-1-2-Event-Time" class="headerlink" title="3.1.2 Event Time"></a>3.1.2 Event Time</h3><p>Event Time 是指事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。</p><p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（事件产生的时间顺序）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p><p>假设所有数据都已到达，Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，不管它们到达的顺序如何（是否按照事件产生的时间）。</p><h3 id="3-1-3-Ingestion-Time"><a href="#3-1-3-Ingestion-Time" class="headerlink" title="3.1.3 Ingestion Time"></a>3.1.3 Ingestion Time</h3><p>Ingestion Time 是事件进入 Flink 的时间。 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，成本可能会高一点，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（只在进入 Flink 的时候分配一次），所以对事件的不同窗口操作将使用相同的时间戳（第一次分配的时间戳），而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序中不必指定如何生成水印。</p><p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，唯一区别就是 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p><h3 id="3-1-4-三种-Time-的对比结果"><a href="#3-1-4-三种-Time-的对比结果" class="headerlink" title="3.1.4 三种 Time 的对比结果"></a>3.1.4 三种 Time 的对比结果</h3><h3 id="3-1-5-使用场景分析"><a href="#3-1-5-使用场景分析" class="headerlink" title="3.1.5 使用场景分析"></a>3.1.5 使用场景分析</h3><h3 id="3-1-6-Time-策略设置"><a href="#3-1-6-Time-策略设置" class="headerlink" title="3.1.6 Time 策略设置"></a>3.1.6 Time 策略设置</h3><h3 id="3-1-7-小结与反思"><a href="#3-1-7-小结与反思" class="headerlink" title="3.1.7 小结与反思"></a>3.1.7 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/znqnMNB">https://t.zsxq.com/znqnMNB</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第三章-——-Flink-中的流计算处理&quot;&gt;&lt;a href=&quot;#第三章-——-Flink-中的流计算处理&quot; class=&quot;headerlink&quot; title=&quot;第三章 —— Flink 中的流计算处理&quot;&gt;&lt;/a&gt;第三章 —— Flink 中的流计算处理&lt;/h1&gt;&lt;p&gt;通过第二章的入门案例讲解，相信你已经知道了 Flink 程序的开发过程，本章将带你熟悉 Flink 中的各种特性，比如多种时间语义、丰富的窗口机制、流计算中常见的运算操作符、Watermark 机制、丰富的 Connectors（Kafka、ElasticSearch、Redis、HBase 等） 的使用方式。除了介绍这些知识点的原理之外，笔者还将通过案例来教会大家如何去实战使用，最后还会讲解这些原理的源码实现，希望你可以更深刻的理解这些特性。&lt;/p&gt;
&lt;h2 id=&quot;3-1-Flink-多种时间语义对比&quot;&gt;&lt;a href=&quot;#3-1-Flink-多种时间语义对比&quot; class=&quot;headerlink&quot; title=&quot;3.1 Flink 多种时间语义对比&quot;&gt;&lt;/a&gt;3.1 Flink 多种时间语义对比&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 案例2：实时处理 Socket 数据</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/10/flink-in-action-2.4/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/10/flink-in-action-2.4/</id>
    <published>2021-07-09T16:00:00.000Z</published>
    <updated>2021-11-08T15:29:24.446Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2-4-案例2：实时处理-Socket-数据"><a href="#2-4-案例2：实时处理-Socket-数据" class="headerlink" title="2.4 案例2：实时处理 Socket 数据"></a>2.4 案例2：实时处理 Socket 数据</h2><p>在 2.3 节中讲解了 Flink 最简单的 WordCount 程序的创建、运行结果查看和代码分析，本节将继续带大家来看一个入门上手的程序：Flink 处理 Socket 数据。</p><a id="more"></a><h3 id="2-4-1-使用-IDEA-创建项目"><a href="#2-4-1-使用-IDEA-创建项目" class="headerlink" title="2.4.1 使用 IDEA 创建项目"></a>2.4.1 使用 IDEA 创建项目</h3><p>使用 IDEA 创建新的 module，结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    ├── main</span><br><span class="line">    │   ├── java</span><br><span class="line">    │   │   └── com</span><br><span class="line">    │   │       └── zhisheng</span><br><span class="line">    │   │           └── socket</span><br><span class="line">    │   │               └── Main.java</span><br><span class="line">    │   └── resources</span><br><span class="line">    │       └── log4j.properties</span><br><span class="line">    └── test</span><br><span class="line">        └── java</span><br></pre></td></tr></table></figure><p>项目创建好了后，我们下一步开始编写 Flink Socket Job 的代码。</p><h3 id="2-4-2-实时处理-Socket-数据应用程序代码实现"><a href="#2-4-2-实时处理-Socket-数据应用程序代码实现" class="headerlink" title="2.4.2 实时处理 Socket 数据应用程序代码实现"></a>2.4.2 实时处理 Socket 数据应用程序代码实现</h3><p>程序代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//参数检查</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String hostname = args[<span class="number">0</span>];</span><br><span class="line">        Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//获取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line">        <span class="comment">//计数</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        sum.print();</span><br><span class="line">        env.execute(<span class="string">"Java WordCount from SocketText"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">            String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>pom.xml</strong> 添加 build：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.flink:force-shading<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!--注意：这里一定要换成你自己的 Job main 方法的启动类--&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.zhisheng.socket.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-4-3-运行实时处理-Socket-数据应用程序"><a href="#2-4-3-运行实时处理-Socket-数据应用程序" class="headerlink" title="2.4.3 运行实时处理 Socket 数据应用程序"></a>2.4.3 运行实时处理 Socket 数据应用程序</h3><p>下面分别讲解在 IDE 和 Flink UI 上运行作业。</p><h4 id="本地-IDE-运行"><a href="#本地-IDE-运行" class="headerlink" title="本地 IDE 运行"></a>本地 IDE 运行</h4><h4 id="UI-运行-Job"><a href="#UI-运行-Job" class="headerlink" title="UI 运行 Job"></a>UI 运行 Job</h4><h3 id="2-4-4-实时处理-Socket-数据应用程序代码分析"><a href="#2-4-4-实时处理-Socket-数据应用程序代码分析" class="headerlink" title="2.4.4 实时处理 Socket 数据应用程序代码分析"></a>2.4.4 实时处理 Socket 数据应用程序代码分析</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/VBEQv3F">https://t.zsxq.com/VBEQv3F</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="2-4-5-Flink-中使用-Lambda-表达式"><a href="#2-4-5-Flink-中使用-Lambda-表达式" class="headerlink" title="2.4.5 Flink 中使用 Lambda 表达式"></a>2.4.5 Flink 中使用 Lambda 表达式</h3><p>因为 Lambda 表达式看起来简洁，所以有时候也是希望在这些 Flink 作业中也可以使用上它，虽然 Flink 中是支持 Lambda，但是个人感觉不太友好。比如上面的应用程序如果将 LineSplitter 该类之间用 Lambda 表达式完成的话则要像下面这样写：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">stream.flatMap((s, collector) -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (String token : s.toLowerCase().split(<span class="string">"\\W+"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>但是这样写完后，运行作业报错如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.flink.api.common.functions.InvalidTypesException: The return type of function &apos;main(LambdaMain.java:34)&apos; could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the &apos;ResultTypeQueryable&apos; interface.</span><br><span class="line">at org.apache.flink.api.dag.Transformation.getOutputType(Transformation.java:417)</span><br><span class="line">at org.apache.flink.streaming.api.datastream.DataStream.getType(DataStream.java:175)</span><br><span class="line">at org.apache.flink.streaming.api.datastream.DataStream.keyBy(DataStream.java:318)</span><br><span class="line">at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:41)</span><br><span class="line">Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of &apos;Collector&apos; are missing. In many cases lambda methods don&apos;t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the &apos;org.apache.flink.api.common.functions.FlatMapFunction&apos; interface. Otherwise the type has to be specified explicitly using type information.</span><br><span class="line">at org.apache.flink.api.java.typeutils.TypeExtractionUtils.validateLambdaType(TypeExtractionUtils.java:350)</span><br><span class="line">at org.apache.flink.api.java.typeutils.TypeExtractionUtils.extractTypeFromLambda(TypeExtractionUtils.java:176)</span><br><span class="line">at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:571)</span><br><span class="line">at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196)</span><br><span class="line">at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:611)</span><br><span class="line">at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:34)</span><br></pre></td></tr></table></figure><p>根据上面的报错信息其实可以知道要怎么解决了，该错误是因为 Flink 在用户自定义的函数中会使用泛型来创建 serializer，当使用匿名函数时，类型信息会被保留。但 Lambda 表达式并不是匿名函数，所以 javac 编译的时候并不会把泛型保存到 class 文件里。解决方法：使用 Flink 提供的 returns 方法来指定 flatMap 的返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用 TupleTypeInfo 来指定 Tuple 的参数类型</span></span><br><span class="line">.returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class))</span><br></pre></td></tr></table></figure><p>在 flatMap 后面加上上面这个 returns 就行了，但是如果算子多了的话，每个都去加一个 returns，其实会很痛苦的，所以通常使用匿名函数或者自定义函数居多。</p><h3 id="2-4-5-小结与反思"><a href="#2-4-5-小结与反思" class="headerlink" title="2.4.5 小结与反思"></a>2.4.5 小结与反思</h3><p>在第一章中介绍了 Flink 的特性，本章主要是让大家能够快速入门，所以在第一节和第二节中分别讲解了 Flink 的环境准备和搭建，在第三节和第四节中通过两个入门的应用程序（WordCount 应用程序和读取 Socket 数据应用程序）让大家可以快速入门 Flink，两个程序都是需要自己动手实操，所以更能加深大家的印象。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2-4-案例2：实时处理-Socket-数据&quot;&gt;&lt;a href=&quot;#2-4-案例2：实时处理-Socket-数据&quot; class=&quot;headerlink&quot; title=&quot;2.4 案例2：实时处理 Socket 数据&quot;&gt;&lt;/a&gt;2.4 案例2：实时处理 Socket 数据&lt;/h2&gt;&lt;p&gt;在 2.3 节中讲解了 Flink 最简单的 WordCount 程序的创建、运行结果查看和代码分析，本节将继续带大家来看一个入门上手的程序：Flink 处理 Socket 数据。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 案例1：WordCount 应用程序</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/09/flink-in-action-2.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/09/flink-in-action-2.3/</id>
    <published>2021-07-08T16:00:00.000Z</published>
    <updated>2021-11-08T15:29:24.442Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2-3-案例1：WordCount-应用程序"><a href="#2-3-案例1：WordCount-应用程序" class="headerlink" title="2.3 案例1：WordCount 应用程序"></a>2.3 案例1：WordCount 应用程序</h2><p>在 2.2 节中带大家讲解了下 Flink 的环境安装，这篇文章就开始我们的第一个 Flink 案例实战，也方便大家快速开始自己的第一个 Flink 应用。大数据里学习一门技术一般都是从 WordCount 开始入门的，那么笔者还是不打破常规了，所以这篇文章笔者也将带大家通过 WordCount 程序来初步了解 Flink。</p><a id="more"></a><h3 id="2-3-1-使用-Maven-创建项目"><a href="#2-3-1-使用-Maven-创建项目" class="headerlink" title="2.3.1 使用 Maven 创建项目"></a>2.3.1 使用 Maven 创建项目</h3><p>Flink 支持 Maven 直接构建模版项目，你在终端使用该命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate                               \</span><br><span class="line">      -DarchetypeGroupId=org.apache.flink              \</span><br><span class="line">      -DarchetypeArtifactId=flink-quickstart-java      \</span><br><span class="line">      -DarchetypeVersion=1.9.0</span><br></pre></td></tr></table></figure><p>在执行的过程中它会提示你输入 groupId、artifactId、和 package 名，你按照要求输入就行，最后就可以成功创建一个项目，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-151203.png" alt=""></p><p>进入到目录你就可以看到已经创建了项目，里面结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~/IdeaProjects/github/Flink-WordCount  tree</span><br><span class="line">.</span><br><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    └── main</span><br><span class="line">        ├── java</span><br><span class="line">        │   └── com</span><br><span class="line">        │       └── zhisheng</span><br><span class="line">        │           ├── BatchJob.java</span><br><span class="line">        │           └── StreamingJob.java</span><br><span class="line">        └── resources</span><br><span class="line">            └── log4j.properties</span><br><span class="line"></span><br><span class="line">6 directories, 4 files</span><br></pre></td></tr></table></figure><p>该项目中包含了两个类 BatchJob 和 StreamingJob，另外还有一个 log4j.properties 配置文件，然后你就可以将该项目导入到 IDEA 了。你可以在该目录下执行 <code>mvn clean package</code> 就可以编译该项目，编译成功后在 target 目录下会生成一个 Job 的 Jar 包，但是这个 Job 还不能执行，因为 StreamingJob 这个类中的 main 方法里面只是简单的创建了 StreamExecutionEnvironment 环境，然后就执行 execute 方法，这在 Flink 中是不算一个可执行的 Job 的，因此如果你提交到 Flink UI 上也是会报错的。</p><p>如下图所示，上传作业程序打包编译的 Jar 包：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-151434.png" alt=""></p><p>运行报错结果如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-152026.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Server Response Message:</span><br><span class="line">Internal server error.</span><br></pre></td></tr></table></figure><p>我们查看 Flink JobManager 的日志，可以看见错误信息如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-152954.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-04-26 17:27:33,706 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler    - Unhandled exception.</span><br><span class="line">org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: No operators defined in streaming topology. Cannot execute.</span><br></pre></td></tr></table></figure><p>因为 execute 方法之前我们是需要补充我们 Job 的一些算子操作的，所以报错还是很正常的，本节下面将会提供完整代码。</p><h3 id="2-3-2-使用-IDEA-创建项目"><a href="#2-3-2-使用-IDEA-创建项目" class="headerlink" title="2.3.2 使用 IDEA 创建项目"></a>2.3.2 使用 IDEA 创建项目</h3><p>一般我们项目可能是由多个 Job 组成，并且代码也都是在同一个工程下面进行管理，上面那种创建方式适合单个 Job 执行，但如果在公司多人合作的时候还是得在同一个工程下面创建项目，每个 Flink Job 对应着一个 module，该 module 负责独立的业务逻辑，比如笔者在 GitHub 的 <a href="https://github.com/zhisheng17/flink-learning">https://github.com/zhisheng17/flink-learning</a> 项目，它的项目结构如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-163154.png" alt=""></p><p>接下来我们需要在父工程的 pom.xml 中加入如下属性（含编码、Flink 版本、JDK 版本、Scala 版本、Maven 编译版本）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Flink 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--JDK 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Scala 2.11 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后加入依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Apache Flink dependencies --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add logging framework, to produce console output when running in the IDE. --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- These dependencies are excluded from the application JAR by default. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面依赖中 flink-java 和 flink-streaming-java 是我们 Flink 必备的核心依赖，为什么设置 scope 为 provided 呢（默认是 compile）？是因为 Flink 其实在自己的安装目录中 lib 文件夹里的 <code>lib/flink-dist_2.11-1.9.0.jar</code> 已经包含了这些必备的 Jar 了，所以我们在给自己的 Flink Job 添加依赖的时候最后打成的 Jar 包可不希望又将这些重复的依赖打进去。有两个好处：</p><ul><li>减小了我们打的 Flink Job Jar 包容量大小</li><li>不会因为打入不同版本的 Flink 核心依赖而导致类加载冲突等问题</li></ul><p>但是问题又来了，我们需要在 IDEA 中调试运行我们的 Job，如果将 scope 设置为 provided 的话，是会报错的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Error: A JNI error has occurred, please check your installation and try again</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/api/common/ExecutionConfig$GlobalJobParameters</span><br><span class="line">at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class="line">at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class="line">at java.lang.Class.privateGetMethodRecursive(Class.java:3048)</span><br><span class="line">at java.lang.Class.getMethod0(Class.java:3018)</span><br><span class="line">at java.lang.Class.getMethod(Class.java:1784)</span><br><span class="line">at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)</span><br><span class="line">at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.common.ExecutionConfig$GlobalJobParameters</span><br><span class="line">at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">... 7 more</span><br></pre></td></tr></table></figure><p>默认 scope 为 compile 的话，本地调试的话就不会出错了。另外测试到底能够减小多少 Jar 包的大小呢？我这里先写了个 Job 测试。当 scope 为 compile 时，编译后的 target 目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  ~/Flink-WordCount/target   master ●✚  ll</span><br><span class="line">total 94384</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    45M  4 26 21:23 Flink-WordCount-1.0-SNAPSHOT.jar</span><br><span class="line">drwxr-xr-x  4 zhisheng  staff   128B  4 26 21:23 classes</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 generated-sources</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 maven-archiver</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 maven-status</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.2K  4 26 21:23 original-Flink-WordCount-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>当 scope 为 provided 时，编译后的 target 目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~/Flink-WordCount/target   master ●✚  ll</span><br><span class="line">total 32</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.5K  4 26 21:27 Flink-WordCount-1.0-SNAPSHOT.jar</span><br><span class="line">drwxr-xr-x  4 zhisheng  staff   128B  4 26 21:27 classes</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 generated-sources</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 maven-archiver</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 maven-status</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.2K  4 26 21:27 original-Flink-WordCount-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>。。。</p><h3 id="2-3-3-流计算-WordCount-应用程序代码实现"><a href="#2-3-3-流计算-WordCount-应用程序代码实现" class="headerlink" title="2.3.3 流计算 WordCount 应用程序代码实现"></a>2.3.3 流计算 WordCount 应用程序代码实现</h3><h3 id="2-3-4-运行流计算-WordCount-应用程序"><a href="#2-3-4-运行流计算-WordCount-应用程序" class="headerlink" title="2.3.4 运行流计算 WordCount 应用程序"></a>2.3.4 运行流计算 WordCount 应用程序</h3><h4 id="本地-IDE-运行"><a href="#本地-IDE-运行" class="headerlink" title="本地 IDE 运行"></a>本地 IDE 运行</h4><h4 id="UI-运行-Job"><a href="#UI-运行-Job" class="headerlink" title="UI 运行 Job"></a>UI 运行 Job</h4><h3 id="2-3-5-流计算-WordCount-应用程序代码分析"><a href="#2-3-5-流计算-WordCount-应用程序代码分析" class="headerlink" title="2.3.5 流计算 WordCount 应用程序代码分析"></a>2.3.5 流计算 WordCount 应用程序代码分析</h3><h3 id="2-3-6-小结与反思"><a href="#2-3-6-小结与反思" class="headerlink" title="2.3.6 小结与反思"></a>2.3.6 小结与反思</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/Z7EAmq3">https://t.zsxq.com/Z7EAmq3</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2-3-案例1：WordCount-应用程序&quot;&gt;&lt;a href=&quot;#2-3-案例1：WordCount-应用程序&quot; class=&quot;headerlink&quot; title=&quot;2.3 案例1：WordCount 应用程序&quot;&gt;&lt;/a&gt;2.3 案例1：WordCount 应用程序&lt;/h2&gt;&lt;p&gt;在 2.2 节中带大家讲解了下 Flink 的环境安装，这篇文章就开始我们的第一个 Flink 案例实战，也方便大家快速开始自己的第一个 Flink 应用。大数据里学习一门技术一般都是从 WordCount 开始入门的，那么笔者还是不打破常规了，所以这篇文章笔者也将带大家通过 WordCount 程序来初步了解 Flink。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 环境搭建</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/08/flink-in-action-2.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/08/flink-in-action-2.2/</id>
    <published>2021-07-07T16:00:00.000Z</published>
    <updated>2021-11-08T15:30:28.152Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2-2-Flink-环境搭建"><a href="#2-2-Flink-环境搭建" class="headerlink" title="2.2 Flink 环境搭建"></a>2.2 Flink 环境搭建</h2><a id="more"></a><p>在 2.1 节中已经将 Flink 的准备环境已经讲完了，本章节将带大家正式开始接触 Flink，那么我们得先安装一下 Flink。Flink 是可以在多个平台（Windows、Linux、Mac）上安装的。在开始写本书的时候最新版本是 1.8 版本，但是写到一半后更新到 1.9 了（合并了大量 Blink 的新特性），所以笔者又全部更新版本到 1.9，书籍后面也都是基于最新的版本讲解与演示。</p><p>Flink 的官网地址是：<a href="https://flink.apache.org/">https://flink.apache.org/</a></p><h3 id="2-2-1-Flink-下载与安装"><a href="#2-2-1-Flink-下载与安装" class="headerlink" title="2.2.1 Flink 下载与安装"></a>2.2.1 Flink 下载与安装</h3><p>Flink 在 Mac、Linux、Window 平台上的安装方式如下。</p><h4 id="在-Mac-和-Linux-下安装"><a href="#在-Mac-和-Linux-下安装" class="headerlink" title="在 Mac 和 Linux 下安装"></a>在 Mac 和 Linux 下安装</h4><p>你可以通过该地址 <a href="https://flink.apache.org/downloads.html">https://flink.apache.org/downloads.html</a> 下载到最新版本的 Flink。这里我们选择 <code>Apache Flink 1.9.0 for Scala 2.11</code> 版本，点击跳转到了一个镜像下载选择的地址，如下图所示，随便选择哪个就行，只是下载速度不一致而已。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-080110.png" alt=""></p><p>下载完后，你就可以直接解压下载的 Flink 压缩包了。接下来我们可以启动一下 Flink，我们进入到 Flink 的安装目录下执行命令 <code>./bin/start-cluster.sh</code> 即可，产生的日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng /usr/local/flink-1.9.0  ./bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><p>如果你的电脑是 Mac 的话，那么你也可以通过 Homebrew 命令进行安装。先通过命令 <code>brew search flink</code> 查找一下包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~  brew search flink</span><br><span class="line">==&gt; Formulae</span><br><span class="line">apache-flink ✔       homebrew/linuxbrew-core/apache-flink</span><br></pre></td></tr></table></figure><p>可以发现找得到 Flink 的安装包，但是这样安装的版本可能不是最新的，如果你要安装的话，则使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install apache-flink</span><br></pre></td></tr></table></figure><p>那么它就会开始进行下载并安装好，安装后的目录应该是在 <code>/usr/local/Cellar/apache-flink</code> 下，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-030606.png" alt=""></p><p>你可以通过下面命令检查安装的 Flink 到底是什么版本的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink --version</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Version: 1.9.0, Commit ID: ff472b4</span><br></pre></td></tr></table></figure><p>这种的话运行是得进入 <code>/usr/local/Cellar/apache-flink/1.9.0/libexec/bin</code> 目录下执行命令 <code>./start-cluster.sh</code> 才可以启动 Flink 的。执行命令后的启动日志如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><h4 id="在-Windows-下安装"><a href="#在-Windows-下安装" class="headerlink" title="在 Windows 下安装"></a>在 Windows 下安装</h4><p>如果你的电脑系统是 Windows 的话，那么你就直接双击 Flink 安装目录下面 bin 文件夹里面的 <code>start-cluster.bat</code> 就行，同样可以将 Flink 起动成功。</p><h3 id="2-2-2-Flink-启动与运行"><a href="#2-2-2-Flink-启动与运行" class="headerlink" title="2.2.2 Flink 启动与运行"></a>2.2.2 Flink 启动与运行</h3><p>启动成功后的话，我们可以通过访问地址<code>http://localhost:8081/</code> 查看 UI 长啥样了，如下图所示：</p><h3 id="2-2-3-Flink-目录配置文件解读"><a href="#2-2-3-Flink-目录配置文件解读" class="headerlink" title="2.2.3 Flink 目录配置文件解读"></a>2.2.3 Flink 目录配置文件解读</h3><h3 id="2-2-4-Flink-源码下载"><a href="#2-2-4-Flink-源码下载" class="headerlink" title="2.2.4 Flink 源码下载"></a>2.2.4 Flink 源码下载</h3><h3 id="2-2-5-Flink-源码编译"><a href="#2-2-5-Flink-源码编译" class="headerlink" title="2.2.5 Flink 源码编译"></a>2.2.5 Flink 源码编译</h3><h3 id="2-2-6-将-Flink-源码导入到-IDE"><a href="#2-2-6-将-Flink-源码导入到-IDE" class="headerlink" title="2.2.6 将 Flink 源码导入到 IDE"></a>2.2.6 将 Flink 源码导入到 IDE</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/JyRzVnU">https://t.zsxq.com/JyRzVnU</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="2-2-7-小结与反思"><a href="#2-2-7-小结与反思" class="headerlink" title="2.2.7 小结与反思"></a>2.2.7 小结与反思</h3><p>本节主要讲了 FLink 在不同系统下的安装和运行方法，然后讲了下怎么去下载源码和将源码导入到 IDE 中。不知道你在将源码导入到 IDE 中是否有遇到什么问题呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2-2-Flink-环境搭建&quot;&gt;&lt;a href=&quot;#2-2-Flink-环境搭建&quot; class=&quot;headerlink&quot; title=&quot;2.2 Flink 环境搭建&quot;&gt;&lt;/a&gt;2.2 Flink 环境搭建&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— Flink 环境准备</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/07/flink-in-action-2.1/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/07/flink-in-action-2.1/</id>
    <published>2021-07-06T16:00:00.000Z</published>
    <updated>2021-11-08T15:18:42.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第二章-——-Flink-入门"><a href="#第二章-——-Flink-入门" class="headerlink" title="第二章 —— Flink 入门"></a>第二章 —— Flink 入门</h1><p>通过第一章对 Flink 的介绍，相信你对 Flink 的概念和特性有了一定的了解，接下来本章将开始正式进入 Flink 的学习之旅，笔者将带你搭建 Flink 的环境和编写两个案例（WordCount 程序、读取 Socket 数据）来入门 Flink。</p><h2 id="2-1-Flink-环境准备"><a href="#2-1-Flink-环境准备" class="headerlink" title="2.1 Flink 环境准备"></a>2.1 Flink 环境准备</h2><a id="more"></a><p>通过前面的章节内容，相信你已经对 Flink 的基础概念等知识已经有一定了解，现在是不是迫切的想把 Flink 给用起来？先别急，我们先把电脑的准备环境给安装好，这样后面才能更愉快地玩耍。</p><p>废话不多说了，直奔主题。因为本书后面章节内容会使用 Kafka、MySQL、ElasticSearch 等组件，并且运行 Flink 程序是需要依赖 Java 的，另外就是我们需要使用 IDE 来开发 Flink 应用程序以及使用 Maven 来管理 Flink 应用程序的依赖，所以本节我们提前安装这个几个组件，搭建好本地的环境，后面如果还要安装其他的组件笔者会在对应的章节中补充，如果你的操作系统已经中已经安装过 JDK、Maven、MySQL、IDEA 等，那么你可以跳过对应的内容，直接看你未安装过的。</p><p>这里笔者再说下自己电脑的系统环境：macOS High Sierra 10.13.5，后面文章的演示环境不作特别说明的话就是都在这个系统环境中。</p><h3 id="2-1-1-JDK-安装与配置"><a href="#2-1-1-JDK-安装与配置" class="headerlink" title="2.1.1 JDK 安装与配置"></a>2.1.1 JDK 安装与配置</h3><p>虽然现在 JDK 已经更新到 12 了，但是为了稳定我们还是安装 JDK 8，如果没有安装过的话，可以去<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html">官网</a> 的<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">下载页面</a>下载对应自己操作系统的最新 JDK8 就行。</p><p>Mac 系统的是 <code>jdk-8u211-macosx-x64.dmg</code> 格式、Linux 系统的是 <code>jdk-8u211-linux-x64.tar.gz</code> 格式。Mac 系统安装的话直接双击然后一直按照提示就行了，最后 JDK 的安装目录在 <code>/Library/Java/JavaVirtualMachines/</code> ，然后在 <code>/etc/hosts</code> 中配置好环境变量（注意：替换你自己电脑本地的路径）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home</span><br><span class="line">export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p>Linux 系统的话就是在某个目录下直接解压就行了，然后在 <code>/etc/profile</code> 添加一下上面的环境变量（注意：替换你自己电脑的路径）。然后执行 <code>java -version</code> 命令可以查看是否安装成功！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng ~  java -version</span><br><span class="line">java version &quot;1.8.0_152&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_152-b16)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)</span><br></pre></td></tr></table></figure><h3 id="2-1-2-Maven-安装与配置"><a href="#2-1-2-Maven-安装与配置" class="headerlink" title="2.1.2 Maven 安装与配置"></a>2.1.2 Maven 安装与配置</h3><p>安装好 JDK 后我们就可以安装 Maven 了，我们在<a href="http://maven.apache.org/download.cgi">官网</a>下载二进制包就行，然后在自己本地软件安装目录解压压缩包就行。接下来你需要配置一下环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export M2_HOME=/Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">export PATH=$PATH:$M2_HOME/bin</span><br></pre></td></tr></table></figure><p>然后执行命令 <code>mvn -v</code> 可以验证是否安装成功，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~ /Users  mvn -v</span><br><span class="line">Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)</span><br><span class="line">Maven home: /Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">Java version: 1.8.0_152, vendor: Oracle Corporation</span><br><span class="line">Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;mac os x&quot;, version: &quot;10.13.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;</span><br></pre></td></tr></table></figure><h3 id="2-1-3-IDE-安装与配置"><a href="#2-1-3-IDE-安装与配置" class="headerlink" title="2.1.3 IDE 安装与配置"></a>2.1.3 IDE 安装与配置</h3><p>安装完 JDK 和 Maven 后，就可以安装 IDE 了，大家可以选择你熟练的 IDE 就行，笔者后面演示的代码都是在 IDEA 中运行的，如果想为了后面不出其他的问题的话，建议尽量和笔者的环境保持一致。</p><p>IDEA 官网下载地址：<a href="https://www.jetbrains.com/idea/download/#section=mac">下载页面的地址</a>，下载后可以双击后然后按照提示一步步安装，安装完成后需要在 IDEA 中配置 JDK 路径和 Maven 的路径，后面我们开发也都是靠 Maven 来管理项目的依赖。</p><h3 id="2-1-4-MySQL-安装与配置"><a href="#2-1-4-MySQL-安装与配置" class="headerlink" title="2.1.4 MySQL 安装与配置"></a>2.1.4 MySQL 安装与配置</h3><h3 id="2-1-5-Kafka-安装与配置"><a href="#2-1-5-Kafka-安装与配置" class="headerlink" title="2.1.5 Kafka 安装与配置"></a>2.1.5 Kafka 安装与配置</h3><h3 id="2-1-6-ElasticSearch-安装与配置"><a href="#2-1-6-ElasticSearch-安装与配置" class="headerlink" title="2.1.6 ElasticSearch 安装与配置"></a>2.1.6 ElasticSearch 安装与配置</h3><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/JyRzVnU">https://t.zsxq.com/JyRzVnU</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="2-1-7-小结与反思"><a href="#2-1-7-小结与反思" class="headerlink" title="2.1.7 小结与反思"></a>2.1.7 小结与反思</h3><p>本节讲解了下 JDK、Maven、IDE、MySQL、Kafka、ElasticSearch 的安装与配置，因为这些都是后面要用的，所以这里单独抽一篇文章来讲解环境准备的安装步骤，当然这里还并不涉及全，因为后面我们还可能会涉及到 HBase、HDFS 等知识，后面我们用到再看，本书的内容主要讲解 Flink，所以更多的环境准备还是得靠大家自己独立完成。</p><p>这里笔者说下笔者自己一般安装环境的选择：</p><p>xxx</p><p>下面章节我们就正式进入 Flink 专题了！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第二章-——-Flink-入门&quot;&gt;&lt;a href=&quot;#第二章-——-Flink-入门&quot; class=&quot;headerlink&quot; title=&quot;第二章 —— Flink 入门&quot;&gt;&lt;/a&gt;第二章 —— Flink 入门&lt;/h1&gt;&lt;p&gt;通过第一章对 Flink 的介绍，相信你对 Flink 的概念和特性有了一定的了解，接下来本章将开始正式进入 Flink 的学习之旅，笔者将带你搭建 Flink 的环境和编写两个案例（WordCount 程序、读取 Socket 数据）来入门 Flink。&lt;/p&gt;
&lt;h2 id=&quot;2-1-Flink-环境准备&quot;&gt;&lt;a href=&quot;#2-1-Flink-环境准备&quot; class=&quot;headerlink&quot; title=&quot;2.1 Flink 环境准备&quot;&gt;&lt;/a&gt;2.1 Flink 环境准备&lt;/h2&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 大数据计算框架对比</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/06/flink-in-action-1.3/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/06/flink-in-action-1.3/</id>
    <published>2021-07-05T16:00:00.000Z</published>
    <updated>2021-11-08T15:14:17.415Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-3-大数据计算框架对比"><a href="#1-3-大数据计算框架对比" class="headerlink" title="1.3 大数据计算框架对比"></a>1.3 大数据计算框架对比</h2><p>在 1.2 节中已经跟大家详细介绍了 Flink，那么在本节就主要 Blink、Spark Streaming、Structured Streaming 和 Storm 的区别。</p><a id="more"></a><h3 id="1-3-1-Flink"><a href="#1-3-1-Flink" class="headerlink" title="1.3.1 Flink"></a>1.3.1 Flink</h3><p>Flink 是一个针对流数据和批数据分布式处理的引擎，在某些对实时性要求非常高的场景，基本上都是采用 Flink 来作为计算引擎，它不仅可以处理有界的批数据，还可以处理无界的流数据，在 Flink 的设计愿想就是将批处理当成是流处理的一种特例。</p><p>如下图所示，在 Flink 的母公司 <a href="https://www.eu-startups.com/2019/01/alibaba-takes-over-berlin-based-streaming-analytics-startup-data-artisans/">Data Artisans 被阿里收购</a>之后，阿里也在开始逐步将内部的 Blink 代码开源出来并合并在 Flink 主分支上。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-01-143012.jpg" alt="阿里巴巴收购 Data Artisans"></p><p>而 Blink 一个很强大的特点就是它的 Table API &amp; SQL 很强大，社区也在 Flink 1.9 版本将 Blink 开源版本大部分代码合进了 Flink 主分支。</p><h3 id="1-3-2-Blink"><a href="#1-3-2-Blink" class="headerlink" title="1.3.2 Blink"></a>1.3.2 Blink</h3><p>Blink 是早期阿里在 Flink 的基础上开始修改和完善后在内部创建的分支，然后 Blink 目前在阿里服务于阿里集团内部搜索、推荐、广告、菜鸟物流等大量核心实时业务，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-01-160059.jpg" alt=""></p><p>Blink 在阿里内部错综复杂的业务场景中锻炼成长着，经历了内部这么多用户的反馈（各种性能、资源使用率、易用性等诸多方面的问题），Blink 都做了针对性的改进。在 Flink Forward China 峰会上，阿里巴巴集团副总裁周靖人宣布 Blink 在 2019 年 1 月正式开源，同时阿里也希望 Blink 开源后能进一步加深与 Flink 社区的联动，</p><p>Blink 开源地址：<a href="https://github.com/apache/flink/tree/blink">https://github.com/apache/flink/tree/blink</a></p><p>开源版本 Blink 的主要功能和优化点：</p><p>1、Runtime 层引入 Pluggable Shuffle Architecture，开发者可以根据不同的计算模型或者新硬件的需要实现不同的 shuffle 策略进行适配；为了性能优化，Blink 可以让算子更加灵活的 chain 在一起，避免了不必要的数据传输开销；在 BroadCast Shuffle 模式中，Blink 优化掉了大量的不必要的序列化和反序列化开销；Blink 提供了全新的 JM FailOver 机制，JM 发生错误之后，新的 JM 会重新接管整个 JOB 而不是重启 JOB，从而大大减少了 JM FailOver 对 JOB 的影响；Blink 支持运行在 Kubernetes 上。</p><p>2、SQL/Table API 架构上的重构和性能的优化是 Blink 开源版本的一个重大贡献。</p><p>3、Hive 的兼容性，可以直接用 Flink SQL 去查询 Hive 的数据，Blink 重构了 Flink catalog 的实现，并且增加了两种 catalog，一个是基于内存存储的 FlinkInMemoryCatalog，另外一个是能够桥接 Hive metaStore 的 HiveCatalog。</p><p>4、Zeppelin for Flink</p><p>5、Flink Web，更美观的 UI 界面，查看日志和监控 Job 都变得更加方便</p><p>对于开源那会看到一个对话让笔者感到很震撼：</p><blockquote><p>Blink 开源后，两个开源项目之间的关系会是怎样的？未来 Flink 和 Blink 也会由不同的团队各自维护吗？</p><p>Blink 永远不会成为另外一个项目，如果后续进入 Apache 一定是成为 Flink 的一部分</p></blockquote><p>对话详情如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-01-162836.jpg" alt=""></p><p>在 Blink 开源那会，笔者就将源码自己编译了一份，然后自己在本地一直运行着，感兴趣的可以看看文章 <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/">阿里巴巴开源的 Blink 实时计算框架真香</a> ，你会发现 Blink 的 UI 还是比较美观和实用的。</p><p>如果你还对 Blink 有什么疑问，可以看看下面两篇文章：</p><p><a href="https://www.infoq.cn/article/wZ_b7Hw9polQWp3mTwVh">阿里重磅开源 Blink：为什么我们等了这么久？</a></p><p><a href="https://www.infoq.cn/article/ZkOGAl6_vkZDTk8tfbbg">重磅！阿里巴巴 Blink 正式开源，重要优化点解读</a></p><h3 id="1-3-3-Spark"><a href="#1-3-3-Spark" class="headerlink" title="1.3.3 Spark"></a>1.3.3 Spark</h3><p>Apache Spark 是一种包含流处理能力的下一代批处理框架。与 Hadoop 的 MapReduce 引擎基于各种相同原则开发而来的 Spark 主要侧重于通过完善的内存计算和处理优化机制加快批处理工作负载的运行速度。Spark 可作为独立集群部署（需要相应存储层的配合），或可与 Hadoop 集成并取代 MapReduce 引擎。</p><p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a> 是 Spark API 核心的扩展，可实现实时数据的快速扩展，高吞吐量，容错处理。数据可以从很多来源（如 Kafka、Flume、Kinesis 等）中提取，并且可以通过很多函数来处理这些数据，处理完后的数据可以直接存入数据库或者 Dashboard 等，如下两图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-06-154210.jpg" alt=""></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-06-134257.jpg" alt=""></p><p><strong>Spark Streaming 的内部实现原理</strong>是接收实时输入数据流并将数据分成批处理，然后由 Spark 引擎处理以批量生成最终结果流，也就是常说的 micro-batch 模式，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-06-134430.jpg" alt=""></p><p><strong>Spark DStreams</strong></p><p>DStreams 是 Spark Streaming 提供的基本的抽象，它代表一个连续的数据流。它要么是从源中获取的输入流，要么是输入流通过转换算子生成的处理后的数据流。在内部实现上，DStream 由连续的序列化 RDD 来表示，每个 RDD 含有一段时间间隔内的数据，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-06-140956.jpg" alt=""></p><p>任何对 DStreams 的操作都转换成了对 DStreams 隐含的 RDD 的操作。例如 flatMap 操作应用于 lines 这个 DStreams 的每个 RDD，生成 words 这个 DStreams 的 RDD 过程如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-06-134718.jpg" alt=""></p><p>通过 Spark 引擎计算这些隐含 RDD 的转换算子。DStreams 操作隐藏了大部分的细节，并且为了更便捷，为开发者提供了更高层的 API。</p><p><strong>Spark 支持的滑动窗口</strong></p><p>它和 Flink 的滑动窗口类似，支持传入两个参数，一个代表窗口长度，一个代表滑动间隔，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-06-134915.jpg" alt=""></p><p><strong>Spark 支持更多的 API</strong></p><p>因为 Spark 是使用 Scala 开发的居多，所以从官方文档就可以看得到对 Scala 的 API 支持的很好，而 Flink 源码实现主要以 Java 为主，因此也对 Java API 更友好，从两者目前支持的 API 友好程度，应该是 Spark 更好，它目前也支持 Python API，但是 Flink 新版本也在不断的支持 Python API。</p><p><strong>Spark 支持更多的 Machine Learning Lib</strong></p><p>你可以很轻松的使用 Spark MLlib 提供的机器学习算法，然后将这些这些机器学习算法模型应用在流数据中，目前 Flink Machine Learning 这块的内容还较少，不过阿里宣称会开源些 Flink Machine Learning 算法，保持和 Spark 目前已有的算法一致，我自己在 GitHub 上看到一个阿里开源的仓库，感兴趣的可以看看 <a href="https://github.com/alibaba/flink-ai-extended">flink-ai-extended</a>。</p><p><strong>Spark Checkpoint</strong></p><p>Spark 和 Flink 一样都支持 Checkpoint，但是 Flink 还支持 Savepoint，你可以在停止 Flink 作业的时候使用 Savepoint 将作业的状态保存下来，当作业重启的时候再从 Savepoint 中将停止作业那个时刻的状态恢复起来，保持作业的状态和之前一致。</p><p><strong>Spark SQL</strong></p><p>Spark 除了 DataFrames 和 Datasets 外，也还有 SQL API，这样你就可以通过 SQL 查询数据，另外 Spark SQL 还可以用于从 Hive 中读取数据。</p><p>从 Spark 官网也可以看到很多比较好的特性，这里就不一一介绍了，如果对 Spark 感兴趣的话也可以去<a href="https://spark.apache.org/docs/latest/index.html">官网</a>了解一下具体的使用方法和实现原理。</p><p><strong>Spark Streaming 优缺点</strong></p><p>1、优点</p><ul><li>Spark Streaming 内部的实现和调度方式高度依赖 Spark 的 DAG 调度器和 RDD，这就决定了 Spark Streaming 的设计初衷必须是粗粒度方式的，也就无法做到真正的实时处理</li><li>Spark Streaming 的粗粒度执行方式使其确保“处理且仅处理一次”的特性，同时也可以更方便地实现容错恢复机制。</li><li>由于 Spark Streaming 的 DStream 本质是 RDD 在流式数据上的抽象，因此基于 RDD 的各种操作也有相应的基于 DStream 的版本，这样就大大降低了用户对于新框架的学习成本，在了解 Spark 的情况下用户将很容易使用 Spark Streaming。</li></ul><p>2、缺点</p><ul><li>Spark Streaming 的粗粒度处理方式也造成了不可避免的数据延迟。在细粒度处理方式下，理想情况下每一条记录都会被实时处理，而在 Spark Streaming 中，数据需要汇总到一定的量后再一次性处理，这就增加了数据处理的延迟，这种延迟是由框架的设计引入的，并不是由网络或其他情况造成的。</li><li>使用的是 Processing Time 而不是 Event Time</li></ul><h3 id="1-3-4-Structured-Streaming"><a href="#1-3-4-Structured-Streaming" class="headerlink" title="1.3.4 Structured Streaming"></a>1.3.4 Structured Streaming</h3><h3 id="1-3-5-Storm"><a href="#1-3-5-Storm" class="headerlink" title="1.3.5 Storm"></a>1.3.5 Storm</h3><h4 id="Storm-核心组件"><a href="#Storm-核心组件" class="headerlink" title="Storm 核心组件"></a>Storm 核心组件</h4><h4 id="Storm-核心概念"><a href="#Storm-核心概念" class="headerlink" title="Storm 核心概念"></a>Storm 核心概念</h4><h4 id="Storm-数据处理流程图"><a href="#Storm-数据处理流程图" class="headerlink" title="Storm 数据处理流程图"></a>Storm 数据处理流程图</h4><h3 id="1-3-6-计算框架对比"><a href="#1-3-6-计算框架对比" class="headerlink" title="1.3.6 计算框架对比"></a>1.3.6 计算框架对比</h3><h4 id="Flink-VS-Spark"><a href="#Flink-VS-Spark" class="headerlink" title="Flink VS Spark"></a>Flink VS Spark</h4><h4 id="Flink-VS-Storm"><a href="#Flink-VS-Storm" class="headerlink" title="Flink VS Storm"></a>Flink VS Storm</h4><h4 id="全部对比结果"><a href="#全部对比结果" class="headerlink" title="全部对比结果"></a>全部对比结果</h4><p>加入知识星球可以看到上面文章：<a href="https://t.zsxq.com/vVjeMBY">https://t.zsxq.com/vVjeMBY</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-25-zsxq.jpg" alt=""></p><h3 id="1-3-7-小结与反思"><a href="#1-3-7-小结与反思" class="headerlink" title="1.3.7 小结与反思"></a>1.3.7 小结与反思</h3><p>因在 1.2 节中已经对 Flink 的特性做了很详细的讲解，所以本节主要介绍其他几种计算框架（Blink、Spark、Spark Streaming、Structured Streaming、Storm），并对比分析了这几种框架的特点与不同。你对这几种计算框架中的哪个最熟悉呢？了解过它们之间的差异吗？你有压测过它们的处理数据的性能吗？</p><p>本章第一节从公司的日常实时计算需求出发，来分析该如何去实现这种实时需求，接着对比了实时计算与离线计算的区别，从而引出了实时计算的优势，接着就在第二节开始介绍本书的重点 —— 实时计算引擎 Flink，把 Flink 的架构、API、特点、优势等方面都做了讲解，在第三节中对比了市面上现有的计算框架，分别对这些框架做了异同点对比，最后还汇总了它们在各个方面的优势和劣势，以供大家公司内部的技术选型。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-3-大数据计算框架对比&quot;&gt;&lt;a href=&quot;#1-3-大数据计算框架对比&quot; class=&quot;headerlink&quot; title=&quot;1.3 大数据计算框架对比&quot;&gt;&lt;/a&gt;1.3 大数据计算框架对比&lt;/h2&gt;&lt;p&gt;在 1.2 节中已经跟大家详细介绍了 Flink，那么在本节就主要 Blink、Spark Streaming、Structured Streaming 和 Storm 的区别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《Flink 实战与性能优化》—— 彻底了解大数据实时计算框架 Flink</title>
    <link href="http://www.54tianzhisheng.cn/2021/07/05/flink-in-action-1.2/"/>
    <id>http://www.54tianzhisheng.cn/2021/07/05/flink-in-action-1.2/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-10-27T14:02:00.366Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-2-彻底了解大数据实时计算框架-Flink"><a href="#1-2-彻底了解大数据实时计算框架-Flink" class="headerlink" title="1.2 彻底了解大数据实时计算框架 Flink"></a>1.2 彻底了解大数据实时计算框架 Flink</h2><p>在 1.1 节中讲解了日常开发常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。</p><a id="more"></a><ul><li>第一代：Hadoop 承载的 MapReduce</li><li>第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务</li><li>第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表</li><li>第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表</li></ul><p>或许会有人不同意以上的分类，笔者觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。</p><p>本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看 <strong>数据集类型</strong> 和 <strong>数据运算模型</strong> 的种类。</p><h4 id="数据集类型"><a href="#数据集类型" class="headerlink" title="数据集类型"></a>数据集类型</h4><p>数据集类型有分无穷和有界数据集：</p><ul><li>无穷数据集：无穷的持续集成的数据集合</li><li>有界数据集：有限不会改变的数据集合</li></ul><p>那么那些常见的无穷数据集有哪些呢？</p><ul><li>用户与客户端的实时交互数据</li><li>应用实时产生的日志</li><li>金融市场的实时交易记录</li><li>…</li></ul><h4 id="数据运算模型"><a href="#数据运算模型" class="headerlink" title="数据运算模型"></a>数据运算模型</h4><p>数据运算模型有分流式处理和批处理：</p><ul><li>流式：只要数据一直在产生，计算就持续地进行</li><li>批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源</li></ul><p>那么我们再来看看 Flink 它是什么呢？</p><h3 id="1-2-1-Flink-简介"><a href="#1-2-1-Flink-简介" class="headerlink" title="1.2.1 Flink 简介"></a>1.2.1 Flink 简介</h3><p>Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集，总结如下图所示。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pRMhfm.jpg" alt=""></p><p>如下图所示，Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/vY6T3M.jpg" alt=""></p><h3 id="1-2-2-Flink-整体架构"><a href="#1-2-2-Flink-整体架构" class="headerlink" title="1.2.2 Flink 整体架构"></a>1.2.2 Flink 整体架构</h3><p>Flink 整体架构从下至上分为：</p><ol><li><p>部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。</p></li><li><p>运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。</p></li><li><p>API：DataStream、DataSet、Table API &amp; SQL。</p></li><li><p>扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。</p></li></ol><p>整体架构如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Drsi9h.jpg" alt=""></p><h3 id="1-2-3-Flink-的多种方式部署"><a href="#1-2-3-Flink-的多种方式部署" class="headerlink" title="1.2.3 Flink 的多种方式部署"></a>1.2.3 Flink 的多种方式部署</h3><p>作为一个计算引擎，如果要做的足够完善，除了它自身的各种特点要包含，还得支持各种生态圈，比如部署的情况，Flink 是支持以 Standalone、YARN、Kubernetes、Mesos 等形式部署的，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-034112.png" alt=""></p><p>每种部署方式介绍如下：</p><ul><li><p>Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群。</p></li><li><p>Standalone：在 Flink 目录下执行 <code>bin/start-cluster.sh</code> 脚本则会启动一个 Standalone 模式的集群。</p></li><li><p>YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下图所示。</p></li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-034029.png" alt=""></p><ul><li>Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 <a href="https://github.com/Aleksandr-Filichkin/flink-k8s/blob/master/flow.jpg">GitHub</a> 看到有下面这种运行架构的。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-19-071249.jpg" alt=""></p><p>通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。</p><h3 id="1-2-4-Flink-分布式运行流程"><a href="#1-2-4-Flink-分布式运行流程" class="headerlink" title="1.2.4 Flink 分布式运行流程"></a>1.2.4 Flink 分布式运行流程</h3><p>Flink 作业提交架构流程如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/p92UrK.jpg" alt=""></p><p>具体流程介绍如下：</p><ol><li><p>Program Code：我们编写的 Flink 应用程序代码</p></li><li><p>Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 JobManager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户</p></li><li><p>JobManager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 Checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 Checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; JobManager 包含 Actor system、Scheduler、Check pointing 三个重要的组件</p></li><li><p>TaskManager：从 JobManager 处接收需要部署的 Task。TaskManager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 TaskManager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 TaskManager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。<br>同一 JVM 中的任务共享 TCP 连接和心跳消息。TaskManager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p></li></ol><h3 id="1-2-5-Flink-API"><a href="#1-2-5-Flink-API" class="headerlink" title="1.2.5 Flink API"></a>1.2.5 Flink API</h3><p>Flink 提供了不同的抽象级别的 API 以开发流式或批处理应用，如下图所示。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ozmU46.jpg" alt=""></p><p>这四种 API 功能分别是：</p><ul><li><p>最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。</p></li><li><p>DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。</p></li><li><p>Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。<br>你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</p></li><li><p>Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。</p></li></ul><p>Flink 除了 DataStream 和 DataSet API，它还支持 Table API &amp; SQL，Flink 也将通过 SQL 来构建统一的大数据流批处理引擎，因为在公司中通常会有那种每天定时生成报表的需求（批处理的场景，每晚定时跑一遍昨天的数据生成一个结果报表），但是也是会有流处理的场景（比如采用 Flink 来做实时性要求很高的需求），于是慢慢的整个公司的技术选型就变得越来越多了，这样开发人员也就要面临着学习两套不一样的技术框架，运维人员也需要对两种不一样的框架进行环境搭建和作业部署，平时还要维护作业的稳定性。当我们的系统变得越来越复杂了，作业越来越多了，这对于开发人员和运维来说简直就是噩梦，没准哪天凌晨晚上就被生产环境的告警电话给叫醒。所以 Flink 系统能通过 SQL API 来解决批流统一的痛点，这样不管是开发还是运维，他们只需要关注一个计算框架就行，从而减少企业的用人成本和后期开发运维成本。</p><h3 id="1-2-6-Flink-程序与数据流结构"><a href="#1-2-6-Flink-程序与数据流结构" class="headerlink" title="1.2.6 Flink 程序与数据流结构"></a>1.2.6 Flink 程序与数据流结构</h3><p>一个完整的 Flink 应用程序结构如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-141653.png" alt=""></p><p>它们的功能分别是：</p><ul><li>Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</li><li>Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</li><li>Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</li></ul><p>代码结构如下图所示：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/u3RagR.jpg" alt=""></p><h3 id="1-2-7-丰富的-Connector"><a href="#1-2-7-丰富的-Connector" class="headerlink" title="1.2.7 丰富的 Connector"></a>1.2.7 丰富的 Connector</h3><p>通过源码可以发现不同版本的 Kafka、不同版本的 ElasticSearch、Cassandra、HBase、Hive、HDFS、RabbitMQ 都是支持的，除了流应用的 Connector 是支持的，另外还支持 SQL，如下图所示。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-10-101956.png" alt=""></p><p>再就是要考虑计算的数据来源和数据最终存储，因为 Flink 在大数据领域的的定位就是实时计算，它不做存储（虽然 Flink 中也有 State 去存储状态数据，这里说的存储类似于 MySQL、ElasticSearch 等存储），所以在计算的时候其实你需要考虑的是数据源来自哪里，计算后的结果又存储到哪里去。庆幸的是 Flink 目前已经支持大部分常用的组件了，比如在 Flink 中已经支持了如下这些 Connector：</p><ul><li>不同版本的 Kafka</li><li>不同版本的 ElasticSearch</li><li>Redis</li><li>MySQL</li><li>Cassandra</li><li>RabbitMQ</li><li>HBase</li><li>HDFS</li><li>…</li></ul><p>这些 Connector 除了支持流作业外，目前还有还有支持 SQL 作业的，除了这些自带的 Connector 外，还可以通过 Flink 提供的接口做自定义 Source 和 Sink（在 3.8 节中）。</p><h3 id="1-2-8-事件时间-amp-处理时间语义"><a href="#1-2-8-事件时间-amp-处理时间语义" class="headerlink" title="1.2.8 事件时间&amp;处理时间语义"></a>1.2.8 事件时间&amp;处理时间语义</h3><p>Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，如下图所示，后面 3.1 节中会很详细的讲解 Flink 中 Time 的概念。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-14-140502.png" alt=""></p><h3 id="1-2-9-灵活的窗口机制"><a href="#1-2-9-灵活的窗口机制" class="headerlink" title="1.2.9 灵活的窗口机制"></a>1.2.9 灵活的窗口机制</h3><p>Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window，如下图所示。后面 3.2 节中会很详细的讲解 Flink 中 Window 的概念。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-12-15-034900.png" alt=""></p><h3 id="1-2-10-并行执行任务机制"><a href="#1-2-10-并行执行任务机制" class="headerlink" title="1.2.10 并行执行任务机制"></a>1.2.10 并行执行任务机制</h3><p>Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ggMHCK.jpg" alt=""></p><h3 id="1-2-11-状态存储和容错"><a href="#1-2-11-状态存储和容错" class="headerlink" title="1.2.11 状态存储和容错"></a>1.2.11 状态存储和容错</h3><p>Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构：</p><ul><li>ValueState</li><li>MapState</li><li>ListState</li><li>ReducingState</li><li>AggregatingState</li></ul><p>另外状态存储也支持多种方式：</p><ul><li>MemoryStateBackend：存储在内存中</li><li>FsStateBackend：存储在文件中</li><li>RocksDBStateBackend：存储在 RocksDB 中</li></ul><p>Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。</p><h3 id="1-2-12-自己的内存管理机制"><a href="#1-2-12-自己的内存管理机制" class="headerlink" title="1.2.12 自己的内存管理机制"></a>1.2.12 自己的内存管理机制</h3><p>Flink 并不是直接把对象存放在堆内存上，而是将对象序列化为固定数量的预先分配的内存段。它采用类似 DBMS 的排序和连接算法，可以直接操作二进制数据，以此将序列化和反序列化开销降到最低。如果需要处理的数据容量超过内存，那么 Flink 的运算符会将部分数据存储到磁盘。Flink 的主动内存管理和操作二进制数据有几个好处：</p><ul><li>保证内存可控，可以防止 OutOfMemoryError</li><li>减少垃圾收集压力</li><li>节省数据的存储空间</li><li>高效的二进制操作</li></ul><p>Flink 是如何分配内存、将对象进行序列化和反序列化以及对二进制数据进行操作的，可以参考文章 <a href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/">Flink 是如何管理好内存的？</a> ，该文中讲解了 Flink 的内存管理机制。</p><h3 id="1-2-13-多种扩展库"><a href="#1-2-13-多种扩展库" class="headerlink" title="1.2.13 多种扩展库"></a>1.2.13 多种扩展库</h3><p>Flink 扩展库中含有机器学习、Gelly 图形处理、CEP 复杂事件处理、State Processing API 等，这些扩展库在一些特殊场景下会比较适用，关于这块内容可以在第六章查看。</p><h3 id="1-2-14-小结与反思"><a href="#1-2-14-小结与反思" class="headerlink" title="1.2.14 小结与反思"></a>1.2.14 小结与反思</h3><p>本节在开始介绍 Flink 之前先讲解了下数据集类型和数据运算模型，接着开始介绍 Flink 的各种特性，对于这些特性，你是否有和其他的计算框架做过对比？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-2-彻底了解大数据实时计算框架-Flink&quot;&gt;&lt;a href=&quot;#1-2-彻底了解大数据实时计算框架-Flink&quot; class=&quot;headerlink&quot; title=&quot;1.2 彻底了解大数据实时计算框架 Flink&quot;&gt;&lt;/a&gt;1.2 彻底了解大数据实时计算框架 Flink&lt;/h2&gt;&lt;p&gt;在 1.1 节中讲解了日常开发常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
