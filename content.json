[{"title":"Flink Table Store ——从计算到存储提升流批统一端到端用户体验","date":"2022-05-11T16:00:00.000Z","path":"2022/05/12/flink-table-store/","text":"该项目用于在 Flink 中为流处理和批处理构建动态表，支持超大流量的数据提取和及时的数据查询。 注意：该项目仍处于 beta 状态，正在快速发展，不建议直接在生产环境中使用它。 Flink Table Store 介绍在过去的几年里，得益于 Flink 社区众多的贡献者和用户，Apache Flink 已经成为最好的分布式计算引擎之一，尤其是在大规模有状态流处理方面。然而，当人们试图从他们的数据中实时获取洞察力时，仍然面临着一些挑战。在这些挑战中，一个突出的问题是缺乏满足所有计算模式的存储。 到目前为止，我们通常会部署一些存储系统来与 Flink 一起用于不同目的是很常见的。典型的设置是用于流处理的消息队列、用于批处理和即席查询的可查询文件系统/对象存储，以及用于查找的 K-V 存储。由于其复杂性和异构性，这种架构在数据质量和系统维护方面提出了挑战。这正在成为影响 Apache Flink 带来的流批统一端到端用户体验的一大问题。 Flink Table Store 的目标就是解决上述问题，这是该项目的重要一步。它将 Flink 的能力从计算扩展到存储领域。 因此，我们可以为用户提供更好的端到端体验。 Flink Table Store 旨在提供统一的存储抽象，让用户不必自己构建混合存储。具体来说，Flink Table Store 提供以下核心能力： 支持超大数据集的存储，并允许以批处理和流方式读取和写入 支持毫秒级别延迟的流式查询 支持秒级别延迟的 Batch/OLAP 查询 默认支持流读增量快照，所以用户不需要自己解决组合不同存储的问题 在这个版本中，架构如上图所示： 用户可以使用 Flink 将数据写入到 Table Store 中，既可以通过流式将数据库中捕获的变更日志写入，也可以通过从数据仓库等其他存储中批量加载数据后再写入 用户可以使用 Flink 以不同的方式查询 Table Store，包括流式查询和 Batch/OLAP 查询。 还值得注意的是，用户也可以使用其他引擎（例如 Apache Hive）从 Table Store 中查询 在底层，Table Store 使用混合存储架构，使用 Lake Store 存储历史数据，使用 Queue 系统（目前支持 Apache Kafka 集成）存储增量数据。 它为混合流式读取提供增量快照 Table Store 的 Lake Store 将数据作为列文件存储在文件系统/对象存储上，并使用 LSM 结构来支持大量的数据更新和高性能查询 非常感谢以下系统的启发：Apache Iceberg 和 RocksDB。 后续进展社区目前正在努力强化核心逻辑，稳定存储格式等，以使 Flink Table Store 可以投入生产。 在即将发布的 0.2.0 版本中，可以期待（至少）以下功能： 生态：支持 Apache Hive Engine 的 Flink Table Store Reader 核心：支持自适应 Bucket 数量 核心：支持仅 append 的数据，Table Store 不仅仅局限于更新场景 核心：完善的 schema 变化 改进基于预览版得到的反馈 从中期来看，你还可以期待： 生态系统：支持 Trino、PrestoDB 和 Apache Spark 的 Flink Table Store Reader Flink Table Store Service 会加速更新，提升查询性能 尝鲜可以通过 https://nightlies.apache.org/flink/flink-table-store-docs-release-0.1/docs/try-table-store/quick-start/ 来尝试一下。 下载链接：https://flink.apache.org/downloads.html 翻译原文链接：https://flink.apache.org/news/2022/05/11/release-table-store-0.1.0.html 原文作者：李劲松 &amp; 秦江杰","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Iceberg Source 并行度推断源码解析","date":"2022-05-01T16:00:00.000Z","path":"2022/05/02/flink-iceberg-source-parallelism/","text":"批读 IcebergIceberg 提供了两个配置： 123456789101112public static final ConfigOption&lt;Boolean&gt; TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM = ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\") .booleanType() .defaultValue(true) .withDescription(\"If is false, parallelism of source are set by config.\\n\" + \"If is true, source parallelism is inferred according to splits number.\\n\");public static final ConfigOption&lt;Integer&gt; TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX = ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\") .intType() .defaultValue(100) .withDescription(\"Sets max infer parallelism for source operator.\"); table.exec.iceberg.infer-source-parallelism：默认是 true，意味着 source 的并行度是根据推断来配置的，如果配置的 false 的话，那么并行度的配置是以配置的为准。 table.exec.iceberg.infer-source-parallelism.max： 默认是 100，source 算子的最大并行度。 这两个参数只在 FileSource 的 inferParallelism 方法中调用： 1234567891011121314151617181920212223242526272829303132int inferParallelism(FlinkInputFormat format, ScanContext context) &#123; // 读取 table.exec.resource.default-parallelism 配置，默认值为 -1 int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM); // 读取 table.exec.iceberg.infer-source-parallelism 配置，默认是 true if (readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) &#123; // 读取 table.exec.iceberg.infer-source-parallelism.max 配置，默认是 100 int maxInferParallelism = readableConfig.get(FlinkConfigOptions .TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX); Preconditions.checkState( maxInferParallelism &gt;= 1, FlinkConfigOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\"); //获取表的 splitNum int splitNum; try &#123; FlinkInputSplit[] splits = format.createInputSplits(0); splitNum = splits.length; &#125; catch (IOException e) &#123; throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e); &#125; parallelism = Math.min(splitNum, maxInferParallelism); &#125; if (context.limit() &gt; 0) &#123; int limit = context.limit() &gt;= Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit(); parallelism = Math.min(parallelism, limit); &#125; // parallelism must be positive. parallelism = Math.max(1, parallelism); return parallelism;&#125; 在下面代码获取到表的 splitNum 1234567891011121314151617181920212223242526272829303132333435363738394041FlinkInputSplit[] splits = format.createInputSplits(0);splitNum = splits.length; public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException &#123; // Called in Job manager, so it is OK to load table from catalog. // 加载 catalog tableLoader.open(); final ExecutorService workerPool = ThreadPools.newWorkerPool(\"iceberg-plan-worker-pool\", context.planParallelism()); try (TableLoader loader = tableLoader) &#123; // 加载表 Table table = loader.loadTable(); // 调用 return FlinkSplitPlanner.planInputSplits(table, context, workerPool); &#125; finally &#123; workerPool.shutdown(); &#125; &#125; static FlinkInputSplit[] planInputSplits(Table table, ScanContext context, ExecutorService workerPool) &#123; // 主要通过 planTasks 方法 try (CloseableIterable&lt;CombinedScanTask&gt; tasksIterable = planTasks(table, context, workerPool)) &#123; List&lt;CombinedScanTask&gt; tasks = Lists.newArrayList(tasksIterable); FlinkInputSplit[] splits = new FlinkInputSplit[tasks.size()]; boolean exposeLocality = context.exposeLocality(); Tasks.range(tasks.size()) .stopOnFailure() .executeWith(exposeLocality ? workerPool : null) .run(index -&gt; &#123; CombinedScanTask task = tasks.get(index); String[] hostnames = null; if (exposeLocality) &#123; hostnames = Util.blockLocations(table.io(), task); &#125; splits[index] = new FlinkInputSplit(index, task, hostnames); &#125;); return splits; &#125; catch (IOException e) &#123; throw new UncheckedIOException(\"Failed to process tasks iterable\", e); &#125; &#125; planTasks 方法中主要靠 scan.planTasks()，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public CloseableIterable&lt;CombinedScanTask&gt; planTasks() &#123; CloseableIterable&lt;FileScanTask&gt; fileScanTasks = planFiles(); CloseableIterable&lt;FileScanTask&gt; splitFiles = TableScanUtil.splitFiles(fileScanTasks, targetSplitSize()); return TableScanUtil.planTasks(splitFiles, targetSplitSize(), splitLookback(), splitOpenFileCost());&#125; // 获取文件情况 public CloseableIterable&lt;FileScanTask&gt; planFiles() &#123; Snapshot snapshot = snapshot(); if (snapshot != null) &#123; LOG.info(\"Scanning table &#123;&#125; snapshot &#123;&#125; created at &#123;&#125; with filter &#123;&#125;\", table, snapshot.snapshotId(), DateTimeUtil.formatTimestampMillis(snapshot.timestampMillis()), context.rowFilter()); Listeners.notifyAll( new ScanEvent(table.name(), snapshot.snapshotId(), context.rowFilter(), schema())); return planFiles(ops, snapshot, context.rowFilter(), context.ignoreResiduals(), context.caseSensitive(), context.returnColumnStats()); &#125; else &#123; LOG.info(\"Scanning empty table &#123;&#125;\", table); return CloseableIterable.empty(); &#125; &#125; // split public static CloseableIterable&lt;FileScanTask&gt; splitFiles(CloseableIterable&lt;FileScanTask&gt; tasks, long splitSize) &#123; Preconditions.checkArgument(splitSize &gt; 0, \"Invalid split size (negative or 0): %s\", splitSize); Iterable&lt;FileScanTask&gt; splitTasks = FluentIterable .from(tasks) .transformAndConcat(input -&gt; input.split(splitSize)); // Capture manifests which can be closed after scan planning return CloseableIterable.combine(splitTasks, tasks); &#125; public static CloseableIterable&lt;CombinedScanTask&gt; planTasks(CloseableIterable&lt;FileScanTask&gt; splitFiles, long splitSize, int lookback, long openFileCost) &#123; Preconditions.checkArgument(splitSize &gt; 0, \"Invalid split size (negative or 0): %s\", splitSize); Preconditions.checkArgument(lookback &gt; 0, \"Invalid split planning lookback (negative or 0): %s\", lookback); Preconditions.checkArgument(openFileCost &gt;= 0, \"Invalid file open cost (negative): %s\", openFileCost); // Check the size of delete file as well to avoid unbalanced bin-packing Function&lt;FileScanTask, Long&gt; weightFunc = file -&gt; Math.max( file.length() + file.deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum(), (1 + file.deletes().size()) * openFileCost); return CloseableIterable.transform( CloseableIterable.combine( new BinPacking.PackingIterable&lt;&gt;(splitFiles, splitSize, lookback, weightFunc, true), splitFiles), BaseCombinedScanTask::new); &#125; 推断到表文件 split 的数量后，那么接下来是决定并行度大小的时候了： 1parallelism = Math.min(splitNum, maxInferParallelism); 取配置的最大并行度和 split 数量的最小值，eg：设置的 source 最大并行度为 50，但是根据表文件划分出来的 split 数量为 40，那么 source 的并行度为 40。 如果没有配置 table.exec.iceberg.infer-source-parallelism 为 true 的话，那么就以 table.exec.resource.default-parallelism 的并行度为准（默认值是 -1）。 继续分析接下来的代码： 1234if (context.limit() &gt; 0) &#123; int limit = context.limit() &gt;= Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit(); parallelism = Math.min(parallelism, limit);&#125; limit 像是 SQL 查询语句里面的 limit 的值，如果配置了 limit，那么也会参与并行度配置的计算的，eg：如果 limti 为 1，那么 parallelism 取前面 parallelism 的值与 1 两者的最小值。 123// parallelism must be positive.parallelism = Math.max(1, parallelism);return parallelism; 最后代码保证并行度的最小值为 1。 来看下 inferParallelism 方法的调用情况，只在 FileSource 类的 build() 方法调用： 123456789101112131415161718192021public DataStream&lt;RowData&gt; build() &#123; Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\"); FlinkInputFormat format = buildFormat(); ScanContext context = contextBuilder.build(); TypeInformation&lt;RowData&gt; typeInfo = FlinkCompatibilityUtil.toTypeInfo(FlinkSchemaUtil.convert(context.project())); if (!context.isStreaming()) &#123; // 只在 批 模式下生效 int parallelism = inferParallelism(format, context); return env.createInput(format, typeInfo).setParallelism(parallelism); &#125; else &#123; StreamingMonitorFunction function = new StreamingMonitorFunction(tableLoader, context); String monitorFunctionName = String.format(\"Iceberg table (%s) monitor\", table); String readerOperatorName = String.format(\"Iceberg table (%s) reader\", table); return env.addSource(function, monitorFunctionName) .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format)); &#125;&#125; 可以看 TestFlinkScanSql 测试类的 testInferedParallelism 方法进行测试： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Testpublic void testInferedParallelism() throws IOException &#123; Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), TestFixtures.SCHEMA, TestFixtures.SPEC); TableLoader tableLoader = TableLoader.fromHadoopTable(table.location()); FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat(); ScanContext scanContext = ScanContext.builder().build(); // Empty table, infer parallelism should be at least 1 int parallelism = FlinkSource.forRowData().inferParallelism(flinkInputFormat, scanContext); Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism); GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER); DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), RandomGenericData.generate(TestFixtures.SCHEMA, 2, 0L)); DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0), RandomGenericData.generate(TestFixtures.SCHEMA, 2, 0L)); helper.appendToTable(dataFile1, dataFile2); // Make sure to generate 2 CombinedScanTasks long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes()); sql(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen); // 2 splits (max infer is the default value 100 , max &gt; splits num), the parallelism is splits num : 2 parallelism = FlinkSource.forRowData().inferParallelism(flinkInputFormat, scanContext); Assert.assertEquals(\"Should produce the expected parallelism.\", 2, parallelism); // 2 splits and limit is 1 , max infer parallelism is default 100， // which is greater than splits num and limit, the parallelism is the limit value : 1 parallelism = FlinkSource.forRowData().inferParallelism(flinkInputFormat, ScanContext.builder().limit(1).build()); Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism); // 2 splits and max infer parallelism is 1 (max &lt; splits num), the parallelism is 1 Configuration configuration = new Configuration(); configuration.setInteger(FlinkConfigOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX, 1); parallelism = FlinkSource.forRowData() .flinkConf(configuration) .inferParallelism(flinkInputFormat, ScanContext.builder().build()); Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism); // 2 splits, max infer parallelism is 1, limit is 3, the parallelism is max infer parallelism : 1 parallelism = FlinkSource.forRowData() .flinkConf(configuration) .inferParallelism(flinkInputFormat, ScanContext.builder().limit(3).build()); Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism); // 2 splits, infer parallelism is disabled, the parallelism is flink default parallelism 1 configuration.setBoolean(FlinkConfigOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM, false); parallelism = FlinkSource.forRowData() .flinkConf(configuration) .inferParallelism(flinkInputFormat, ScanContext.builder().limit(3).build()); Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);&#125; 注意⚠️：该代码模块是在 Iceberg 项目的 flink module 下 流读 Iceberg并没有针对流读配置 Source 并行度 加入知识星球可以看到上面文章：https://t.zsxq.com/6YjyJYR","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Hive Source 并行度推断源码解析","date":"2022-04-30T16:00:00.000Z","path":"2022/05/01/flink-hive-source-parallelism/","text":"批读 HiveHiveOptions 中有两个配置 1234567891011public static final ConfigOption&lt;Boolean&gt; TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM = key(\"table.exec.hive.infer-source-parallelism\") .defaultValue(true) .withDescription( \"If is false, parallelism of source are set by config.\\n\" + \"If is true, source parallelism is inferred according to splits number.\\n\");public static final ConfigOption&lt;Integer&gt; TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX = key(\"table.exec.hive.infer-source-parallelism.max\") .defaultValue(1000) .withDescription(\"Sets max infer parallelism for source operator.\"); table.exec.hive.infer-source-parallelism：默认值是 true，表示 source 的并行度是根据数据分区数和文件数推断的，如果设置为 false 的话表示并行度是以配置的为准 table.exec.hive.infer-source-parallelism.max：默认值是 1000，表示读取 Hive 数据的 source 最大并行度 这两个参数只在 HiveParallelismInference 类中使用，观察到 HiveParallelismInference 类是专门针对 Hive 并行度配置的工具类，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * A utility class to calculate parallelism for Hive connector considering various factors. */class HiveParallelismInference &#123; private static final Logger LOG = LoggerFactory.getLogger(HiveParallelismInference.class); private final ObjectPath tablePath; private final boolean infer; private final int inferMaxParallelism; private int parallelism; HiveParallelismInference(ObjectPath tablePath, ReadableConfig flinkConf) &#123; this.tablePath = tablePath; // 获取 table.exec.hive.infer-source-parallelism 配置并赋值， this.infer = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM); // 获取 table.exec.hive.infer-source-parallelism.max 配置并赋值 this.inferMaxParallelism = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX); Preconditions.checkArgument( inferMaxParallelism &gt;= 1, HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\"); // 获取 table.exec.resource.default-parallelism 配置 this.parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM); &#125; /** * Apply limit to calculate the parallelism. * Here limit is the limit in query &lt;code&gt;SELECT * FROM xxx LIMIT [limit]&lt;/code&gt;. */ int limit(Long limit) &#123; if (limit != null) &#123; parallelism = Math.min(parallelism, (int) (limit / 1000)); &#125; // make sure that parallelism is at least 1 return Math.max(1, parallelism); &#125; //根据 /** * Infer parallelism by number of files and number of splits. * If &#123;@link HiveOptions#TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM&#125; is not set this method does nothing. */ HiveParallelismInference infer( SupplierWithException&lt;Integer, IOException&gt; numFiles, SupplierWithException&lt;Integer, IOException&gt; numSplits) &#123; //如果设置 table.exec.hive.infer-source-parallelism 为 false，则直接跳过了 if (!infer) &#123; return this; &#125; try &#123; // `createInputSplits` is costly, // so we try to avoid calling it by first checking the number of files // which is the lower bound of the number of splits int lowerBound = logRunningTime(\"getNumFiles\", numFiles); if (lowerBound &gt;= inferMaxParallelism) &#123; parallelism = inferMaxParallelism; return this; &#125; int splitNum = logRunningTime(\"createInputSplits\", numSplits); parallelism = Math.min(splitNum, inferMaxParallelism); &#125; catch (IOException e) &#123; throw new FlinkHiveException(e); &#125; return this; &#125; private int logRunningTime( String operationName, SupplierWithException&lt;Integer, IOException&gt; supplier) throws IOException &#123; long startTimeMillis = System.currentTimeMillis(); int result = supplier.get(); LOG.info( \"Hive source(&#123;&#125;&#125;) &#123;&#125; use time: &#123;&#125; ms, result: &#123;&#125;\", tablePath, operationName, System.currentTimeMillis() - startTimeMillis, result); return result; &#125;&#125; 可以看到注释主要是 infer 方法去做的的并行度推断，该方法有两个参数 numFiles 和 numSplits，该方法只在HiveTableSource 类中的 getDataStream 方法中调用，可以查看下图： 那就来看看这两个方法的实现： getNumFiles 方法是用来获取 Hive 表分区下面的文件数量的: 1234567891011121314151617public static int getNumFiles(List&lt;HiveTablePartition&gt; partitions, JobConf jobConf) throws IOException &#123; int numFiles = 0; FileSystem fs = null; for (HiveTablePartition partition : partitions) &#123; StorageDescriptor sd = partition.getStorageDescriptor(); org.apache.hadoop.fs.Path inputPath = new org.apache.hadoop.fs.Path(sd.getLocation()); if (fs == null) &#123; fs = inputPath.getFileSystem(jobConf); &#125; // it's possible a partition exists in metastore but the data has been removed if (!fs.exists(inputPath)) &#123; continue; &#125; numFiles += fs.listStatus(inputPath).length; &#125; return numFiles;&#125; createInputSplits 方法是用来将 Hive 表分区下的文件分割成逻辑上的 InputSplit，这里是在 Flink Hive Connector 里面定义了一个 HiveSourceSplit 类来包装 InputSplit，包含了 Hive 表分区的信息。 123456789101112131415161718192021222324252627282930313233343536public static List&lt;HiveSourceSplit&gt; createInputSplits( int minNumSplits, List&lt;HiveTablePartition&gt; partitions, JobConf jobConf) throws IOException &#123; List&lt;HiveSourceSplit&gt; hiveSplits = new ArrayList&lt;&gt;(); FileSystem fs = null; for (HiveTablePartition partition : partitions) &#123; StorageDescriptor sd = partition.getStorageDescriptor(); org.apache.hadoop.fs.Path inputPath = new org.apache.hadoop.fs.Path(sd.getLocation()); if (fs == null) &#123; fs = inputPath.getFileSystem(jobConf); &#125; // it's possible a partition exists in metastore but the data has been removed if (!fs.exists(inputPath)) &#123; continue; &#125; InputFormat format; try &#123; format = (InputFormat) Class.forName(sd.getInputFormat(), true, Thread.currentThread().getContextClassLoader()).newInstance(); &#125; catch (Exception e) &#123; throw new FlinkHiveException(\"Unable to instantiate the hadoop input format\", e); &#125; ReflectionUtils.setConf(format, jobConf); jobConf.set(INPUT_DIR, sd.getLocation()); //TODO: we should consider how to calculate the splits according to minNumSplits in the future. org.apache.hadoop.mapred.InputSplit[] splitArray = format.getSplits(jobConf, minNumSplits); for (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) &#123; Preconditions.checkState(inputSplit instanceof FileSplit, \"Unsupported InputSplit type: \" + inputSplit.getClass().getName()); hiveSplits.add(new HiveSourceSplit((FileSplit) inputSplit, partition, null)); &#125; &#125; return hiveSplits;&#125; 因为上面两个方法的执行可能需要一点时间，所以专门还写了一个 logRunningTime 记录其执行的时间。 如果文件数大于配置的最大并行度，那么作业的并行度直接以配置的最大并行度为准；否则取 InputSplit 个数与配置的最大并行度两者最小值。 12345678int lowerBound = logRunningTime(\"getNumFiles\", numFiles);if (lowerBound &gt;= inferMaxParallelism) &#123; parallelism = inferMaxParallelism; return this;&#125;int splitNum = logRunningTime(\"createInputSplits\", numSplits);parallelism = Math.min(splitNum, inferMaxParallelism); 然后就是 limit 方法的限制并行度了： 123456789101112/** * Apply limit to calculate the parallelism. * Here limit is the limit in query &lt;code&gt;SELECT * FROM xxx LIMIT [limit]&lt;/code&gt;. */int limit(Long limit) &#123; if (limit != null) &#123; parallelism = Math.min(parallelism, (int) (limit / 1000)); &#125; // make sure that parallelism is at least 1 return Math.max(1, parallelism);&#125; 这个方法的注释的意思是根据查询语句的 limit 来配置并行度，判断前面得到的并行度与 limit/1000 的大小，取两者最小值。举个例子，前面判断这个 Hive 表分区有非常多的文件，比如 10001 个，那大于默认的最大值 1000，那么返回的并行度是 1000，但是因为查询 Hive 的 SQL 只是 100 条，那么这里取值得到的最小值是 0，最后通过 Math.max(1, parallelism) 返回的 source 并行度是 1。 注意⚠️：上面的并行度配置仅仅针对于批作业查 Hive 数据，不针对流读 Hive 数据。 流读 Hive在 HiveTableSource 类中的 getDataStream 方法中并没有针对流读配置 Source 并行度。 加入知识星球可以看到上面文章：https://t.zsxq.com/E6Mj6uv","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"如何提高 Flink K8s 集群资源使用率？","date":"2022-03-25T16:00:00.000Z","path":"2022/03/26/flink-k8s-pod-add-request-and-limit/","text":"问题在 flink on k8s 默认提交作业的命令下，我们会指定作业的 JM/TM 的 CPU 和 Memory，最后作业生成的 pod 它的 CPU/Memory 的 request/limit 都是一样的资源，但是作业真实运行时使用的资源远达不到 limit 的值，这样就会造成机器资源浪费（水位不高，但是机器又不能再申请 pod）。 比如下面命令：（指定了 TM 的资源，未指定 JM 资源） 12345678./bin/flink run-application -p 1 -t kubernetes-application -c com.zhisheng.Test \\ -Dkubernetes.cluster-id=flink-log-alert-test1 \\ -Dtaskmanager.memory.process.size=6g \\ -Djobmanager.memory.process.size=2g \\ -Dkubernetes.jobmanager.cpu=0.5 \\ -Dkubernetes.taskmanager.cpu=1 \\ -Dtaskmanager.numberOfTaskSlots=1 \\ .... JM: TM: 解决方案1、分别为 JM/TM 的 内存和 CPU 添加参数设置 request 和 limit，如果行得通的话，这种方式要增加 8 个参数才能满足需求，但因 Flink 内存模型使得单独设置内存的 request/limit 变得非常复杂，只能设置 CPU 参数，而且之前的参数也将变得不可以使用。 2、分别为 JM/TM 的 内存和 CPU 添加参数 limit 因子，用户配置的内存或者 CPU 的值默认为 request 的值，limit 因子必须 &gt;= 1，这种方式需要增加四个参数，相比第一种方法这种方法较为简单，但是目前 YARN 集群用户的资源配置，大多数作业已经是有一定的资源浪费（申请的资源远大于实际使用的资源），如果使用该方式，用户作业无感迁移到 K8s 集群后，其实资源浪费问题并没有解决。 3、分别为 JM/TM 的 内存和 CPU 添加参数 request 因子，用户配置的内存或者 CPU 的值默认为 limit 的值，request 因子必须 &lt;= 1，我们可以根据生产的数据配置一个合理的值，比如为 0.5。这种方式同样需要增加四个参数，但是这种方法对比第二种带来的好处是，大多数用户作业的资源配置将会更合理，机器同等资源能运行更多的 pod，从而可以提高机器的资源水位。 代码开发 1、在 KubernetesConfigOptions 增加配置 KubernetesConfigOptions.java 12345678910111213141516171819202122232425262728293031323334// jobmanagerpublic static final ConfigOption&lt;Double&gt; JOB_MANAGER_CPU_REQUEST_FACTOR = key(\"kubernetes.jobmanager.cpu.request-factor\") .doubleType() .defaultValue(1.0) .withDescription( \"The request factor of cpu used by job manager. \" + \"The resources request cpu will be set to cpu * request-factor.\");public static final ConfigOption&lt;Double&gt; JOB_MANAGER_MEMORY_REQUEST_FACTOR = key(\"kubernetes.jobmanager.memory.request-factor\") .doubleType() .defaultValue(1.0) .withDescription( \"The request factor of memory used by job manager. \" + \"The resources request memory will be set to memory * request-factor.\");// taskmanagerpublic static final ConfigOption&lt;Double&gt; TASK_MANAGER_CPU_REQUEST_FACTOR = key(\"kubernetes.taskmanager.cpu.request-factor\") .doubleType() .defaultValue(1.0) .withDescription( \"The request factor of cpu used by task manager. \" + \"The resources request cpu will be set to cpu * request-factor.\");public static final ConfigOption&lt;Double&gt; TASK_MANAGER_MEMORY_REQUEST_FACTOR = key(\"kubernetes.taskmanager.memory.request-factor\") .doubleType() .defaultValue(1.0) .withDescription( \"The request factor of memory used by task manager. \" + \"The resources request memory will be set to memory * request-factor.\"); 2、在 KubernetesJobManagerParameters 和 KubernetesTaskManagerParameters 中分别提供获取参数的方法 KubernetesJobManagerParameters.java 12345678910111213141516171819public double getJobManagerCPURequestFactor() &#123; final double requestFactor = flinkConfig.getDouble(KubernetesConfigOptions.JOB_MANAGER_CPU_REQUEST_FACTOR); checkArgument( requestFactor &lt;= 1, \"%s should be less than or equal to 1.\", KubernetesConfigOptions.JOB_MANAGER_CPU_REQUEST_FACTOR.key()); return requestFactor;&#125;public double getJobManagerMemoryRequestFactor() &#123; final double requestFactor = flinkConfig.getDouble(KubernetesConfigOptions.JOB_MANAGER_MEMORY_REQUEST_FACTOR); checkArgument( requestFactor &lt;= 1, \"%s should be less than or equal to 1.\", KubernetesConfigOptions.JOB_MANAGER_MEMORY_REQUEST_FACTOR.key()); return requestFactor;&#125; KubernetesTaskManagerParameters.java 12345678910111213141516171819public double getTaskManagerCPURequestFactor() &#123; final double requestFactor = flinkConfig.getDouble(KubernetesConfigOptions.TASK_MANAGER_CPU_REQUEST_FACTOR); checkArgument( requestFactor &lt;= 1, \"%s should be less than or equal to 1.\", KubernetesConfigOptions.TASK_MANAGER_CPU_REQUEST_FACTOR.key()); return requestFactor;&#125;public double getTaskManagerMemoryRequestFactor() &#123; final double requestFactor = flinkConfig.getDouble(KubernetesConfigOptions.TASK_MANAGER_MEMORY_REQUEST_FACTOR); checkArgument( requestFactor &lt;= 1, \"%s should be less than or equal to 1.\", KubernetesConfigOptions.TASK_MANAGER_MEMORY_REQUEST_FACTOR.key()); return requestFactor;&#125; 3、KubernetesUtils.getResourceRequirements() 方法做如下改变，增加 request 因子参数 KubernetesUtils.java 12345678910111213141516171819202122232425262728293031323334353637383940/** * Get resource requirements from memory and cpu. * * @param mem Memory in mb. * @param memoryRequestFactor Memory request factor. * @param cpu cpu. * @param cpuRequestFactor cpu request factor. * @param externalResources external resources * @return KubernetesResource requirements. */public static ResourceRequirements getResourceRequirements( int mem, double memoryRequestFactor, double cpu, double cpuRequestFactor, Map&lt;String, Long&gt; externalResources) &#123; //todo：cpu 和 内存分别设置一个因子，默认是 0.5，用户设置的资源配置为 limit；request = limit * 因子 final Quantity cpuQuantity = new Quantity(String.valueOf(cpu)); final Quantity cpuRequestQuantity = new Quantity(String.valueOf(cpu * cpuRequestFactor)); final Quantity memQuantity = new Quantity(mem + Constants.RESOURCE_UNIT_MB); final Quantity memRequestQuantity = new Quantity(((int) (mem * memoryRequestFactor)) + Constants.RESOURCE_UNIT_MB); ResourceRequirementsBuilder resourceRequirementsBuilder = new ResourceRequirementsBuilder() .addToRequests(Constants.RESOURCE_NAME_MEMORY, memRequestQuantity) .addToRequests(Constants.RESOURCE_NAME_CPU, cpuRequestQuantity) .addToLimits(Constants.RESOURCE_NAME_MEMORY, memQuantity) .addToLimits(Constants.RESOURCE_NAME_CPU, cpuQuantity); // Add the external resources to resource requirement. for (Map.Entry&lt;String, Long&gt; externalResource: externalResources.entrySet()) &#123; final Quantity resourceQuantity = new Quantity(String.valueOf(externalResource.getValue())); resourceRequirementsBuilder .addToRequests(externalResource.getKey(), resourceQuantity) .addToLimits(externalResource.getKey(), resourceQuantity); LOG.info(\"Request external resource &#123;&#125; with config key &#123;&#125;.\", resourceQuantity.getAmount(), externalResource.getKey()); &#125; return resourceRequirementsBuilder.build();&#125; 4、在 InitJobManagerDecorator 和 InitTaskManagerDecorator 调用上面方法的地方做相应的修改 最终效果可以在 flink-conf.yaml 中定义配置如下： 1234kubernetes.jobmanager.cpu.request-factor: 0.5kubernetes.jobmanager.memory.request-factor: 0.8kubernetes.taskmanager.cpu.request-factor: 0.5kubernetes.taskmanager.memory.request-factor: 0.8 当然，用户的作业提交参数中也可以使用上面的参数进行覆盖，最终效果如下： 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"宕机一台机器，结果一百多个 Flink 作业挂了","date":"2021-11-10T16:00:00.000Z","path":"2021/11/11/flink-akka-framesize/","text":"背景因宕机了一台物理机器，实时集群不少作业发生 failover，其中大部分作业都能 failover 成功，某个部门的部分作业一直在 failover，始终未成功，到 WebUI 查看作业异常日志如下： 12345678910111213141516171819202122232021-11-09 16:01:11java.util.concurrent.CompletionException: java.lang.reflect.UndeclaredThrowableException at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.reflect.UndeclaredThrowableException at com.sun.proxy.$Proxy54.submitTask(Unknown Source) at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72) at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) ... 7 moreCaused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize. at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.createRpcInvocationMessage(AkkaInvocationHandler.java:276) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:205) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:134) ... 11 more 解决异常过程从上面的异常日志中我们提取到关键信息： 1Caused by: java.io.IOException: The rpc invocation size 56424326 exceeds the maximum akka framesize. 看起来是 RPC 的消息大小超过了默认的 akka framesize 的最大值了，所以我们来了解一下这个值的默认值，从 官网 我们可以看的到该值的默认大小为 “10485760b”，并且该参数的描述为： 1Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier. 翻译过来的意思就是：这个参数是 JobManager 和 TaskManagers 之间通信允许的最大消息大小，如果 Flink 作业因为通信消息大小超过了该值，你可以通过增加该值的大小来解决，该参数需要指定一个单位。 分析原因Flink 使用 Akka 作为组件（JobManager/TaskManager/ResourceManager）之间的 RPC 框架，在 JobManager 和 TaskManagers 之间发送的消息的最大大小默认为 10485760b，如果消息超过这个限制就会失败，报错。这个可以看下抛出异常处的源码： 123456789101112131415161718192021protected RpcInvocation createRpcInvocationMessage(String methodName, Class&lt;?&gt;[] parameterTypes, Object[] args) throws IOException &#123; Object rpcInvocation; if (this.isLocal) &#123; rpcInvocation = new LocalRpcInvocation(methodName, parameterTypes, args); &#125; else &#123; try &#123; RemoteRpcInvocation remoteRpcInvocation = new RemoteRpcInvocation(methodName, parameterTypes, args); if (remoteRpcInvocation.getSize() &gt; this.maximumFramesize) &#123; // 异常所在位置 throw new IOException(&quot;The rpc invocation size exceeds the maximum akka framesize.&quot;); &#125; rpcInvocation = remoteRpcInvocation; &#125; catch (IOException var6) &#123; LOG.warn(&quot;Could not create remote rpc invocation message. Failing rpc invocation because...&quot;, var6); throw var6; &#125; &#125; return (RpcInvocation)rpcInvocation;&#125; 至于为什么 JobManager 和 TaskManager 之间的 RPC 消息大小会如此之大，初步的解释是在 task 出现异常之后，它需要调用 updateTaskExecutionState(TaskExecutionState，taskExecutionState) 这个 RPC 接口去通知 Flink Jobmanager 去改变对应 task 的状态并且重启 task。但是呢，taskExecutionState 这个参数里面有个 error 属性，当我的 task 打出来的错误栈太多的时候，在序列化的之后超过了 rpc 接口要求的最大数据大小（也就是 maximum akka framesize），导致调用 updateTaskExecutionState 这个 rpc 接口失败，Jobmanager 无法获知这个 task 已经处于 fail 的状态，也无法重启，然后就导致了一系列连锁反应。 解决办法任务停止，在 flink-conf.yaml 中加入 akka.framesize 参数，调大该值。 1akka.framesize: &quot;62914560b&quot; 然后将任务重启，可以观察 Jobmanager Configration 看看参数是否生效。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"实时平台如何管理多个 Flink 版本？—— 为啥会出现多个版本？","date":"2021-09-25T16:00:00.000Z","path":"2021/09/26/realtime-platform-flink-version/","text":"为啥会出现多个版本？ Flink 社区本身迭代速度非常快，目前阿里云有一大波的人专职做 Flink 开源，另外还拥有活跃的社区贡献者，所以功能开发较快，bug 修复速度较快，几乎每 4 个月一个大版本，每个大版本之间迭代的功能非常多，代码变动非常大，API 接口变动也大，动不动就干翻自己了。 社区迭代快就快呗，为什么公司也要要不断跟着社区鼻子走？社区迭代快意味着功能多，修复的 bug 多，相对于早期版本意味着稳定性也高些。除了国内一二线公司有特别多的专职人去负责这块，大多数中小公司最简单最快捷体验到稳定性最高、功能性最多、性能最好的 Flink 版本无非是直接使用最新的 Flink 版本。举个例子：Flink SQL 从最早期（1.9）的功能、性能到目前 1.14，差别真的大很多，优化了特别多的地方，增强了很多功能。原先使用 Flink SQL 完成一个流处理任务非常麻烦，还不如直接写几十行代码来的快，目前我情愿写 SQL 去处理一个流任务。那么自然会跟着升级到新版本。 用户 A 问 Flink SQL 支持单独设置并行度吗？用户 B 问实时平台现在支持 Flink 1.13 版本的 Window TVF？这个要 Flink xxx 版本才能支持，要不你升级一下 Flink 版本到 xxx？这样就能支持了，类似的场景还有很多，对于中小公司的实时平台负责人来说，这无非最省事；对于大公司的负责实时开发的人来说，这无疑是一个噩梦，每次升级新版本都要将在老版本开发的各种功能都想尽办法移植到新版本上来，碰到 API 接口变动大的无非相当于重写了，或者将新版本的某些特别需要的功能通过打 patch 的方式打到老版本里面去。 新版本香是真的香，可是为啥有的人不用呢？问题就是，实时作业大多数是长期运行的，如果一个作业没啥错误，在生产运行的好好的，也不出啥故障，稳定性和性能也都能接受（并不是所有作业数据量都很大，会遇到性能问题），那么用户为啥要使用新版本？用户才不管你新版本功能多牛逼，性能多屌呢，老子升级还要改依赖版本、改接口代码、测试联调、性能测试（谁知道你说的性能提升是不是吹牛逼的）、稳定性测试（可能上线双跑一段时间验证），这些不需要时间呀，你叫我升级就升级，滚犊子吧，你知道我还有多少业务需求要做吗？ 那么就落下这个场地了，又要使用新版本的功能去解决问题，老作业的用户跟他各种扯皮也打动不了他升级作业的版本，那么自然就不断的出现了多个版本了。 这样，如果不对版本做好规划，那么摊子就逐渐越来越大，越来越难收拾了？ 那么该如何管理公司的 Flink 版本？如果管理和兼容多个 Flink 版本的作业提交？如何兼容 Jar 包和 SQL 作业的提交 怎么管理多个 Flink 版本的作业提交？尽请期待下篇文章","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 基于 Flink 的实时监控告警系统","date":"2021-08-20T16:00:00.000Z","path":"2021/08/21/flink-in-action-12.3/","text":"12.3 基于 Flink 的实时监控告警系统在如今微服务、云原生等技术盛行的时代，当谈到说要从 0 开始构建一个监控系统，大家无非就首先想到三个词：Metrics、Tracing、Logging。 12.3.1 监控系统的诉求国外一篇比较火的文章 Metrics, Tracing, and Logging 内有个图很好的总结了一个监控系统的诉求，分别是 Metrics、Logging、Tracing，如下图所示。 Metrics 的特点：它自己提供了五种基本的度量类型 Gauge、Counter、Histogram、Timer、Meter。 Tracing 的特点：提供了一个请求从接收到处理完毕整个生命周期的跟踪路径，通常请求都是在分布式的系统中处理，所以也叫做分布式链路追踪。 Logging 的特点：提供项目应用运行的详细信息，例如方法的入参、运行的异常记录等。 这三者在监控系统中缺一不可，它们之间的关系是：基于 Metrics 的异常告警事件，然后通过 Tracing 定位问题可疑模块，根据模块详细的日志定位到错误根源，最后再返回来调整 Metrics 的告警规则，以便下次更早的预警，提前预防出现此类问题。 12.3.2 监控系统包含的内容针对提到的三个点，笔者找到国内外的开源监控系统做了对比，发现真正拥有全部功能的比较少，有的系统比较专注于 Logging、有的系统比较专注于 Tracing，而大部分其他的监控系统无非是只是监控系统的一部分，比如是作为一款数据库存储监控数据、作为一个可视化图表的系统去展示各种各样的监控数据信息。 拿 Logging 来说，开源用的最多最火的技术栈是 ELK，Tracing 这块有 Skywalking、Pinpoint 等技术，它们的对比如 APM 巅峰对决：Skywalking PK Pinpoint 一文介绍。而存储监控数据的时序数据库那就比较多了，常见的比如 InfluxDB、Prometheus、OpenTSDB 等，它们之间的对比介绍如下图所示。 监控可视化图表的开源系统个人觉得最好看的就是 Grafana，在 8.2 节中搭建 Flink 监控系统的数据展示也是用的 Grafana，当然还可以利用 ECharts、BizCharts 等数据图表库做二次开发来适配公司的数据展示图表。 上面说了这么多，这里笔者根据自己的工作经验先谈谈几点自己对监控系统的心得： 告警是监控系统第一入口，图表展示体现监控的价值：告警是唯一可以第一时间反映运行状态，它承担着系统与人之间的沟通桥梁，通常告警消息又会携带链接跳转到图表展示，它作为第一入口并衔接上了整个监控系统。 数据采集是监控的源泉：数据采集是监控系统的源泉，如果采集的数据是错误的，将导致后面的链路（告警、数据展示）全处于无效状态，所以千万千万要保证数据采集的准确性和完整性。 数据存储是监控最大挑战：当机器、系统应用和监控指标等变得越多来多时，采集上来的数据是爆炸性增长的，将海量的监控数据实时存储到任何一个数据库，挑战都是不小的。 说完心得再来讲解到底一个监控系统真正该包含哪些东西呢？笔者觉得首先分 6 层：数据采集层、数据传输层、数据计算层、告警、数据存储、数据展示，如下图所示。 监控系统这六层的主要功能分别是： 数据采集层：该层主要功能就是去采集各种各样的数据，比如 Metrics、Logging、Tracing 数据。 数据传输层：该层主要功能就是传输采集到的监控数据，一般使用消息队列居多，比如 Kafka、RocketMQ 等。 数据计算层：该层的主要功能是将采集到的数据进行数据清洗和计算，一般采用 Flink、Spark 等计算引擎来处理。 告警：该层其实也属于数据计算层，但是因为告警涉及的内容太多，比如告警规则、告警计算、告警通知等，所以可以单独作为一个重要点来讲。 数据存储层：该层的主要功能是存储所有的监控数据，为后面的数据可视化提供数据源。 数据展示层：该层的主要功能是将监控数据通过可视化图表来展示出来，通过图表可以知道服务器、应用的运行状态。 12.3.3 Metrics／Tracing／Logging 数据实时采集在 12.1 节中讲解了日志数据如何采集，那么对于 Metrics 和 Tracing 数据该怎么采集呢？ MetricsTracing12.3.4 消息队列如何撑住高峰流量RabbitMQKafkaRocketMQ12.3.5 指标数据实时计算12.3.6 提供及时且准确的根因分析告警告警本质告警通知对象告警通知方式告警规则的设计根因分析告警告警事件解耦告警工单记录告警解决过程12.3.7 AIOps 智能运维道路探索12.3.8 如何保障高峰流量实时写入存储系统的稳定性12.3.9 监控数据使用可视化图表展示Grafana 介绍12.3.10 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/IeAYbEy 本章总结本章讲了三个公司常见场景案例：日志处理、去重、监控告警。在日志处理案例中讲解了日志的采集的需求，以及分析了整个日志采集案例的架构设计分层，关于日志采集工具的选型在 11.5 节中有讲过，所以该案例中就直接讲述采集工具的使用和安装，并将采集到的数据发送到 Kafka，然后使用 Flink 清洗日志数据并将异常日志告警通知到相应负责人，接着将数据写入到 ElasticSearch，最后通过 Kibana 可以展示和搜索日志数据。 在百亿数据实时去重案例中通过对比通用解决方法、使用 BloomFilter、使用 HBase 和使用 Flink KeyedState 几种方案来分析实时去重的解决方案，并在该案例中还提及到如何去优化去重的效果。 在实时监控告警案例中讲述了公司通用的监控告警需求，包括 Metrics、Logging、Tracing，然后设计出整个监控告警系统的架构分层和技术选型，其中分层包含数据采集层、数据传输层、数据计算层、数据存储层、数据展示层。关于数据采集工具、消息队列、数据存储中间件、数据展示的技术选型，笔者也分别做了对比介绍，以便大家可以根据自己公司的情况做架构选型。另外针对实时告警这块，笔者也详述了很多，包括告警规则的设计、告警通知对象、告警通知方式、告警消息收敛、告警消息根因分析等，最后还讲了对监控告警的展望，希望能够在 AIOps 做更多的探索，对于这块的内容，笔者在社区钉钉群做过视频直播，大家可以去笔者的博客查看录播的视频。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 基于 Flink 的百亿数据去重实践","date":"2021-08-19T16:00:00.000Z","path":"2021/08/20/flink-in-action-12.2/","text":"12.2 基于 Flink 的百亿数据去重实践在工作中经常会遇到去重的场景，例如基于 APP 的用户行为日志分析系统：用户的行为日志从手机 APP 端上报到 Nginx 服务端，然后通过 Logstash、Flume 或其他工具将日志从 Nginx 写入到 Kafka 中。由于用户手机客户端的网络可能出现不稳定，所以手机 APP 端上传日志的策略是：宁可重复上报，也不能漏报日志，所以导致 Kafka 中可能会出现日志重复的情况，即：同一条日志出现了 2 条或 2 条以上。通常情况下，Flink 任务的数据源都是 Kafka，若 Kafka 中数据出现了重复，在实时 ETL 或者流计算时都需要考虑基于日志主键对日志进行去重，否则会导致流计算结果偏高或结果不准确的问题，例如用户 a 在某个页面只点击了一次，但由于日志重复上报，所以用户 a 在该页面的点击日志在 Kafka 中出现了 2 次，最后统计该页面的点击数时，结果就会偏高。这里只阐述了一种可能造成 Kafka 中数据重复的情况，在生产环境中很多情况都可能造成 Kafka 中数据重复，这里不一一列举，本节主要讲述出现了数据重复后，该如何处理。 12.2.1 去重的通用解决方案Kafka 中数据出现重复后，各种解决方案都比较类似，一般需要一个全局 Set 集合来维护历史所有数据的主键。当处理新日志时，需要拿到当前日志的主键与历史数据的 Set 集合按照规则进行比较，若 Set 集合中已经包含了当前日志的主键，说明当前日志在之前已经被处理过了，则当前日志应该被过滤掉，否则认为当前日志不应该被过滤应该被处理，而且处理完成后需要将新日志的主键加入到 Set 集合中，Set 集合永远存放着所有已经被处理过的数据。这种去重的通用解决方案的流程图如下图所示。 处理流程很简单，关键在于如何维护这个 Set 集合，可以简单估算一下这个 Set 集合需要占用多大空间。本小节要解决的问题是百亿数据去重，所以就按照每天 1 百亿的数据量来计算。由于每天数据量巨大，因此主键占用空间通常会比较大，如果主键占用空间小意味着表示的数据范围就比较小，就可能导致主键冲突，例如：4 个字节的 int 类型表示数据范围是为 -2147483648 ~ 2147483647，总共可以表示 42 亿个数，如果这里每天百亿的数据量选用 int 类型做为主键的话，很明显会有大量的主键发生冲突，会将不重复的数据认为是发生了重复。用户的行为日志是在手机客户端生成的，没有全局发号器，一般会选取 UUID 做为日志的主键，UUID 会生成 36 位的字符串，例如：”f106c4a1-4c6f-41c1-9d30-bbb2b271284a”。每个主键占用 36 字节，每天 1 百亿数据，36字节 100亿 ≈ 360 GB。这仅仅是一天的数据量，所以该 Set 集合要想存储空间不发生持续地爆炸式增长，必须增加一个功能，那就是给所有的主键增加 TTL（过期时间）。如果不增加 TTL，10 天数据量的主键占用空间就 3.6T，100 天数据量的主键占用空间 36T，所以在设计之初必须考虑为主键设定 TTL。如果要求按天进行去重或者认为日志发生重复上报的时间间隔不可能大于 24 小时，那么为了系统的可靠性 TTL 可以设置为 36 小时。每天数据量 1 百亿，且 Set 集合中存放着 36 小时的数据量，即 100 亿 1.5 = 150 亿，所以 Set 集合中需要维护 150 亿的数据量，且 Set 集合中每条数据都增加了 TTL，意味着 Set 集合需要为每条数据再附带保存一个时间戳，来确定该数据什么时候过期。例如 Redis 中为一个 key 设置了 TTL，如果没有为这个 key 附带时间戳，那么根本无法判断该 key 什么时候应该被清理。所以在考虑每条数据占用空间时，不仅要考虑数据本身，还需要考虑是否需要其他附带的存储。主键本身占用 36 字节加上 long 类型的时间戳 8 字节，所以每条数据至少需要占用 44 字节，150 亿 * 44 字节 = 660 GB。所以每天百亿的数据量，如果我们使用 Set 集合的方案来实现，至少需要占用 660 GB 以上的存储空间。 12.2.2 使用 BloomFilter 实现去重有些流计算的场景对准确性要求并不是很高，例如传统的 Lambda 架构中，都会有离线去矫正实时计算的结果，所以根据业务场景，当业务要求可以接受结果有小量误差时，可以选择使用一些低成本的数据结构。BloomFilter 和 HyperLogLog 都是相对低成本的数据结构，分别有自己的应用场景，且两种数据结构都有一定误差。HyperLogLog 可以估算出 HyperLogLog 中插入了多少个不重复的元素，而不能告诉我们之前是否插入了哪些元素。BloomFilter 则恰好相反，相对而言 BloomFilter 更像是一个 Set 集合，BloomFilter 可以告诉你 BloomFilter 中肯定不包含元素 a，或者告诉你 BloomFilter 中可能包含元素 b，但 BloomFilter 不能告诉你 BloomFilter 中插入了多少个元素。接下来了解一下 BloomFilter 的实现原理。 bitmap 位图了解 BloomFilter，从 bitmap（位图）开始说起。现在有 1 千万个整数，数据范围在 0 到 2 千万之间。如何快速查找某个整数是否在这 1 千万个整数中呢？可以将这 1 千万个数保存在 HashMap 中，不考虑对象头及其他空间，1000 万个 int 类型数据需要占用大约 1000万 * 4 字节 ≈ 40 MB 存储空间。有没有其他方案呢？因为数据范围是 0 到 2 千万，所以可以申请一个长度为 2000 万、boolean 类型的数组，将这 2 千万个整数作为数组下标，将其对应的数组默认值设置成 false，如下图所示，数组下标为 2、666、999 的位置存储的数据为 true，表示 1 千万个数中包含了 2、666、999 等。当查询某个整数 K 是否在这 1 千万个整数中时，只需要将对应的数组值 array[K] 取出来，看是否等于 true。如果等于 true，说明 1 千万整数中包含这个整数 K，否则表示不包含这个整数 K。 Java 的 boolean 基本类型占用一个字节（8bit）的内存空间，所以上述方案需要申请 2000 万字节。如下图所示，可以通过编程语言用二进制位来模拟布尔类型，二进制的 1 表示true、二进制的 0 表示false。通过二进制模拟布尔类型的方案，只需要申请 2000 万 bit 即可，相比 boolean 类型而言，存储空间占用仅为原来的 1/8。2000 万 bit ≈ 2.4 MB，相比存储原始数据的方案 40 MB 而言，占用的存储空间少了很多。 假如这 1 千万个整数的数据范围是 0 到 100 亿，那么就需要申请 100 亿个 bit 约等于 1200 MB，比存储原始数据方案的 40MB 还要大很多。该情况下，直接使用位图使用的存储空间更多了，怎么解决呢？可以只申请 1 亿 bit 的存储空间，对 1000 万个数求hash，映射到 1 亿的二进制位上，最后大约占用 12 MB 的存储空间，但是可能存在 hash 冲突的情况。例如 3 和 100000003（一亿零三）这两个数对一亿求余都为 3，所以映射到长度为 1 亿的位图上，这两个数会占用同一个 bit，就会导致一个问题：1 千万个整数中包含了一亿零三，所以位图中下标为 3 的位置存储着二进制 1。当查询 1 千万个整数中是否包含数字 3 时，同样也是去位图中下标 3 的位置去查找，发现下标为 3 的位置存储着二进制 1，所以误以为 1 千万个整数中包含数字 3。为了减少 hash 冲突，于是诞生了 BloomFilter。 BloomFilter 原理介绍hash 存在 hash 冲突（碰撞）的问题，两个不同的 key 通过同一个 hash 函数得到的值有可能相同。为了减少冲突，可以引入多个 hash 函数，如果通过其中的一个 hash 函数发现某元素不在集合中，那么该元素肯定不在集合中。当所有的 hash 函数告诉我们该元素在集合中时，才能确定该元素存在于集合中，这便是BloomFilter的基本思想。 如下图所示，是往 BloomFilter 中插入元素 a、b 的过程，有 3 个 hash 函数，元素 a 经过 3 个 hash 函数后对应的 2、8、10 这三个二进制位，所以将这三个二进制位置为 1，元素 b 经过 3 个 hash 函数后，对应的 5、10、14 这三个二进制位，将这三个二进制位也置为 1，其中下标为 10 的二进制位被 a、b 元素都涉及到。 如下图所示，是从 BloomFilter 中查找元素 c、d 的过程，同样包含了 3 个 hash 函数，元素 c 经过 3 个 hash 函数后对应的 2、6、9 这三个二进制位，其中下标 6 和 9 对应的二进制位为 0，所以会认为 BloomFilter 中不存在元素 c。元素 d 经过 3 个 hash 函数后对应的 5、8、14 这三个二进制位，这三个位对应的二进制位都为 1，所以会认为 BloomFilter 中存在元素 d，但其实 BloomFilter 中并不存在元素 d，是因为元素 a 和元素 b 也对应到了 5、8、14 这三个二进制位上，所以 BloomFilter 会有误判。但是从实现原理来看，当 BloomFilter 告诉你不包含元素 c 时，BloomFilter 中肯定不包含元素 c，当 BloomFilter 告诉你 BloomFilter 中包含元素 d 时，它只是可能包含，也有可能不包含。 使用 BloomFilter 实现数据去重Redis 4.0 之后 BloomFilter 以插件的形式加入到 Redis 中，关于 api 的具体使用这里不多赘述。BloomFilter 在创建时支持设定一个预期容量和误判率，预期容量即预计插入的数据量，误判率即：当 BloomFilter 中插入的数据达到预期容量时，误判的概率，如果 BloomFilter 中插入数据较少的话，误判率会更低。 经笔者测试，申请一个预期容量为 10 亿，误判率为千分之一的 BloomFilter，BloomFilter 会申请约 143 亿个 bit，即：14G左右，相比之前 660G 的存储空间小太多了。但是在使用过程中，需要记录 BloomFilter 中插入元素的个数，当插入元素个数达到 10 亿时，为了保障误差率，可以将当前 BloomFilter 清除，重新申请一个新的 BloomFilter。 通过使用 Redis 的 BloomFilter，我们可以通过相对较小的内存实现百亿数据的去重，但是 BloomFilter 有误差，所以只能使用在那些对结果能承受一定误差的应用场景，对于广告计费等对数据精度要求非常高的场景，极力推荐大家使用精准去重的方案来实现。 12.2.3 使用 HBase 维护全局 Set 实现去重通过之前分析，我们知道要想实现百亿数据量的精准去重，需要维护 150 亿数据量的 Set 集合，每条数据占用 44 KB，总共需要 660 GB 的存储空间。注意这里说的是存储空间而不是内存空间，为什么呢？因为 660 G 的内存实在是太贵了，660G 的 Redis 云服务一个月至少要 2 万 RMB 以上，俗话说设计架构不考虑成本等于耍流氓。这里使用 Redis 确实可以解决问题，但是成本较高。HBase 基于 RowKey Get 的效率比较高，所以这里可以考虑将这个大的 Set 集合以 HBase RowKey 的形式存放到 HBase 中。HBase 表设置 TTL 为 36 小时，最近 36 小时的 150 亿条日志的主键都存放到 HBase 中，每来一条数据，先拿到主键去 HBase 中查询，如果 HBase 表中存在该主键，说明当前日志已经被处理过了，当前日志应该被过滤。如果 HBase 表中不存在该主键，说明当前日志之前没有被处理过，此时应该被处理，且处理完成后将当前主键 Put 到 HBase 表中。由于数据量比较大，所以一定要提前对 HBase 表进行预分区，将压力分散到各个 RegionServer 上。 使用 HBase RowKey 去重带来的问题12.2.4 使用 Flink 的 KeyedState 实现去重下面就教大家如何使用 Flink 的 KeyedState 实现去重。 使用 Flink 状态来维护 Set 集合的优势如何使用 KeyedState 维护 Set 集合优化主键来减少状态大小，且提高吞吐量12.2.5 使用 RocksDBStateBackend 的优化方法在使用上述方案的过程中，可能会出现吞吐量时高时低，或者吞吐量比笔者的测试性能要低一些，当出现这类问题的时候，可以尝试从以下几个方面进行优化。 设置本地 RocksDB 的数据目录Checkpoint 参数相关配置RocksDB 参数相关配置12.2.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/IeAYbEy","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 基于 Flink 实时处理海量日志","date":"2021-08-18T16:00:00.000Z","path":"2021/08/19/flink-in-action-12.1/","text":"第十二章 —— Flink 案例本章将介绍 Flink 在多个场景下落地实现的大型案例，第一个是实时处理海量的日志，将从日志的收集、日志的传输、日志的实时清洗和异常检测、日志存储、日志展示等方面去介绍 Flink 在其中起的作用，希望整个日志处理的架构大家可以灵活的运用在自己的公司；第二个是百亿数据量的情况下如何使用 Flink 实时去重，在这个案例中将对比介绍其他几种常见的去重实现方案；第三个是 Flink 在监控告警系统中的落地实现，在这个案例中同样很详细的介绍了一个监控告警系统的全链路，每一个关节都不可或缺，并且还介绍了 Flink 在未来结合机器学习算法做一些 AIOps 的事情。三个案例都比较典型，如果你也在做类似的项目，希望对你们的技术选型有一定的帮助。 12.1 基于 Flink 实时处理海量日志在 11.5 节中讲解了 Flink 如何实时处理异常的日志，并且对比分析了几种常用的日志采集工具。我们也知道通常在排查线上异常故障的时候，日志是必不可缺的一部分，通过异常日志我们可以快速的定位到问题的根因。那么通常在公司对于日志处理有哪些需求呢？ 12.1.1 实时处理海量日志需求分析现在公司都在流行构建分布式、微服务、云原生的架构，在这类架构下，项目应用的日志都被分散到不同的机器上，日志查询就会比较困难，所以统一的日志收集几乎也是每家公司必不可少的。据笔者调研，不少公司现在是有日志统一的收集，也会去做日志的实时 ETL，利用一些主流的技术比如 ELK 去做日志的展示、搜索和分析，但是却缺少了日志的实时告警。总结来说，大部分公司对于日志这块的现状是： 日志分布零散：分布式应用导致日志分布在不同的机器上，人肉登录到机器上操作复杂，需要统一的日志收集工具。 异常日志无告警：出错时无异常日志告警，导致错过最佳定位问题的时机，需要异常错误日志的告警。 日志查看不友好：登录服务器上在终端查看日志不太方便，需要一个操作友好的页面去查看日志。 无日志搜索分析：历史日志文件太多，想找某种日志找不到了，需要一个可以搜索日志的功能。 在本节中，笔者将为大家讲解日志的全链路，包含了日志的实时采集、日志的 ETL、日志的实时监控告警、日志的存储、日志的可视化图表展示与搜索分析等。 12.1.2 实时处理海量日志架构设计分析完我们这个案例的需求后，接下来对整个项目的架构做一个合理的设计。 整个架构分为五层：日志接入层、日志削峰层、日志处理层、日志存储层、日志展示层。 日志接入层：日志采集的话使用的是 Filebeat 组件，需要在每台机器上部署一个 Filebeat。 日志削峰层：防止日志流量高峰，使用 Kafka 消息队列做削峰。 日志处理层：Flink 作业同时消费 Kafka 数据做日志清洗、ETL、实时告警。 日志存储层：使用 ElasticSearch 做日志的存储。 日志展示层：使用 Kibana 做日志的展示与搜索查询界面。 12.1.3 日志实时采集在 11.5.1 中对比了这几种比较流行的日志采集工具（Logstash、Filebeat、Fluentd、Logagent），从功能完整性、性能、成本、使用难度等方面综合考虑后，这里演示使用的是 Filebeat。 安装 Filebeat在服务器上下载 Fliebeat 6.3.2 安装包（请根据自己服务器和所需要的版本进行下载），下载后进行解压。 1tar xzf filebeat-6.3.2-linux-x86_64.tar.gz 配置 Filebeat配置 Filebeat 需要编辑 Filebeat 的配置文件 filebeat.yml，不同安装方式配置文件的存放路径有一些不同，对于解压包安装的方式，配置文件存在解压目录下面；对于 rpm 和 deb 的方式, 配置文件路径的是 /etc/filebeat/filebeat.yml 下。 因为 Filebeat 是要实时采集日志的，所以得让 Filebeat 知道日志的路径是在哪里，下面在配置文件中定义一下日志文件的路径。通常建议在服务器上固定存放日志的路径，然后应用的日志都打在这个固定的路径中，这样 Filebeat 的日志路径配置只需要填写一次，其他机器上可以拷贝同样的配置就能将 Filebeat 运行起来，配置如下。 123456- type: log # 配置为 true 表示开启 enabled: true # 日志的路径 paths: - /var/logs/*.log 上面的配置表示将对 /var/logs 目录下所有以 .log 结尾的文件进行采集，接下来配置日志输出的方式，这里使用的是 Kafka，配置如下。 12345678output.kafka: # 填写 Kafka 地址信息 hosts: [&quot;localhost:9092&quot;] # 数据发到哪个 topic topic: zhisheng-log partition.round_robin: reachable_only: false required_acks: 1 上面讲解的两个配置，笔者这里将它们写在一个新建的配置文件中 kafka.yml，然后启动 Filebeat 的时候使用该配置。 1234567891011filebeat.inputs:- type: log enabled: true paths: - /var/logs/*.logoutput.kafka: hosts: [\"localhost:9092\"] topic: zhisheng_log partition.round_robin: reachable_only: false required_acks: 1 启动 Filebeat日志路径的配置和 Kafka 的配置都写好后，则接下来通过下面命令将 Filebeat 启动： 1bin/filebeat -e -c kafka.yml 执行完命令后出现的日志如下则表示启动成功了，另外还可以看得到会在终端打印出 metrics 数据出来。 验证 Filebeat 是否将日志数据发到 Kafka那么此时就得去查看是否真正就将这些日志数据发到 Kafka 了呢，你可以通过 Kafka 的自带命令去消费这个 Topic 看是否不断有数据发出来，命令如下： 1bin/kafka-console-consumer.sh --zookeeper 106.54.248.27:2181 --topic zhisheng_log --from-beginning 如果出现数据则代表是已经有数据发到 Kafka 了，如果你不喜欢使用这种方式验证，可以自己写个 Flink Job 去读取 Kafka 该 Topic 的数据，比如写了个作业运行结果如下就代表着日志数据已经成功发送到 Kafka。 发到 Kafka 的日志结构既然数据都已经发到 Kafka 了，通过消费 Kafka 该 Topic 的数据我们可以发现这些数据的格式否是 JSON，结构如下： 12345678910111213141516171819202122232425262728293031&#123; \"@timestamp\": \"2019-10-26T08:18:18.087Z\", \"@metadata\": &#123; \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.8.4\", \"topic\": \"zhisheng_log\" &#125;, \"prospector\": &#123; \"type\": \"log\" &#125;, \"input\": &#123; \"type\": \"log\" &#125;, \"beat\": &#123; \"name\": \"VM_0_2_centos\", \"hostname\": \"VM_0_2_centos\", \"version\": \"6.8.4\" &#125;, \"host\": &#123; \"name\": \"VM_0_2_centos\" &#125;, \"source\": \"/var/logs/middleware/kafka.log\", \"offset\": 9460, \"log\": &#123; \"file\": &#123; \"path\": \"/var/logs/middleware/kafka.log\" &#125; &#125;, \"message\": \"2019-10-26 16:18:11 TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)\"&#125; 这个日志结构里面包含了很多字段，比如 timestamp、metadata、host、source、message 等，但是其中某些字段我们其实根本不需要的，你可以根据公司的需求丢弃一些字段，把要丢弃的字段也配置在 kafka.yml 中，如下所示。 123processors:- drop_fields: fields: [&quot;prospector&quot;,&quot;input&quot;,&quot;beat&quot;,&quot;log&quot;,&quot;offset&quot;,&quot;@metadata&quot;] 然后再次启动 Filebeat ，发现上面配置的字段在新的数据中没有了（除 @metadata 之外），另外经笔者验证：不仅 @metadata 字段不能丢弃，如果 @timestamp 这个字段在 drop_fields 中配置了，也是不起作用的，它们两不允许丢弃。通常来说一行日志已经够长了，再加上这么多我们不需要的字段，就会增加数据的大小，对于生产环境的话，日志数据量非常大，那无疑会对后面所有的链路都会造成一定的影响，所以一定要在底层数据源头做好精简。另外还可以在发送 Kafka 的时候对数据进行压缩，可以在配置文件中配置一个 compression: gzip。精简后的日志数据结构如下： 1234567891011121314&#123; \"@timestamp\": \"2019-10-26T09:23:16.848Z\", \"@metadata\": &#123; \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.8.4\", \"topic\": \"zhisheng_log\" &#125;, \"host\": &#123; \"name\": \"VM_0_2_centos\" &#125;, \"source\": \"/var/logs/middleware/kafka.log\", \"message\": \"2019-10-26 17:23:11 TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)\"&#125; 12.1.4 日志格式统一因为 Filebeat 是在机器上采集的日志，这些日志的种类比较多，常见的有应用程序的运行日志、作业构建编译打包的日志、中间件服务运行的日志等。通常在公司是可以给开发约定日志打印的规则，但是像中间件这类服务的日志是不固定的，如果将 Kafka 中的消息直接存储到 ElasticSearch 的话，后面如果要做区分筛选的话可能会有问题。为了避免这个问题，我们得在日志存入 ElasticSearch 之前做一个数据格式化和清洗的工作，因为 Flink 处理数据的速度比较好，而且可以做到实时，所以选择在 Flink Job 中完成该工作。 在该作业中的要将 message 解析，一般该行日志信息会包含很多信息，比如日志打印时间、日志级别、应用名、唯一性 ID（用来关联各个请求）、请求上下文。那么我们就需要一个新的日志结构对象来统一日志的格式，定义如下： 12.1.5 日志实时清洗12.1.6 日志实时告警12.1.7 日志实时存储12.1.8 日志实时展示12.1.9 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/IeAYbEy","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何实时将应用 Error 日志告警？","date":"2021-08-17T16:00:00.000Z","path":"2021/08/18/flink-in-action-11.5/","text":"11.5 如何实时将应用 Error 日志告警？大数据时代，随着公司业务不断的增长，数据量自然也会跟着不断的增长，那么业务应用和集群服务器的的规模也会逐渐扩大，几百台服务器在一般的公司已经是很常见的了。那么将应用服务部署在如此多的服务器上，对开发和运维人员来说都是一个挑战。一个优秀的系统运维平台是需要将部署在这么多服务器上的应用监控信息汇总成一个统一的数据展示平台，方便运维人员做日常的监测、提升运维效率，还可以及时反馈应用的运行状态给应用开发人员。举个例子，应用的运行日志需要按照时间排序做一个展示，并且提供日志下载和日志搜索等服务，这样如果应用出现问题开发人员首先可以根据应用日志的错误信息进行问题的排查。那么该如何实时的将应用的 Error 日志推送给应用开发人员呢，接下来我们将讲解日志的处理方案。 11.5.1 日志处理方案的演进日志处理的方案也是有一个演进的过程，要想弄清楚整个过程，我们先来看下日志的介绍。 什么是日志？日志是带时间戳的基于时间序列的数据，它可以反映系统的运行状态，包括了一些标识信息（应用所在服务器集群名、集群机器 IP、机器设备系统信息、应用名、应用 ID、应用所属项目等） 日志处理方案演进日志处理方案的演进过程： 日志处理 v1.0: 应用日志分布在很多机器上，需要人肉手动去机器查看日志信息。 日志处理 v2.0: 利用离线计算引擎统一的将日志收集，形成一个日志搜索分析平台，提供搜索让用户根据关键字进行搜索和分析，缺点就是及时性比较差。 日志处理 v3.0: 利用 Agent 实时的采集部署在每台机器上的日志，然后统一发到日志收集平台做汇总，并提供实时日志分析和搜索的功能，这样从日志产生到搜索分析出结果只有简短的延迟（在用户容忍时间范围之内），优点是快，但是日志数据量大的情况下带来的挑战也大。 11.5.2 日志采集工具对比上面提到的日志采集，其实现在已经有很多开源的组件支持去采集日志，比如 Logstash、Filebeat、Fluentd、Logagent 等，这里简单做个对比。 LogstashLogstash 是一个开源数据收集引擎，具有实时管道功能。Logstash 可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地。如下图所示，Logstash 将采集到的数据用作分析、监控、告警等。 优势：Logstash 主要的优点就是它的灵活性，它提供很多插件，详细的文档以及直白的配置格式让它可以在多种场景下应用。而且现在 ELK 整个技术栈在很多公司应用的比较多，所以基本上可以在往上找到很多相关的学习资源。 劣势：Logstash 致命的问题是它的性能以及资源消耗(默认的堆大小是 1GB)。尽管它的性能在近几年已经有很大提升，与它的替代者们相比还是要慢很多的，它在大数据量的情况下会是个问题。另一个问题是它目前不支持缓存，目前的典型替代方案是将 Redis 或 Kafka 作为中心缓冲池： Filebeat作为 Beats 家族的一员，Filebeat 是一个轻量级的日志传输工具，它的存在正弥补了 Logstash 的缺点，Filebeat 作为一个轻量级的日志传输工具可以将日志推送到 Kafka、Logstash、ElasticSearch、Redis。它的处理流程如下图所示： 优势：Filebeat 只是一个二进制文件没有任何依赖。它占用资源极少，尽管它还十分年轻，正式因为它简单，所以几乎没有什么可以出错的地方，所以它的可靠性还是很高的。它也为我们提供了很多可以调节的点，例如：它以何种方式搜索新的文件，以及当文件有一段时间没有发生变化时，何时选择关闭文件句柄。 劣势：Filebeat 的应用范围十分有限，所以在某些场景下我们会碰到问题。例如，如果使用 Logstash 作为下游管道，我们同样会遇到性能问题。正因为如此，Filebeat 的范围在扩大。开始时，它只能将日志发送到 Logstash 和 Elasticsearch，而现在它可以将日志发送给 Kafka 和 Redis，在 5.x 版本中，它还具备过滤的能力。 FluentdFluentd 创建的初衷主要是尽可能的使用 JSON 作为日志输出，所以传输工具及其下游的传输线不需要猜测子字符串里面各个字段的类型。这样它为几乎所有的语言都提供库，这也意味着可以将它插入到自定义的程序中。它的处理流程如下图所示： 优势：和多数 Logstash 插件一样，Fluentd 插件是用 Ruby 语言开发的非常易于编写维护。所以它数量很多，几乎所有的源和目标存储都有插件(各个插件的成熟度也不太一样)。这也意味这可以用 Fluentd 来串联所有的东西。 劣势：因为在多数应用场景下得到 Fluentd 结构化的数据，它的灵活性并不好。但是仍然可以通过正则表达式来解析非结构化的数据。尽管性能在大多数场景下都很好，但它并不是最好的，它的缓冲只存在与输出端，单线程核心以及 Ruby GIL 实现的插件意味着它大的节点下性能是受限的。 LogagentLogagent 是 Sematext 提供的传输工具，它用来将日志传输到 Logsene(一个基于 SaaS 平台的 Elasticsearch API)，因为 Logsene 会暴露 Elasticsearch API，所以 Logagent 可以很容易将数据推送到 Elasticsearch 。 优势：可以获取 /var/log 下的所有信息，解析各种格式的日志，可以掩盖敏感的数据信息。它还可以基于 IP 做 GeoIP 丰富地理位置信息。同样，它轻量又快速，可以将其置入任何日志块中。Logagent 有本地缓冲，所以在数据传输目的地不可用时不会丢失日志。 劣势：没有 Logstash 灵活。 11.5.3 日志结构设计前面介绍了日志和对比了常用日志采集工具的优势和劣势，通常在不同环境，不同机器上都会部署日志采集工具，然后采集工具会实时的将新的日志采集发送到下游，因为日志数据量毕竟大，所以建议发到 MQ 中，比如 Kafka，这样再想怎么处理这些日志就会比较灵活。假设我们忽略底层采集具体是哪种，但是规定采集好的日志结构化数据如下： 12345678public class LogEvent &#123; private String type;//日志的类型(应用、容器、...) private Long timestamp;//日志的时间戳 private String level;//日志的级别(debug/info/warn/error) private String message;//日志内容 //日志的标识(应用 ID、应用名、容器 ID、机器 IP、集群名、...) private Map&lt;String, String&gt; tags = new HashMap&lt;&gt;();&#125; 然后上面这种 LogEvent 的数据（假设采集发上来的是这种结构数据的 JSON 串，所以需要在 Flink 中做一个反序列化解析）就会往 Kafka 不断的发送数据，样例数据如下： 123456789101112&#123; \"type\": \"app\", \"timestamp\": 1570941591229, \"level\": \"error\", \"message\": \"Exception in thread \\\"main\\\" java.lang.NoClassDefFoundError: org/apache/flink/api/common/ExecutionConfig$GlobalJobParameters\", \"tags\": &#123; \"cluster_name\": \"zhisheng\", \"app_name\": \"zhisheng\", \"host_ip\": \"127.0.0.1\", \"app_id\": \"21\" &#125;&#125; 那么在 Flink 中如何将应用异常或者错误的日志做实时告警呢？ 11.5.4 异常日志实时告警项目架构整个异常日志实时告警项目的架构如下图所示。 应用日志散列在不同的机器，然后每台机器都有部署采集日志的 Agent（可以是上面的 Filebeat、Logstash 等），这些 Agent 会实时的将分散在不同机器、不同环境的应用日志统一的采集发到 Kafka 集群中，然后告警这边是有一个 Flink 作业去实时的消费 Kafka 数据做一个异常告警计算处理。如果还想做日志的搜索分析，可以起另外一个作业去实时的将 Kafka 的日志数据写入进 ElasticSearch，再通过 Kibana 页面做搜索和分析。 11.5.5 日志数据发送到 Kafka上面已经讲了日志数据 LogEvent 的结构和样例数据，因为要在服务器部署采集工具去采集应用日志数据对于本地测试来说可能稍微复杂，所以在这里就只通过代码模拟构造数据发到 Kafka 去，然后在 Flink 作业中去实时消费 Kafka 中的数据，下面演示构造日志数据发到 Kafka 的工具类，这个工具类主要分两块，构造 LogEvent 数据和发送到 Kafka。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384@Slf4jpublic class BuildLogEventDataUtil &#123; //Kafka broker 和 topic 信息 public static final String BROKER_LIST = \"localhost:9092\"; public static final String LOG_TOPIC = \"zhisheng_log\"; public static void writeDataToKafka() &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", BROKER_LIST); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 0; i &lt; 10000; i++) &#123; //模拟构造 LogEvent 对象 LogEvent logEvent = new LogEvent().builder() .type(\"app\") .timestamp(System.currentTimeMillis()) .level(logLevel()) .message(message(i + 1)) .tags(mapData()) .build();// System.out.println(logEvent); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(LOG_TOPIC, null, null, GsonUtil.toJson(logEvent)); producer.send(record); &#125; producer.flush(); &#125; public static void main(String[] args) &#123; writeDataToKafka(); &#125; public static String message(int i) &#123; return \"这是第 \" + i + \" 行日志！\"; &#125; public static String logLevel() &#123; Random random = new Random(); int number = random.nextInt(4); switch (number) &#123; case 0: return \"debug\"; case 1: return \"info\"; case 2: return \"warn\"; case 3: return \"error\"; default: return \"info\"; &#125; &#125; public static String hostIp() &#123; Random random = new Random(); int number = random.nextInt(4); switch (number) &#123; case 0: return \"121.12.17.10\"; case 1: return \"121.12.17.11\"; case 2: return \"121.12.17.12\"; case 3: return \"121.12.17.13\"; default: return \"121.12.17.10\"; &#125; &#125; public static Map&lt;String, String&gt; mapData() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"app_id\", \"11\"); map.put(\"app_name\", \"zhisheng\"); map.put(\"cluster_name\", \"zhisheng\"); map.put(\"host_ip\", hostIp()); map.put(\"class\", \"BuildLogEventDataUtil\"); map.put(\"method\", \"main\"); map.put(\"line\", String.valueOf(new Random().nextInt(100))); //add more tag return map; &#125;&#125; 如果之前 Kafka 中没有 zhisheng_log 这个 topic，运行这个工具类之后也会自动创建这个 topic 了。 11.5.6 Flink 实时处理日志数据11.5.7 处理应用异常日志加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M 本章属于 Flink 实战篇，前面章节讲了很多 Flink 相关的技术知识点，这章主要是通过技术点来教大家如何去完成一些真实的需求，比如通过 State 去实时统计网站各页面一天的 PV 和 UV、通过 ProcessFunction 去做定时器处理一些延迟的事件（宕机告警）、通过 Async IO 读取告警规则、通过广播变量动态的更新告警规则、如何实时的做到日志告警。 虽然这些需求换到你们公司去可能不一样，但是这些技术知识点是可以运用到你的项目需求中去的，这里介绍的这些需求，你要学会去分析，然后去判断这些需求到底该使用什么技术来实现会更好，这样才可以做到活学活用。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何利用广播变量动态更新告警规则？","date":"2021-08-16T16:00:00.000Z","path":"2021/08/17/flink-in-action-11.4/","text":"11.4 如何利用广播变量动态更新告警规则？一个在生产环境运行的流作业有时候会想变更一些作业的配置或者数据流的配置，然后作业可以读取并使用新的配置，而不是通过修改配置然后重启作业来读取配置，毕竟重启一个有状态的流作业代价挺大，本节将带你熟悉 Broadcast，并通过一个案例来教会你如何去动态的更新作业的配置。 11.4.1 BroadcastVariable 简介BroadcastVariable 中文意思是广播变量，其实可以理解是一个公共的共享变量（可能是固定不变的数据集合，也可能是动态变化的数据集合），在作业中将该共享变量广播出去，然后下游的所有任务都可以获取到该共享变量，这样就可以不用将这个变量拷贝到下游的每个任务中。之所以设计这个广播变量的原因主要是因为在 Flink 中多并行度的情况下，每个算子或者不同算子运行所在的 Slot 不一致，这就导致它们不会共享同一个内存，也就不可以通过静态变量的方式去获取这些共享变量值。对于这个问题，有不少读者在问过我为啥我设置的静态变量值在本地运行是可以获取到的，在集群环境运行作业就出现空指针啊，该问题其实笔者自己也在生产环境遇到过，所以接下来好好教大家使用！ 11.4.2 如何使用 BroadcastVariable ？在 3.4 节中讲过如何 broadcast 算子和 BroadcastStream 如何使用，在 4.1 节中讲解了 Broadcast State 如何使用以及需要注意的地方，注意 BroadcastVariable 只能应用在批作业中，如果要应用在流作业中则需要要使用 BroadcastStream。 在批作业中通过使用 withBroadcastSet(DataSet, String) 来广播一个 DataSet 数据集合，并可以给这份数据起个名字，如果要获取数据的时候，可以通过 getRuntimeContext().getBroadcastVariable(String) 获取广播出去的变量数据。下面演示一下广播一个 DataSet 变量和获取变量的样例。 12345678910111213141516171819202122final ParameterTool params = ParameterTool.fromArgs(args);final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();//1. 待广播的数据DataSet&lt;Integer&gt; toBroadcast = env.fromElements(1, 2, 3);env.fromElements(\"a\", \"b\") .map(new RichMapFunction&lt;String, String&gt;() &#123; List&lt;Integer&gt; broadcastData; @Override public void open(Configuration parameters) throws Exception &#123; // 3. 获取广播的 DataSet 数据 作为一个 Collection broadcastData = getRuntimeContext().getBroadcastVariable(\"zhisheng\"); &#125; @Override public String map(String value) throws Exception &#123; return broadcastData.get(1) + value; &#125; &#125;).withBroadcastSet(toBroadcast, \"zhisheng\")// 2. 广播 DataSet .print(); 注意广播的时候设置的名称和获取的名称要一致，然后运行的结果如下图所示。 流作业中通常使用 BroadcastStream 的方式将变量集合在数据流中传递，可能数据集合会做修改更新，但是修改后其实并不想重启作业去读取这些新修改的配置，因为对于一个流作业来说重启带来的代价很高（需要考虑数据堆积和如何恢复至重启前的状态等问题），那么这种情况下就可以在广播数据流处定时查询数据，这样就能够获取更改后的数据，通常在这种广播数据处获取数据只需要设置一个并行度就好，时间根据需求来判断及时性，一般 1 分钟内的数据变更延迟都是在容忍范围之内。广播流中的元素保证流所有的元素最终都会发到下游的所有并行实例，但是元素到达下游的并行实例的顺序可能不相同。因此，对广播状态的修改不能依赖于输入数据的顺序。在进行 Checkpoint 时，所有的任务都会 Checkpoint 下它们的广播状态。 另外需要注意的是：广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大，因为广播出去的数据，会一致在内存中存在，除非程序执行结束。个人建议：如果数据集在几十兆或者百兆的时候，可以选择进行广播，如果数据集的大小上 G 的话，就不建议进行广播了。 上面介绍了下广播变量的在批作业的使用方式，下面通过一个案例来教大家如何在流作业中使用广播变量。 11.4.3 利用广播变量动态更新告警规则数据需求分析在 11.3.3 节中有设计一张简单的告警规则表，通常告警规则是会对外提供接口进行增删改查的，那么随着业务应用上线，开发人员会对其应用服务新增或者修改告警规则（更改之前规则中的阈值），那么更改之后就需要让告警的作业能够去感知到之前的规则发生了变动，所以就需要在作业中想个什么办法去获取到更改后的数据。有两种方式可以让作业知道规则的变更： push 和 pull 模式。 push 模式则需要在更新、删除、新增接口中不仅操作数据库，还需要额外的发送更新、删除、新增规则的事件到消息队列中，然后作业消费消息队列的数据再去做更新、删除、新增规则，这种及时性有保证，但是可能会有数据不统一的风险（如果消息队列的数据丢了，但是在接口中还是将规则的数据变更存储到数据库）；pull 模式下就需要作业定时去查找一遍所有的告警规则数据，然后存在作业内存中，这个时间可以设置的比较短，比如 1 分钟，这样就能既保证数据的一致性，时间延迟也是在容忍范围之内。 对于这种动态变化的规则数据，在 Flink 中通常是使用广播流来处理的。那么接下来就演示下如何利用广播变量动态更新告警规则数据，假设我们在数据库中新增告警规则或者修改告警规则指标的阈值，然后看作业中是否会出现相应的变化。 11.4.4 读取告警规则数据11.4.5 监控数据连接规则数据11.4.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何利用 Async I/O 读取告警规则？","date":"2021-08-15T16:00:00.000Z","path":"2021/08/16/flink-in-action-11.3/","text":"11.3 如何利用 Async I/O 读取告警规则？Async 中文是异步的意思，在流计算中，使用异步 I/O 能够提升作业整体的计算能力，本节中不仅会讲解异步 I/O 的 API 原理，还会通过一个实战需求（读取告警规则）来讲解异步 I/O 的使用。 11.3.1 为什么需要 Async I/O？在大多数情况下，IO 操作都是一个耗时的过程，尤其在流计算中，如果在具体的算子里面还有和第三方外部系统（比如数据库、Redis、HBase 等存储系统）做交互，比如在一个 MapFunction 中每来一条数据就要去查找 MySQL 中某张表的数据，然后跟查询出来的数据做关联（同步交互）。查询请求到数据库，再到数据库响应返回数据的整个流程的时间对于流作业来说是比较长的。那么该 Map 算子处理数据的速度就会降下来，在大数据量的情况下很可能会导致整个流作业出现反压问题（在 9.1 节中讲过），那么整个作业的消费延迟就会增加，影响作业整体吞吐量和实时性，从而导致最终该作业处于不可用的状态。 这种同步（Sync）的与数据库做交互操作，会因耗时太久导致整个作业延迟，如果换成异步的话，就可以同时处理很多请求并同时可以接收响应，这样的话，等待数据库响应的时间就会与其他发送请求和接收响应的时间重叠，相同的等待时间内会处理多个请求，从而比同步的访问要提高不少流处理的吞吐量。虽然也可以通过增大该算子的并行度去执行查数据库，但是这种解决办法需要消耗更多的资源（并行度增加意味着消费的 slot 个数也会增加），这种方法和使用异步处理的方法对比一下，还是使用异步的查询数据库这种方法值得使用。同步操作（Sync I/O）和异步操作（Async I/O）的处理流程如下图所示。 左侧表示的是在流处理中同步的数据库请求，右侧是异步的数据库请求。假设左侧是数据流中 A 数据来了发送一个查询数据库的请求看是否之前存在 A，然后等待查询结果返回，只有等 A 整个查询请求响应后才会继续开始 B 数据的查询请求，依此继续；而右侧是连续的去数据库查询是否存在 A、B、C、D，后面哪个请求先响应就先处理哪个，不需要和左侧的一样要等待上一个请求全部完成才可以开始下一个请求，所以异步的话吞吐量自然就高起来了。但是得注意的是：使用异步这种方法前提是要数据库客户端支持异步的请求，否则可能需要借助线程池来实现异步请求，但是现在主流的数据库通常都支持异步的操作，所以不用太担心。 11.3.2 Async I/O APIFlink 的 Async I/O API 允许用户在数据流处理中使用异步请求，并且还支持超时处理、处理顺序、事件时间、容错。在 Flink 中，如果要使用 Async I/O API，是非常简单的，需要通过下面三个步骤来执行对数据库的异步操作。 继承 RichAsyncFunction 抽象类或者实现用来分发请求的 AsyncFunction 接口 返回异步请求的结果的 Future 在 DataStream 上使用异步操作 官网也给出案例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142class AsyncDatabaseRequest extends RichAsyncFunction&lt;String, Tuple2&lt;String, String&gt;&gt; &#123; //数据库的客户端，它可以发出带有 callback 的并发请求 private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception &#123; client = new DatabaseClient(host, post, credentials); &#125; @Override public void close() throws Exception &#123; client.close(); &#125; @Override public void asyncInvoke(String key, final ResultFuture&lt;Tuple2&lt;String, String&gt;&gt; resultFuture) throws Exception&#123; //发出异步请求，接收 future 的结果 final Future&lt;String&gt; result = client.query(key); //设置客户端请求完成后执行的 callback，callback 只是将结果转发给 ResultFuture CompletableFuture.supplyAsync(new Supplier&lt;String&gt;() &#123; @Override public String get() &#123; try &#123; return result.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; return null; &#125; &#125; &#125;).thenAccept( (String dbResult) -&gt; &#123; resultFuture.complete(Collections.singleton(new Tuple2&lt;&gt;(key, dbResult))); &#125;); &#125;&#125;//原始数据DataStream&lt;String&gt; stream = ...;//应用异步 I/O 转换DataStream&lt;Tuple2&lt;String, String&gt;&gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100); 注意：ResultFuture 在第一次调用 resultFuture.complete 时就已经完成了，后面所有 resultFuture.complete 的调用都会被忽略。 下面两个参数控制了异步操作： Timeout：timeout 定义了异步操作过了多长时间后会被丢弃，这个参数是防止了死的或者失败的请求 Capacity：这个参数定义可以同时处理多少个异步请求。虽然异步请求会带来更好的吞吐量，但是该操作仍然可能成为流作业的性能瓶颈。限制并发请求的数量可确保操作不会不断累积处理请求，一旦超过 Capacity 值，它将触发反压。 超时处理结果顺序事件时间容错性保证实践技巧注意点11.3.3 利用 Async I/O 读取告警规则需求分析监控数据样例告警规则表设计告警规则实体类11.3.4 如何使用 Async I/O 读取告警规则数据11.3.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何使用 Flink ProcessFunction 处理宕机告警?","date":"2021-08-14T16:00:00.000Z","path":"2021/08/15/flink-in-action-11.2/","text":"11.2 如何使用 Flink ProcessFunction 处理宕机告警?在 3.3 节中讲解了 Process 算子的概念，本节中将更详细的讲解 Flink ProcessFunction，然后教大家如何使用 ProcessFunction 来解决公司中常见的问题 —— 宕机，这个宕机不仅仅包括机器宕机，还包含应用宕机，通常出现宕机带来的影响是会很大的，所以能及时收到告警会减少损失。 11.2.1 ProcessFunction 简介在 1.2.5 节中讲了 Flink 的 API 分层，其中可以看见 Flink 的底层 API 就是 ProcessFunction，它是一个低阶的流处理操作，它可以访问流处理程序的基础构建模块：Event、State、Timer。ProcessFunction 可以被认为是一种提供了对 KeyedState 和定时器访问的 FlatMapFunction。每当数据源中接收到一个事件，就会调用来此函数来处理。对于容错的状态，ProcessFunction 可以通过 RuntimeContext 访问 KeyedState。 定时器可以对处理时间和事件时间的变化做一些处理。每次调用 processElement() 都可以获得一个 Context 对象，通过该对象可以访问元素的事件时间戳以及 TimerService。TimerService 可以为尚未发生的事件时间/处理时间实例注册回调。当定时器到达某个时刻时，会调用 onTimer() 方法。在调用期间，所有状态再次限定为定时器创建的 key，允许定时器操作 KeyedState。如果要访问 KeyedState 和定时器，那必须在 KeyedStream 上使用 KeyedProcessFunction，比如在 keyBy 算子之后使用： 123dataStream.keyBy(...).process(new KeyedProcessFunction&lt;&gt;()&#123; &#125;) KeyedProcessFunction 是 ProcessFunction 函数的一个扩展，它可以在 onTimer 和 processElement 方法中获取到分区的 Key 值，这对于数据传递是很有帮助的，因为经常有这样的需求，经过 keyBy 算子之后可能还需要这个 key 字段，那么在这里直接构建成一个新的对象（新增一个 key 字段），然后下游的算子直接使用这个新对象中的 key 就好了，而不在需要重复的拼一个唯一的 key。 12345678910public void processElement(String value, Context ctx, Collector&lt;String&gt; out) throws Exception &#123; System.out.println(ctx.getCurrentKey()); out.collect(value);&#125;@Overridepublic void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception &#123; System.out.println(ctx.getCurrentKey()); super.onTimer(timestamp, ctx, out);&#125; 11.2.2 CoProcessFunction 简介如果要在两个输入流上进行操作，可以使用 CoProcessFunction，这个函数可以传入两个不同的数据流输入，并为来自两个不同数据源的事件分别调用 processElement1() 和 processElement2() 方法。可以按照下面的步骤来实现一个典型的 Join 操作： 为一个数据源的数据建立一个状态对象 从数据源处有新数据流过来的时候更新这个状态对象 在另一个数据源接收到元素时，关联状态对象并对其产生出连接的结果 比如，将监控的 metric 数据和告警规则数据进行一个连接，在流数据的状态中存储了告警规则数据，当有监控数据过来时，根据监控数据的 metric 名称和一些 tag 去找对应告警规则计算表达式，然后通过规则的表达式对数据进行加工处理，判断是否要告警，如果是要告警则会关联构造成一个新的对象，新对象中不仅有初始的监控 metric 数据，还有含有对应的告警规则数据以及通知策略数据，组装成这样一条数据后，下游就可以根据这个数据进行通知，通知还会在状态中存储这个告警状态，表示它在什么时间告过警了，下次有新数据过来的时候，判断新数据是否是恢复的，如果属于恢复则把该状态清除。 11.2.3 Timer 简介Timer 提供了一种定时触发器的功能，通过 TimerService 接口注册 timer。TimerService 在内部维护两种类型的定时器（处理时间和事件时间定时器）并排队执行。处理时间定时器的触发依赖于 ProcessingTimeService，它负责管理所有基于处理时间的触发器，内部使用 ScheduledThreadPoolExecutor 调度定时任务；事件时间定时器的触发依赖于系统当前的 Watermark。需要注意的一点就是：Timer 只能在 KeyedStream 中使用。 TimerService 会删除每个 Key 和时间戳重复的定时器，即每个 Key 在同一个时间戳上最多有一个定时器。如果为同一时间戳注册了多个定时器，则只会调用一次 onTimer（） 方法。Flink 会同步调用 onTimer() 和 processElement() 方法，因此不必担心状态的并发修改问题。TimerService 不仅提供了注册和删除 Timer 的功能，还可以通过它来获取当前的系统时间和 Watermark 的值。TimerService 类中的方法如下图所示。 容错定时器具有容错能力，并且会与应用程序的状态一起进行 Checkpoint，如果发生故障重启会从 Checkpoint／Savepoint 中恢复定时器的状态。如果有处理时间定时器原本是要在恢复起来的那个时间之前触发的，那么在恢复的那一刻会立即触发该定时器。定时器始终是异步的进行 Checkpoint（除 RocksDB 状态后端存储、增量的 Checkpoint、基于堆的定时器外）。因为定时器实际上也是一种特殊状态的状态，在 Checkpoint 时会写入快照中，所以如果有大量的定时器，则无非会增加一次 Checkpoint 所需要的时间，必要的话得根据实际情况合并定时器。 合并定时器由于 Flink 仅为每个 Key 和时间戳维护一个定时器，因此可以通过降低定时器的频率来进行合并以减少定时器的数量。对于频率为 1 秒的定时器（基于事件时间或处理时间），可以将目标时间向下舍入为整秒数，则定时器最多提前 1 秒触发，但不会迟于我们的要求，精确到毫秒。因此，每个键每秒最多有一个定时器。 12long coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerProcessingTimeTimer(coalescedTime); 由于事件时间计时器仅在 Watermark 到达时才触发，因此可以将当前 Watermark 与下一个 Watermark 的定时器一起调度和合并： 12long coalescedTime = ctx.timerService().currentWatermark() + 1;ctx.timerService().registerEventTimeTimer(coalescedTime); 定时器也可以类似下面这样移除： 1234567//删除处理时间定时器long timestampOfTimerToStop = ...ctx.timerService().deleteProcessingTimeTimer(timestampOfTimerToStop);//删除事件时间定时器long timestampOfTimerToStop = ...ctx.timerService().deleteEventTimeTimer(timestampOfTimerToStop); 如果没有该时间戳的定时器，则删除定时器无效。 11.2.4 如果利用 ProcessFunction 处理宕机告警？宕机告警需求分析宕机告警代码实现11.2.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何统计网站各页面一天内的 PV 和 UV？","date":"2021-08-13T16:00:00.000Z","path":"2021/08/14/flink-in-action-11.1/","text":"第十一章 —— Flink 实战本章主要是 Flink 实战，介绍了一些常见的需求，比如实时统计网站页面的 PV/UV、宕机告警、动态更新配置、应用 Error 日志实时告警等，然后分别去分析这些需求的实现方式，明白该使用 Flink 中的哪些知识点才能够很好的完成这种需求，并提供完整的案例代码供大家参考。在实现完成这些需求之后，笔者还将会更深一步的去讲解下这些知识点背后的实现方式，希望可以加深你对这些知识点的印象，以便后面你可以灵活的处理类似的需求。 11.1 如何统计网站各页面一天内的 PV 和 UV？大数据开发最常统计的需求可能就是 PV、UV。PV 全拼 PageView，即页面访问量，用户每次对网站的访问均被记录，按照访问量进行累计，假如用户对同一页面访问了 5 次，那该页面的 PV 就应该加 5。UV 全拼为 UniqueVisitor，即独立访问用户数，访问该页面的一台电脑客户端为一个访客，假如用户对同一页面访问了 5 次，那么该页面的 UV 只应该加 1，因为 UV 计算的是去重后的用户数而不是访问次数。当然如果是按天统计，那么当天 0 点到 24 点相同的客户端只被计算一次，如果过了今天 24 点，第二天该用户又访问了该页面，那么第二天该页面的 UV 应该加 1。 概念明白了那如何使用 Flink 来统计网站各页面的 PV 和 UV 呢？通过本节来详细描述。 11.1.1 统计网站各页面一天内的 PV在 9.5.2 节端对端如何保证 Exactly Once 中的幂等性写入如何保证端对端 Exactly Once 部分已经用案例讲述了如何通过 Flink 的状态来计算 APP 的 PV，并能够保证 Exactly Once。如果在工作中需要计算网站各页面一天内的 PV，只需要将案例中的 APP 替换成各页面的 id 或者各页面的 url 进行统计即可，按照各页面 id 和日期组合做为 key 进行 keyBy，相同页面、相同日期的数据发送到相同的实例中进行 PV 值的累加，每个 key 对应一个 ValueState，将 PV 值维护在 ValueState 即可。如果一些页面属于爆款页面，例如首页或者活动页面访问特别频繁就可能出现某些 subtask 上的数据量特别大，导致各个 subtask 之前出现数据倾斜的问题，关于数据倾斜的解决方案请参考 9.6 节。 11.1.2 统计网站各页面一天内 UV 的三种方案PV 统计相对来说比较简单，每来一条用户的访问日志只需要从日志中提取出相应的页面 id 和日期，将其对应的 PV 值加一即可。相对而言统计 UV 就有难度了，同一个用户一天内多次访问同一个页面，只能计数一次。所以每来一条日志，日志中对应页面的 UV 值是否需要加一呢？存在两种情况：如果该用户今天第一次访问该页面，那么 UV 应该加一。如果该用户今天不是第一次访问该页面，表示 UV 中已经记录了该用户，UV 要基于用户去重，所以此时 UV 值不应该加一。难点就在于如何判断该用户今天是不是第一次访问该页面呢？ 把问题简单化，先不考虑日期，现在统计网站各页面的累积 UV，可以为每个页面维护一个 Set 集合，假如网站有 10 个页面，那么就维护 10 个 Set 集合，集合中存放着所有访问过该页面用户的 user_id。每来一条用户的访问日志，我们都需要从日志中解析出相应的页面 id 和用户 user_id，去该页面 id 对应的 Set 中查找该 user_id 之前有没有访问过该页面，如果 Set 中包含该 user_id 表示该用户之前访问过该页面，所以该页面的 UV 值不应该加一，如果 Set 中不包含该 user_id 表示该用户之前没有访问过该页面，所以该页面的 UV 值应该加一，并且将该 user_id 插入到该页面对应的 Set 中，表示该用户访问过该页面了。要按天去统计各页面 UV，只需要将日期和页面 id 看做一个整体 key，每个 key 对应一个 Set，其他流程与上述类似。具体的程序流程图如下图所示。 使用 Redis 的 set 来维护用户集合每个 key 都需要维护一个 Set，这个 Set 存放在哪里呢？这里每条日志都需要访问一次 Set，对 Set 访问比较频繁，对存储介质的延迟要求比较高，所以可以使用 Redis 的 set 数据结构，Redis 的 set 数据结构也会对数据进行去重。可以将页面 id 和日期拼接做为 Redis 的 key，通过 Redis 的 sadd 命令将 user_id 放到 key 对应的 set 中即可。Redis 的 set 中存放着今天访问过该页面所有用户的 user_id。 在真实的工作中，Flink 任务可能不需要维护一个 UV 值，Flink 任务承担的角色是实时计算，而查询 UV 可能是一个 Java Web 项目。Web 项目只需要去 Redis 查询相应 key 对应的 set 中元素的个数即可，Redis 的 set 数据结构有 scard 命令可以查询 set 中元素个数，这里的元素个数就是我们所要统计的网站各页面每天的 UV 值。所以使用 Redis set 数据结构的方案 Flink 任务的代码很简单，只需要从日志中解析出相应的日期、页面id 和 user_id，将日期和页面 id 组合做为 Redis 的 key，最后将 user_id 通过 sadd 命令添加到 set 中，Flink 任务的工作就结束了，之后 Web 项目就能从 Redis 中查询到实时增加的 UV 了。下面来看详细的代码实现。 用户访问网站页面的日志实体类： 123456789101112public class UserVisitWebEvent &#123; // 日志的唯一 id private String id; // 日期，如：20191025 private String date; // 页面 id private Integer pageId; // 用户的唯一标识，用户 id private String userId; // 页面的 url private String url;&#125; 生成测试数据的核心代码如下: 123456789101112131415String yyyyMMdd = new DateTime(System.currentTimeMillis()).toString(\"yyyyMMdd\");int pageId = random.nextInt(10); // 随机生成页面 idint userId = random.nextInt(100); // 随机生成用户 idUserVisitWebEvent userVisitWebEvent = UserVisitWebEvent.builder() .id(UUID.randomUUID().toString()) // 日志的唯一 id .date(yyyyMMdd) // 日期 .pageId(pageId) // 页面 id .userId(Integer.toString(userId)) // 用户 id .url(\"url/\" + pageId) // 页面的 url .build();// 对象序列化为 JSON 发送到 KafkaProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, GsonUtil.toJson(userVisitWebEvent));producer.send(record); 统计 UV 的核心代码如下，对 Redis Connector 不熟悉的请参阅 3.11 节如何使用 Flink Connectors —— Redis： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class RedisSetUvExample &#123; public static void main(String[] args) throws Exception &#123; // 省略了 env初始化及 Checkpoint 相关配置 Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, UvExampleUtil.broker_list); props.put(ConsumerConfig.GROUP_ID_CONFIG, \"app-uv-stat\"); FlinkKafkaConsumerBase&lt;String&gt; kafkaConsumer = new FlinkKafkaConsumer011&lt;&gt;( UvExampleUtil.topic, new SimpleStringSchema(), props) .setStartFromLatest(); FlinkJedisPoolConfig conf = new FlinkJedisPoolConfig .Builder().setHost(\"192.168.30.244\").build(); env.addSource(kafkaConsumer) .map(string -&gt; &#123; // 反序列化 JSON UserVisitWebEvent userVisitWebEvent = GsonUtil.fromJson( string, UserVisitWebEvent.class); // 生成 Redis key，格式为 日期_pageId，如: 20191026_0 String redisKey = userVisitWebEvent.getDate() + \"_\" + userVisitWebEvent.getPageId(); return Tuple2.of(redisKey, userVisitWebEvent.getUserId()); &#125;) .returns(new TypeHint&lt;Tuple2&lt;String, String&gt;&gt;()&#123;&#125;) .addSink(new RedisSink&lt;&gt;(conf, new RedisSaddSinkMapper())); env.execute(\"Redis Set UV Stat\"); &#125; // 数据与 Redis key 的映射关系 public static class RedisSaddSinkMapper implements RedisMapper&lt;Tuple2&lt;String, String&gt;&gt; &#123; @Override public RedisCommandDescription getCommandDescription() &#123; // 这里必须是 sadd 操作 return new RedisCommandDescription(RedisCommand.SADD); &#125; @Override public String getKeyFromData(Tuple2&lt;String, String&gt; data) &#123; return data.f0; &#125; @Override public String getValueFromData(Tuple2&lt;String, String&gt; data) &#123; return data.f1; &#125; &#125;&#125; Redis 中统计结果如下图所示，左侧展示的 Redis key，20191026_1 表示 2019年10月26日浏览过 pageId 为 1 的页面对应的 key，右侧展示 key 对应的 set 集合，表示 userId 为 [0,6,27,30,66,67,79,88] 的用户在 2019年10月26日浏览过 pageId 为 1 的页面。 要想获取 20191026_1 对应的 UV 值，可通过 scard 命令获取 set 中 user_id 的数量，具体操作如下所示： 12redis&gt; scard 20191026_18 通过上述代码即可通过 Redis 的 set 数据结构来统计网站各页面的 UV。 使用 Flink 的 KeyedState 来维护用户集合使用 Redis 的 HyperLogLog 来统计 UV11.1.3 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何设置 Flink Job RestartStrategy（重启策略）？","date":"2021-08-12T16:00:00.000Z","path":"2021/08/13/flink-in-action-10.2/","text":"10.2 如何使用 Flink ParameterTool 读取配置？在使用 Flink 中不知道你有没有觉得配置的管理很不方便，比如像算子的并行度配置、Kafka 数据源的配置（broker 地址、topic 名、group.id）、Checkpoint 是否开启、状态后端存储路径、数据库地址、用户名和密码等，反正各种各样的配置都杂乱在一起，当然你可能说我就在代码里面写死不就好了，但是你有没有想过你的作业是否可以不修改任何配置就直接在各种环境（开发、测试、预发、生产）运行呢？可能每个环境的这些配置对应的值都是不一样的，如果你是直接在代码里面写死的配置，那这下子就比较痛苦了，每次换个环境去运行测试你的作业，你都要重新去修改代码中的配置，然后编译打包，提交运行，这样你就要花费很多时间在这些重复的劳动力上了。有没有什么办法可以解决这种问题呢？ 10.2.1 Flink Job 配置在 Flink 中其实是有几种方法来管理配置，下面分别来讲解一下。 使用 ConfigurationFlink 提供了 withParameters 方法，它可以传递 Configuration 中的参数给，要使用它，需要实现那些 Rich 函数，比如实现 RichMapFunction，而不是 MapFunction，因为 Rich 函数中有 open 方法，然后可以重写 open 方法通过 Configuration 获取到传入的参数值。 12345678910111213141516171819202122232425262728ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();// Configuration 类来存储参数Configuration configuration = new Configuration();configuration.setString(\"name\", \"zhisheng\");env.fromElements(WORDS) .flatMap(new RichFlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; String name; @Override public void open(Configuration parameters) throws Exception &#123; //读取配置 name = parameters.getString(\"name\", \"\"); &#125; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] splits = value.toLowerCase().split(\"\\\\W+\"); for (String split : splits) &#123; if (split.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(split + name, 1)); &#125; &#125; &#125; &#125;).withParameters(configuration) //将参数传递给函数 .print(); 但是要注意这个 withParameters 只在批程序中支持，流程序中是没有该方法的，并且这个 withParameters 要在每个算子后面使用才行，并不是一次使用就所有都可以获取到，如果所有算子都要该配置，那么就重复设置多次就会比较繁琐。 10.2.2 ParameterTool 管理配置上面通过 Configuration 的局限性很大，其实在 Flink 中还可以通过使用 ParameterTool 类读取配置，它可以读取环境变量、运行参数、配置文件，下面分别讲下每种如何使用。 读取运行参数我们知道 Flink UI 上是支持为每个 Job 单独传入 arguments（参数）的，它的格式要求是如下这种。 123--brokers 127.0.0.1:9200--username admin--password 123456 或者这种 123-brokers 127.0.0.1:9200-username admin-password 123456 然后在 Flink 程序中你可以直接使用 ParameterTool.fromArgs(args) 获取到所有的参数，然后如果你要获取某个参数对应的值的话，可以通过 parameterTool.get(&quot;username&quot;) 方法。那么在这个地方其实你就可以将配置放在一个第三方的接口，然后这个参数值中传入一个接口，拿到该接口后就能够通过请求去获取更多你想要的配置。 读取系统属性ParameterTool 还支持通过 ParameterTool.fromSystemProperties() 方法读取系统属性。 读取配置文件除了上面两种外，ParameterTool 还支持 ParameterTool.fromPropertiesFile(&quot;/application.properties&quot;) 读取 properties 配置文件。你可以将所有要配置的地方（比如并行度和一些 Kafka、MySQL 等配置）都写成可配置的，然后其对应的 key 和 value 值都写在配置文件中，最后通过 ParameterTool 去读取配置文件获取对应的值。 ParameterTool 获取值ParameterTool 类提供了很多便捷方法去获取值，如下图所示。 你可以在应用程序的 main() 方法中直接使用这些方法返回的值，例如：你可以按如下方法来设置一个算子的并行度： 123ParameterTool parameters = ParameterTool.fromArgs(args);int parallelism = parameters.get(\"mapParallelism\", 2);DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = data.flatMap(new Tokenizer()).setParallelism(parallelism); 因为 ParameterTool 是可序列化的，所以你可以将它当作参数进行传递给自定义的函数。 12ParameterTool parameters = ParameterTool.fromArgs(args);DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = dara.flatMap(new Tokenizer(parameters)); 然后在函数内部使用 ParameterTool 来获取命令行参数，这样就意味着你在作业任何地方都可以获取到参数，而不是像 withParameters 一样需要每次都设置。 注册全局参数在 ExecutionConfig 中可以将 ParameterTool 注册为全作业参数的参数，这样就可以被 JobManager 的 web 端以及用户自定义函数中以配置值的形式访问。 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args)); 然后就可以在用户自定义的 Rich 函数中像如下这样获取到参数值了。 12345678910111213env.addSource(new RichSourceFunction&lt;String&gt;() &#123; @Override public void run(SourceContext&lt;String&gt; sourceContext) throws Exception &#123; while (true) &#123; ParameterTool parameterTool = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); sourceContext.collect(System.currentTimeMillis() + parameterTool.get(\"os.name\") + parameterTool.get(\"user.home\")); &#125; &#125; @Override public void cancel() &#123; &#125;&#125;) 在笔者公司内通常是以 Job 运行的环境变量为准，比如我们是运行在 K8s 上面，那么我们会为我们的这个 Flink Job 设置很多环境变量，设置的环境变量的值就得通过 ParameterTool 类去获取，我们是会优先根据环境变量的值为准，如果环境变量的值没有就会去读取应用运行参数，如果应用运行参数也没有才会去读取之前已经写好在配置文件中的配置。大概代码如下： 12345678910111213141516public static ParameterTool createParameterTool(final String[] args) throws Exception &#123; return ParameterTool .fromPropertiesFile(ExecutionEnv.class.getResourceAsStream(\"/application.properties\")) .mergeWith(ParameterTool.fromArgs(args)) .mergeWith(ParameterTool.fromSystemProperties()) .mergeWith(ParameterTool.fromMap(getenv()));// mergeWith 会使用最新的配置&#125;//获取 Job 设置的环境变量private static Map&lt;String, String&gt; getenv() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, String&gt; entry : System.getenv().entrySet()) &#123; map.put(entry.getKey().toLowerCase().replace('_', '.'), entry.getValue()); &#125; return map;&#125; 这样如果 Job 要更改一些配置，直接在 Job 在 K8s 上面的环境变量进行配置就好了，修改配置后然后重启 Job 就可以运行起来了，整个过程都不需要再次将作业重新编译打包的。但是这样其实也有一定的坏处，重启一个作业的代价很大，因为在重启后你又要去保证状态要恢复到之前未重启时的状态，尽管 Flink 中的 Checkpoint 和 Savepoint 已经很强大了，但是对于复杂的它来说我们多一事不如少一事，所以其实更希望能够直接动态的获取配置，如果配置做了更改，作业能够感知到。在 Flink 中有的配置是不能够动态设置的，但是比如应用业务配置却是可以做到动态的配置，这时就需要使用比较强大的广播变量，广播变量在之前 3.4 节已经介绍过了，如果忘记可以再回去查看，另外在 11.4 节中会通过一个实际案例来教你如何使用广播变量去动态的更新配置数据。 10.2.3 ParameterTool 源码分析10.2.4 自定义配置参数类10.2.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M 本章讲了两个实践相关的内容，一个是作业的重启策略，从分析真实线上故障来教大家如何去配置重启策略，以及介绍重启策略的种类，另一个是使用 ParameterTool 去管理配置。两个实践都是比较真实且有一定帮助作用的，希望你也可以应用在你的项目中去。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何设置 Flink Job RestartStrategy（重启策略）？","date":"2021-08-11T16:00:00.000Z","path":"2021/08/12/flink-in-action-10.1/","text":"第十章 —— Flink 最佳实践本章将介绍两个最佳实践，第一个是如何合理的配置重启策略，笔者通过自己的亲身经历来讲述配置重启策略的重要性，接着介绍了 Flink 中的重启策略和恢复策略的发展实现过程；第二个是如何去管理 Flink 作业的配置。两个实践大家可以参考，不一定要照搬运用在自己的公司，同时也希望你可以思考下自己是否有啥最佳实践可以分享。 10.1 如何设置 Flink Job RestartStrategy（重启策略）？从使用 Flink 到至今，遇到的 Flink 有很多，解决的问题更多（含帮助微信好友解决问题），所以对于 Flink 可能遇到的问题及解决办法都比较清楚，那么在这章就给大家讲解下几个 Flink 中比较常遇到的问题的解决办法。 10.1.1 常见错误导致 Flink 作业重启不知道大家是否有遇到过这样的问题：整个 Job 一直在重启，并且还会伴随着一些错误（可以通过 UI 查看 Exceptions 日志），以下三张图片中的错误信息是笔者曾经生产环境遇到过的一些问题。 笔者就曾因为上图中的一个异常报错，作业一直重启，在深夜线上发版的时候，同事发现这个问题，凌晨两点的时候打电话把我叫醒起来修 BUG，真是惨的教训，哈哈哈，估计这辈子都忘不掉了！ 其实遇到上面这种问题比较常见的，比如有时候因为数据的问题（不合规范、为 null 等），这时在处理这些脏数据的时候可能就会遇到各种各样的异常错误，比如空指针、数组越界、数据类型转换错误等。可能你会说只要过滤掉这种脏数据就行了，或者进行异常捕获就不会导致 Job 不断重启的问题了。 确实如此，如果做好了脏数据的过滤和异常的捕获，Job 的稳定性确实有保证，但是复杂的 Job 下每个算子可能都会产生出脏数据（包含源数据可能也会为空或者不合法的数据），你不可能在每个算子里面也用一个大的 try catch 做一个异常捕获，所以脏数据和异常简直就是防不胜防，不过我们还是要尽力的保证代码的健壮性，但是也要配置好 Flink Job 的 RestartStrategy（重启策略）。 10.1.2 RestartStrategy 简介RestartStrategy，重启策略，在遇到机器或者代码等不可预知的问题时导致 Job 或者 Task 挂掉的时候，它会根据配置的重启策略将 Job 或者受影响的 Task 拉起来重新执行，以使得作业恢复到之前正常执行状态。Flink 中的重启策略决定了是否要重启 Job 或者 Task，以及重启的次数和每次重启的时间间隔。 10.1.3 为什么需要 RestartStrategy？重启策略会让 Job 从上一次完整的 Checkpoint 处恢复状态，保证 Job 和挂之前的状态保持一致，另外还可以让 Job 继续处理数据，不会出现 Job 挂了导致消息出现大量堆积的问题，合理的设置重启策略可以减少 Job 不可用时间和避免人工介入处理故障的运维成本，因此重启策略对于 Flink Job 的稳定性来说有着举足轻重的作用。 10.1.4 如何配置 RestartStrategy？既然 Flink 中的重启策略作用这么大，那么该如何配置呢？其实如果 Flink Job 没有单独设置重启重启策略的话，则会使用集群启动时加载的默认重启策略，如果 Flink Job 中单独设置了重启策略则会覆盖默认的集群重启策略。默认重启策略可以在 Flink 的配置文件 flink-conf.yaml 中设置，由 restart-strategy 参数控制，有 fixed-delay（固定延时重启策略）、failure-rate（故障率重启策略）、none（不重启策略）三种可以选择，如果选择的参数不同，对应的其他参数也不同。下面分别介绍这几种重启策略和如何配置。 FixedDelayRestartStrategy（固定延时重启策略）FixedDelayRestartStrategy 是固定延迟重启策略，程序按照集群配置文件中或者程序中额外设置的重启次数尝试重启作业，如果尝试次数超过了给定的最大次数，程序还没有起来，则停止作业，另外还可以配置连续两次重启之间的等待时间，在 flink-conf.yaml 中可以像下面这样配置。 123restart-strategy: fixed-delayrestart-strategy.fixed-delay.attempts: 3 #表示作业重启的最大次数，启用 checkpoint 的话是 Integer.MAX_VALUE，否则是 1。restart-strategy.fixed-delay.delay: 10 s #如果设置分钟可以类似 1 min，该参数表示两次重启之间的时间间隔，当程序与外部系统有连接交互时延迟重启可能会有帮助，启用 checkpoint 的话，延迟重启的时间是 10 秒，否则使用 akka.ask.timeout 的值。 在程序中设置固定延迟重启策略的话如下： 12345ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 尝试重启的次数 Time.of(10, TimeUnit.SECONDS) // 延时)); FailureRateRestartStrategy（故障率重启策略）FailureRateRestartStrategy 是故障率重启策略，在发生故障之后重启作业，如果固定时间间隔之内发生故障的次数超过设置的值后，作业就会失败停止，该重启策略也支持设置连续两次重启之间的等待时间。 1234restart-strategy: failure-raterestart-strategy.failure-rate.max-failures-per-interval: 3 #固定时间间隔内允许的最大重启次数，默认 1restart-strategy.failure-rate.failure-rate-interval: 5 min #固定时间间隔，默认 1 分钟restart-strategy.failure-rate.delay: 10 s #连续两次重启尝试之间的延迟时间，默认是 akka.ask.timeout 可以在应用程序中这样设置来配置故障率重启策略： 123456ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // 固定时间间隔允许 Job 重启的最大次数 Time.of(5, TimeUnit.MINUTES), // 固定时间间隔 Time.of(10, TimeUnit.SECONDS) // 两次重启的延迟时间)); NoRestartStrategy（不重启策略）NoRestartStrategy 作业不重启策略，直接失败停止，在 flink-conf.yaml 中配置如下： 1restart-strategy: none 在程序中如下设置即可配置不重启： 12ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart()); Fallback（备用重启策略）如果程序没有启用 Checkpoint，则采用不重启策略，如果开启了 Checkpoint 且没有设置重启策略，那么采用固定延时重启策略，最大重启次数为 Integer.MAX_VALUE。 在应用程序中配置好了固定延时重启策略，可以测试一下代码异常后导致 Job 失败后重启的情况，然后观察日志，可以看到 Job 重启相关的日志： 123[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Try to restart or fail the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) if no longer possible.[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) switched from state FAILING to RESTARTING.[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Restarting the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0). 最后重启次数达到配置的最大重启次数后 Job 还没有起来的话，则会停止 Job 并打印日志： 1[flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Could not restart the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) because the restart strategy prevented it. Flink 中几种重启策略的设置如上，大家可以根据需要选择合适的重启策略，比如如果程序抛出了空指针异常，但是你配置的是一直无限重启，那么就会导致 Job 一直在重启，这样无非再浪费机器资源，这种情况下可以配置重试固定次数，每次隔多久重试的固定延时重启策略，这样在重试一定次数后 Job 就会停止，如果对 Job 的状态做了监控告警的话，那么你就会收到告警信息，这样也会提示你去查看 Job 的运行状况，能及时的去发现和修复 Job 的问题。 10.1.5 RestartStrategy 源码分析再介绍重启策略应用程序代码配置的时候不知道你有没有看到设置重启策略都是使用 RestartStrategies 类，通过该类的方法就可以创建不同的重启策略，在 RestartStrategies 类中提供了五个方法用来创建四种不同的重启策略（有两个方法是创建 FixedDelay 重启策略的，只不过方法的参数不同），如下图所示。 在每个方法内部其实调用的是 RestartStrategies 中的内部静态类，分别是 NoRestartStrategyConfiguration、FixedDelayRestartStrategyConfiguration、FailureRateRestartStrategyConfiguration、FallbackRestartStrategyConfiguration，这四个类都继承自 RestartStrategyConfiguration 抽象类，如下图所示。 上面是定义的四种重启策略的配置类，在 Flink 中是靠 RestartStrategyResolving 类中的 resolve 方法来解析 RestartStrategies.RestartStrategyConfiguration，然后根据配置使用 RestartStrategyFactory 创建 RestartStrategy。RestartStrategy 是一个接口，它有 canRestart 和 restart 两个方法，它有四个实现类： FixedDelayRestartStrategy、FailureRateRestartStrategy、ThrowingRestartStrategy、NoRestartStrategy，如下图所示。 10.1.6 Failover Strategies（故障恢复策略）重启所有的任务基于 Region 的局部故障重启策略10.1.7 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RBYj66M","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何处理 Flink 中数据倾斜问题？","date":"2021-08-10T16:00:00.000Z","path":"2021/08/11/flink-in-action-9.6/","text":"9.6 如何处理 Flink 中数据倾斜问题？在大数据计算场景，无论使用 MapReduce、Spark 还是 Flink 计算框架，无论是批处理还是流处理都存在数据倾斜的问题，通过本节学习产生数据倾斜的原因及如何在生产环境解决数据倾斜。 9.6.1 数据倾斜简介分析一个计算各 app PV 的案例，如下图所示，圆球表示 app1 的日志，方块表示 app2 的日志，Source 端从外部系统读取用户上报的各 app 行为日志，要计算各 app 的 PV，所以按照 app 进行 keyBy，相同 app 的数据发送到同一个 Operator 实例中处理，keyBy 后对 app 的 PV 值进行累加来，最后将计算的 PV 结果输出到外部 Sink 端。 可以看到在任务运行过程中，计算 Count 的算子有两个并行度，其中一个并行度处理 app1 的数据，另一个并行度处理 app2 的数据。由于 app1 比较热门，所以 app1 的日志量远大于 app2 的日志量，造成计算 app1 PV 的并行度压力过大成为整个系统的瓶颈，而计算 app2 PV 的并行度数据量较少所以 CPU、内存以及网络资源的使用率整体都比较低，这就是产生数据倾斜的案例。 随着业务的不断发展，如果 app1 的日志量暴增，单个节点的单个并行度已经承担不了计算 app1 PV 的任务，此时如何来解决呢？对于不了解数据倾斜的同学看到 Flink 任务出现了延迟，结合之前学习的反压章节，定位整个 Flink 任务的瓶颈在于 Count 算子，所以认为 Count 算子的并行度不够，于是解决思路就是调大 Count 算子的并行度至 4 来提高 Count 算子的计算能力，调大并行度以后发现 Flink 任务的吞吐量并没有提升，而且通过反压机制定位到系统的瓶颈还在于 Count 算子，难道 Count 算子的并行度需要从 2 调大到 10 吗？No，上述情况就算把并行度调大到 100，依然不能解决任务瓶颈。为什么出现这种情况呢？要计算各 app 的 PV 数据，那么相同 app 的数据必然要发送到相同的 Operator 实例去处理，现在只有两个 app，最多只能分配到两个并行度上去执行，如果 Count 算子的并行度大于 2，意味着肯定有一些并行度分配不到数据，所以上述情况调大 Count 算子的并行度不能解决问题。那使用 Flink 如何来解决数据倾斜呢，我们先学习 Flink 中如何来判断是否发生了数据倾斜。 9.6.2 判断是否存在数据倾斜这里再通过一个案例来讲述 Flink 任务如何来判断是否存在数据倾斜，如下图所示，是 Flink Web UI Job 页面展示的任务执行计划，可以看到任务经过 Operator Chain 后，总共有两个 Task，上游 Task 将数据 keyBy 后发送到下游 Task，如何判断第二个 Task 计算的数据是否存在数据呢？ 如下图所示，通过 Flink Web UI 中 Job 页面的第一个 Subtasks 选项卡，可以看到任务的两个 Task，点击 Task，可以看到 Task 相应的 Subtask 详情。例如 Subtask 的启动时间、结束时间、持续时长、接收数据量的字节数以及接收数据的个数。图中可以看到，相同 Task 的多个 Subtask 中，有的 Subtask 接收到 1.69 TB 的数据量，有的 Subtask 接收到 17.6 TB 的数据量，通过 Flink Web UI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜，接下来学习 Flink 中如何来解决数据倾斜。 9.6.3 分析和解决数据倾斜问题在 Flink 中，很多因素都会导致数据倾斜，例如 9.6.1 节描述的 keyBy 后的聚合操作存在数据倾斜。keyBy 之前的数据直接来自于数据源，一般不会出现数据倾斜，除非数据源中的数据发生了数据倾斜。本小节将从多个角度来解决数据倾斜。 keyBy 后的聚合操作存在数据倾斜keyBy 之前发生数据倾斜9.6.4 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/uFEEYzJ 在前一章中讲解了 Flink 监控系统的重要性，这章主要讲解 Flink 作业的性能调优。当作业出现各种各样的问题时，其实这时就体现了前面章节提到的监控的重要性，所以本章的内容也比较依赖于监控系统，然后才能够更好的去排查问题，然后去解决问题。 在本章中讲解的反压问题、并行度设置问题、数据倾斜问题等都是开发作业时要注意的点，本章不仅讲解了这些问题出现后的解决方案，还深入的剖析了这些问题为啥会出现，只有知其原因后，后面开发新的作业时才会去注意这些问题。本章的内容属于高阶玩家要掌握的，希望你也能够好好理解，在你们公司遇到同样问题的时候可以站出来去解决。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 中如何保证 Exactly Once？","date":"2021-08-10T16:00:00.000Z","path":"2021/08/11/flink-in-action-9.5/","text":"9.5 Flink 中如何保证 Exactly Once？在分布式场景下，我们的应用程序随时可能出现任何形式的故障，例如：机器硬件故障、程序 OOM 等。当应用程序出现故障时，Flink 为了保证数据消费的 Exactly Once，需要有相应的故障容错能力。Flink 是通过周期性 Checkpoint 的方式来实现故障容错，这里使用的是基于 Chandy-Lamport 改进的算法。本节会介绍 Flink 内部如何保证 Exactly Once 以及端对端如何保证 Exactly Once。 9.5.1 Flink 内部如何保证 Exactly Once？Flink 官网的定义是 Stateful Computations over Data Streams（数据流上的有状态计算），那到底什么是状态呢？举一个无状态计算的例子，比如：我们只是进行一个字符串拼接，输入a，输出a_666,输入b，输出 b_666。无状态表示计算输出的结果跟之前的状态没关系，符合幂等性。幂等性就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生副作用。而计算 PV、UV 就属于有状态计算。实时计算 PV 时，每次都需要从某个存储介质的结果表中拿到之前的 PV 值，+1 后 set 到结果表中。有状态计算表示输出的结果跟之前的状态有关系，不符合幂等性，访问多次，PV 会增加。 Flink的 Checkpoint 功能简介Flink Checkpoint 机制的存在就是为了解决 Flink 任务在运行过程中由于各种原因导致任务失败后，能够正常恢复任务。那 Checkpoint 具体做了哪些功能，为什么任务挂掉之后，通过 Checkpoint 机制能使得任务恢复呢？Checkpoint 是通过给程序做快照的方式使得将整个程序某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。问题来了，快照是什么东西？SnapShot翻译为快照，是指将程序中某些信息存一份，后期可以用这些信息来恢复任务。对于一个 Flink 任务来讲，快照里面到底保存着什么信息呢？理论知识一般比较晦涩难懂，我们分析一个案例，用案例辅助大家理解快照里面到底存储什么信息。计算各个 app 的 PV，使用 Flink 该怎么统计呢？ 可以把要统计的 app_id 做为 key，对应的 PV 值做为 value，将统计的结果放到一个 Map 集合中，这个 Map 集合可以是内存里的 HashMap 或其他 kv 数据库，例如放到Redis 的 key、value 结构中。从 Kafka 读取到一条条日志，由于要统计各 app 的 PV，所以我们需要从日志中解析出 app_id 字段，每来一条日志，只需要从 Map 集合将相应 app_id 的 PV 值拿出来，+1 后 put 到 Map 中，这样我们的 Map 中永远保存着所有 app 最新的 PV 数据。详细流程如下图所示： 图中包含三部分：第一个是 Kafka 的一个名为 test 的 Topic，我们的数据来源于这个 Topic，第二个是 Flink 的 Source Task，是 Flink 应用程序读取数据的 Task，第三个是计算 PV 的 Flink Task，用于统计各个 app 的 PV 值，并将 PV 结果输出到 Map 集合。 Flink 的 Source Task 记录了当前消费到 test Topic 所有 partition 的 offset，为了方便理解 Checkpoint 的作用，这里先用一个 partition 进行讲解，假设名为 test 的 Topic只有一个partition0。例：（0，60000）表示0号partition 目前消费到 offset 为 60000 的数据。Flink 的 PV task 记录了当前计算的各 app 的 PV 值，为了方便讲解，这里假设有两个app：app1、app2。例：（app1，50000）（app2，10000）表示 app1 当前 PV 值为50000、app2 当前 PV 值为 10000。计算过程中，每来一条数据，只需要确定相应 app_id，将相应的 PV 值 +1 后 put 到 map 中即可。 该案例中，Checkpoint 到底记录了什么信息呢？记录的其实就是第 n 次 Checkpoint 消费的 offset 信息和各app 的 PV 值信息，记录下发生 Checkpoint 当前的状态信息，并将该状态信息保存到相应的状态后端。（注：状态后端是保存状态的地方，决定状态如何保存，如何保证状态高可用，我们只需要知道，我们能从状态后端拿到 offset 信息和 PV 信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过 Checkpoint 来恢复我们的应用程序）。下面列出了第 100 次 Checkpoint 的时候，状态后端保存的状态信息： 123chk-100 - offset：（0，60000） - PV：（app1，50000）（app2，10000） 该状态信息表示第 100 次 Checkpoint 的时候， partition 0 offset 消费到了 60000，PV 统计结果为（app1，50000）（app2，10000） 。如果任务挂了，如何恢复？ 假如我们设置了一分钟进行一次 Checkpoint，第 100 次 Checkpoint 成功后，过了十秒钟，offset已经消费到 （0，60100），PV 统计结果变成了（app1，50080）（app2，10020），突然任务挂了，怎么办？其实很简单，Flink 只需要从最近一次成功的 Checkpoint，也就是从第 100 次 Checkpoint 保存的 offset（0，60000）处接着消费即可，当然 PV 值也要从第 100 次 Checkpoint 里保存的 PV 值（app1，50000）（app2，10000）进行累加，不能从（app1，50080）（app2，10020）处进行累加，因为 partition 0 offset消费到 60000 时，对应的 PV 统计结果为（app1，50000）（app2，10000）。当然如果你想从offset （0，60100）PV（app1，50080）（app2，10020）这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来，只有在 Checkpoint 的时候，才会把这些完整的状态保存到状态后端，供我们恢复任务。我们能做的最高效方式就是从最近一次成功的 Checkpoint 处恢复，也就是一直所说的 chk-100。以上基本就是 Checkpoint 承担的工作，为了方便理解，描述的业务场景比较简单。 补充两个问题：计算 PV 的 task 在一直运行，它怎么知道什么时候去做 Checkpoint 呢？计算 PV 的 task 怎么保证它自己计算的 PV 值（app1，50000）（app2，10000）就是offset（0，60000）那一刻的统计结果呢？Flink 在数据中加了一个叫做 barrier（栅栏） 的东西，如下图所示，用圈标注的就是 barrier。 barrier 从 Source Task 处生成，一直流到 Sink Task，期间所有的 Task 只要碰到 barrier，就会触发自身进行快照。如上图所示，Checkpoint barrier n-1 处做的快照就是指 Job 从开始处理到 barrier n-1 所有的状态数据，barrier n 处做的快照就是指从 Job 开始到处理到 barrier n 所有的状态数据。对应到 PV 案例中就是，Source Task 接收到 JobManager 的编号为 chk-100 的 Checkpoint 触发请求后，发现自己恰好接收到 kafka offset（0，60000）处的数据，所以会往 offset（0，60000）数据之后 offset（0，60001）数据之前插入一个barrier，然后自己开始做快照，也就是将offset（0，60000）保存到状态后端 chk-100 中。然后，Source Task 会把 barrier 和我们要处理的数据一块往下游发送，当统计 PV 的 task 接收到 barrier 后，意味着 barrier 之前的数据已经被 PV task 处理完了，此时也会暂停处理 barrier 之后的数据，将自己内存中保存的 PV 信息（app1，50000）（app2，10000）保存到状态后端 chk-100 中。Flink 大概就是通过以上过程来保存快照的。 上述过程中，barrier 的作用就是为了把数据区分开，barrier 之前的数据是本次 Checkpoint 之前必须处理完的数据，barrier 之后的数据在本次 Checkpoint 之前不能被处理。Checkpoint 过程中有一个同步做快照的环节不能处理 barrier 之后的数据，为什么呢？如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据。结合案例来讲就是，PV task 在对（app1，50000）（app2，10000）做快照的同时，如果 barrier 之后的数据还在处理，可能会导致状态信息还没保存到磁盘，状态已经变成了（app1，50001）（app2，10001），导致我们最后快照里保存的 PV 值变成了（app1，50001）（app2，10001），这样如果从 Checkpoint 恢复任务时，我们从 offset 60000 开始消费，PV 值从 （app1，50001）（app2，10001） 开始累加，就会造成计算的 PV 结果偏高，结果不准确，就不能保证 Exactly Once。所以，Checkpoint 同步做快照的过程中，不能处理 barrier 之后的数据。Checkpoint 将快照信息写入到磁盘后，为了保证快照信息的高可用，需要将快照上传到 HDFS，这个上传快照到 HDFS 的过程是异步进行的，这个过程也可以处理 barrier 之后的数据，处理 barrier 之后的数据不会影响到磁盘上的快照信息。 从 PV 案例再分析 Flink 是如何做 Checkpoint 并从 Checkpoint 处恢复任务的，首先 JobManager 端会向所有 SourceTask 发送 Checkpoint，Source Task 会在数据流中安插 Checkpoint barrier，如下图所示。 Source Task 安插好 barrier 后，会将 barrier 跟数据一块发送给下游，然后自身开始做快照，并将快照信息 offset (0,60000) 发送到高可用的持久化存储介质，例如 HDFS 上，发送流程如下图所示。 下游的 PV task 接收到 barrier 后，也会做快照，并将快照信息 PV：(app1,50000) (app2,10000) 发送到 HDFS 上，如下图所示。 假设第 100 次 Checkpoint 完成后，一段时间后任务挂了，Flink 任务会自动从状态后端恢复任务。Source Task 去读取自己需要的状态信息 offset (0,60000) ，并从 offset 为 60000 的位置接着开始消费数据，PV task 也会去读取需要的状态信息 PV：(app1,50000) (app2,10000)，并在该状态值的基础上，往上累积计算 PV 值，流程如下图所示。 多并行度、多 Operator 情况下，Checkpoint 的过程上一节中讲述了单并行度情况下 Checkpoint 的过程，但是生产环境中，一般都是多并行度，而且算子也会比较多，这种情况下 Checkpoint 的过程就会变得复杂。分布式状态容错面临的问题与挑战： 如何确保状态拥有精确一次的容错保证？ 如何在分布式场景下替多个拥有本地状态的算子产生一个全域一致的快照？ 如何在不中断运算的前提下产生快照？ 多并行度、多 Operator 实例的情况下，如何做全域一致的快照？所有的 Operator 运行过程中接收到所有上游算子发送 barrier 后，对自身的状态进行一次快照，保存到相应状态后端，流程如下图所示。 当任务从状态恢复时，每个 Operator 从状态后端读取自己相应的状态信息，数据源会从状态中保存的位置开始重新消费，后续的其他算子也会基于 Checkpoint 中保存的状态进行计算，如下图所示。 整个 Checkpoint 的过程跟之前单并行度类似，图中有 4 个带状态的 Operator 实例，相应的状态后端就可以想象成 4 个格子。整个 Checkpoint 的过程可以当做 Operator 实例填自己格子的过程，Operator 实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单的认为一次完整的 Checkpoint 做完了。 上面只是快照的过程，Checkpoint 执行过程如下： 1、JobManager 端的 CheckPointCoordinator 向所有 Source Task 发送 CheckPointTrigger，Source Task会在数据流中安插 Checkpoint barrier 2、当 task 收到所有的 barrier 后，向自己的下游继续传递 barrier，然后自身执行快照，并将自己的状态异步写入到持久化存储中 增量 CheckPoint 只是把最新的一部分数据更新写入到外部存储 为了下游尽快开始做 CheckPoint，所以会先发送 barrier 到下游，自身再同步进行快照 3、当 task 对状态的快照信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的 CheckPointCoordinator 如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有 状态数据全部删除 4、CheckPointCoordinator 把整个 StateHandle 封装成 Completed Checkpoint Meta，写入到 HDFS，整个 Checkpoint 结束 barrier 对齐什么是 barrier 对齐？如图所示，当前的 Operator 实例接收上游两个流的数据，一个是字母流，一个是数字流。 当 Checkpoint 时，上游字母流和数字流都会往 Operator 实例发送 Checkpoint barrier，但是由于每个算子的执行速率不同，所以不可能保证上游两个流的 barrier 同时到达 Operator 实例，那图中的 Operator 实例到底什么时候进行快照呢？接收到任意一个 barrier 就可以开始进行快照了吗，还是接收到所有的 barrier 才能开始进行快照呢？答案是：当一个 Operator 实例有多个输入流时，Operator 实例需要在做快照之前进行 barrier 对齐，等待所有输入流的 barrier 都到达。barrier 对齐的详细过程如下所示： 1、对于一个有多个输入流的 Operator 实例，当 Operator 实例从其中一个输入流接收到 Checkpoint barrier n 时，就不能处理来自该流的任何数据记录了，直到它从其他所有输入流接收到 barrier n为止，否则 Operator 实例 Checkpoint n 的快照会混入快照 n 的记录和快照 n + 1 的记录。如上图中第 1 个小图所示，数字流的 barrier 先到达了。 2、接收到 barrier n 的流暂时被搁置，从这些流接收的记录不会被处理，而是放入输入缓冲区。图 2 中，我们可以看到虽然数字流对应的 barrier 已经到达了，但是barrier之后的 1、2、3 这些数据只能放到缓冲区中，等待字母流的barrier到达。 3、一旦最后所有输入流都接收到 barrier n，Operator 实例就会把 barrier 之前所有已经处理完成的数据和 barrier n 一块发送给下游。然后 Operator 实例就可以对状态信息进行快照。如图 3 所示，Operator 实例接收到上游所有流的 barrier n，此时 Operator 实例就可以将 barrier 和 barrier 之前的数据发送到下游，然后自身状态进行快照。 4、快照做完后，Operator 实例将继续处理缓冲区的记录，然后就可以处理输入流的数据。如图 4 所示，先处理完缓冲区数据，就可以正常处理输入流的数据了。 上面的过程就是 Flink 在 Operator 实例有多个输入流的情况下，整个 barrier 对齐的过程。那什么是 barrier 不对齐呢？barrier 不对齐是指当还有其他流的 barrier 还没到达时，为了提高 Operator 实例的处理性能，Operator 实例会直接处理 barrier 之后的数据，等到所有流的 barrier 都到达后，就可以对该 Operator 做 Checkpoint 快照了。对应到图中就是，barrier 不对齐时会直接把 barrier 之后的数据 1、2、3 直接处理掉，而不是放到缓冲区中等待其他的输入流的 barrier 到达，当所有输入流的 barrier 都到达后，才开始对 Operator 实例的状态信息进行快照，这样会导致做快照之前，Operator 实例已经处理了一些 barrier n 之后的数据。Checkpoint 的目的是为了保存快照信息，如果 barrier 不对齐，那么 Operator 实例在做第 n 次 Checkpoint 之前，已经处理了一些 barrier n 之后的数据，当程序从第 n 次 Checkpoint 恢复任务时，程序会从第 n 次 Checkpoint 保存的 offset 位置开始消费数据，就会导致一些数据被处理了两次，就出现了重复消费。如果进行 barrier 对齐，就不会出现这种重复消费的问题，所以 barrier 对齐就可以实现 Exactly Once，barrier 不对齐就变成了At Least Once。 再结合计算 PV 的案例来证明一下，为什么 barrier 对齐就可以实现 Exactly Once，barrier 不对齐就变成了 At Least Once。之前的案例为了简单，描述的 kafka topic 只有 1 个 partition，这里为了讲述 barrier 对齐，假设 topic 有 2 个 partittion，且计算的是我们平台的总 PV，也就是说不需要区分 app，每条一条数据，我们都需要将其 PV 值 +1 即可。如下图所示，Flink 应用程序有两个 Source Task，一个计算 PV 的 Task，这里计算 PV 的 Task 就出现了存在多个输入流的情况。 假设 barrier 不对齐，那么 Checkpoint 过程是怎么样呢？如下图所示： 如上图左部分所示，Source Subtask 0 和 Subtask 1 已经完成了快照操作，他们的状态信息为 offset(0,10000)(1,10005) 表示 partition0 消费到 offset 为 10000 的位置，partition 1 消费到 offset 为 10005 的位置。当 Source Subtask 1 的 barrier 到达 PV task 时，计算的 PV 结果为 20002，但 PV task 还没有接收到 Source Subtask 0 发送的 barrier，所以 PV task 还不能对自身状态信息进行快照。由于设置的 barrier 不对齐，所以此时 PV task 会继续处理 Source Subtask 0 和 Source Subtask 1 传来的数据。很快，如上图右部分所示，PV task 接收到 Source Subtask 0 发来的 barrier，但是 PV task 已经处理了 Source Subtask 1 barrier 之后的三条数据，所以 PV 值目前已经为 20008了，这里的 PV=20008 实际上已经处理到 partition 1 offset 为 10008 的位置，此时 PV task 会对自身的状态信息（PV = 20008）做快照，整体的快照信息为 offset(0,10000)(1,10005) PV=20008。 接着程序在继续运行，过了 10 秒，由于某个服务器故障，导致我们的 Operator 实例有一个挂了，所以 Flink 会从最近一次 Checkpoint 保存的状态恢复。那具体是怎么恢复的呢？Flink 同样会起三个 Operator 实例，我还称他们是 Source Subtask 0 、Source Subtask 1 和 PV task。三个 Operator 会从状态后端读取保存的状态信息。Source Subtask 0 会从 partition 0 offset 为 10000 的位置开始消费，Source Subtask 1 会从 partition 1 offset 为 10005 的位置开始消费，PV task 会基于 PV=20008 进行累加统计。然后就会发现的 PV 值 20008 实际上已经包含了 partition 1 的 offset 10005~10008 的数据，所以 partition 1 从 offset 10005 恢复任务时，partition1 的 offset 10005~10008 的数据被消费了两次，出现了重复消费的问题，所以 barrier 不对齐只能保证 At Least Once。 如果设置为 barrier 对齐，这里能保证 Exactly Once 吗？如下图所示，当 PV task 接收到 Source Subtask 1 的 barrier 后，并不会处理 Source Subtask 1 barrier 之后的数据，而是把这些数据放到 PV task 的输入缓冲区中，直到等到 Source Subtask 0 的 barrier 到达后，PV task 才会对自身状态信息进行快照，此时 PV task 会把 PV=20005 保存到快照信息中，整体的快照状态信息为 offset(0,10000)(1,10005) PV=20005，当任务从 Checkpoint 恢复时，Source Subtask 0 会从 partition 0 offset 为 10000 的位置开始消费，Source Subtask 1 会从 partition 1 offset 为 10005 的位置开始消费，PV task 会基于 PV=20005 进行累加统计，所以 barrier 对齐能保证 Flink 内部的 Exactly Once。在 Flink 应用程序中，当 Checkpoint 语义设置 Exactly Once 或 At Least Once 时，唯一的区别就是 barrier 对不对齐。当设置为 Exactly Once 时，就会 barrier 对齐，当设置为 At Least Once 时，就会 barrier 不对齐。 通过本案例，我们应该发现了 barrier 在 Flink 的 Checkpoint 中起着非常大的作用。barrier 告诉 Flink 应用程序，Checkpoint 之前哪些数据不应该被处理，barrier 对齐的过程其实就是为了防止 Flink 应用程序处理重复的数据。总结一下，满足哪些条件时，会出现 barrier 对齐？在代码中设置了 Flink 的 Checkpoint 语义是 Exactly Once，其次 Operator 实例必须有多个输入流才会出现 barrier 对齐。对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以必须有多个输入流才可能存在对齐。barrier 对齐就是上游多个流配合使得数据对齐的过程。言外之意：如果 Operator 实例只有一个输入流，就根本不存在 barrier 对齐，自己跟自己默认永远都是对齐的，所以当我们的应用程序从 Source 到 Sink 所有算子的并行度都是 1 的话，就算设置的 At Least Once，无形中也实现了 barrier 对齐，此时 Checkpoint 设置成 Exactly Once 和 At Least Once 一点区别都没有，都可以保证 Exactly Once。看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么 barrier 对齐就能保证 Exactly Once，为什么 barrier 不对齐就是 At Least Once。 barrier 对齐其实是要付出代价的，从 barrier 对齐的过程可以看出，PV task 明明可以更高效的处理数据，但因为 barrier 对齐，导致 Source Subtask 1 barrier 之后的数据被放到缓冲区中，暂时性地没有被处理，假如生产环境中，Source Subtask 0 的 barrier 迟迟没有到达，比 Source Subtask 1 延迟了 30 秒，那么这 30 秒期间，Source Subtask 1 barrier 之后的数据不能被处理，所以 PV task 相当于被闲置了。所以，当我们的一些业务场景对 Exactly Once 要求不高时，我们可以设置 Flink 的 Checkpoint 语义是 At Least Once 来小幅度的提高应用程序的执行效率。Flink Web UI 的 Checkpoint 选项卡中可以看到 barrier 对齐的耗时，如果发现耗时比较长，且对 Exactly Once 语义要求不高时，可以考虑使用该优化方案。 前面提到如何在不中断运算的前提下产生快照？在 Flink 的 Checkpoint 过程中，无论下游算子有没有做完快照，只要上游算子将 barrier 发送到下游且上游算子自身已经做完快照时，那么上游算子就可以处理 barrier 之后的数据了，从而使得整个系统 Checkpoint 的过程影响面尽量缩到最小，来提升系统整体的吞吐量。 在整个 Checkpoint 的过程中，还存在一个问题，假设我们设置的 10 分钟一次 Checkpoint。在第 n 次 Checkpoint 成功后，过了 9 分钟，任务突然挂了，我们需要从最近一次成功的 Checkpoint 处恢复任务，也就是从 9 分钟之前的状态恢复任务，就需要把这 9分钟的数据全部再消费一次，成本比较大。有的同学可能会想，那可以不可以设置为 100 ms就做一次 Checkpoint 呢？这样的话，当任务出现故障时，就不需要从 9 分钟前的状态进行恢复了，直接从 100 ms之前的状态恢复即可，恢复就会很快，不需要处理大量重复数据了。但是，这样做会导致应用程序频繁的访问状态后端，一般我们为了高可用，会把状态里的数据比如 offset：（0，60000）PV：（app1，50000）（app2，10000） 信息保存到 HDFS 中，如果频繁访问 HDFS，肯定会造成吞吐量下降，所以一般我们的 Checkpoint 时间间隔可以设置为分钟级别，例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，我们甚至可以设置为 5 分钟一次 Checkpoint，毕竟我们的应用程序挂的概率并不高，偶尔一次从 5 分钟前的状态恢复，我们是可以接受的。可以根据业务场景合理地调节 Checkpoint 的间隔时长，对于状态很小的 Job Checkpoint 会很快，我们可以调小时间间隔，对于状态比较大的 Job Checkpoint 会比较慢，我们可以调大 Checkpoint 时间间隔。 有的同学可能还有疑问，明明说好的 Exactly Once，但在 Checkpoint 成功后 10s 发生了故障，从最近一次成功的 Checkpoint 处恢复时，由于发生故障前的 10s Flink 也在处理数据，所以 Flink 应用程序肯定是把一些数据重复处理了呀。在面对任意故障时，不可能保证每个算子中用户定义的逻辑在每个事件中只执行一次，因为用户代码被部分执行的可能性是永远存在的。那么，当引擎声明 Exactly Once 处理语义时，它们能保证什么呢？如果不能保证用户逻辑只执行一次，那么哪些逻辑只执行一次？当引擎声明 Exactly Once 处理语义时，它们实际上是在说，它们可以保证引擎管理的状态更新只提交一次到持久的后端存储。换言之，无论以什么维度计算 PV、无论 Flink 应用程序发生多少次故障导致重启从 Checkpoint 恢复，Flink 都可以保证 PV 结果是准确的，不会因为各种任务重启而导致 PV 值计算偏高。 为了下游尽快做 Checkpoint，所以会先发送 barrier 到下游，自身再同步进行快照。这一步，如果向下发送barrier后，自己同步快照慢怎么办？下游已经同步好了，自己还没？可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游做快照更及时了，我只要保证下游把barrier之前的数据都处理了，并且不处理 barrier 之后的数据，然后做快照，那么下游也同样支持 Exactly Once。这个问题不要从全局思考，单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意，如果有一个Operator 的 Checkpoint 失败了或者因为 Checkpoint 超时也会导致失败，那么 JobManager 会认为整个 Checkpoint 失败。失败的 Checkpoint 是不能用来恢复任务的，必须所有的算子的 Checkpoint 都成功，那么这次 Checkpoint 才能认为是成功的，才能用来恢复任务。对应到 PV 案例就是，PV task 做快照速度较快，PV=20005 较早地写入到了 HDFS，但是 offset(0,10000)(1,10005) 过了几秒才写入到 HDFS，这种情况就算出现了，也不会影响计算结果，因为我们的快照信息是完全正确的。 再分享一个案例，Flink 的 Checkpoint 语义设置了 Exactly Once，程序中设置了 1 分钟 1 次 Checkpoint，5 秒向 MySQL 写一次数据，并commit。最后发现 MySQL 中数据重复了。为什么会重复呢？Flink要求端对端的 Exactly Once 都必须实现 TwoPhaseCommitSinkFunction。如果你的 Checkpoint 成功了，过了30秒突然程序挂了，由于 5 秒 commit 一次，所以在应用程序挂之前的 30 秒实际上已经写入了 6 批数据进入 MySQL。从 Checkpoint 处恢复时，之前提交的 6 批数据就会重复写入，所以出现了重复消费。Flink 的 Exactly Once 有两种情况，一个是我们本节所讲的 Flink 内部的 Exactly Once，一个是端对端的 Exactly Once。关于端对端如何保证 Exactly Once，我们在下一节中深入分析。 9.5.2 端对端如何保证 Exactly Once？Flink 与外部存储介质之间进行数据交互统称为端对端或 end to end 数据传输。上一节讲述了 Flink 内部如何保证 Exactly Once，这一节来分析端对端的 Exactly Once。正如上述 Flink 写 MySQL 的案例所示，在第 n 次 Checkpoint 结束后，第 n+1 次 Checkpoint 之前，如果 Flink 应用程序已经向外部的存储介质中成功写入并提交了一些数据后，Flink 应用程序由于某些原因挂了，导致任务从第 n 次 Checkpoint 处恢复。这种情况下，就会导致第 n 次 Checkpoint 结束后且任务失败之前往外部存储介质中写入的那一部分数据重复写入两次，可能会导致相同的数据在存储介质中存储了两份，从而端对端的一致性语义保证从 Exactly Once 退化为 At Least Once。这里只考虑了数据重复的情况，为什么不考虑丢数据的情况呢？在写数据时可以对异常进行捕获增加重试策略，如果重试多次还没有成功可以让 Flink 任务失败，Flink 任务就会从最近一次成功的 Checkpoint 处恢复，就不会出现丢数据的情况，所以我们本节内容主要用来解决数据重复的问题。 针对上述端对端 Exactly Once 的问题，我们可以使用以下方案来解决： 1、假如我们使用的存储介质支持按照全局主键去重，那么比较容易实现 Exactly Once，无论相同的数据往外部存储中写入了几次，外部存储都会进行去重，只保留一条数据。例如，app1 的 PV 值为 10，现在把 （key=app1，value=10） 往 Redis 中写入 10 次，只是说把 value 值覆盖了 10次，并不会导致结果错误，这种方案属于幂等性写入。 2、我们上述案例中为什么会导致重复写入数据到外部存储呢？是因为在下一次 Checkpoint 之前如果任务失败时，一些数据已经成功写入到了外部存储中，没办法删除那些数据。既然问题是这样，那可以想办法把 “向外部存储中提交数据” 与 “Checkpoint” 强关联，两次 Checkpoint 之间不允许向外部存储介质中提交数据，Checkpoint 的时候再向外部存储提交。如果提交成功，则 Checkpoint 成功，提交失败，则 Checkpoint 也失败。这样在下一次 Checkpoint 之前，如果任务失败，也没有重复数据被提交到外部存储。这里只是描述一下大概思想，好多细节这里并没有详细描述，会在下文中详细描述。基于上述思想，Flink 实现了 TwoPhaseCommitSinkFunction，它提取了两阶段提交协议的通用逻辑，使得通过 Flink 来构建端到端的Exactly Once 程序成为可能。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的 Exactly Once 语义。不过这种方案必须要求我们的输出端 (Sink 端) 必须支持事务。 下面我们通过两部分来详细介绍上述两种方案。 幂等性写入如何保证端对端的 Exactly Once实时 ETL 当 HBase 做为 Sink 端时，就是典型的应用场景。把日志中的主键做为 HBase 的 RowKey，就可以保证数据不重复，实现比较简单，这里不多赘述。 继续探讨实时计算各 app PV 的案例，将统计结果以普通键值对的形式保存到 Redis 中供业务方查询。到底如何实现，才能保证 Redis 中的结果是精准的呢？在之前 Strom 或 Spark Streaming 的方案中，将统计的 PV 结果保存在 Redis 中，每来一条数据，从 Redis 中获取相应 app 对应的 PV 值然后内存中进行 +1 后，再将 PV 值 put 到 Redis 中。例如：Redis 中保存 app1 的 PV 为 10，现在来了一条 app1 的日志，首先从 Redis 中获取 app1 的 PV 值=10，内存中 10+1=11，将 (app1,11) put 到 Redis 中，这里的 11 就是我们统计的 app1 的 PV 结果。可以将这种方案优化为 incr 或 incrby，直接对 Redis 中的 10 进行累加，不需要手动在内存中进行累加操作。当然 Flink 也可以用上述的这种方案来统计各 app 的 PV，但是上述方案并不能保证 Exactly Once，为什么呢？当第 n 次 Checkpoint 时，app1 的 PV 结果为 10000，第 n 次 Checkpoint 结束后运行了 10 秒，Redis 中 app1 的 PV 结果已经累加到了 10200。此时如果任务挂了，从第 n 次 Checkpoint 恢复任务时，会继续按照 Redis 中保存的 PV=10200 进行累加，但是正确的结果应该是从 PV=10000 开始累加。如果按照上面的方案统计 PV，就可能会出现统计值偏高的情况。这里也证实了一点：并不是说 Flink 程序的 Checkpoint 语义设置为 Exactly Once，就能保证我们的统计结果或者各种输出结果都能满足 Exactly Once。为了编写真正满足 Exactly Once 的代码，我们需要对 Flink 的 Checkpoint 原理做一些了解，编写对 Exactly Once 友好的代码。 那如何编写代码才能使得最后在 Redis 中保存的 PV 结果满足 Exactly Once 呢？上一节中，讲述了 Flink 内部状态可以保证 Exactly Once，这里可以将统计的 PV 结果保存在 Flink 内部的状态里，每次基于状态进行累加操作，并将累加到的结果 put 到 Redis 中，这样当任务从 Checkpoint 处恢复时，并不是基于 Redis 中实时统计的 PV 值进行累加，而是基于 Checkpoint 中保存的 PV 值进行累加，Checkpoint 中会保存每次 Checkpoint 时对应的 PV 快照信息，例如：第 n 次 Checkpoint 会把当时 pv=10000 保存到快照信息里，同时状态后端还保存着一份实时的状态信息用于实时累加。示例代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 1 分钟一次Checkpointenv.enableCheckpointing(TimeUnit.MINUTES.toMillis(1));CheckpointConfig checkpointConf = env.getCheckpointConfig();// Checkpoint 语义 EXACTLY ONCEcheckpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);checkpointConf.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);Properties props = new Properties();props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");props.put(ConsumerConfig.GROUP_ID_CONFIG, \"app-pv-stat\");DataStreamSource&lt;String&gt; appInfoSource = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( // kafka topic， String 序列化 \"app-topic\", new SimpleStringSchema(), props));// 按照 appId 进行 keyByappInfoSource.keyBy((KeySelector&lt;String, String&gt;) appId -&gt; appId) .map(new RichMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; private ValueState&lt;Long&gt; pvState; private long pv = 0; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); // 初始化状态 pvState = getRuntimeContext().getState( new ValueStateDescriptor&lt;&gt;(\"pvStat\", TypeInformation.of(new TypeHint&lt;Long&gt;() &#123;&#125;))); &#125; @Override public Tuple2&lt;String, Long&gt; map(String appId) throws Exception &#123; // 从状态中获取该 app 的 PV 值，+1后，update 到状态中 if(null == pvState.value())&#123; pv = 1; &#125; else &#123; pv = pvState.value(); pv += 1; &#125; pvState.update(pv); return new Tuple2&lt;&gt;(appId, pv); &#125; &#125;) .print();env.execute(\"Flink PV stat\"); 代码中设置 1 分钟一次 Checkpoint，Checkpoint 语义 EXACTLY ONCE，从 Kafka 中读取数据，这里为了简化代码，所以 Kafka 中读取的直接就是 String 类型的 appId，按照 appId KeyBy 后，执行 RichMapFunction，RichMapFunction 的 open 方法中会初始化 ValueState 类型的 pvState，pvState 就是上文一直强调的状态信息，每次 Checkpoint 的时候，会把 pvState 的状态信息快照一份到 HDFS 来提供恢复。这里按照 appId 进行 keyBy，所以每一个 appId 都会对应一个 pvState，pvState 里存储着该 appId 对应的 pv 值。每来一条数据都会执行一次 map 方法，当这条数据对应的 appId 是新 app 时，pvState 里就没有存储这个 appId 当前的 pv 值，将 pv 值赋值为 1，当 pvState 里存储的 value 不为 null 时，拿出 pv 值 +1后 update 到 pvState 里。map 方法再将 appId 和 pv 值发送到下游算子，下游直接调用了 print 进行输出，这里完全可以替换成相应的 RedisSink 或 HBaseSink。本案例中计算 pv 的工作交给了 Flink 内部的 ValueState，不依赖外部存储介质进行累加，外部介质承担的角色仅仅是提供数据给业务方查询，所以无论下游使用什么形式的 Sink，只要 Sink 端能够按照主键去重，该统计方案就可以保证 Exactly Once。本案例使用的 ValueState，关于 State 的详细使用请参阅第3.1节。 TwoPhaseCommitSinkFunction 如何保证端对端的 Exactly OnceFlink 的源码中有这么一段注释：This is a recommended base class for all of the {@link SinkFunction} that intend to implement exactly-once semantic。意思是对于打算实现 Exactly Once 语义的所有 SinkFunction 都推荐继承该抽象类。在介绍 TwoPhaseCommitSinkFunction 之前，先了解一下 2PC 分布式一致性协议。 在分布式系统中，每一个机器节点虽然都能明确地知道自己在进行事务操作过程中的结果是成功或失败，但无法直接获取到其他分布式节点的操作结果。因此，当一个事务操作需要跨越多个分布式节点的时候，为了让每个节点都能够获取到其他节点的事务执行状况，需要引入一个”协调者（Coordinator）”节点来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点被称为”参与者（Participant）”。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正的提交。普通的事务可以保证单个事务内所有操作要么全部成功，要么全部失败，而分布式系统中具体如何保证多台节点上执行的事务要么所有节点事务都成功，要么所有节点事务都失败呢？先了解一下 2PC 一致性协议。 2PC 是 Two-Phase Commit 的缩写，即两阶段提交。2PC 将分布式事务分为了两个阶段，分别是提交事务请求（投票）和执行事务提交。协调者会根据参与者在第一阶段的投票结果，来决定第二阶段是否真正的执行事务，具体流程如下。 提交事务请求（投票）阶段提交事务请求阶段如下所示： 协调者向所有参与者发送 prepare 请求与事务内容，询问是否可以准备事务提交，并等待参与者的响应 各参与者执行事务操作，并记录 Undo日志（用于回滚）和 Redo日志（用于重放），但不真正提交 参与者向协调者返回事务操作的执行结果，执行成功返回 Yes，否则返回 No 执行事务提交阶段分为成功与失败两种情况： 若第一阶段所有参与者都返回 Yes，说明事务可以提交 协调者向所有参与者发送 Commit 请求 参与者收到 Commit 请求后，会正式执行事务提交操作，并在提交完成后释放事务资源 完成事务提交后，向协调者发送 Ack 消息 协调者收到所有参与者的 Ack 消息，完成事务 事务提交成功的流程如下图所示： 若第一阶段有参与者返回 No 或者超时未返回，说明事务中断，需要回滚 协调者向所有参与者发送 Rollback 请求 参与者收到 Rollback 请求后，根据 Undo日志回滚到事务执行前的状态，释放占用的事务资源 参与者在完成事务回滚后，向协调者返回 Ack 协调者收到所有参与者的 Ack 消息，事务回滚完成 事务提交中断，需要回滚的流程如下图所示： 简单来讲，2PC 将一个事务的处理过程分为了投票和执行两个阶段，其核心是每个事务都采用先尝试后提交的处理方式。2PC 的优缺点如下所示： 优点：原理简单，实现方便 缺点： 协调者单点问题：协调者在整个 2PC 协议中非常重要，一旦协调者故障，则 2PC 将无法运转 过于保守：在 2PC 的阶段一，如果参与者出现故障而导致协调者无法获取到参与者的响应信息，这时协调者只能依靠自身的超时机制来判断是否需要中断事务，这种策略比较保守。换言之，2PC 没有涉及较为完善的容错机制，任意一个节点失败都会导致整个事务的失败 同步阻塞：执行过程是完全同步的，各个参与者在等待其他参与者投票响应的的过程中，将无法进行其他任何操作 数据不一致：在二阶段提交协议的阶段二，当协调者向所有的参与者发送 Commit 请求后，出现了局部网络异常或局部参与者机器故障等因素导致一部分的参与者执行了 Commit 操作，而发生故障的参与者没有执行 Commit，于是整个分布式系统便出现了数据不一致现象 Flink 的 TwoPhaseCommitSinkFunction 是基于 2PC 实现的。Flink 的 JobManager 对应到 2PC 中的协调者，Operator 实例对应到 2PC 中的参与者。TwoPhaseCommitSinkFunction 实现了 CheckpointedFunction 和 CheckpointListener 接口。CheckpointedFunction 接口中有\b两个方法 snapshotState 和 initializeState，snapshotState 方法会在 Checkpoint \b时且做快照之前被调用，initializeState 方法会在自定义 Function 初始化恢复状态时被调用。CheckpointListener 接口中有一个 notifyCheckpointComplete 方法，Operator 实例的 Checkpoint 成功后，会反馈给 JobManager，当 JobManager 接收到所有 Operator 实例 Checkpoint 成功的通知后，就认为本次 Checkpoint 成功了，会给所有 Operator 实例发送一个 Checkpoint 完成的\b通知，Operator 实例接收到通知后，就会调用 notifyCheckpointComplete 方法。 TwoPhaseCommitSinkFunction定义了如下 5 个抽象方法： 12345678910// 处理每一条数据protected abstract void invoke(TXN transaction, IN value, Context context) throws Exception;// 开始一个事务，返回事务信息的句柄protected abstract TXN beginTransaction() throws Exception;// 预提交（即提交请求）阶段的逻辑protected abstract void preCommit(TXN transaction) throws Exception;// 正式提交阶段的逻辑protected abstract void commit(TXN transaction);// 取消事务,Rollback 相关的逻辑protected abstract void abort(TXN transaction); 9.5.3 分析 FlinkKafkaConsumer 的设计思想kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡Source 端并行度改变了，如何来恢复 offset如何实现自动发现当前消费 topic 下新增的 partition9.5.4 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/uFEEYzJ","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何合理的设置 Flink 作业并行度？","date":"2021-08-09T16:00:00.000Z","path":"2021/08/10/flink-in-action-9.4/","text":"9.4 如何合理的设置 Flink 作业并行度？在 9.2 节中讲解了 Flink Job 中的执行计划，并详细分析了 Flink 中的 operator chain 在一起的各种条件，在 9.3 节中也通过真实生产环境的案例来分享并行度与 Slot 的概念与关系。相信大家也都有一定的理解，但是有时候生产环境如果 Job 突然消费不及时了，或者 Job 就根本不在消费数据了，那么该怎么办？首先得看下相关的监控查看 Job 是否在正常运行，是否出现反压的情况，是否这会生产数据量过大然而并行度却是根据之前数据量设置的，种种原因都需要一个个排查一下，然后找到根因才能够对应的去解决。这节来讲解下遇到这种问题后如何合理配置并行度呢？ 9.4.1 Source 端并行度的配置假设数据源端是 Kafka，在出现作业消费不及时的时候，首先看下 Kafka 的监控是不是现在生产者生产的数据上涨速度较快，从而导致作业目前的消费速度就是跟不上 Kafka 生产者的生产速度，如果是这样的话，那么就得查看作业的并行度和 Kafka 的分区数是否一致，如果小于 Kafka 的分区数，那么可以增大并行度至 Kafka 的分区数，然后再观察作业消费速度是否可以跟上数据生产速度；如果已经等于 Kafka 的分区数了，那得考虑下是否 Kafka 要扩大分区，但是这样可能会带来 Kafka 其他的问题，这个操作需要谨慎。 Kafka 中数据出现堆积的话，还可以分析下数据的类型，如果数据不重要，但是又要保证数据的及时性，可以修改作业让作业始终从最新的数据开始消费，丢弃之前堆积的数据，这样就可以保证数据的及时性。举个例子，假如一个实时告警作业它突然消费不及时，Kafka 中堆积了几亿条数据（数据延迟几个小时），那么如果作业调高并行度重启后，它还是从上一次提交的 offset 处开始消费的话，这样告警作业即使现在消费速度跟的上了，但是它要处理掉之前堆积的几亿条数据也是要一段时间的，那么就意味着这个作业仍将有段时间处于 ‘不可用’。因为即使判断出来要告警，可能这个告警信息的原数据已经是几个小时前的了，没准这个告警此时已经恢复了，但是还发出来告警这就意味着延迟性比较大，还会对告警消息接收者造成一定的干扰，所以这种场景下建议重启作业就直接开始从最新的数据开始消费。当然不同的场景可能不一样，如果金融行业的交易数据，那么是肯定不能允许这样丢数据的，即使堆积了，也要慢慢的去消费堆积的数据，直到后面追平至最新的数据。 在 Source 端设置并行度的话，如果数据源是 Kafka 的话，建议并行度不要超过 Kafka 的分区数，因为一个并行度会去处理一至多个分区的数据，如果设置过多的话，会出现部分并行度空闲。如果是其他的数据源，可以根据实际情况合理增大并行度来提高作业的处理数据能力。 9.4.2 中间 Operator 并行度的配置数据从 Source 端流入后，通常会进行一定的数据转换、聚合才能够满足需求，在数据转换中可能会和第三方系统进行交互，在交互的过程中可能会因为网络原因或者第三方服务原因导致有一定的延迟，从而导致这个数据交互的算子处理数据的吞吐量会降低，可能会造成反压，从而会影响上游的算子的消费。那么在这种情况下这些与第三方系统有交互的算子得稍微提高并行度，防止出现这种反压问题（当然反压问题不一定就这样可以解决，具体如何处理参见 9.1 节）。 除了这种与第三方服务做交互的外，另外可能的性能瓶颈也会出现在这类算子中，比如你 Kafka 过来的数据是 JSON 串的 String，然后需要转换成对象，在大数据量的情况下这个转换也是比较耗费性能的。 所以数据转换中间过程的算子也是非常重要的，如果哪一步算子的并行度设置的不合理，可能就会造成各种各样的问题出现。 9.4.3 Sink 端并行度的配置Sink 端是数据流向下游的地方，可以根据 Sink 端的数据量进行评估，可能有的作业是 Source 端的数据量最大，然后数据量不断的变少，最后到 Sink 端的数据就一点点了，比较常见的就是监控告警的场景。Source 端的数据是海量的，但是通过逐层的过滤和转换，到最后判断要告警的数据其实已经减少很多很多了，那么在最后的这个地方就可以将并行度设置的小一些。 当然也可能会有这样的情况，在 Source 端的数据量是最小的，拿到 Source 端流过来的数据后做了细粒度的拆分，那么数据量就不断的增加了，到 Sink 端的数据量就非常非常的大了。那么在 Sink 到下游的存储中间件的时候就需要提高并行度。 另外 Sink 端也是要与下游的服务进行交互，并行度还得根据下游的服务抗压能力来设置，如果在 Flink Sink 这端的数据量过大的话，然后在 Sink 处并行度也设置的很大，但是下游的服务完全撑不住这么大的并发写入，也是可能会造成下游服务直接被写挂的，下游服务可能还要对外提供一些其他的服务，如果稳定性不能保证的话，会造成很大的影响，所以最终还是要在 Sink 处的并行度做一定的权衡。 9.4.4 Operator Chain对于一般的作业（无特殊耗性能处），可以尽量让算子的并行度从 Source 端到 Sink 端都保持一致，这样可以尽可能的让 Job 中的算子进行 chain 在一起，形成链，数据在链中可以直接传输，而不需要再次进行序列化与反序列化，这样带来的性能消耗就会得到降低。在 9.2 节中具体讲解了算子 chain 在一起的条件，忘记的话可以去回顾一下。 9.4.5 小结与反思本节讲了作业执行过程中 Source 端、中间算子和 Sink 端的并行度设置的一些技巧。并行度修改后（增大或者减小）重启 Job，如果是减小并行度，之前原有的并行度的状态该怎么办；如果是新增并行度，如何确保和原来的并行度状态保持一致？ 加入知识星球可以看到上面文章：https://t.zsxq.com/uFEEYzJ","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Parallelism 和 Slot 深度理解","date":"2021-08-08T16:00:00.000Z","path":"2021/08/09/flink-in-action-9.3/","text":"9.3 Flink Parallelism 和 Slot 深度理解相信使用过 Flink 的你或多或少遇到过下面这个问题（笔者自己的项目曾经也出现过这样的问题），错误信息如下： 123Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;. 错误信息的完整截图如下图所示。 跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：FLINK-9056 issues，看到该 Issue 下面的评论说出现该问题的原因是因为 TaskManager 的 Slot 数量不足导致的 Job 提交失败，在 Flink 1.63 中已经修复了，变成抛出异常了，修复的代码如下图所示。 竟然知道了是因为 Slot 不足的原因了，那么我们就要先了解下 Slot 是什么呢？不过在了解 Slot 之前这里先介绍下 Parallelism。 9.3.1 Parallelism 简介Parallelism 翻译成中文是并行的意思，在 Flink 作业里面代表算子的并行度，适当的提高并行度可以大大提高 Job 的执行效率，比如你的 Job 消费 Kafka 数据过慢，适当调大可能就消费正常了。那么在 Flink 中怎么设置并行度呢？ 9.3.2 如何设置 Parallelism？在 Flink 配置文件中默认并行度是 1，你可以通过下面的命令查看到配置文件中的默认并行度： 1cat flink-conf.yaml | grep parallelism 结果如下图所示： 所以如果在你的 Flink Job 里面不设置任何 Parallelism 的话，那么它也会有一个默认的 Parallelism（默认为 1），那也意味着可以修改这个配置文件的默认并行度来提高 Job 的执行效率。如果是使用命令行启动你的 Flink Job，那么你也可以这样设置并行度(使用 -p n 参数)： 1./bin/flink run -p 10 /Users/zhisheng/word-count.jar 你也可以在作业中通过 env.setParallelism(n) 代码来设置整个作业程序的并行度。 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(10); 注意：这样设置的并行度是整个程序的并行度，那么后面如果每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是以这里设置的并行度为准了。如何给每个算子单独设置并行度呢？ 1234data.keyBy(new xxxKey()) .flatMap(new XxxFlatMapFunction()).setParallelism(5) .map(new XxxMapFunction).setParallelism(5) .addSink(new XxxSink()).setParallelism(1) 如上就是给每个算子单独设置并行度，这样的话，就算程序设置了 env.setParallelism(10) 也是会被覆盖的。这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度。 并行度讲到这里应该都懂了，下面就继续讲什么是 Slot？ 9.3.3 Slot 简介其实 Slot 的概念在 1.2 节中已经提及到，这里再细讲一点。Flink 的作业提交的架构流程如下图所示： 图中 TaskManager 是从 JobManager 处接收需要部署的 Task，任务能配置的最大并行度由 TaskManager 上可用的 Slot 决定。每个任务代表分配给任务槽的一组资源，Slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 Slot 中，这样就可以并行的执行程序。 例如，如果 TaskManager 有四个 Slot，那么它将为每个 Slot 分配 25％ 的内存。 可以在一个 Slot 中运行一个或多个线程。 同一 Slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。TaskManager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 Task 的 subtask，只要它们来自相同的 Job，这种共享模式可以大大的提高资源利用率。 如下图所示，有两个 TaskManager，每个 TaskManager 有三个 Slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 Slot 里面可以执行 1 至多个子任务。那么再看下图，source/map/keyby/window/apply 算子最大可以设置 6 个并行度，sink 只设置了 1 个并行度。 每个 Flink TaskManager 在集群中提供 Slot，Slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例（一般情况下 Slot 个数是每个 TaskManager 的 CPU 核数）。Flink 配置文件中设置的一个 TaskManager 默认的 Slot 是 1，配置如下图所示。 taskmanager.numberOfTaskSlots: 1 该参数可以根据实际情况做一定的修改。 9.3.4 Slot 和 Parallelism 的关系9.3.5 可能会遇到 Slot 和 Parallelism 的问题9.3.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/uFEEYzJ","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何查看 Flink 作业执行计划？","date":"2021-08-07T16:00:00.000Z","path":"2021/08/08/flink-in-action-9.2/","text":"9.2 如何查看 Flink 作业执行计划？当一个应用程序需求比较简单的情况下，数据转换涉及的 operator（算子）可能不多，但是当应用的需求变得越来越复杂时，可能在一个 Job 里面算子的个数会达到几十个、甚至上百个，在如此多算子的情况下，整个应用程序就会变得非常复杂，所以在编写 Flink Job 的时候要是能够随时知道 Job 的执行计划那就很方便了。 刚好，Flink 是支持可以获取到整个 Job 的执行计划的，另外 Flink 官网还提供了一个可视化工具 visualizer（可以将执行计划 JSON 绘制出执行图），如下图所示。 9.2.1 如何获取执行计划 JSON？既然知道了将执行计划 JSON 绘制出可查看的执行图的工具，那么该如何获取执行计划 JSON 呢？方法很简单，你只需要在你的 Flink Job 的 Main 方法 里面加上这么一行代码： 1System.out.println(env.getExecutionPlan()); 然后就可以在 IDEA 中右键 Run 一下你的 Flink Job，从打印的日志里面可以查看到执行计划的 JSON 串，例如下面这种： 1&#123;\"nodes\":[&#123;\"id\":1,\"type\":\"Source: Custom Source\",\"pact\":\"Data Source\",\"contents\":\"Source: Custom Source\",\"parallelism\":5&#125;,&#123;\"id\":2,\"type\":\"Sink: flink-connectors-kafka\",\"pact\":\"Data Sink\",\"contents\":\"Sink: flink-connectors-kafka\",\"parallelism\":5,\"predecessors\":[&#123;\"id\":1,\"ship_strategy\":\"FORWARD\",\"side\":\"second\"&#125;]&#125;]&#125; IDEA 中运行打印出来的执行计划的 JSON 串如下图所示： 9.2.2 生成执行计划图获取到执行计划 JSON 了，那么利用 Flink 自带的工具来绘出执行计划图，将获得到的 JSON 串复制粘贴到刚才那网址去，如下图所示。 点击上图的 Draw 按钮，就会生成如下图所示的执行流程图了。 从图中我们可以看到哪些内容呢？ operator name（算子）：比如 source、sink 每个 operator 的并行度：比如 Parallelism: 5 数据下发的类型：比如 FORWARD 你还可以点击下图中的 Data Source(ID = 1) 查看具体详情，如下图所示： 随着需求的不段增加，可能算子的个数会增加，所以执行计划也会变得更为复杂，如下图所示。 看到上图是不是觉得就有点很复杂了，笔者相信可能你自己的业务需求比这还会复杂得更多，不过从这图我们可以看到比上面那个简单的执行计划图多了一种数据下发类型就是 HASH。但是大家可能会好奇的说：为什么我平时从 Flink UI 上查看到的 Job ”执行计划图“ 却不是这样子的呀？ 这里我们复现一下这个问题，我们把这个稍微复杂的 Flink Job 提交到 Flink UI 上去查看一下到底它在 UI 上的执行计划图是个什么样子？我们提交 Jar 包后不运行，直接点击 show plan 的结果如下图所示。 我们再运行一下，查看运行的时候的展示的 “执行计划图” 又是不一样的，如下图所示。 9.2.3 深入探究 Flink 作业执行计划我们可以发现这两个 “执行计划图” 都和在 Flink 官网提供的 visualizer 工具生成的执行计划图是不一样的。粗略观察可以发现：在 Flink UI 上面的 “执行计划图” 变得更加简洁了，有些算子合在一起了，所以整体看起来就没这么复杂了。其实，这是 Flink 内部做的一个优化。我们先来看下 env.getExecutionPlan() 这段代码它背后的逻辑： 1234567891011/** * Creates the plan with which the system will execute the program, and * returns it as a String using a JSON representation of the execution data * flow graph. Note that this needs to be called, before the plan is * executed. * * @return The execution plan of the program, as a JSON String. */public String getExecutionPlan() &#123; return getStreamGraph().getStreamingPlanAsJSON();&#125; 代码注释的大概意思是： 创建程序执行计划，并将执行数据流图的 JSON 作为 String 返回，请注意，在执行计划之前需要调用此方法。 这个 getExecutionPlan 方法有两步操作： 1、获取到 Job 的 StreamGraph 关于如何获取到 StreamGraph，笔者在博客里面写了篇源码解析 如何获取 StreamGraph？ 。 2、将 StreamGraph 转换成 JSON 12345678public String getStreamingPlanAsJSON() &#123; try &#123; return new JSONGenerator(this).getJSON(); &#125; catch (Exception e) &#123; throw new RuntimeException(\"JSON plan creation failed\", e); &#125;&#125; 跟进 getStreamingPlanAsJSON 方法看见它构造了一个 JSONGenerator 对象（含参 StreamGraph），然后调用 getJSON 方法，我们来看下这个方法： 1234567891011121314151617public String getJSON() &#123; ObjectNode json = mapper.createObjectNode(); ArrayNode nodes = mapper.createArrayNode(); json.put(\"nodes\", nodes); List&lt;Integer&gt; operatorIDs = new ArrayList&lt;Integer&gt;(streamGraph.getVertexIDs()); Collections.sort(operatorIDs, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer idOne, Integer idTwo) &#123; boolean isIdOneSinkId = streamGraph.getSinkIDs().contains(idOne); boolean isIdTwoSinkId = streamGraph.getSinkIDs().contains(idTwo); // put sinks at the back ... &#125; &#125;); visit(nodes, operatorIDs, new HashMap&lt;Integer, Integer&gt;()); return json.toString();&#125; 一开始构造外部的对象，然后调用 visit 方法继续构造内部的对象，visit 方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private void visit(ArrayNode jsonArray, List&lt;Integer&gt; toVisit, Map&lt;Integer, Integer&gt; edgeRemapings) &#123; Integer vertexID = toVisit.get(0); StreamNode vertex = streamGraph.getStreamNode(vertexID); if (streamGraph.getSourceIDs().contains(vertexID) || Collections.disjoint(vertex.getInEdges(), toVisit)) &#123; ObjectNode node = mapper.createObjectNode(); decorateNode(vertexID, node); if (!streamGraph.getSourceIDs().contains(vertexID)) &#123; ArrayNode inputs = mapper.createArrayNode(); node.put(PREDECESSORS, inputs); for (StreamEdge inEdge : vertex.getInEdges()) &#123; int inputID = inEdge.getSourceId(); Integer mappedID = (edgeRemapings.keySet().contains(inputID)) ? edgeRemapings .get(inputID) : inputID; decorateEdge(inputs, inEdge, mappedID); &#125; &#125; jsonArray.add(node); toVisit.remove(vertexID); &#125; else &#123; Integer iterationHead = -1; for (StreamEdge inEdge : vertex.getInEdges()) &#123; int operator = inEdge.getSourceId(); if (streamGraph.vertexIDtoLoopTimeout.containsKey(operator)) &#123; iterationHead = operator; &#125; &#125; ObjectNode obj = mapper.createObjectNode(); ArrayNode iterationSteps = mapper.createArrayNode(); obj.put(STEPS, iterationSteps); obj.put(ID, iterationHead); obj.put(PACT, \"IterativeDataStream\"); obj.put(PARALLELISM, streamGraph.getStreamNode(iterationHead).getParallelism()); obj.put(CONTENTS, \"Stream Iteration\"); ArrayNode iterationInputs = mapper.createArrayNode(); obj.put(PREDECESSORS, iterationInputs); toVisit.remove(iterationHead); visitIteration(iterationSteps, toVisit, iterationHead, edgeRemapings, iterationInputs); jsonArray.add(obj); &#125; if (!toVisit.isEmpty()) &#123; visit(jsonArray, toVisit, edgeRemapings); &#125;&#125; 最后就将这个 StreamGraph 构造成一个 JSON 串返回出去，所以其实这里返回的执行计划图就是 Flink Job 的 StreamGraph，然而我们在 Flink UI 上面看到的 “执行计划图” 是对应 Flink 中的 JobGraph，同样，笔者在博客里面也写了篇源码解析的文章 源码解析——如何获取 JobGraph？。 9.2.4 Flink 中算子链接（chain）起来的条件Flink 在内部会将多个算子串在一起作为一个 operator chain（执行链）来执行，每个执行链会在 TaskManager 上的一个独立线程中执行，这样不仅可以减少线程的数量及线程切换带来的资源消耗，还能降低数据在算子之间传输序列化与反序列化带来的消耗。 举个例子，拿一个 Flink Job （算子的并行度都设置为 5）生成的 StreamGraph JSON 渲染出来的执行流程图如下图所示。 提交到 Flink UI 上的 JobGraph 如下图所示。 可以看到 Flink 它内部将三个算子（source、filter、sink）都串成在一个执行链里。但是我们修改一下 filter 这个算子的并行度为 4，我们再次提交到 Flink UI 上运行，效果如下图所示。 你会发现它竟然拆分成三个了，我们继续将 sink 的并行度也修改成 4，继续打包运行后的效果如下图所示。 神奇不，它变成了 2 个了，将 filter 和 sink 算子串在一起了执行了。经过简单的测试，我们可以发现其实如果想要把两个不一样的算子串在一起执行确实还不是那么简单的，的确，它背后的条件可是比较复杂的，这里笔者给出源码出来，感兴趣的可以独自阅读下源码。 1234567891011121314151617181920public static boolean isChainable(StreamEdge edge, StreamGraph streamGraph) &#123; //获取StreamEdge的源和目标StreamNode StreamNode upStreamVertex = edge.getSourceVertex(); StreamNode downStreamVertex = edge.getTargetVertex(); //获取源和目标StreamNode中的StreamOperator StreamOperator&lt;?&gt; headOperator = upStreamVertex.getOperator(); StreamOperator&lt;?&gt; outOperator = downStreamVertex.getOperator(); return downStreamVertex.getInEdges().size() == 1 &amp;&amp; outOperator != null &amp;&amp; headOperator != null &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex) &amp;&amp; outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS &amp;&amp; (headOperator.getChainingStrategy() == ChainingStrategy.HEAD || headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS) &amp;&amp; (edge.getPartitioner() instanceof ForwardPartitioner) &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism() &amp;&amp; streamGraph.isChainingEnabled();&#125; 从源码最后的 return 可以看出它有九个条件： … 所以看到上面的这九个条件，你是不是在想如果我们代码能够合理的写好，那么就有可能会将不同的算子串在一个执行链中，这样也就可以提高代码的执行效率了。 9.2.5 如何禁止 Operator chain？加入知识星球可以看到上面文章：https://t.zsxq.com/uFEEYzJ 9.2.6 小结与反思本节内容从查看作业的执行计划来分析作业的执行情况，接着分析了作业算子 chain 起来的条件，并通过程序演示来验证，最后讲解了如何禁止算子 chain 起来。 本节代码地址：chain Job visualizer 工具 源码解析——如何获取 StreamGraph","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何处理 Flink Job BackPressure （反压）问题?","date":"2021-08-06T16:00:00.000Z","path":"2021/08/07/flink-in-action-9.1/","text":"第九章 —— Flink 性能调优通过第八章的监控图表信息，我们可以发现问题，在发现问题后，需要去分析为什么会发生这些问题以及我们该如何去解决这些问题。本章将会介绍很多 Flink 生产环境遇到的问题，比如作业出现反压、作业并行度配置不合理、作业数据倾斜等，除了引出这种常见问题之外，笔者还将和你一起去分析这种问题造成的原因以及如何去优化作业。比如合理的配置并行度、让作业算子尽可能的 chain 在一起已达到最优等。希望通过本章的内容，你可以将这些解决方法运用在你的公司，帮助公司解决类似的问题。 9.1 如何处理 Flink Job BackPressure （反压）问题?反压（BackPressure）机制被广泛应用到实时流处理系统中，流处理系统需要能优雅地处理反压问题。反压通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。反压机制是指系统能够自己检测到被阻塞的 Operator，然后自适应地降低源头或上游数据的发送速率，从而维持整个系统的稳定。Flink 任务一般运行在多个节点上，数据从上游算子发送到下游算子需要网络传输，若系统在反压时想要降低数据源头或上游算子数据的发送速率，那么肯定也需要网络传输。所以下面先来了解一下 Flink 的网络流控（Flink 对网络数据流量的控制）机制。 9.1.1 Flink 流处理为什么需要网络流控下图是一个简单的 Flink 流任务执行图：任务首先从 Kafka 中读取数据、通过 map 算子对数据进行转换、keyBy 按照指定 key 对数据进行分区（key 相同的数据经过 keyBy 后分到同一个 subtask 实例中），keyBy 后对数据进行 map 转换，然后使用 Sink 将数据输出到外部存储。 众所周知，在大数据处理中，无论是批处理还是流处理，单点处理的性能总是有限的，我们的单个 Job 一般会运行在多个节点上，通过多个节点共同配合来提升整个系统的处理性能。图中，任务被切分成 4 个可独立执行的 subtask 分别是 A0、A1、B0、B1，在数据处理过程中就会存在 shuffle。例如，subtask A0 处理完的数据经过 keyBy 后被发送到 subtask B0、B1 所在节点去处理。那么问题来了，subtask A0 应该以多快的速度向 subtask B0、B1 发送数据呢？把上述问题抽象化，如下图所示，将 subtask A0 当作 Producer，subtask B0 当做 Consumer，上游 Producer 向下游 Consumer 发送数据，在发送端和接收端有相应的 Send Buffer 和 Receive Buffer，但是上游 Producer 生产数据的速率比下游 Consumer 消费数据的速率大，Producer 生产数据的速率为 2MB/s， Consumer 消费数据速率为 1MB/s，Receive Buffer 容量只有 5MB，所以过了 5 秒后，接收端的 Receive Buffer 满了。 下游消费速率慢，且接收区的 Receive Buffer 有限，如果上游一直有源源不断的数据，那么将会面临着以下两种情况： 下游消费者的缓冲区放不下数据，导致下游消费者会丢弃新到达的数据 为了不丢弃数据，所以下游消费者的 Receive Buffer 持续扩张，最后耗尽消费者的内存，导致 OOM 程序挂掉 常识告诉我们，这两种情况在生产环境下都是不能接受的，第一种会丢数据、第二种会把应用程序挂掉。所以，该问题的解决方案不应该是下游 Receive Buffer 一直累积数据，而是上游 Producer 发现下游 Consumer 消费比较慢的时候，应该在 Producer 端做出限流的策略，防止在下游 Consumer 端无限制地堆积数据。那上游 Producer 端该如何做限流呢？可以采用如下图所示的静态限流策略： 静态限速的思想就是，提前已知下游 Consumer 端的消费速率，然后在上游 Producer 端使用类似令牌桶的思想，限制 Producer 端生产数据的速率，从而控制上游 Producer 端向下游 Consumer 端发送数据的速率。但是静态限速会存在问题： 通常无法事先预估下游 Consumer 端能承受的最大速率 就算通过某种方式预估出下游 Consumer 端能承受的最大速率，在运行过程中也可能会因为网络抖动、 CPU 共享竞争、内存紧张、IO阻塞等原因造成下游 Consumer 的吞吐量降低，但是上游 Producer 的吞吐量正常，然后又会出现之前所说的下游接收区的 Receive Buffer 有限，上游一直有源源不断的数据发送到下游的问题，还是会造成下游要么丢数据，要么为了不丢数据 buffer 不断扩充导致下游 OOM 的问题 综上所述，我们发现了，上游 Producer 端必须有一个限流的策略，且静态限流是不可靠的，于是就需要一个动态限流的策略。可以采用如下图所示的动态反馈策略： 下游 Consumer 端会频繁地向上游 Producer 端进行动态反馈，告诉 Producer 下游 Consumer 的负载能力，从而使 Producer 端可以动态调整向下游 Consumer 发送数据的速率，以实现 Producer 端的动态限流。当 Consumer 端处理较慢时，Consumer 将负载反馈到 Producer 端，Producer 端会根据反馈适当降低 Producer 自身从上游或者 Source 端读数据的速率来降低向下游 Consumer 发送数据的速率。当 Consumer 处理负载能力提升后，又及时向 Producer 端反馈，Producer 会通过提升自身从上游或 Source 端读数据的速率来提升向下游发送数据的速率，通过动态反馈的策略来动态调整系统整体的吞吐量。 读到这里，应该知道 Flink 为什么需要网络流控机制了，并且知道 Flink 的网络流控机制必须是一个动态反馈的策略。但是还有以下几个问题： Flink 中数据具体是怎么从上游 Producer 端发送到下游 Consumer 端的？ Flink 的动态限流具体是怎么实现的？下游的负载能力和压力是如何传递给上游的？ 带着这两个问题，学习下面的 Flink 网络流控与反压机制。 9.1.2 Flink 1.5 之前网络流控机制介绍在 Flink 1.5 之前，Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！Flink 利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。 如下图所示，Job 分为 Task A、B、C，Task A 是 Source Task、Task B 处理转换数据、Task C 是 Sink Task。Task A 从外部 Source 端读取到数据后将数据序列化放到 Send Buffer 中，再由 Task A 的 Send Buffer 发送到 Task B 的 Receive Buffer，Task B 的算子从 Task B 的 Receive Buffer 中将数据反序列后进行处理，将处理后数据序列化放到 Task B 的 Send Buffer 中，再由 Task B 的 Send Buffer 发送到 Task C 的 Receive Buffer，Task C 再从 Task C 的 Receive Buffer 中将数据反序列后输出到外部 Sink 端，这就是所有数据的传输和处理流程。 Flink 中，动态反馈策略原理比较简单，假如 Task C 由于各种原因吞吐量急剧降低，那么肯定会造成 Task C 的 Receive Buffer 中堆积大量数据，此时 Task B 还在给 Task C 发送数据，但是毕竟内存是有限的，持续一段时间后 Task C 的 Receive Buffer 满了，此时 Task B 发现 Task C 的 Receive Buffer 满了后，就不会再往 Task C 发送数据了，Task B 处理完的数据就开始往 Task B 的 Send Buffer 积压，一段时间后 Task B 的 Send Buffer 也满了，Task B 的处理就会被阻塞，这时 Task A 还在往 Task B 的 Receive Buffer 发送数据。同样的道理，Task B 的 Receive Buffer 很快满了，导致 Task A 不再往 Task B 发送数据，Task A 的 Send Buffer 也会被用完，Task A 是 Source Task 没有上游，所以 Task A 直接降低从外部 Source 端读取数据的速率甚至完全停止读取数据。通过以上原理，Flink 将下游的压力传递给上游。如果下游 Task C 的负载能力恢复后，如何将负载提升的信息反馈给上游呢？实际上 Task B 会一直向 Task C 发送探测信号，检测 Task C 的 Receive Buffer 是否有足够的空间，当 Task C 的负载能力恢复后，Task C 会优先消费 Task C Receive Buffer 中的数据，Task C Receive Buffer 中有足够的空间时，Task B 会从 Send Buffer 继续发送数据到 Task C 的 Receive Buffer，Task B 的 Send Buffer 有足够空间后，Task B 又开始正常处理数据，很快 Task B 的 Receive Buffer 中也会有足够空间，同理，Task A 会从 Send Buffer 继续发送数据到 Task B 的 Receive Buffer，Task A 的 Receive Buffer 有足够空间后，Task A 就可以从外部的 Source 端开始正常读取数据了。通过以上原理，Flink 将下游负载过低的消息传递给上游。所以说 Flink 利用自身纯数据流引擎的优势优雅地响应反压问题，并没有任何复杂的机制来解决反压。上述流程，就是 Flink 动态限流（反压机制）的简单描述，可以看到 Flink 的反压是从下游往上游传播的，一直往上传播到 Source Task 后，Source Task 最终会降低或提升从外部 Source 端读取数据的速率。 如下图所示，对于一个 Flink 任务，动态反馈要考虑如下两种情况： 1.跨 Task，动态反馈具体如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer 当下游 Task C 的 Receive Buffer 满了，如何告诉上游 Task B 应该降低数据发送速率 当下游 Task C 的 Receive Buffer 空了，如何告诉上游 Task B 应该提升数据发送速率 注：这里又分了两种情况，Task B 和 Task C 可能在同一个 TaskManager 上运行，也有可能不在同一个 TaskManager 上运行 Task B 和 Task C 在同一个 TaskManager 运行指的是：一个 TaskManager 包含了多个 Slot，Task B 和 Task C 都运行在这个 TaskManager 上。此时 Task B 给 Task C 发送数据实际上是同一个 JVM 内的数据发送，所以不存在网络通信 Task B 和 Task C 不在同一个 TaskManager 运行指的是：Task B 和 Task C 运行在不同的 TaskManager 中。此时 Task B 给 Task C 发送数据是跨节点的，所以会存在网络通信 2.Task 内，动态反馈如何从内部的 Send Buffer 反馈给内部的 Receive Buffer 当 Task B 的 Send Buffer 满了，如何告诉 Task B 内部的 Receive Buffer，自身的 Send Buffer 已经满了？要让 Task B 的 Receive Buffer 感受到压力，才能把下游的压力传递到 Task A 当 Task B 的 Send Buffer 空了，如何告诉 Task B 内部的 Receive Buffer 下游 Send Buffer 空了，并把下游负载很低的消息传递给 Task A 到目前为止，动态反馈的具体细节抽象成了三个问题： 跨 Task 且 Task 不在同一个 TaskManager 内，动态反馈具体如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer 跨 Task 且 Task 在同一个 TaskManager 内，动态反馈具体如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer Task 内，动态反馈具体如何从 Task 内部的 Send Buffer 反馈给内部的 Receive Buffer TaskManager 之间网络传输相关组件TaskManager 之间数据传输流向如下图所示，可以看到 Source Task 给 Task B 发送数据，Source Task 做为 Producer，Task B 做为 Consumer，Producer 端产生的数据最后通过网络发送给 Consumer 端。Producer 端 Operator 实例对一条条的数据进行处理，处理完的数据首先缓存到 ResultPartition 内的 ResultSubPartition 中。ResultSubPartition 中一个 Buffer 写满或者超时后，就会触发将 ResultSubPartition 中的数据拷贝到 Producer 端 Netty 的 Buffer 中，之后又把数据拷贝到 Socket 的 Send Buffer 中，这里有一个从用户态拷贝到内核态的过程，最后通过 Socket 发送网络请求，把 Send Buffer 中的数据发送到 Consumer 端的 Receive Buffer。数据到达 Consumer 端后，再依次从 Socket 的 Receive Buffer 拷贝到 Netty 的 Buffer，再拷贝到 Consumer Operator InputGate 内的 InputChannel 中，最后 Consumer Operator 就可以读到数据进行处理了。这就是两个 TaskManager 之间的数据传输过程，我们可以看到发送方和接收方各有三层的 Buffer。当 Task B 往下游发送数据时，整个流程与 Source Task 给 Task B 发送数据的流程类似。 根据上述流程，下表中对 Flink 通信相关的一些术语进行介绍： 概念/术语 解释 ResultPartition 生产者生产的数据首先写入到 ResultPartition 中，一个 Operator 实例对应一个ResultPartition。 ResultSubpartition 一个 ResultPartition 是由多个 ResultSubpartition 组成。当 Producer Operator 实例生产的数据要发送给下游 Consumer Operator n 个实例时，那么该 Producer Operator 实例对应的 ResultPartition 中就包含 n 个 ResultSubpartition。 InputGate 消费者消费的数据来自于 InputGate 中，一个 Operator 实例对应一个InputGate。网络中传输的数据会写入到 Task 的 InputGate。 InputChannel 一个 InputGate 是由多个 InputChannel 组成。当 Consumer Operator 实例读取的数据来自于上游 Producer Operator n 个实例时，那么该 Consumer Operator 实例对应的 InputGate 中就包含 n 个 InputChannel。 RecordReader 用于将记录从Buffer中读出。 RecordWriter 用于将记录写入Buffer。 LocalBufferPool 为 ResultPartition 或 InputGate 分配内存，每一个 ResultPartition 或 InputGate分别对应一个 LocalBufferPool。 NetworkBufferPool 为 LocalBufferPool 分配内存，NetworkBufferPool 是 Task 之间共享的，每个 TaskManager 只会实例化一个。 InputGate 和 ResultPartition 的内存是如何申请的呢？如下图所示，了解一下 Flink 网络传输相关的内存管理。在 TaskManager 初始化时，Flink 会在 NetworkBufferPool 中生成一定数量的内存块 MemorySegment，内存块的总数量就代表了网络传输中所有可用的内存。 NetworkBufferPool 是 Task 之间共享的，每个 TaskManager 只会实例化一个。Task 线程启动时，会为 Task 的 InputChannel 和 ResultSubPartition 分别创建一个 LocalBufferPool。InputGate 或 ResultPartition 需要写入数据时，会向相对应的 LocalBufferPool 申请内存（图中①），当 LocalBufferPool 没有足够的内存且还没到达 LocalBufferPool 设置的上限时，就会向 NetworkBufferPool 申请内存（图中②），并将内存分配给相应的 InputChannel 或 ResultSubPartition （图③④）。虽然可以申请，但是必须明白内存申请肯定是有限制的，不可能无限制的申请，我们在启动任务时可以指定该任务最多可能申请多大的内存空间用于 NetworkBufferPool。当 InputChannel 的内存块被 Operator 读取消费掉或 ResultSubPartition 的内存块已经被写入到了 Netty 中，那么 InputChannel 和 ResultSubPartition 中的内存块就可以还给 LocalBufferPool 了（图中⑤），如果 LocalBufferPool 中有较多空闲的内存块，就会还给 NetworkBufferPool （图中⑥）。 了解了 Flink 网络传输相关的内存管理，我们来分析 3 种动态反馈的具体细节。 跨 Task 且 Task 不在同一个 TaskManager 内时，反压如何向上游传播跨 Task 且 Task 在同一个 TaskManager 内，反压如何向上游传播Task 内部，反压如何向上游传播9.1.3 基于 Credit 的反压机制Flink 1.5 之前反压机制存在的问题基于 Credit 的反压机制原理9.1.4 Flink 如何定位产生反压的位置？Flink 反压监控原理介绍利用 Flink Web UI 定位产生反压的位置利用 Flink Metrics 定位产生反压的位置9.1.5 定位到反压来源后，该如何处理？系统资源垃圾收集（GC）CPU/线程瓶颈线程竞争负载不平衡外部依赖9.1.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/f66iAMz","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何搭建一套 Flink 监控系统?","date":"2021-08-05T16:00:00.000Z","path":"2021/08/06/flink-in-action-8.2/","text":"8.2 搭建一套 Flink 监控系统8.1 节中讲解了 JobManager、TaskManager 和 Flink Job 的监控，以及需要关注的监控指标有哪些。本节带大家讲解一下如何搭建一套完整的 Flink 监控系统，如果你所在的公司没有专门的监控平台，那么可以根据本节的内容来为公司搭建一套属于自己公司的 Flink 监控系统。 8.2.1 利用 API 获取监控数据熟悉 Flink 的朋友都知道 Flink 的 UI 上面已经详细地展示了很多监控指标的数据，并且这些指标还是比较重要的，所以如果不想搭建额外的监控系统，那么直接利用 Flink 自身的 UI 就可以获取到很多重要的监控信息。这里要讲的是这些监控信息其实也是通过 Flink 自身的 Rest API 来获取数据的，所以其实要搭建一个粗糙的监控平台，也是可以直接利用现有的接口定时去获取数据，然后将这些指标的数据存储在某种时序数据库中，最后用些可视化图表做个展示，这样一个完整的监控系统就做出来了。 这里通过 Chrome 浏览器的控制台来查看一下有哪些 REST API 是用来提供监控数据的。 1.在 Chrome 浏览器中打开 http://localhost:8081/overview 页面，可以获取到整个 Flink 集群的资源信息：TaskManager 个数（TaskManagers）、Slot 总个数（Total Task Slots）、可用 Slot 个数（Available Task Slots）、Job 运行个数（Running Jobs）、Job 运行状态（Finished 0 Canceled 0 Failed 0）等，如下图所示。 2.通过 http://localhost:8081/taskmanagers 页面查看 TaskManager 列表，可以知道该集群下所有 TaskManager 的信息（数据端口号（Data Port）、上一次心跳时间（Last Heartbeat）、总共的 Slot 个数（All Slots）、空闲的 Slot 个数（Free Slots）、以及 CPU 和内存的分配使用情况，如下图所示。 3.通过 http://localhost:8081/taskmanagers/tm_id 页面查看 TaskManager 的具体情况（这里的 tm_id 是个随机的 UUID 值）。在这个页面上，除了上一条的监控信息可以查看，还可以查看该 TaskManager 的 JVM（堆和非堆）、Direct 内存、网络、GC 次数和时间，如下图所示。内存和 GC 这些信息非常重要，很多时候 TaskManager 频繁重启的原因就是 JVM 内存设置得不合理，导致频繁的 GC，最后使得 OOM 崩溃，不得不重启。 另外如果你在 /taskmanagers/tm_id 接口后面加个 /log 就可以查看该 TaskManager 的日志，注意，在 Flink 中的日志和平常自己写的应用中的日志是不一样的。在 Flink 中，日志是以 TaskManager 为概念打印出来的，而不是以单个 Job 打印出来的，如果你的 Job 在多个 TaskManager 上运行，那么日志就会在多个 TaskManager 中打印出来。如果一个 TaskManager 中运行了多个 Job，那么它里面的日志就会很混乱，查看日志时会发现它为什么既有这个 Job 打出来的日志，又有那个 Job 打出来的日志，如果你之前有这个疑问，那么相信你看完这里，就不会有疑问了。 对于这种设计是否真的好，不同的人有不同的看法，在 Flink 的 Issue 中就有人提出了该问题，Issue 中的描述是希望日志可以是 Job 与 Job 之间的隔离，这样日志更方便采集和查看，对于排查问题也会更快。对此国内有公司也对这一部分做了改进，不知道正在看本书的你是否有什么好的想法可以解决 Flink 的这一痛点。 4.通过 http://localhost:8081/#/job-manager/config 页面可以看到可 JobManager 的配置信息，另外通过 http://localhost:8081/jobmanager/log 页面可以查看 JobManager 的日志详情。 5.通过 http://localhost:8081/jobs/job_id 页面可以查看 Job 的监控数据，如下图所示，由于指标（包括了 Job 的 Task 数据、Operator 数据、Exception 数据、Checkpoint 数据等）过多，大家可以自己在本地测试查看。 上面列举了几个 REST API（不是全部），主要是为了告诉大家，其实这些接口我们都知道，那么我们也可以利用这些接口去获取对应的监控数据，然后绘制出更酷炫的图表，用更直观的页面将这些数据展示出来，这样就能更好地控制。 除了利用 Flink UI 提供的接口去定时获取到监控数据，其实 Flink 还提供了很多的 reporter 去上报监控数据，比如 JMXReporter、PrometheusReporter、PrometheusPushGatewayReporter、InfluxDBReporter、StatsDReporter 等，这样就可以根据需求去定制获取到 Flink 的监控数据，下面教大家使用几个常用的 reporter。 相关 Rest API 可以查看官网链接：rest-api-integration 8.2.2 Metrics 类型简介可以在继承自 RichFunction 的函数中通过 getRuntimeContext().getMetricGroup() 获取 Metric 信息，常见的 Metrics 的类型有 Counter、Gauge、Histogram、Meter。 CounterGaugeHistogramMeter8.2.3 利用 JMXReporter 获取监控数据8.2.4 利用 PrometheusReporter 获取监控数据8.2.5 利用 PrometheusPushGatewayReporter 获取监控数据8.2.6 利用 InfluxDBReporter 获取监控数据8.2.7 安装 InfluxDB 和 Grafana安装 InfluxDB安装 Grafana8.2.8 配置 Grafana 展示监控数据加入知识星球可以看到上面文章：https://t.zsxq.com/f66iAMz 8.2.9 小结与反思本节讲了如何利用 API 去获取监控数据，对 Metrics 的类型进行介绍，然后还介绍了怎么利用 Reporter 去将 Metrics 数据进行上报，并通过 InfluxDB + Grafana 搭建了一套 Flink 的监控系统。另外你还可以根据公司的需要使用其他的存储方案来存储监控数据，Grafana 也支持不同的数据源，你们公司的监控系统架构是怎么样的，是否可以直接接入这套监控系统？ 作业部署上线后的监控尤其重要，虽说 Flink UI 自身提供了不少的监控信息，但是个人觉得还是比较弱，还是得去搭建一套完整的监控系统去监控 Flink 中的 JobManager、TaskManager 和作业。本章中讲解了 Flink UI 上获取监控数据的方式，还讲解了如何利用 Flink 自带的 Metrics Reporter 去采集各种监控数据，从而利用时序数据库存储这些监控数据，最后用 Grafana 这种可视化比较好的去展示这些监控数据，从而达到作业真正的监控运维效果。 整套监控系统也希望你可以运用在你们公司，当然你不一定非得选用相同的存储时序数据库，这样可以为你们节省不少作业出问题后的排查时间。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何实时监控 Flink 及其作业？","date":"2021-08-04T16:00:00.000Z","path":"2021/08/05/flink-in-action-8.1/","text":"第八章 —— Flink 监控Flink 相关的组件和作业的稳定性通常是比较关键的，所以得需要对它们进行监控，如果有异常，则需要及时告警通知。本章先会教会教会大家如何利用现有 Flink UI 上面的信息去发现和排查问题，会指明一些比较重要和我们非常关心的指标，通过这些指标我们能够立马定位到问题的根本原因。接着笔者会教大家如何去利用现有的 Metrics Reporter 去构建一个 Flink 的监控系统，它可以收集到所有作业的监控指标，并会存储这些监控指标数据，最后还会有一个监控大盘做数据可视化，通过这个大盘可以方便排查问题。 8.1 实时监控 Flink 及其作业 当将 Flink JobManager、TaskManager 都运行起来了，并且也部署了不少 Flink Job，那么它到底是否还在运行、运行的状态如何、资源 TaskManager 和 Slot 的个数是否足够、Job 内部是否出现异常、计算速度是否跟得上数据生产的速度 等这些问题其实对我们来说是比较关注的，所以就很迫切的需要一个监控系统帮我们把整个 Flink 集群的运行状态给展示出来。通过监控系统我们能够很好的知道 Flink 内部的整个运行状态，然后才能够根据项目生产环境遇到的问题 ‘对症下药’。下面分别来讲下 JobManager、TaskManager、Flink Job 的监控以及最关心的一些监控指标。 8.1.1 监控 JobManager我们知道 JobManager 是 Flink 集群的中控节点，类似于 Apache Storm 的 Nimbus 以及 Apache Spark 的 Driver 的角色。它负责作业的调度、作业 Jar 包的管理、Checkpoint 的协调和发起、与 TaskManager 之间的心跳检查等工作。如果 JobManager 出现问题的话，就会导致作业 UI 信息查看不了，TaskManager 和所有运行的作业都会受到一定的影响，所以这也是为啥在 7.1 节中强调 JobManager 的高可用问题。 在 Flink 自带的 UI 上 JobManager 那个 Tab 展示的其实并没有显示其对应的 Metrics，那么对于 JobManager 来说常见比较关心的监控指标有哪些呢？ 基础指标因为 Flink JobManager 其实也是一个 Java 的应用程序，那么它自然也会有 Java 应用程序的指标，比如内存、CPU、GC、类加载、线程信息等。 内存：内存又分堆内存和非堆内存，在 Flink 中还有 Direct 内存，每种内存又有初始值、使用值、最大值等指标，因为在 JobManager 中的工作其实相当于 TaskManager 来说比较少，也不存储事件数据，所以通常 JobManager 占用的内存不会很多，在 Flink JobManager 中自带的内存 Metrics 指标有： 123456789101112jobmanager_Status_JVM_Memory_Direct_Countjobmanager_Status_JVM_Memory_Direct_MemoryUsedjobmanager_Status_JVM_Memory_Direct_TotalCapacityjobmanager_Status_JVM_Memory_Heap_Committedjobmanager_Status_JVM_Memory_Heap_Maxjobmanager_Status_JVM_Memory_Heap_Usedjobmanager_Status_JVM_Memory_Mapped_Countjobmanager_Status_JVM_Memory_Mapped_MemoryUsedjobmanager_Status_JVM_Memory_Mapped_TotalCapacityjobmanager_Status_JVM_Memory_NonHeap_Committedjobmanager_Status_JVM_Memory_NonHeap_Maxjobmanager_Status_JVM_Memory_NonHeap_Used CPU：JobManager 分配的 CPU 使用情况，如果使用类似 K8S 等资源调度系统，则需要对每个容器进行设置资源，比如 CPU 限制不能超过多少，在 Flink JobManager 中自带的 CPU 指标有： 12jobmanager_Status_JVM_CPU_Loadjobmanager_Status_JVM_CPU_Time GC：GC 信息对于 Java 应用来说是避免不了的，每种 GC 都有时间和次数的指标可以供参考，提供的指标有： 1234jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Countjobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Timejobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Countjobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Time Checkpoint 指标因为 JobManager 负责了作业的 Checkpoint 的协调和发起功能，所以 Checkpoint 相关的指标就有表示 Checkpoint 执行的时间、Checkpoint 的时间长短、完成的 Checkpoint 的次数、Checkpoint 失败的次数、Checkpoint 正在执行 Checkpoint 的个数等，其对应的指标如下： 123456789jobmanager_job_lastCheckpointAlignmentBufferedjobmanager_job_lastCheckpointDurationjobmanager_job_lastCheckpointExternalPathjobmanager_job_lastCheckpointRestoreTimestampjobmanager_job_lastCheckpointSizejobmanager_job_numberOfCompletedCheckpointsjobmanager_job_numberOfFailedCheckpointsjobmanager_job_numberOfInProgressCheckpointsjobmanager_job_totalNumberOfCheckpoints 重要的指标另外还有比较重要的指标就是 Flink UI 上也提供的，类似于 Slot 总共个数、Slot 可使用的个数、TaskManager 的个数（通过查看该值可以知道是否有 TaskManager 发生异常重启）、正在运行的作业数量、作业运行的时间和完成的时间、作业的重启次数，对应的指标如下： 12345678jobmanager_job_uptimejobmanager_numRegisteredTaskManagersjobmanager_numRunningJobsjobmanager_taskSlotsAvailablejobmanager_taskSlotsTotaljobmanager_job_downtimejobmanager_job_fullRestartsjobmanager_job_restartingTime 8.1.2 监控 TaskManager…. 12345678910111213141516171819202122232425taskmanager_Status_JVM_CPU_Loadtaskmanager_Status_JVM_CPU_Timetaskmanager_Status_JVM_ClassLoader_ClassesLoadedtaskmanager_Status_JVM_ClassLoader_ClassesUnloadedtaskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Counttaskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Timetaskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Counttaskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Timetaskmanager_Status_JVM_Memory_Direct_Counttaskmanager_Status_JVM_Memory_Direct_MemoryUsedtaskmanager_Status_JVM_Memory_Direct_TotalCapacitytaskmanager_Status_JVM_Memory_Heap_Committedtaskmanager_Status_JVM_Memory_Heap_Maxtaskmanager_Status_JVM_Memory_Heap_Usedtaskmanager_Status_JVM_Memory_Mapped_Counttaskmanager_Status_JVM_Memory_Mapped_MemoryUsedtaskmanager_Status_JVM_Memory_Mapped_TotalCapacitytaskmanager_Status_JVM_Memory_NonHeap_Committedtaskmanager_Status_JVM_Memory_NonHeap_Maxtaskmanager_Status_JVM_Memory_NonHeap_Usedtaskmanager_Status_JVM_Threads_Counttaskmanager_Status_Network_AvailableMemorySegmentstaskmanager_Status_Network_TotalMemorySegmentstaskmanager_Status_Shuffle_Netty_AvailableMemorySegmentstaskmanager_Status_Shuffle_Netty_TotalMemorySegments 8.1.3 监控 Flink 作业… 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647taskmanager_job_task_Shuffle_Netty_Input_Buffers_outPoolUsagetaskmanager_job_task_Shuffle_Netty_Input_Buffers_outputQueueLengthtaskmanager_job_task_Shuffle_Netty_Output_Buffers_inPoolUsagetaskmanager_job_task_Shuffle_Netty_Output_Buffers_inputExclusiveBuffersUsagetaskmanager_job_task_Shuffle_Netty_Output_Buffers_inputFloatingBuffersUsagetaskmanager_job_task_Shuffle_Netty_Output_Buffers_inputQueueLengthtaskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocaltaskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocalPerSecondtaskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemotetaskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemotePerSecondtaskmanager_job_task_Shuffle_Netty_Output_numBytesInLocaltaskmanager_job_task_Shuffle_Netty_Output_numBytesInLocalPerSecondtaskmanager_job_task_Shuffle_Netty_Output_numBytesInRemotetaskmanager_job_task_Shuffle_Netty_Output_numBytesInRemotePerSecondtaskmanager_job_task_buffers_inPoolUsagetaskmanager_job_task_buffers_inputExclusiveBuffersUsagetaskmanager_job_task_buffers_inputFloatingBuffersUsagetaskmanager_job_task_buffers_inputQueueLengthtaskmanager_job_task_buffers_outPoolUsagetaskmanager_job_task_buffers_outputQueueLengthtaskmanager_job_task_checkpointAlignmentTimetaskmanager_job_task_currentInputWatermarktaskmanager_job_task_numBuffersInLocaltaskmanager_job_task_numBuffersInLocalPerSecondtaskmanager_job_task_numBuffersInRemotetaskmanager_job_task_numBuffersInRemotePerSecondtaskmanager_job_task_numBuffersOuttaskmanager_job_task_numBuffersOutPerSecondtaskmanager_job_task_numBytesIntaskmanager_job_task_numBytesInLocaltaskmanager_job_task_numBytesInLocalPerSecondtaskmanager_job_task_numBytesInPerSecondtaskmanager_job_task_numBytesInRemotetaskmanager_job_task_numBytesInRemotePerSecondtaskmanager_job_task_numBytesOuttaskmanager_job_task_numBytesOutPerSecondtaskmanager_job_task_numRecordsIntaskmanager_job_task_numRecordsInPerSecondtaskmanager_job_task_numRecordsOuttaskmanager_job_task_numRecordsOutPerSecondtaskmanager_job_task_operator_currentInputWatermarktaskmanager_job_task_operator_currentOutputWatermarktaskmanager_job_task_operator_numLateRecordsDroppedtaskmanager_job_task_operator_numRecordsIntaskmanager_job_task_operator_numRecordsInPerSecondtaskmanager_job_task_operator_numRecordsOuttaskmanager_job_task_operator_numRecordsOutPerSecond 8.1.4 最关心的性能指标JobManagerTaskManagerFlink Job8.1.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/f66iAMz","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？","date":"2021-08-03T16:00:00.000Z","path":"2021/08/04/flink-in-action-7.2/","text":"7.2 Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？前面章节已经有很多学习案列带大家使用 Flink，不仅有讲将 Flink 应用程序在 IDEA 中运行，也有讲将 Flink Job 编译打包上传到 Flink UI 上运行，在这 UI 背后可能是通过 Standalone、YARN、Mesos、Kubernetes 等运行启动的 Flink。那么这节就系统讲下如何部署和运行我们的 Flink Job，大家可以根据自己公司的场景进行选择使用哪种方式进行部署 Flink 作业！ 7.2.1 Standalone第一种方式就是 Standalone 模式，这种模式笔者在前面 2.2 节里面演示的就是这种，我们通过执行命令：./bin/start-cluster.sh 启动一个 Flink Standalone 集群。 1234zhisheng@zhisheng /usr/local/flink-1.9.0 ./bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host zhisheng.Starting taskexecutor daemon on host zhisheng. 默认的话是启动一个 JobManager 和一个 TaskManager，我们可以通过 jps 查看进程有： 12365425 Jps51572 TaskManagerRunner51142 StandaloneSessionClusterEntrypoint 其中上面的 TaskManagerRunner 代表的是 TaskManager 进程，StandaloneSessionClusterEntrypoint 代表的是 JobManager 进程。上面运行产生的只有一个 JobManager 和一个 TaskManager，如果是生产环境的话，这样的配置肯定是不够运行多个 Job 的，那么我们该如何在生产环境中配置 standalone 模式的集群呢？我们就需要修改 Flink 安装目录下面的 conf 文件夹里面配置： 123flink-conf.yamlmastersslaves 将 slaves 中再添加一个 localhost，这样就可以启动两个 TaskManager 了。接着启动脚本 start-cluster.sh，启动日志显示如下图所示。 可以看见有两个 TaskManager 启动了，再看下 UI 显示的也是有两个 TaskManager，如下图所示。 那么如果还想要添加一个 JobManager 或者 TaskManager 怎么办？总不能再次重启修改配置文件后然后再重启吧！这里你可以这样操作。 增加一个 JobManager： 1bin/jobmanager.sh ((start|start-foreground) [host] [webui-port])|stop|stop-all 但是注意 Standalone 下一台机器最多只能运行一个 JobManager。 增加一个 TaskManager： 1bin/taskmanager.sh start|start-foreground|stop|stop-all 比如我执行了 ./bin/taskmanager.sh start 命令后，运行结果如下图所示。 Standalone 模式下可以先对 Flink Job 通过 mvn clean package 编译打包，得到 Jar 包后，可以在 UI 上直接上传 Jar 包，然后点击 Submit 就可以运行了。 7.2.2 YARN7.2.3 MesosSession 集群Per Job 集群7.3.4 Kubernetes7.2.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/f66iAMz 本章讲解了 Flink 中所有的配置文件，每个配置文件中的配置有啥作用，并且如果在不同环境下配置 JobManager 的高可用，另外还介绍了 Flink 的部署问题，因为 Flink 本身是支持在不同的环境下部署的，比如 Standalone、K8S、YARN、Mesos 等，其中在调度平台上又有 Session 模式和 Per Job 模式，每种模式都有自己的特点，所以你可能需要根据公司的情况来做一定的选型，每种的部署也可能会有点不一样，遇到问题的化还得根据特殊情况进行特殊处理，希望你可以在公司灵活的处理这种问题。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 配置详解及如何配置高可用？","date":"2021-08-02T16:00:00.000Z","path":"2021/08/03/flink-in-action-7.1/","text":"第七章 —— Flink 作业环境部署在第一章中介绍过 Flink 是可以以多种方式部署的，比如 Standalone、YARN、Mesos、K8S。本章将先对 Flink 中的所有配置文件做一个详细的讲解，接下来将讲解 JobManager 高可用部署相关的配置，最后会分别讲解如何在不同的平台上部署运行 Flink 作业。虽然在你们公司可能只会用到其中的一种，但是仍然建议你将每种方式都熟悉一下。 7.1 Flink 配置详解及如何配置高可用？ 在讲解如何部署 Flink 作业（在 7.2 节中会讲）之前，先来详细的看一下 Flink 中的所有配置文件以及文件中的各种配置代表的内容，这样对于后面部署和调优 Flink 作业有一定的帮助。 7.1.1 Flink 配置详解先来看下 Flink 配置文件目录中最重要的配置文件 flink-conf.yaml 的配置。 flink-conf.yaml基础配置如下所示： 1234567891011121314151617181920# jobManager 的IP地址jobmanager.rpc.address: localhost# JobManager 的端口号jobmanager.rpc.port: 6123# JobManager JVM heap 内存大小jobmanager.heap.size: 1024m# TaskManager JVM heap 内存大小taskmanager.heap.size: 1024m# 每个 TaskManager 提供的任务 slots 数量大小taskmanager.numberOfTaskSlots: 1# 程序默认并行计算的个数parallelism.default: 1# 文件系统来源# fs.default-scheme 高可用性相关的配置如下所示： 1234567891011# 可以选择 &apos;NONE&apos; 或者 &apos;zookeeper&apos;.# high-availability: zookeeper# 文件系统路径，让 Flink 在高可用性设置中持久保存元数据# high-availability.storageDir: hdfs:///flink/ha/# zookeeper 集群中仲裁者的机器 ip 和 port 端口号# high-availability.zookeeper.quorum: localhost:2181# 默认是 open，如果 zookeeper security 启用了该值会更改成 creator# high-availability.zookeeper.client.acl: open 容错和 Checkpoint 相关的配置如下所示： 1234567891011# 用于存储和检查点状态# state.backend: filesystem# 存储检查点的数据文件和元数据的默认目录# state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints# savepoints 的默认目标目录(可选)# state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints# 用于启用/禁用增量 checkpoints 的标志# state.backend.incremental: false Web 前端相关的配置如下所示： 12345678# 基于 Web 的运行时监视器侦听的地址.#jobmanager.web.address: 0.0.0.0# Web 的运行时监视器端口rest.port: 8081# 是否从基于 Web 的 jobmanager 启用作业提交# jobmanager.web.submit.enable: false 高级配置如下所示： 123456789101112# io.tmp.dirs: /tmp# 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存# taskmanager.memory.preallocate: false# 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类# classloader.resolve-order: child-first# 用于网络缓冲区的 JVM 内存的分数。 这决定了 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度。 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值。 另请注意，“taskmanager.network.memory.min”和“taskmanager.network.memory.max”可能会覆盖此分数# taskmanager.network.memory.fraction: 0.1# taskmanager.network.memory.min: 67108864# taskmanager.network.memory.max: 1073741824 Flink 集群安全配置如下所示： 1234567891011# 指示是否从 Kerberos ticket 缓存中读取# security.kerberos.login.use-ticket-cache: true# 包含用户凭据的 Kerberos 密钥表文件的绝对路径# security.kerberos.login.keytab: /path/to/kerberos/keytab# 与 keytab 关联的 Kerberos 主体名称# security.kerberos.login.principal: flink-user# 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）# security.kerberos.login.contexts: Client,KafkaClient Zookeeper 安全配置如下所示： 12345# 覆盖以下配置以提供自定义 ZK 服务名称# zookeeper.sasl.service-name: zookeeper# 该配置必须匹配 &quot;security.kerberos.login.contexts&quot; 中的列表（含有一个）# zookeeper.sasl.login-context-name: Client HistoryServer 相关的配置如下所示： 12345678910111213141516# 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer# 将已完成的作业上传到的目录# jobmanager.archive.fs.dir: hdfs:///completed-jobs/# 基于 Web 的 HistoryServer 的地址# historyserver.web.address: 0.0.0.0# 基于 Web 的 HistoryServer 的端口号# historyserver.web.port: 8082# 以逗号分隔的目录列表，用于监视已完成的作业# historyserver.archive.fs.dir: hdfs:///completed-jobs/# 刷新受监控目录的时间间隔（以毫秒为单位）# historyserver.archive.fs.refresh-interval: 10000 mastersmasters 配置文件中以 host:port 构成就行，如下所示： 1localhost:8081 slavesslaves 文件里面是每个 worker 节点的 IP/Hostname，每一个 worker 结点之后都会运行一个 TaskManager，一个一行，如下所示。 1localhost 7.1.2 Log 的配置在 Flink 的日志配置文件（logback.xml 或 log4j.properties）中有配置日志存储的地方，logback.xml 配置日志存储的路径是： 1234567&lt;appender name=\"file\" class=\"ch.qos.logback.core.FileAppender\"&gt; &lt;file&gt;$&#123;log.file&#125;&lt;/file&gt; &lt;append&gt;false&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;60&#125; %X&#123;sourceThread&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; log4j.properties 和 log4j-cli.properties 的配置日志存储的路径是： 1log4j.appender.file.file=$&#123;log.file&#125; 从上面两个配置可以看到日志的路径都是由 log.file 变量控制的，如果系统变量没有配置的话，则会使用 bin／flink 脚本里配置的值。 12log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-client-$HOSTNAME.loglog_setting=(-Dlog.file=&quot;$log&quot; -Dlog4j.configuration=file:&quot;$FLINK_CONF_DIR&quot;/log4j-cli.properties -Dlogback.configurationFile=file:&quot;$FLINK_CONF_DIR&quot;/logback.xml) 从上面可以看到 log 里配置的 FLINK_LOG_DIR 变量是在 bin 目录下的 config.sh 里初始化的。 123456DEFAULT_FLINK_LOG_DIR=$FLINK_HOME_DIR_MANGLED/logKEY_ENV_LOG_DIR=\"env.log.dir\"if [ -z \"$&#123;FLINK_LOG_DIR&#125;\" ]; then FLINK_LOG_DIR=$(readFromConfig $&#123;KEY_ENV_LOG_DIR&#125; \"$&#123;DEFAULT_FLINK_LOG_DIR&#125;\" \"$&#123;YAML_CONF&#125;\")fi 从上面可以知道日志默认就是在 Flink 的 log 目录下，你可以通过在 flink-conf.yaml 配置文件中配置 env.log.dir 参数来更改保存日志的目录。另外通过源码可以发现，如果找不到 log.file 环境变量，则会去找 web.log.path 的配置，但是该配置在 Standalone 下是不起作用的，日志依旧是会在 log 目录，在 YARN 下是会起作用的。 123456789101112131415161718192021222324252627282930313233public static LogFileLocation find(Configuration config) &#123; final String logEnv = \"log.file\"; String logFilePath = System.getProperty(logEnv); if (logFilePath == null) &#123; LOG.warn(\"Log file environment variable '&#123;&#125;' is not set.\", logEnv); logFilePath = config.getString(WebOptions.LOG_PATH); //该值为 web.log.path &#125; // not configured, cannot serve log files if (logFilePath == null || logFilePath.length() &lt; 4) &#123; LOG.warn(\"JobManager log files are unavailable in the web dashboard. \" + \"Log file location not found in environment variable '&#123;&#125;' or configuration key '&#123;&#125;'.\", logEnv, WebOptions.LOG_PATH); return new LogFileLocation(null, null); &#125; String outFilePath = logFilePath.substring(0, logFilePath.length() - 3).concat(\"out\"); LOG.info(\"Determined location of main cluster component log file: &#123;&#125;\", logFilePath); LOG.info(\"Determined location of main cluster component stdout file: &#123;&#125;\", outFilePath); return new LogFileLocation(resolveFileLocation(logFilePath), resolveFileLocation(outFilePath));&#125;/** * The log file location (may be in /log for standalone but under log directory when using YARN). */public static final ConfigOption&lt;String&gt; LOG_PATH = key(\"web.log.path\") .noDefaultValue() .withDeprecatedKeys(\"jobmanager.web.log.path\") .withDescription(\"Path to the log file (may be in /log for standalone but under log directory when using YARN).\"); 另外可能会在本地 IDE 中运行作业出不来日志的情况，这时请检查是否有添加日志的依赖。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt; 7.1.3 如何配置 JobManager 高可用？JobManager 协调每个 Flink 作业的部署，它负责调度和资源管理。默认情况下，每个 Flink 集群只有一个 JobManager 实例，这样就可能会产生单点故障，如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。如果保证 JobManager 的高可用，则可以避免这个问题。下面分别下如何搭建 Standalone 集群和 YARN 集群高可用的 JobManager。 搭建 Standalone 集群高可用 JobManagerStandalone 集群的 JobManager 高可用性的概念是：任何时候只有一个主 JobManager 和多个备 JobManager，以便在主节点失败时有新的 JobManager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别，每个 JobManager 都可以充当主备节点。例如，请考虑以下三个 JobManager 实例的设置。 如何配置 要启用 JobManager 高可用性功能，首先必须在配置文件 flink-conf.yaml 中将高可用性模式设置为 ZooKeeper，配置 ZooKeeper quorum，将所有 JobManager 主机及其 Web UI 端口写入配置文件。每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 ZooKeeper。另外就是高可用存储目录，JobManager 元数据保存在 high-availability.storageDir 指定的文件系统中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS、S3、Ceph、NFS 等，该文件系统中保存了 JobManager 恢复状态需要的所有元数据。 123high-availability: zookeeperhigh-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181high-availability.storageDir: hdfs:///flink/ha/ Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。它在我们的 Flink 安装路径下面 /conf/zoo.cfg 。 搭建 YARN 集群高可用 JobManager7.1.4 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/f66iAMz","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 扩展库——Gelly","date":"2021-08-01T16:00:00.000Z","path":"2021/08/02/flink-in-action-6.5/","text":"6.5 Flink 扩展库——Gelly在 1.9 版本中还剩最后一个扩展库就是 Gelly，本节将带你了解一下 Gelly 的功能以及如何使用。 6.5.1 Gelly 简介Gelly 是 Flink 的图 API 库，它包含了一组旨在简化 Flink 中图形分析应用程序开发的方法和实用程序。在 Gelly 中，可以使用类似于批处理 API 提供的高级函数来转换和修改图。Gelly 提供了创建、转换和修改图的方法以及图算法库。 6.5.2 使用 Gelly因为 Gelly 是 Flink 项目中库的一部分，它本身不在 Flink 的二进制包中，所以运行 Gelly 项目（Java 应用程序）是需要将 opt/flink-gelly_2.11-1.9.0.jar 移动到 lib 目录中，如果是 Scala 应用程序则需要将 opt/flink-gelly-scala_2.11-1.9.0.jar 移动到 lib 中，接着运行下面的命令就可以运行一个 flink-gelly-examples 项目。 1234./bin/flink run examples/gelly/flink-gelly-examples_2.11-1.9.0.jar \\ --algorithm GraphMetrics --order directed \\ --input RMatGraph --type integer --scale 20 --simplify directed \\ --output print 接下来可以在 UI 上看到运行的结果如下图所示： 如果是自己创建的 Gelly Java 应用程序，则需要添加如下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-gelly_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 如果是 Gelly Scala 应用程序，添加下面的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-gelly-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 6.5.3 Gelly API引入好依赖后，接着将介绍一下 Gelly 该如何使用。 Graph 介绍在 Gelly 中，一个图（Graph）由顶点的数据集（DataSet）和边的数据集（DataSet）组成。图中的顶点由 Vertex 类型来表示，一个 Vertex 由唯一的 ID 和一个值来表示。其中 Vertex 的 ID 必须是全局唯一的值，且实现了 Comparable 接口。如果节点不需要由任何值，则该值类型可以声明成 NullValue 类型。 12345//创建一个 Vertex&lt;Long，String&gt;Vertex&lt;Long, String&gt; v = new Vertex&lt;Long, String&gt;(1L, \"foo\");//创建一个 Vertex&lt;Long，NullValue&gt;Vertex&lt;Long, NullValue&gt; v = new Vertex&lt;Long, NullValue&gt;(1L, NullValue.getInstance()); Graph 中的边由 Edge 类型来表示，一个 Edge 通常由源顶点的 ID，目标顶点的 ID 以及一个可选的值来表示。其中源顶点和目标顶点的类型必须与 Vertex 的 ID 类型相同。同样的，如果边不需要由任何值，则该值类型可以声明成 NullValue 类型。 1234Edge&lt;Long, Double&gt; e = new Edge&lt;Long, Double&gt;(1L, 2L, 0.5);//反转此 edge 的源和目标Edge&lt;Long, Double&gt; reversed = e.reverse();Double weight = e.getValue(); // weight = 0.5 在 Gelly 中，一个 Edge 总是从源顶点指向目标顶点。如果图中每条边都能匹配一个从目标顶点到源顶点的 Edge，那么这个图可能是个无向图。同样地，无向图可以用这个方式来表示。 创建 Graph可以通过以下几种方式创建一个 Graph： 从一个 Edge 数据集合和一个 Vertex 数据集合中创建图。 123456ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();DataSet&lt;Vertex&lt;String, Long&gt;&gt; vertices = ...DataSet&lt;Edge&lt;String, Double&gt;&gt; edges = ...Graph&lt;String, Long, Double&gt; graph = Graph.fromDataSet(vertices, edges, env); 从一个表示边的 Tuple2 数据集合中创建图。Gelly 会将每个 Tuple2 转换成一个 Edge，其中第一个元素表示源顶点的 ID，第二个元素表示目标顶点的 ID，图中的顶点和边的 value 值均被设置为 NullValue。 12345ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();DataSet&lt;Tuple2&lt;String, String&gt;&gt; edges = ...Graph&lt;String, NullValue, NullValue&gt; graph = Graph.fromTuple2DataSet(edges, env); 从一个 Tuple3 数据集和一个可选的 Tuple2 数据集中生成图。在这种情况下，Gelly 会将每个 Tuple3 转换成 Edge，其中第一个元素域是源顶点 ID，第二个域是目标顶点 ID，第三个域是边的值。同样的，每个 Tuple2 会转换成一个顶点 Vertex，其中第一个域是顶点的 ID，第二个域是顶点的 value。 1234567ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();DataSet&lt;Tuple2&lt;String, Long&gt;&gt; vertexTuples = env.readCsvFile(\"path/to/vertex/input\").types(String.class, Long.class);DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; edgeTuples = env.readCsvFile(\"path/to/edge/input\").types(String.class, String.class, Double.class);Graph&lt;String, Long, Double&gt; graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env); 从一个表示边数据的CSV文件和一个可选的表示节点的CSV文件中生成图。在这种情况下，Gelly会将表示边的CSV文件中的每一行转换成一个Edge，其中第一个域表示源顶点ID，第二个域表示目标顶点ID，第三个域表示边的值。同样的，表示节点的CSV中的每一行都被转换成一个Vertex，其中第一个域表示顶点的ID，第二个域表示顶点的值。为了通过GraphCsvReader生成图，需要指定每个域的类型，可以使用 types、edgeTypes、vertexTypes、keyType 中的方法。 123456//创建一个具有字符串 Vertex id、Long Vertex 和双边缘的图Graph&lt;String, Long, Double&gt; graph = Graph.fromCsvReader(\"path/to/vertex/input\", \"path/to/edge/input\", env) .types(String.class, Long.class, Double.class);//创建一个既没有顶点值也没有边值的图Graph&lt;Long, NullValue, NullValue&gt; simpleGraph = Graph.fromCsvReader(\"path/to/edge/input\", env).keyType(Long.class); 从一个边的集合和一个可选的顶点的集合中生成图。如果在图创建的时候顶点的集合没有传入，Gelly 会依据数据的边数据集合自动地生成一个 Vertex 集合。这种情况下，创建的节点是没有值的。或者也可以像下面一样，在创建图的时候提供一个 MapFunction 方法来初始化节点的值。 12345678910111213List&lt;Vertex&lt;Long, Long&gt;&gt; vertexList = new ArrayList...List&lt;Edge&lt;Long, String&gt;&gt; edgeList = new ArrayList...Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(vertexList, edgeList, env);//将顶点值初始化为顶点IDGraph&lt;Long, Long, String&gt; graph = Graph.fromCollection(edgeList, new MapFunction&lt;Long, Long&gt;() &#123; public Long map(Long value) &#123; return value; &#125; &#125;, env); Graph 属性Gelly 提供了下列方法来查询图的属性和指标： 123456789101112DataSet&lt;Vertex&lt;K, VV&gt;&gt; getVertices()//获取边缘数据集DataSet&lt;Edge&lt;K, EV&gt;&gt; getEdges()//获取顶点的 id 数据集DataSet&lt;K&gt; getVertexIds()DataSet&lt;Tuple2&lt;K, K&gt;&gt; getEdgeIds()DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; inDegrees()DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; outDegrees()DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; getDegrees()long numberOfVertices()long numberOfEdges()DataSet&lt;Triplet&lt;K, VV, EV&gt;&gt; getTriplets() Graph 转换Graph 转换方式有下面几种方式： Map：Gelly 提供了专门的用于转换顶点值和边值的方法。mapVertices 和 mapEdges 会返回一个新图，图中的每个顶点和边的 ID 不会改变，但是顶点和边的值会根据用户自定义的映射方法进行修改。这些映射方法同时也可以修改顶点和边的值的类型。 Translate：Gelly 还提供了专门用于根据用户定义的函数转换顶点和边的 ID 和值的值及类型的方法（translateGraphIDs/translateVertexValues/translateEdgeValues），是Map 功能的升级版，因为 Map 操作不支持修订顶点和边的 ID。 Filter：Gelly 支持在图中的顶点上或边上执行一个用户指定的 filter 转换。filterOnEdges 会根据提供的在边上的断言在原图的基础上生成一个新的子图，注意，顶点的数据不会被修改。同样的 filterOnVertices 在原图的顶点上进行 filter 转换，不满足断言条件的源节点或目标节点会在新的子图中移除。该子图方法支持同时对顶点和边应用 filter 函数，如下图所示。 Reverse：Gelly中得reverse()方法用于在原图的基础上，生成一个所有边方向与原图相反的新图。 Undirected：在前面的内容中，我们提到过，Gelly中的图通常都是有向的，而无向图可以通过对所有边添加反向的边来实现，出于这个目的，Gelly提供了getUndirected()方法，用于获取原图的无向图。 Union：Gelly的union()操作用于联合当前图和指定的输入图，并生成一个新图，在输出的新图中，相同的节点只保留一份，但是重复的边会保留。如下图所示： Difference：Gelly提供了difference()方法用于发现当前图与指定的输入图之间的差异。 Intersect：Gelly提供了intersect()方法用于发现两个图中共同存在的边，并将相同的边以新图的方式返回。相同的边指的是具有相同的源顶点，相同的目标顶点和相同的边值。返回的新图中，所有的节点没有任何值，如果需要节点值，可以使用joinWithVertices()方法去任何一个输入图中检索。 Graph 变化Gelly 内置下列方法以支持对一个图进行节点和边的增加/移除操作： 12345678Graph&lt;K, VV, EV&gt; addVertex(final Vertex&lt;K, VV&gt; vertex)Graph&lt;K, VV, EV&gt; addVertices(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToAdd)Graph&lt;K, VV, EV&gt; addEdge(Vertex&lt;K, VV&gt; source, Vertex&lt;K, VV&gt; target, EV edgeValue)Graph&lt;K, VV, EV&gt; addEdges(List&lt;Edge&lt;K, EV&gt;&gt; newEdges)Graph&lt;K, VV, EV&gt; removeVertex(Vertex&lt;K, VV&gt; vertex)Graph&lt;K, VV, EV&gt; removeVertices(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToBeRemoved)Graph&lt;K, VV, EV&gt; removeEdge(Edge&lt;K, EV&gt; edge)Graph&lt;K, VV, EV&gt; removeEdges(List&lt;Edge&lt;K, EV&gt;&gt; edgesToBeRemoved) Neighborhood MethodsGraph 验证6.5.4 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/nMR7ufq 本章所讲的内容属于 Flink 的扩展库，包含了 CEP 复杂事件处理、State Processor API、Machine Learning 和 Gelly，各种都有讲解一些样例，但是没有过多深入的讲，但还是希望你可以在书本外自己去扩充这些内容的知识点。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 扩展库——Machine Learning","date":"2021-07-31T16:00:00.000Z","path":"2021/08/01/flink-in-action-6.4/","text":"6.4 Flink 扩展库——Machine Learning随着人工智能的火热，机器学习这门技术也变得异常重要，Flink 作为一个数据处理的引擎，虽然目前在该方面还较弱，但是在 Flink Forward Asia 2019 北京站后，阿里开源了 Alink 平台的核心代码，并上传了一系列的算法库，该项目是基于 Flink 的通用算法平台，开发者和数据分析师可以利用 Alink 提供的一系列算法来构建软件功能，例如统计分析、机器学习、实时预测、个性化推荐和异常检测。相信未来 Flink 的机器学习库将会应用到更多的场景去，本节将带你了解一下 Flink 中的机器学习库。 6.4.1 Flink-ML 简介ML 是 Machine Learning 的简称，Flink-ML 是 Flink 的机器学习类库。在 Flink 1.9 之前该类库是存在 flink-libraries 模块下的，但是在 Flink 1.9 版本中，为了支持 FLIP-39 ，所以该类库被移除了。 建立 FLIP-39 的目的主要是增强 Flink-ML 的可伸缩性和易用性。通常使用机器学习的有两类人，一类是机器学习算法库的开发者，他们需要一套标准的 API 来实现算法，每个机器学习算法会在这些 API 的基础上实现；另一类用户是直接利用这些现有的机器学习算法库去训练数据模型，整个训练是要通过很多转换或者算法才能完成的，所以如果能够提供 ML Pipeline，那么对于后一类用户来说绝对是一种福音。虽然在 1.9 中移除了之前的 Flink-ML 模块，但是在 Flink 项目下出现了一个 flink-ml-parent 的模块，该模块有两个子模块 flink-ml-api 和 flink-ml-lib。 flink-ml-api 模块增加了 ML Pipeline 和 MLLib 的接口，它的类结构图如下图所示。 主要接口如下所示： Transformer: Transformer 是一种可以将一个表转换成另一个表的算法 Model: Model 是一种特别的 Transformer，它继承自 Transformer。它通常是由 Estimator 生成，Model 用于推断，输入一个数据表会生成结果表。 Estimator: Estimator 是一个可以根据一个数据表生成一个模型的算法。 Pipeline: Pipeline 描述的是机器学习的工作流，它将很多 Transformer 和 Estimator 连接在一起成一个工作流。 PipelineStage: PipelineStage 是 Pipeline 的基础节点，Transformer 和 Estimator 两个都继承自 PipelineStage 接口。 Params: Params 是一个参数容器。 WithParams: WithParams 有一个保存参数的 Params 容器。通常会使用在 PipelineStage 里面，因为几乎所有的算法都需要参数。 Flink-ML 的 pipeline 流程如下图所示： flink-ml-lib 模块包括了 DenseMatrix、DenseVector、SparseVector 等类的基本操作。这两个模块是 Flink-ML 的基础模块，相信社区在后面的稳定版本一定会带来更加完善的 Flink-ML 库。 6.4.2 使用 Flink-ML虽然在 Flink 1.9 中已经移除了 Flink-ML 模块，但是在之前的版本还是支持的，如果你们公司使用的是低于 1.9 的版本，那么还是可以使用的，在使用之前引入依赖（假设使用的是 Flink 1.8 版本）： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-ml_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt;&lt;/dependency&gt; 另外如果是要运行的话还是要将 opt 目录下的 flink-ml_2.11-1.8.0.jar 移到 lib 目录下。下面演示下如何训练多元线性回归模型： 12345678910111213141516171819//带标签的特征向量val trainingData: DataSet[LabeledVector] = ...val testingData: DataSet[Vector] = ...val dataSet: DataSet[LabeledVector] = ...//使用 Splitter 将数据集拆分成训练数据和测试数据val trainTestData: DataSet[TrainTestDataSet] = Splitter.trainTestSplit(dataSet)val trainingData: DataSet[LabeledVector] = trainTestData.trainingval testingData: DataSet[Vector] = trainTestData.testing.map(lv =&gt; lv.vector)val mlr = MultipleLinearRegression() .setStepsize(1.0) .setIterations(100) .setConvergenceThreshold(0.001)mlr.fit(trainingData)//已经形成的模型可以用来预测数据了val predictions: DataSet[LabeledVector] = mlr.predict(testingData) 6.4.3 使用 Flink-ML Pipeline6.4.4 Alink 介绍6.4.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/nMR7ufq","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 扩展库——State Processor API","date":"2021-07-29T16:00:00.000Z","path":"2021/07/30/flink-in-action-6.3/","text":"6.3 Flink 扩展库——State Processor APIState Processor API 功能是在 1.9 版本中新增加的一个功能，本节将带你了解一下其功能和如何使用？ 6.3.1 State Processor API 简介能够从外部访问 Flink 作业的状态一直用户迫切需要的功能之一，在 Apache Flink 1.9.0 中新引入了 State Processor API，该 API 让用户可以通过 Flink DataSet 作业来灵活读取、写入和修改 Flink 的 Savepoint 和 Checkpoint。 6.3.2 在 Flink 1.9 之前是如何处理状态的？一般来说，大多数的 Flink 作业都是有状态的，并且随着作业运行的时间越来越久，就会累积越多越多的状态，如果因为故障导致作业崩溃可能会导致作业的状态都丢失，那么对于比较重要的状态来说，损失就会很大。为了保证作业状态的一致性和持久性，Flink 从一开始使用的就是 Checkpoint 和 Savepoint 来保存状态，并且可以从 Savepoint 中恢复状态。在 Flink 的每个新 Release 版本中，Flink 社区添加了越来越多与状态相关的功能以提高 Checkpoint 的速度和恢复速度。 有的时候，用户可能会有这些需求场景，比如从第三方外部系统访问作业的状态、将作业的状态信息迁移到另一个应用程序等，目前现有支持查询作业状态的功能 Queryable State，但是在 Flink 中目前该功能只支持根据 Key 查找，并且不能保证返回值的一致性。另外该功能不支持添加和修改作业的状态，所以适用的场景还是比较有限。 6.3.3 使用 State Processor API 读写作业状态在 1.9 版本中的 State Processor API，它完全和之前不一致，该功能使用 InputFormat 和 OutputFormat 扩展了 DataSet API 以读取和写入 Checkpoint 和 Savepoint 数据。由于 DataSet 和 Table API 的互通性，所以也可以使用 Table API 或者 SQL 查询和分析状态的数据。例如，再获取到正在运行的流作业状态的 Checkpoint 后，可以使用 DataSet 批处理程序对其进行分析，以验证该流作业的运行是否正确。另外 State Processor API 还可以修复不一致的状态信息，它提供了很多方法来开发有状态的应用程序，这些方法在以前的版本中因为设计的问题导致作业在启动后不能再修改，否则状态可能会丢失。现在，你可以任意修改状态的数据类型、调整算子的最大并行度、拆分或合并算子的状态、重新分配算子的 uid 等。 如果要使用 State Processor API 去读写作业的状态，你需要添加下面的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-state-processor-api_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 6.3.4 使用 DataSet 读取作业状态State Processor API 将作业的状态映射到一个或多个可以单独处理的数据集，为了能够使用该 API，需要先了解这个映射的工作方式，首先来看下有状态的 Flink 作业是什么样子的。Flink 作业是由很多算子组成，通常是一个或多个数据源（Source）、一些实际处理数据的算子（比如 Map／Filter／FlatMap 等）和一个或者多个 Sink。每个算子会在一个或者多个任务中并行运行（取决于并行度），并且可以使用不同类型的状态，算子可能会有零个、一个或多个 Operator State，这些状态会组成一个以算子任务为范围的列表。如果是算子应用在 KeyedStream，它还有零个、一个或者多个 Keyed State，它们的作用域范围是从每个已处理数据中提取 Key，可以将 Keyed State 看作是一个分布式的 Map。 State Processor API 现在提供了读取、新增和修改 Savepoint 数据的方法，比如从已加载的 Savepoint 中读取数据集，然后将数据集转换为状态并将其保存到 Savepoint。下面分别讲解下这三种方法该如何使用。 读取现有的 Savepoint读取状态首先需要指定一个 Savepoint（或者 Checkpoint） 的路径和状态后端存储的类型。 12ExecutionEnvironment bEnv = ExecutionEnvironment.getExecutionEnvironment();ExistingSavepoint savepoint = Savepoint.load(bEnv, \"hdfs://path/\", new RocksDBStateBackend()); 读取 Operator State 时，只需指定算子的 uid、状态名称和类型信息。 12345DataSet&lt;Integer&gt; listState = savepoint.readListState(\"zhisheng-uid\", \"list-state\", Types.INT);DataSet&lt;Integer&gt; unionState = savepoint.readUnionState(\"zhisheng-uid\", \"union-state\", Types.INT); DataSet&lt;Tuple2&lt;Integer, Integer&gt;&gt; broadcastState = savepoint.readBroadcastState(\"zhisheng-uid\", \"broadcast-state\", Types.INT, Types.INT); 如果在状态描述符（StateDescriptor）中使用了自定义类型序列化器 TypeSerializer，也可以指定它： 123DataSet&lt;Integer&gt; listState = savepoint.readListState( \"zhisheng-uid\", \"list-state\", Types.INT, new MyCustomIntSerializer()); 写入新的 Savepoint修改现有的 Savepoint6.3.5 为什么要使用 DataSet API？6.3.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/nMR7ufq","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 使用 Flink CEP 处理复杂事件","date":"2021-07-28T16:00:00.000Z","path":"2021/07/29/flink-in-action-6.2/","text":"6.2 使用 Flink CEP 处理复杂事件6.1 节中介绍 Flink CEP 和其使用场景，本节将详细介绍 Flink CEP 的 API，教会大家如何去使用 Flink CEP。 6.2.1 准备依赖要开发 Flink CEP 应用程序，首先你得在项目的 pom.xml 中添加依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 这个依赖有两种，一个是 Java 版本的，一个是 Scala 版本，你可以根据项目的开发语言自行选择。 6.2.2 Flink CEP 入门应用程序准备好依赖后，我们开始第一个 Flink CEP 应用程序，这里我们只做一个简单的数据流匹配，当匹配成功后将匹配的两条数据打印出来。首先定义实体类 Event 如下： 1234public class Event &#123; private Integer id; private String name;&#125; 然后构造读取 Socket 数据流将数据进行转换成 Event，代码如下： 123456789101112SingleOutputStreamOperator&lt;Event&gt; eventDataStream = env.socketTextStream(\"127.0.0.1\", 9200) .flatMap(new FlatMapFunction&lt;String, Event&gt;() &#123; @Override public void flatMap(String s, Collector&lt;Event&gt; collector) throws Exception &#123; if (StringUtil.isNotEmpty(s)) &#123; String[] split = s.split(\",\"); if (split.length == 2) &#123; collector.collect(new Event(Integer.valueOf(split[0]), split[1])); &#125; &#125; &#125; &#125;); 接着就是定义 CEP 中的匹配规则了，下面的规则表示第一个事件的 id 为 42，紧接着的第二个事件 id 要大于 10，满足这样的连续两个事件才会将这两条数据进行打印出来。 12345678910111213141516171819202122232425262728Pattern&lt;Event, ?&gt; pattern = Pattern.&lt;Event&gt;begin(\"start\").where( new SimpleCondition&lt;Event&gt;() &#123; @Override public boolean filter(Event event) &#123; log.info(\"start &#123;&#125;\", event.getId()); return event.getId() == 42; &#125; &#125;).next(\"middle\").where( new SimpleCondition&lt;Event&gt;() &#123; @Override public boolean filter(Event event) &#123; log.info(\"middle &#123;&#125;\", event.getId()); return event.getId() &gt;= 10; &#125; &#125;);CEP.pattern(eventDataStream, pattern).select(new PatternSelectFunction&lt;Event, String&gt;() &#123; @Override public String select(Map&lt;String, List&lt;Event&gt;&gt; p) throws Exception &#123; StringBuilder builder = new StringBuilder(); log.info(\"p = &#123;&#125;\", p); builder.append(p.get(\"start\").get(0).getId()).append(\",\").append(p.get(\"start\").get(0).getName()).append(\"\\n\") .append(p.get(\"middle\").get(0).getId()).append(\",\").append(p.get(\"middle\").get(0).getName()); return builder.toString(); &#125;&#125;).print();//打印结果 然后笔者在终端开启 Socket，输入的两条数据如下： 1242,zhisheng20,zhisheng 作业打印出来的日志如下图所示： 整个作业 print 出来的结果如下图所示： 好了，一个完整的 Flink CEP 应用程序如上，相信你也能大概理解上面的代码，接着来详细的讲解一下 Flink CEP 中的 Pattern API。 6.2.3 Pattern API你可以通过 Pattern API 去定义从流数据中匹配事件的 Pattern，每个复杂 Pattern 都是由多个简单的 Pattern 组成的，拿前面入门的应用来讲，它就是由 start 和 middle 两个简单的 Pattern 组成的，在其每个 Pattern 中都只是简单的处理了流数据。在处理的过程中需要标示该 Pattern 的名称，以便后续可以使用该名称来获取匹配到的数据，如 p.get(&quot;start&quot;).get(0) 它就可以获取到 Pattern 中匹配的第一个事件。接下来我们先来看下简单的 Pattern 。 单个 Pattern数量 条件 组合 PatternGroup Pattern事件匹配跳过策略6.2.4 检测 Pattern选择 Pattern6.2.5 CEP 时间属性根据事件时间处理延迟数据时间上下文加入知识星球可以看到上面文章：https://t.zsxq.com/nMR7ufq 6.2.6 小结与反思本节开始通过一个 Flink CEP 案例教大家上手，后面通过讲解 Flink CEP 的 Pattern API，更多详细的还是得去看官网文档，其实也建议大家好好的跟着官网的文档过一遍所有的 API，并跟着敲一些样例来实现，这样在开发需求的时候才能够及时的想到什么场景下该使用哪种 API，接着教了大家如何将 Pattern 与数据流结合起来匹配并获取匹配的数据，最后讲了下 CEP 中的时间概念。 你公司有使用 Flink CEP 吗？通常使用哪些 API 居多？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink CEP 简介及其使用场景","date":"2021-07-27T16:00:00.000Z","path":"2021/07/28/flink-in-action-6.1/","text":"第六章 —— 扩展库Flink 源码中有单独的 flink-libraries 模块用来存放一些扩展库，比如 CEP、Gelly、Machine Learning、State Processor API。本章将分别介绍这几种扩展库以及如何应用在我们的项目中。 6.1 Flink CEP 简介及其使用场景 Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案，它可以在实时数据中匹配复杂事件，用处很广，本节将带大家了解其功能和应用场景。 6.1.1 CEP 简介？CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。它可以用于处理实时数据并在事件流到达时从事件流中提取信息，并根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。 因为这种事件匹配通常是根据提前制定好的规则去匹配的，而这些规则一般来说不仅多，而且复杂，所以就会引入一些规则引擎来处理这种复杂事件匹配。市面上常用的规则引擎有如下这些。 6.1.2 规则引擎对比目前开源的规则引擎有很多种，接下来将对比一下 Drools、Aviator、EasyRules、Esper、Flink CEP 之间的优势和劣势。 DroolsDrools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。 优势: 功能较为完善，具有如系统监控、操作平台等功能。 规则支持动态更新。 劣势: 以内存实现时间窗功能，无法支持较长跨度的时间窗。 无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。 AviatorAviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。 优势： 支持大部分运算操作符。 支持函数调用和自定义函数。 支持正则表达式匹配。 支持传入变量并且性能优秀。 劣势： 没有 if else、do while 等语句，没有赋值语句，没有位运算符。 EasyRulesEasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。 优势： 轻量级框架，学习成本低。 基于 POJO。 为定义业务引擎提供有用的抽象和简便的应用 支持从简单的规则组建成复杂规则 EsperEsper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。 优势: 轻量级可嵌入开发，常用的 CEP 功能简单好用。 EPL 语法与 SQL 类似，学习成本较低。 劣势: 单机全内存方案，需要整合其他分布式和存储。 以内存实现时间窗功能，无法支持较长跨度的时间窗。 无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。 Flink CEPFlink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。 优势: 继承了 Flink 高吞吐的特点 事件支持存储到外部，可以支持较长跨度的时间窗。 可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现） 劣势: 无法动态更新规则（痛点） 6.1.3 Flink CEP 简介6.1.4 Flink CEP 动态更新规则6.1.5 Flink CEP 使用场景分析实时反作弊和风控实时营销实时网络攻击检测6.1.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/nMR7ufq","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Table & SQL 概念与通用 API","date":"2021-07-26T16:00:00.000Z","path":"2021/07/27/flink-in-action-5.1/","text":"第五章 —— Table API &amp; SQLFlink 中除了 DataStream 和 DataSet API，还有比较高级的 Table API &amp; SQL，它可以帮助我们简化开发的过程，能够快读的运用 Flink 去完成一些需求，本章将对 Flink Table API &amp; SQL 进行讲解，并将与其他的 API 结合对比分析。 5.1 Flink Table &amp; SQL 概念与通用 API 前面的内容都是讲解 DataStream 和 DataSet API 相关的，在 1.2.5 节中讲解 Flink API 时提及到 Flink 的高级 API —— Table API &amp; SQL，本节将开始 Table &amp; SQL 之旅。 5.1.1 新增 Blink SQL 查询处理器在 Flink 1.9 版本中，合进了阿里巴巴开源的 Blink 版本中的大量代码，其中最重要的贡献就是 Blink SQL 了。在 Blink 捐献给 Apache Flink 之后，社区就致力于为 Table API &amp; SQL 集成 Blink 的查询优化器和 runtime。先来看下 1.8 版本的 Flink Table 项目结构如下图所示。 1.9 版本的 Flink Table 项目结构图如下图所示： 可以发现新增了 flink-sql-parser、flink-table-planner-blink、flink-table-runtime-blink、flink-table-uber-blink 模块，对 Flink Table 模块的重构详细内容可以参考 FLIP-32。这样对于 Java 和 Scala API 模块、优化器以及 runtime 模块来说，分层更清楚，接口更明确。 另外 flink-table-planner-blink 模块中实现了新的优化器接口，所以现在有两个插件化的查询处理器来执行 Table API &amp; SQL：1.9 以前的 Flink 处理器和新的基于 Blink 的处理器。基于 Blink 的查询处理器提供了更好的 SQL 覆盖率、支持更广泛的查询优化、改进了代码生成机制、通过调优算子的实现来提升批处理查询的性能。除此之外，基于 Blink 的查询处理器还提供了更强大的流处理能力，包括了社区一些非常期待的新功能（如维表 Join、TopN、去重）和聚合场景缓解数据倾斜的优化，以及内置更多常用的函数，具体可以查看 flink-table-runtime-blink 代码。目前整个模块的结构如下图所示： 注意：两个查询处理器之间的语义和功能大部分是一致的，但未完全对齐，因为基于 Blink 的查询处理器还在优化中，所以在 1.9 版本中默认查询处理器还是 1.9 之前的版本。如果你想使用 Blink 处理器的话，可以在创建 TableEnvironment 时通过 EnvironmentSettings 配置启用。被选择的处理器必须要在正在执行的 Java 进程的类路径中。对于集群设置，默认两个查询处理器都会自动地加载到类路径中。如果要在 IDE 中运行一个查询，需要在项目中添加 planner 依赖。 5.1.2 为什么选择 Table API &amp; SQL？在 1.2 节中介绍了 Flink 的 API 是包含了 Table API &amp; SQL，在 1.3 节中也介绍了在 Flink 1.9 中阿里开源的 Blink 分支中的很强大的 SQL 功能合并进 Flink 主分支，另外通过阿里 Blink 相关的介绍，可以知道阿里在 SQL 功能这块是做了很多的工作。从前面章节的内容可以发现 Flink 的 DataStream／DataSet API 的功能已经很全并且很强大了，常见复杂的数据处理问题也都可以处理，那么社区为啥还在一直推广 Table API &amp; SQL 呢？ 其实通过观察其它的大数据组件，就不会好奇了，比如 Spark、Storm、Beam、Hive 、KSQL（面向 Kafka 的 SQL 引擎）、Elasticsearch、Phoenix（使用 SQL 进行 HBase 数据的查询）等，可以发现 SQL 已经成为各个大数据组件必不可少的数据查询语言，那么 Flink 作为一个大数据实时处理引擎，笔者对其支持 SQL 查询流数据也不足为奇了，但是还是来稍微介绍一下 Table API &amp; SQL。 Table API &amp; SQL 是一种关系型 API，用户可以像操作数据库一样直接操作流数据，而不再需要通过 DataStream API 来写很多代码完成计算需求，更不用手动去调优你写的代码，另外 SQL 最大的优势在于它是一门学习成本很低的语言，普及率很高，用户基数大，和其他的编程语言相比，它的入门相对简单。 除了上面的原因，还有一个原因是：可以借助 Table API &amp; SQL 统一流处理和批处理，因为在 DataStream／DataSet API 中，用户开发流作业和批作业需要去了解两种不同的 API，这对于公司有些开发能力不高的数据分析师来说，学习成本有点高，他们其实更擅长写 SQL 来分析。Table API &amp; SQL 做到了批与流上的查询具有同样的语法语义，因此不用改代码就能同时在批和流上执行。 总结来说，为什么选择 Table API &amp; SQL： 声明式语言表达业务逻辑 无需代码编程 —— 易于上手 查询能够被有效的优化 查询可以高效的执行 5.1.3 Flink Table 项目模块在上文中提及到 Flink Table 在 1.8 和 1.9 的区别，这里还是要再讲解一下这几个依赖，因为只有了解清楚了之后，我们在后面开发的时候才能够清楚挑选哪种依赖。它有如下几个模块： flink-table-common：table 中的公共模块，可以用于通过自定义 function，format 等来扩展 Table 生态系统 flink-table-api-java：支持使用 Java 语言，纯 Table＆SQL API flink-table-api-scala：支持使用 Scala 语言，纯 Table＆SQL API flink-table-api-java-bridge：支持使用 Java 语言，包含 DataStream/DataSet API 的 Table＆SQL API（推荐使用） flink-table-api-scala-bridge：支持使用 Scala 语言，带有 DataStream/DataSet API 的 Table＆SQL API（推荐使用） flink-sql-parser：SQL 语句解析层，主要依赖 calcite flink-table-planner：Table 程序的 planner 和 runtime flink-table-uber：将上诉模块打成一个 fat jar，在 lib 目录下 flink-table-planner-blink：Blink 的 Table 程序的 planner（阿里开源的版本） flink-table-runtime-blink：Blink 的 Table 程序的 runtime（阿里开源的版本） flink-table-uber-blink：将 Blink 版本的 planner 和 runtime 与前面模块（除 flink-table-planner 模块）打成一个 fat jar，在 lib 目录下，如下图所示。 flink-sql-client：SQL 客户端 5.1.4 两种 planner 之间的区别上面讲了两种不同的 planner 之间包含的模块有点区别，但是具体有什么区别如下所示： Blink planner 将批处理作业视为流的一种特殊情况。因此不支持 Table 和 DataSet 之间的转换，批处理作业会转换成 DataStream 程序，而不会转换成 DataSet 程序，流作业还是转换成 DataStream 程序。 Blink planner 不支持 BatchTableSource，而是使用有界的（bounded） StreamTableSource 代替它。 Blink planner 仅支持全新的 Catalog，不支持已经废弃的 ExternalCatalog。 以前的 planner 中 FilterableTableSource 的实现与现在的 Blink planner 有冲突，在以前的 planner 中是叠加 PlannerExpressions（在未来的版本中会移除），而在 Blink planner 中是 Expressions。 基于字符串的 KV 键值配置选项仅可以在 Blink planner 中使用。 PlannerConfig 的实现（CalciteConfig）在两种 planner 中不同。 Blink planner 会将多个 sink 优化在同一个 DAG 中（只在 TableEnvironment 中支持，StreamTableEnvironment 中不支持），而以前的 planner 是每个 sink 都有一个 DAG 中，相互独立的。 以前的 planner 不支持 catalog 统计，而 Blink planner 支持。 在了解到了两种 planner 的区别后，接下来开始 Flink Table API &amp; SQL 之旅。 5.1.5 添加项目依赖因为在 Flink 1.9 版本中有两个 planner，所以得根据你使用的 planner 来选择对应的依赖，假设你选择的是最新的 Blink 版本，那么添加下面的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 如果是以前的 planner，则使用下面这个依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 如果要自定义 format 格式或者自定义 function，则需要添加 flink-table-common 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 5.1.6 创建一个 TableEnvironmentTableEnvironment 是 Table API 和 SQL 的统称，它负责的内容有： 在内部的 catalog 注册 Table 注册一个外部的 catalog 执行 SQL 查询 注册用户自定义的 function 将 DataStream 或者 DataSet 转换成 Table 保持对 ExecutionEnvironment 和 StreamExecutionEnvironment 的引用 Table 总是会绑定在一个指定的 TableEnvironment，不能在同一个查询中组合不同 TableEnvironment 的 Table，比如 join 或 union 操作。你可以使用下面的几种静态方法创建 TableEnvironment。 1234567891011121314151617181920212223//创建 StreamTableEnvironmentstatic StreamTableEnvironment create(StreamExecutionEnvironment executionEnvironment) &#123; return create(executionEnvironment, EnvironmentSettings.newInstance().build());&#125;static StreamTableEnvironment create(StreamExecutionEnvironment executionEnvironment, EnvironmentSettings settings) &#123; return StreamTableEnvironmentImpl.create(executionEnvironment, settings, new TableConfig());&#125;/** @deprecated */@Deprecatedstatic StreamTableEnvironment create(StreamExecutionEnvironment executionEnvironment, TableConfig tableConfig) &#123; return StreamTableEnvironmentImpl.create(executionEnvironment, EnvironmentSettings.newInstance().build(), tableConfig);&#125;//创建 BatchTableEnvironmentstatic BatchTableEnvironment create(ExecutionEnvironment executionEnvironment) &#123; return create(executionEnvironment, new TableConfig());&#125;static BatchTableEnvironment create(ExecutionEnvironment executionEnvironment, TableConfig tableConfig) &#123; //&#125; 你需要根据你的程序来使用对应的 TableEnvironment，是 BatchTableEnvironment 还是 StreamTableEnvironment。默认两个 planner 都是在 Flink 的安装目录下 lib 文件夹中存在的，所以应该在你的程序中指定使用哪种 planner。 1234567891011121314151617181920212223242526272829// Flink Streaming queryimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);//或者 TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);// Flink Batch queryimport org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.table.api.java.BatchTableEnvironment;ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);// Blink Streaming queryimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);//或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);// Blink Batch queryimport org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings); 如果在 lib 目录下只存在一个 planner，则可以使用 useAnyPlanner 来创建指定的 EnvironmentSettings。 5.1.7 Table API &amp; SQL 应用程序的结构批处理和流处理的 Table API &amp; SQL 作业都有相同的模式，它们的代码结构如下： 123456789101112131415161718192021//根据前面内容创建一个 TableEnvironment，指定是批作业还是流作业TableEnvironment tableEnv = ...; //用下面的其中一种方式注册一个 TabletableEnv.registerTable(\"table1\", ...) tableEnv.registerTableSource(\"table2\", ...); tableEnv.registerExternalCatalog(\"extCat\", ...);//注册一个 TableSinktableEnv.registerTableSink(\"outputTable\", ...);//根据一个 Table API 查询创建一个 TableTable tapiResult = tableEnv.scan(\"table1\").select(...);//根据一个 SQL 查询创建一个 TableTable sqlResult = tableEnv.sqlQuery(\"SELECT ... FROM table2 ... \");//将 Table API 或者 SQL 的结果发送给 TableSinktapiResult.insertInto(\"outputTable\");//运行tableEnv.execute(\"java_job\"); 5.1.8 Catalog 中注册 TableTable 有两种类型，输入表和输出表，可以在 Table API &amp; SQL 查询中引用输入表并提供输入数据，输出表可以用于将 Table API &amp; SQL 的查询结果发送到外部系统。输出表可以通过 TableSink 来注册，输入表可以从各种数据源进行注册： 已经存在的 Table 对象，通过是 Table API 或 SQL 查询的结果 连接了外部系统的 TableSource，比如文件、数据库、MQ 从 DataStream 或 DataSet 程序中返回的 DataStream 和 DataSet 注册 Table在 TableEnvironment 中可以像下面这样注册一个 Table： 12345678//创建一个 TableEnvironmentTableEnvironment tableEnv = ...; // see \"Create a TableEnvironment\" section//projTable 是一个简单查询的结果Table projTable = tableEnv.scan(\"X\").select(...);//将 projTable 表注册为 projectedTable 表tableEnv.registerTable(\"projectedTable\", projTable); 注册 TableSource注册 TableSink5.1.9 注册外部的 Catalog5.1.10 查询 TableTable APISQLTable API &amp; SQL5.1.11 提交 Table5.1.12 翻译并执行查询加入知识星球可以看到上面文章：https://t.zsxq.com/MNBEYvf 5.1.13 小结与反思本节介绍了 Flink 新的 planner，然后详细的和之前的 planner 做了对比，然后对 Table API &amp; SQL 中的概念做了介绍，还通过样例去介绍了它们的通用 API。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Table API & SQL 功能","date":"2021-07-26T16:00:00.000Z","path":"2021/07/27/flink-in-action-5.2/","text":"5.2 Flink Table API &amp; SQL 功能在 5.1 节中对 Flink Table API &amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。 5.2.1 Flink Table 和 SQL 与 DataStream 和 DataSet 集成两个 planner 都可以与 DataStream API 集成，只有以前的 planner 才可以集成 DataSet API，所以下面讨论 DataSet API 都是和以前的 planner 有关。 Table API &amp; SQL 查询与 DataStream 和 DataSet 程序集成是非常简单的，比如可以通过 Table API 或者 SQL 查询外部表数据，进行一些预处理后，然后使用 DataStream 或 DataSet API 继续处理一些复杂的计算，另外也可以将 DataStream 或 DataSet 处理后的数据利用 Table API 或者 SQL 写入到外部表去。总而言之，它们之间互相转换或者集成比较容易。 Scala 的隐式转换Scala Table API 提供了 DataSet、DataStream 和 Table 类的隐式转换，可以通过导入 org.apache.flink.table.api.scala._ 或者 org.apache.flink.api.scala._ 包来启用这些转换。 将 DataStream 或 DataSet 注册为 TableDataStream 或者 DataSet 可以注册为 Table，结果表的 schema 取决于已经注册的 DataStream 和 DataSet 的数据类型。你可以像下面这种方式转换： 123456789StreamTableEnvironment tableEnv = ...;DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...//将 DataStream 注册为 myTable 表tableEnv.registerDataStream(\"myTable\", stream);//将 DataStream 注册为 myTable2 表（表中的字段为 myLong、myString）tableEnv.registerDataStream(\"myTable2\", stream, \"myLong, myString\"); 将 DataStream 或 DataSet 转换为 Table除了可以将 DataStream 或 DataSet 注册为 Table，还可以将它们转换为 Table，代码如下所示，转换之后再去使用 Table API 查询就比较方便了。 123456789StreamTableEnvironment tableEnv = ...;DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...//将 DataStream 转换成 TableTable table1 = tableEnv.fromDataStream(stream);//将 DataStream 转换成 TableTable table2 = tableEnv.fromDataStream(stream, \"myLong, myString\"); 将 Table 转换成 DataStream 或 DataSetTable 可以转换为 DataStream 或 DataSet，这样就可以在 Table API 或 SQL 查询的结果上运行自定义的 DataStream 或 DataSet 程序。当将一个 Table 转换成 DataStream 或 DataSet 时，需要指定结果 DataStream 或 DataSet 的数据类型，最方便的数据类型是 Row，下面几个数据类型表示不同的功能： Row：字段按位置映射，任意数量的字段，支持 null 值，没有类型安全访问。 POJO：字段按名称映射，POJO 属性必须按照 Table 中的属性来命名，任意数量的字段，支持 null 值，类型安全访问 Case Class：字段按位置映射，不支持 null 值，类型安全访问。 Tuple：按位置映射字段，限制为 22（Scala）或 25（Java）字段，不支持 null 值，类型安全访问。 原子类型：Table 必须具有单个字段，不支持 null 值，类型安全访问。 将 Table 转换成 DataStream流查询的结果表会动态更新，即每个新的记录到达输入流时结果就会发生变化。所以在将 Table 转换成 DataStream 就需要对表的更新进行编码，有两种将 Table 转换为 DataStream 的模式： 追加模式(Append Mode)：这种模式只能在动态表仅通过 INSERT 更改修改时才能使用，即仅追加，之前发出的结果不会更新。 撤回模式(Retract Mode)：任何时刻都可以使用此模式，它使用一个 boolean 标志来编码 INSERT 和 DELETE 的更改。 两种模式的代码如下所示： 1234567891011121314StreamTableEnvironment tableEnv = ...;//有两个字段(name、age) 的 TableTable table = ...//通过指定类，将表转换为一个 append DataStreamDataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);//将表转换为 Tuple2&lt;String, Integer&gt; 的 append DataStreamTupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);//将表转换为一个 Retract DataStream RowDataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class); 将 Table 转换成 DataSet将 Table 转换成 DataSet 的样例如下： 1234567891011BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);//有两个字段(name、age) 的 TableTable table = ...//通过指定一个类将表转换为一个 Row DataSetDataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);//将表转换为 Tuple2&lt;String, Integer&gt; 的 DataSetTupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType); 5.2.2 查询优化Flink 使用 Calcite 来优化和翻译查询，以前的 planner 不会去优化 join 的顺序，而是按照查询中定义的顺序去执行。通过提供一个 CalciteConfig 对象来调整在不同阶段应用的优化规则集，这个可以通过调用 CalciteConfig.createBuilder() 获得的 builder 来创建，并且可以通过调用tableEnv.getConfig.setCalciteConfig(calciteConfig) 来提供给 TableEnvironment。而在 Blink planner 中扩展了 Calcite 来执行复杂的查询优化，这包括一系列基于规则和成本的优化，比如： 基于 Calcite 的子查询去相关性 Project pruning Partition pruning Filter push-down 删除子计划中的重复数据以避免重复计算 重写特殊的子查询，包括两部分： 将 IN 和 EXISTS 转换为 left semi-joins 将 NOT IN 和 NOT EXISTS 转换为 left anti-join 重排序可选的 join 通过启用 table.optimizer.join-reorder-enabled 注意：IN/EXISTS/NOT IN/NOT EXISTS 目前只支持子查询重写中的连接条件。 解释 TableTable API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。你可以通过 TableEnvironment.explain(table) 或者 TableEnvironment.explain() 方法来完成。explain(table) 会返回给定计划的 Table，explain() 会返回多路 Sink 计划的结果（主要用于 Blink planner）。它返回一个描述三个计划的字符串： 关系查询的抽象语法树，即未优化的逻辑查询计划 优化的逻辑查询计划 实际执行计划 以下代码演示了一个 Table 示例： 1234567891011StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(new Tuple2&lt;&gt;(1, \"hello\"));DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(new Tuple2&lt;&gt;(1, \"hello\"));Table table1 = tEnv.fromDataStream(stream1, \"count, word\");Table table2 = tEnv.fromDataStream(stream2, \"count, word\");Table table = table1.where(\"LIKE(word, 'F%')\").unionAll(table2);System.out.println(tEnv.explain(table)); 通过 explain(table) 方法返回的结果： 123456789101112131415161718192021222324252627282930== Abstract Syntax Tree ==LogicalUnion(all=[true]) LogicalFilter(condition=[LIKE($1, _UTF-16LE&apos;F%&apos;)]) FlinkLogicalDataStreamScan(id=[1], fields=[count, word]) FlinkLogicalDataStreamScan(id=[2], fields=[count, word])== Optimized Logical Plan ==DataStreamUnion(all=[true], union all=[count, word]) DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE&apos;F%&apos;)]) DataStreamScan(id=[1], fields=[count, word]) DataStreamScan(id=[2], fields=[count, word])== Physical Execution Plan ==Stage 1 : Data Source content : collect elements with CollectionInputFormatStage 2 : Data Source content : collect elements with CollectionInputFormat Stage 3 : Operator content : from: (count, word) ship_strategy : REBALANCE Stage 4 : Operator content : where: (LIKE(word, _UTF-16LE&apos;F%&apos;)), select: (count, word) ship_strategy : FORWARD Stage 5 : Operator content : from: (count, word) ship_strategy : REBALANCE 5.2.3 数据类型5.2.4 时间属性Processing TimeEvent time5.2.5 SQL Connector使用代码 使用 YAML 文件 使用 DDL 5.2.6 SQL Client5.2.7 Hive加入知识星球可以看到上面文章：https://t.zsxq.com/MNBEYvf 5.2.8 小结与反思本章节继续介绍了 Flink Table API &amp; SQL 中的部分 API，然后讲解了 Flink 之前的 planner 和 Blink planner 在某些特性上面的区别，还讲解了 SQL Connector，最后介绍了 SQL Client 和 Hive。 本章讲解了 Flink Table API &amp; SQL 相关的概述，另外还介绍了它们的 API 使用方式，除此之外还对 Connectors、SQL Client、Hive 做了一定的讲解。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Checkpoint 和 Savepoint 的区别及其配置使用","date":"2021-07-25T16:00:00.000Z","path":"2021/07/26/flink-in-action-4.3/","text":"4.3 Flink Checkpoint 和 Savepoint 的区别及其配置使用Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。 4.3.1 Checkpoint 简介及使用在 Flink 任务运行过程中，为了保障故障容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。 Flink 的 Checkpoint 有以下先决条件： 需要具有持久性且支持重放一定时间范围内数据的数据源。例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。 需要一个能保存状态的持久化存储介质，例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。 Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。下面来介绍 Checkpoint 具体如何使用。 首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。Checkpoint 配置相关的 Java 代码如下所示： 12345678910111213141516171819202122232425262728StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutinEnvironment();// 开启 Checkpoint，每 1000毫秒进行一次 Checkpointenv.enableCheckpointing(1000);// Checkpoint 语义设置为 EXACTLY_ONCEenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// CheckPoint 的超时时间env.getCheckpointConfig().setCheckpointTimeout(60000);// 同一时间，只允许 有 1 个 Checkpoint 在发生env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复env.getCheckpointConfig().setPreferCheckpointForRecovery(true);// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）env.getCheckpointConfig().setTolerableCheckpointFailureNumber(1);// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(true) 以上 Checkpoint 相关的参数描述如下所示： Checkpoint 语义： EXACTLY_ONCE 或 AT_LEAST_ONCE，EXACTLY_ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT_LEAST_ONCE 表示要消费的数据至少处理一次，可能会重复消费。 Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。 同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。 两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。 当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为： ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。 ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。 任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。 作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。 关于 Checkpoint 时，状态后端相关的配置请参阅本书 4.2 节。 4.3.2 Savepoint 简介及使用Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 uid(String) 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。 12345678910DataStream&lt;String&gt; stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(\"source-id\") // ID for the source operator .shuffle() // Stateful mapper with ID .map(new StatefulMapper()) .uid(\"mapper-id\") // ID for the mapper // Stateless printing sink .print(); // Auto-generated ID 如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 uid(String) 手动指定了 Operator ID，就不会存在 上述问题了。 Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示： 1bin/flink savepoint :jobId [:targetDirectory] 这将触发 ID 为 :jobId 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。 使用 YARN 触发 Savepoint 的方式如下所示： 1bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId 这将触发 ID 为 :jobId 和 YARN 应用程序 ID :yarnAppId 的作业进行 Savepoint，并返回创建的 Savepoint 路径。 使用 Savepoint 取消 Flink 任务： 1bin/flink cancel -s [:targetDirectory] :jobId 这将自动触发 ID 为 :jobid 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。 4.3.3 Savepoint 与 Checkpoint 的区别前面分别介绍了 Savepoint 和 Checkpoint，可以发现它们有不少相似之处，下面来看下它们之间的区别。 Checkpoint Savepoint 由 Flink 的 JobManager 定时自动触发并管理 由用户手动触发并管理 主要用于任务发生故障时，为任务提供给自动恢复机制 主要用户升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复 当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照 仅支持全量快照 Flink 任务停止后，Checkpoint 的状态快照信息默认被清除 一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除 Checkpoint 设计目标：轻量级且尽可能快地恢复任务 Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作 4.3.4 Checkpoint 流程Flink 任务 Checkpoint 的详细流程如下面流程所示。 JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier，如下图所示。 当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中，如下图所示。 如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中 为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照 注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分 当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除 如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束，如下图所示。 如果对上述 Checkpoint 过程不理解，在后续 9.5 节 Flink 如何保障 Exactly Once 中会详细介绍 Flink 的 Checkpoint 过程以及为什么这么做。 基于 RocksDB 的增量 Checkpoint 实现原理当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下图所示。 第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。 基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。 基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。 4.3.5 如何从 Checkpoint 中恢复状态4.3.6 如何从 Savepoint 中恢复状态4.3.7 如何优雅地删除 Checkpoint 目录4.3.8 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RNJeqFy 本章属于属于本书的进阶篇，在第一节中讲解了 State 的各种类型，包括每种 State 的使用方式、实现原理和部分源码剖析，在第二节中介绍 Flink 中的状态后端存储的使用方式，对比分析了现有的几种方案的使用场景，在第三节中介绍了 Checkpoint 和 Savepoint 的区别，如果从状态中恢复作业，以及整个作业的 Checkpoint 流程。 本章的内容是 Flink 作业开发的重点，通常线上遇到 State、Checkpoint、Savepoint 的问题比较多，所以希望你能好好的弄清楚它们的原理，这样在出问题后才能够对症下药去解决问题。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 状态后端存储","date":"2021-07-24T16:00:00.000Z","path":"2021/07/25/flink-in-action-4.2/","text":"4.2 Flink 状态后端存储在 4.1 节中介绍了 Flink 中的状态，那么在生产环境中，随着作业的运行时间变长，状态会变得越来越大，那么如何将这些状态存储也是 Flink 要解决的一大难点，本节来讲解下 Flink 中不同类型的状态后端存储。 4.2.1 State Backends当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，为此在 Flink 源码中还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置： state.backend: 用于存储和进行状态 Checkpoint 的状态后端存储方式，无默认值 state.checkpoints.num-retained: 要保留的已完成 Checkpoint 的最大数量，默认值为 1 state.backend.async: 状态后端是否使用异步快照方法，默认值为 true state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用 taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录 state.savepoints.dir: 存储 savepoints 的目录 state.checkpoints.dir: 存储 Checkpoint 的数据文件和元数据 state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024 虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下： 1234567StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(new MemoryStateBackend()); //设置堆内存存储//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints)); //设置文件存储//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints)); //设置 RocksDB 存储 StateBackend 接口的三种实现类如下图所示： 上面三种方式取一种就好了。但是有三种方式，我们该如何去挑选用哪种去存储状态呢？下面讲讲这三种的特点以及该如何选择。 4.2.2 MemoryStateBackend 的用法及分析如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。如果你细心的话，可以从你的 Job 中看到类似日志如下： 12019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: &apos;null&apos;, savepoints: &apos;null&apos;, asynchronous: TRUE, maxStateSize: 5242880) 上面日志的意思就是说如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 TaskManagers 的内存（JVM 堆）中，当应用程序触发 Checkpoint 时，会将此时的状态进行快照然后存储在 JobManager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制，比如： 不太适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 JobManager 中，如果状态较大的话，那么会使得 JobManager 的内存比较紧张，从而导致 JobManager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。 每个单独的状态大小不能超过最大的 DEFAULT_MAX_STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。 Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。 另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有： TRUE（表示使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题） FALSE（同步） UNDEFINED（默认值） 在构造 MemoryStateBackend 的默认函数时是使用的 UNDEFINED，而不是异步： 123public MemoryStateBackend() &#123; this(null, null, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);//使用的是 UNDEFINED&#125; 网上有人说默认是异步的，这里给大家解释清楚一下，从上面的那条日志打印的确实也是表示异步，但是前提是你对 State 无任何操作，笔者跟了下源码，当你没有配置任何的 state 时，它是会在 StateBackendLoader 类中通过 MemoryStateBackendFactory 来创建的 state 的，如下图所示。 继续跟进 MemoryStateBackendFactory 可以发现他这里创建了一个 MemoryStateBackend 实例并通过 configure 方法进行配置，大概流程代码是： 12345678910111213141516171819202122//MemoryStateBackendFactory 类public MemoryStateBackend createFromConfig(Configuration config, ClassLoader classLoader) &#123; return new MemoryStateBackend().configure(config, classLoader);&#125;//MemoryStateBackend 类中的 config 方法public MemoryStateBackend configure(Configuration config, ClassLoader classLoader) &#123; return new MemoryStateBackend(this, config, classLoader);&#125;//私有的构造方法private MemoryStateBackend(MemoryStateBackend original, Configuration configuration, ClassLoader classLoader) &#123; ... this.asynchronousSnapshots = original.asynchronousSnapshots.resolveUndefined( configuration.getBoolean(CheckpointingOptions.ASYNC_SNAPSHOTS));&#125;//根据 CheckpointingOptions 类中的 ASYNC_SNAPSHOTS 参数进行设置的public static final ConfigOption&lt;Boolean&gt; ASYNC_SNAPSHOTS = ConfigOptions .key(\"state.backend.async\") .defaultValue(true) //默认值就是 true，代表异步 .withDescription(...) 可以发现最终是通过读取 state.backend.async 参数的默认值（true）来配置是否要异步的进行快照，但是如果你手动配置 MemoryStateBackend 的话，利用无参数的构造方法，那么就不是默认异步，如果想使用异步的话，需要利用下面这个构造函数（需要传入一个 boolean 值，true 代表异步，false 代表同步）： 123public MemoryStateBackend(boolean asynchronousSnapshots) &#123; this(null, null, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.fromBoolean(asynchronousSnapshots));&#125; 如果你再细看了这个 MemoryStateBackend 类的话，那么你可能会发现这个构造函数： 123public MemoryStateBackend(@Nullable String checkpointPath, @Nullable String savepointPath) &#123; this(checkpointPath, savepointPath, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);//需要你传入 checkpointPath 和 savepointPath&#125; 这个也是用来创建一个 MemoryStateBackend 的，它需要传入的参数是两个路径（checkpointPath、savepointPath），其中 checkpointPath 是写入 Checkpoint 元数据的路径，savepointPath 是写入 savepoint 的路径。 这个来看看 MemoryStateBackend 的继承关系图可以更明确的知道它是继承自 AbstractFileStateBackend，然后 AbstractFileStateBackend 这个抽象类就是为了能够将状态存储中的数据或者元数据进行文件存储的，如下图所示。 所以 FsStateBackend 和 MemoryStateBackend 都会继承该类。 4.2.3 FsStateBackend 的用法及分析这种状态后端存储也是将工作状态存储在 TaskManager 中的内存（JVM 堆）中，但是 Checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置，比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 TaskManager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。 同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 Checkpoint 的，异步可以避免在状态 Checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX_FILE_STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 state.backend.fs.memory-threshold 来配置）。 那么我们该什么时候使用 FsStateBackend 呢？ 如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合 使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复 使用 FsStateBackend 需要注意的地方有什么呢？ 工作状态仍然是存储在 TaskManager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 TaskManager 的内存 4.2.4 RocksDBStateBackend 的用法及分析4.2.5 如何选择状态后端存储？4.2.6 小结与反思加入知识星球可以看到上面文章： https://t.zsxq.com/RNJeqFy","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 深度讲解 Flink 中的状态","date":"2021-07-23T16:00:00.000Z","path":"2021/07/24/flink-in-action-4.1/","text":"第四章 —— Flink 中的状态及容错机制Flink 对比其他的流处理框架最大的特点是其支持状态，本章将深度的讲解 Flink 中的状态分类，如何在不同的场景使用不同的状态，接着会介绍 Flink 中的多种状态存储，最后会介绍 Checkpoint 和 Savepoint 的使用方式以及如何恢复状态。 4.1 深度讲解 Flink 中的状态 在基础篇中的 1.2 节中介绍了 Flink 是一款有状态的流处理框架。那么大家可能有点疑问，这个状态是什么意思？拿 Flink 最简单的 Word Count 程序来说，它需要不断的对 word 出现的个数进行结果统计，那么后一个结果就需要利用前一个的结果然后再做 +1 的操作，这样前一个计算就需要将 word 出现的次数 count 进行存着（这个 count 那么就是一个状态）然后后面才可以进行累加。 4.1.1 为什么需要 State？对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）： Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费 Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费 上面两种情况用图片描述如下图所示。 为了解决上面两种情况（数据重复消费或者数据没有消费）的发生，那么是不是就得需要个什么东西做个记录将这种数据消费状态，Flink state 就这样诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态），当 Job 因为某种错误或者其他原因导致重启时，就能够从 Checkpoint（定时将 state 做一个全局快照，在 Flink 中，为了能够让 Job 在运行的过程中保证容错性，才会对这些 state 做一个快照，在 4.3 节中会详细讲） 中的 state 数据进行恢复。 4.1.2 State 的种类在 Flink 中有两个基本的 state：Keyed state 和 Operator state，下面来分别介绍一下这两种 State。 4.1.3 Keyed StateKeyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。你可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。逻辑上，Keyed State 总是对应着一个 二元组，在某种程度上，因为每个具体的 key 总是属于唯一一个具体的 parallel-operator-instance（并行操作实例），这种情况下，那么就可以简化认为是 。Keyed State 可以进一步组织成 Key Group，Key Group 是 Flink 重新分配 Keyed State 的最小单元，所以有多少个并行，就会有多少个 Key Group。在执行过程中，每个 keyed operator 的并行实例会处理来自不同 key 的不同 Key Group。 4.1.4 Operator State对 Operator State 而言，每个 operator state 都对应着一个并行实例。Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。 当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。 在 Flink 源码中，在 flink-core module 下的 org.apache.flink.api.common.state 中可以看到 Flink 中所有和 State 相关的类，如下图所示。 4.1.5 Raw State 和 Managed StateKeyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。 原始状态是 Operator（算子）保存它们自己的数据结构中的 state，当 Checkpoint 时，原始状态会以字节流的形式写入进 Checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。 托管状态可以使用 Flink runtime 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink runtime 会对这些状态进行编码然后将它们写入到 Checkpoint 中。 DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在实现 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。 注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。 4.1.6 如何使用托管的 Keyed State托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。 我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的： ValueState: 保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。 ListState: 保存一个值的列表，用 add(T) 或者 addAll(List) 来添加，用 Iterable get() 来获取。 ReducingState: 保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。 AggregatingState: 保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。 FoldingState: 与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。 MapState: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map) 添加，或者使用 get(UK) 获取。 所有类型的状态都有一个 clear() 方法来清除当前的状态。 注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。 需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。 要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。 状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态： ValueState getState(ValueStateDescriptor) ReducingState getReducingState(ReducingStateDescriptor) ListState getListState(ListStateDescriptor) AggregatingState getAggregatingState(AggregatingState) FoldingState getFoldingState(FoldingStateDescriptor) MapState getMapState(MapStateDescriptor) 上面讲了这么多概念，那么来一个例子来看看如何使用状态： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; &#123; //ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; @Override public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; //访问状态的 value 值 Tuple2&lt;Long, Long&gt; currentSum = sum.value(); //更新 count currentSum.f0 += 1; //更新 sum currentSum.f1 += input.f1; //更新状态 sum.update(currentSum); //如果 count 等于 2, 发出平均值并清除状态 if (currentSum.f0 &gt;= 2) &#123; out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); &#125; &#125; @Override public void open(Configuration config) &#123; ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( \"average\", //状态名称 TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), //类型信息 Tuple2.of(0L, 0L)); //状态的默认值 sum = getRuntimeContext().getState(descriptor);//获取状态 &#125;&#125;env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(0) .flatMap(new CountWindowAverage()) .print();//结果会打印出 (1,4) 和 (1,5) 这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。 4.1.7 State TTL(存活时间)随着作业的运行时间变长，作业的状态也会逐渐的变大，那么很有可能就会影响作业的稳定性，这时如果有状态的过期这种功能就可以将历史的一些状态清除，对应在 Flink 中的就是 State TTL，接下来将对其做详细介绍。 State TTL 介绍TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能： 123456789101112import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build(); ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;(\"zhisheng\", String.class);stateDescriptor.enableTimeToLive(ttlConfig); //开启 ttl 上面配置中有几个选项需要注意： 1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。 2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）： StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新 StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新 3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired： StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值 StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回 在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据. 另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。 注意： 状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。 目前仅支持参考 processing time 的 TTL 使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。 TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。 只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。 清除过期 State默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。 注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。 此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置： 1234567import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot() .build(); 此配置不适用于 RocksDB 状态后端中的增量 Checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。 除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理： 12345import org.apache.flink.api.common.state.StateTtlConfig;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInBackground() .build(); 要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。 我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。 在该类中的属性如下图所示： 这些属性的功能如下： DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选 StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选 Time：设置 TTL 的时间，这里有两个参数 unit 和 size CleanupStrategies：TTL 清理策略，在该类中有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL_STATE_SCAN_SNAPSHOT、INCREMENTAL_CLEANUP 和 ROCKSDB_COMPACTION_FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器），该类和其实现类如下图所示。 如果对 State TTL 还有不清楚的可以看看 Flink 源码 flink-runtime module 中的 state ttl 相关的实现类，如下图所示： 4.1.8 如何使用托管的 Operator StateCheckpointedFunctionListCheckpointed4.1.9 Stateful Source Functions4.1.10 Broadcast StateFlink 中的 Broadcast State 在很多场景下也有使用，下面来讲解下其使用方式。 Broadcast State 如何使用使用 Broadcast state 需要注意4.1.11 Queryable State加入知识星球可以看到上面文章： https://t.zsxq.com/ZVByvzN 4.1.12 小结与反思本节一开始讲解了 State 出现的原因，接着讲解了 Flink 中的 State 分类，然后对 Flink 中的每种 State 做了详细的讲解，希望可以好好消化这节的内容。你对本节的内容有什么不理解的地方吗？在使用 State 的过程中有遇到什么问题吗？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 使用 Side Output 分流","date":"2021-07-22T16:00:00.000Z","path":"2021/07/23/flink-in-action-3.12/","text":"3.12 使用 Side Output 分流通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。 3.12.1 使用 Filter 分流使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下： 12345DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env); //从 Kafka 获取到所有的数据流SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; \"machine\".equals(m.getTags().get(\"type\"))); //过滤出机器的数据SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; \"docker\".equals(m.getTags().get(\"type\"))); //过滤出容器的数据SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; \"application\".equals(m.getTags().get(\"type\"))); //过滤出应用的数据SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; \"middleware\".equals(m.getTags().get(\"type\"))); //过滤出中间件的数据 3.12.2 使用 Split 分流先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下： 123456789101112131415161718192021222324252627282930DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env); //从 Kafka 获取到所有的数据流SplitStream&lt;MetricEvent&gt; splitData = data.split(new OutputSelector&lt;MetricEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(MetricEvent metricEvent) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); String type = metricEvent.getTags().get(\"type\"); switch (type) &#123; case \"machine\": tags.add(\"machine\"); break; case \"docker\": tags.add(\"docker\"); break; case \"application\": tags.add(\"application\"); break; case \"middleware\": tags.add(\"middleware\"); break; default: break; &#125; return tags; &#125;&#125;);DataStream&lt;MetricEvent&gt; machine = splitData.select(\"machine\");DataStream&lt;MetricEvent&gt; docker = splitData.select(\"docker\");DataStream&lt;MetricEvent&gt; application = splitData.select(\"application\");DataStream&lt;MetricEvent&gt; middleware = splitData.select(\"middleware\"); 上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ ，当时排查这个问题还查到两个相关的 Flink Issue。 FLINK-5031 Issue FLINK-11084 Issue 这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。 3.12.3 使用 Side Output 分流要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下： 123456789//创建 output tagprivate static final OutputTag&lt;MetricEvent&gt; machineTag = new OutputTag&lt;MetricEvent&gt;(\"machine\") &#123;&#125;;private static final OutputTag&lt;MetricEvent&gt; dockerTag = new OutputTag&lt;MetricEvent&gt;(\"docker\") &#123;&#125;;private static final OutputTag&lt;MetricEvent&gt; applicationTag = new OutputTag&lt;MetricEvent&gt;(\"application\") &#123;&#125;;private static final OutputTag&lt;MetricEvent&gt; middlewareTag = new OutputTag&lt;MetricEvent&gt;(\"middleware\") &#123;&#125;; 定义好 OutputTag 后，可以使用下面几种函数来处理数据： ProcessFunction KeyedProcessFunction CoProcessFunction ProcessWindowFunction ProcessAllWindowFunction 在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示： 12345678910111213141516171819DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env); //从 Kafka 获取到所有的数据流SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(new ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123; @Override public void processElement(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector) throws Exception &#123; String type = metricEvent.getTags().get(\"type\"); switch (type) &#123; case \"machine\": context.output(machineTag, metricEvent); case \"docker\": context.output(dockerTag, metricEvent); case \"application\": context.output(applicationTag, metricEvent); case \"middleware\": context.output(middlewareTag, metricEvent); default: collector.collect(metricEvent); &#125; &#125;&#125;); 好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如： 1234DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag); 这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。 3.12.4 小结与反思本节讲了下 Flink 中将数据分流的三种方式，完整代码的 GitHub 地址：sideoutput 本章全部在介绍 Flink 的技术点，比如多种时间语义的对比分析和应用场景分析、多种灵活的窗口的使用方式及其原理实现、平时开发使用较多的一些算子、深入讲解了 DataStream 中的流类型及其对应的方法实现、如何将 Watermark 与 Window 结合来处理延迟数据、Flink Connector 的使用方式。 因为 Flink 的 Connector 比较多，所以本书只挑选了些平时工作用的比较多的 Connector，比如 Kafka、ElasticSearch、HBase、Redis 等，并教会了大家如何去自定义 Source 和 Sink，这样就算 Flink 中的 Connector 没有你需要的，那么也可以通过这种自定义的方法来实现读取和写入数据。 本章讲解的内容更侧重于在 Flink DataStream 和 Connector 的使用技巧和优化，在讲解这些时还提供了详细的代码实现，目的除了大家可以参考外，其实更期望大家能够自己跟着多动手去实现和优化，这样才可以提高自己的编程水平。另外本章还介绍了自己在使用这些 Connector 时遇到的一些问题，是怎么解决的，也希望大家在工作的时候遇到问题可以静下来自己独立的思考下出现问题的原因是啥，该如何解决，多独立思考后，相信遇到问题后你也能够从容的分析和解决问题。 加入知识星球可以看到更多文章","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Connector —— Redis 的用法","date":"2021-07-21T16:00:00.000Z","path":"2021/07/22/flink-in-action-3.11/","text":"3.11 Flink Connector —— Redis 的用法在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。 3.11.1 安装 Redis首先介绍下 Redis 的的安装和启动运行。 下载安装先从 官网 下载 Redis，然后解压。 1234wget http://download.redis.io/releases/redis-5.0.4.tar.gztar xzf redis-5.0.4.tar.gzcd redis-5.0.4make 通过 HomeBrew 安装1brew install redis 如果需要后台运行 Redis 服务，使用命令： 1brew services start redis 要运行命令，可以直接到 /usr/local/bin 目录下，有： 12redis-serverredis-cli 两个命令，执行 redis-server 可以打开服务端，启动后结果如下图所示： 然后另外开一个终端，运行 redis-cli 命令可以运行客户端，执行后效果如下图所示： 3.11.2 将商品数据发送到 Kafka这里我打算将从 Kafka 读取到所有到商品的信息，然后将商品信息中的 商品ID 和 商品价格 提取出来，然后写入到 Redis 中，供第三方服务根据商品 ID 查询到其对应的商品价格。 首先定义我们的商品类 （其中 id 和 price 字段是我们最后要提取的）为： ProductEvent.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Desc: 商品 * blog：http://www.54tianzhisheng.cn/ * 微信公众号：zhisheng */@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class ProductEvent &#123; /** * Product Id */ private Long id; /** * Product 类目 Id */ private Long categoryId; /** * Product 编码 */ private String code; /** * Product 店铺 Id */ private Long shopId; /** * Product 店铺 name */ private String shopName; /** * Product 品牌 Id */ private Long brandId; /** * Product 品牌 name */ private String brandName; /** * Product name */ private String name; /** * Product 图片地址 */ private String imageUrl; /** * Product 状态（1(上架),-1(下架),-2(冻结),-3(删除)） */ private int status; /** * Product 类型 */ private int type; /** * Product 标签 */ private List&lt;String&gt; tags; /** * Product 价格（以分为单位） */ private Long price;&#125; 然后写个工具类不断的模拟商品数据发往 Kafka，工具类 ProductUtil.java 的代码如下： 1234567891011121314151617181920212223242526public class ProductUtil &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"zhisheng\"; //kafka topic 需要和 flink 程序用同一个 topic public static final Random random = new Random(); public static void main(String[] args) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 1; i &lt;= 10000; i++) &#123; ProductEvent product = ProductEvent.builder().id((long) i) //商品的 id .name(\"product\" + i) //商品 name .price(random.nextLong() / 10000000000000L) //商品价格（以分为单位） .code(\"code\" + i).build(); //商品编码 ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, GsonUtil.toJson(product)); producer.send(record); System.out.println(\"发送数据: \" + GsonUtil.toJson(product)); &#125; producer.flush(); &#125;&#125; 3.11.3 Flink 消费 Kafka 中的商品数据我们需要在 Flink 中消费 Kafka 数据，然后将商品中的两个数据（商品 id 和 price）取出来。先来看下这段 Flink Job 代码： 1234567891011121314151617181920212223public class Main &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL; Properties props = KafkaConfigUtil.buildKafkaProps(parameterTool); SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; product = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( parameterTool.get(METRICS_TOPIC), //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)) .map(string -&gt; GsonUtil.fromJson(string, ProductEvent.class)) //反序列化 JSON .flatMap(new FlatMapFunction&lt;ProductEvent, Tuple2&lt;String, String&gt;&gt;() &#123; @Override public void flatMap(ProductEvent value, Collector&lt;Tuple2&lt;String, String&gt;&gt; out) throws Exception &#123; //收集商品 id 和 price 两个属性 out.collect(new Tuple2&lt;&gt;(value.getId().toString(), value.getPrice().toString())); &#125; &#125;); product.print(); env.execute(\"flink redis connector\"); &#125;&#125; 然后 IDEA 中启动运行 Job，再运行上面的 ProductUtil 发送 Kafka 数据的工具类（注意：也得提前启动 Kafka），运行结果如下图所示。 上图左半部分是工具类发送数据到 Kafka 打印的日志，右半部分是 Job 执行的结果，可以看到它已经将商品的 id 和 price 数据获取到了。 那么接下来我们需要的就是将这种 Tuple2&lt;Long, Long&gt; 格式的 KV 数据写入到 Redis 中去。要将数据写入到 Redis 的话是需要先添加依赖的。 3.11.4 Redis Connector 简介Redis Connector 提供用于向 Redis 发送数据的接口的类。接收器可以使用三种不同的方法与不同类型的 Redis 环境进行通信： 单 Redis 服务器 Redis 集群 Redis Sentinel 添加依赖需要添加 Flink Redis Sink 的 Connector，这个 Redis Connector 官方只有老的版本，后面也一直没有更新，所以可以看到网上有些文章都是添加老的版本的依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.5&lt;/version&gt;&lt;/dependency&gt; 包括该部分的文档都是很早之前的啦，可以查看 flink-docs-release-1.1 redis。 另外在 flink-streaming-redis 也看到一个 Flink Redis Connector 的依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 两个依赖功能都是一样的，我们还是就用官方的那个 Maven 依赖来进行演示。 3.11.5 Flink 写入数据到 Redis3.11.6 项目运行及验证3.11.7 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/zr76I66","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Connector —— HBase 的用法","date":"2021-07-20T16:00:00.000Z","path":"2021/07/21/flink-in-action-3.10/","text":"3.10 Flink Connector —— HBase 的用法HBase 是一个分布式的、面向列的开源数据库，同样，很多公司也有使用该技术存储数据的，本节将对 HBase 做些简单的介绍，以及利用 Flink HBase Connector 读取 HBase 中的数据和写入数据到 HBase 中。 3.10.1 准备环境和依赖下面分别讲解 HBase 的环境安装、配置、常用的命令操作以及添加项目需要的依赖。 HBase 安装如果是苹果系统，可以使用 HomeBrew 命令安装： 1brew install hbase HBase 最终会安装在路径 /usr/local/Cellar/hbase/ 下面，安装版本不同，文件名也不同。 配置 HBase打开 libexec/conf/hbase-env.sh 修改里面的 JAVA_HOME： 12# The java implementation to use. Java 1.7+ required.export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&quot; 根据你自己的 JAVA_HOME 来配置这个变量。 打开 libexec/conf/hbase-site.xml 配置 HBase 文件存储目录: 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;!-- 配置HBase存储文件的目录 --&gt; &lt;value&gt;file:///usr/local/var/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;!-- 配置HBase存储内建zookeeper文件的目录 --&gt; &lt;value&gt;/usr/local/var/zookeeper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.dns.interface&lt;/name&gt; &lt;value&gt;lo0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.dns.interface&lt;/name&gt; &lt;value&gt;lo0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.dns.interface&lt;/name&gt; &lt;value&gt;lo0&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行 HBase执行启动的命令： 1./bin/start-hbase.sh 执行后打印出来的日志如： 1starting master, logging to /usr/local/var/log/hbase/hbase-zhisheng-master-zhisheng.out 验证是否安装成功使用 jps 命令： 12345zhisheng@zhisheng /usr/local/Cellar/hbase/1.2.9/libexec jps91302 HMaster62535 RemoteMavenServer110091471 Jps 出现 HMaster 说明安装运行成功。 启动 HBase Shell执行下面命令： 1./bin/hbase shell 运行结果如下图所示： 停止 HBase执行下面的命令： 1./bin/stop-hbase.sh 运行结果如下图所示： HBase 常用命令HBase 中常用的命令有：list（列出已存在的表）、create（创建表）、put（写数据）、get（读数据）、scan（读数据，读全表）、describe（显示表详情），如下图所示。 简单使用上诉命令的结果如下： 添加依赖在 pom.xml 中添加 HBase 相关的依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-hbase_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt;&lt;/dependency&gt; Flink HBase Connector 中，HBase 不仅可以作为数据源，也还可以写入数据到 HBase 中去，我们先来看看如何从 HBase 中读取数据。 3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据这里我们使用 TableInputFormat 来读取 HBase 中的数据，首先准备数据。 准备数据先往 HBase 中插入五条数据如下： 12345put &apos;zhisheng&apos;, &apos;first&apos;, &apos;info:bar&apos;, &apos;hello&apos;put &apos;zhisheng&apos;, &apos;second&apos;, &apos;info:bar&apos;, &apos;zhisheng001&apos;put &apos;zhisheng&apos;, &apos;third&apos;, &apos;info:bar&apos;, &apos;zhisheng002&apos;put &apos;zhisheng&apos;, &apos;four&apos;, &apos;info:bar&apos;, &apos;zhisheng003&apos;put &apos;zhisheng&apos;, &apos;five&apos;, &apos;info:bar&apos;, &apos;zhisheng004&apos; scan 整个 zhisheng 表的话，有五条数据，运行结果如下图所示： Flink Job 代码Flink 读取 HBase 数据的程序代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Desc: 读取 HBase 数据 */public class HBaseReadMain &#123; //表名 public static final String HBASE_TABLE_NAME = \"zhisheng\"; // 列族 static final byte[] INFO = \"info\".getBytes(ConfigConstants.DEFAULT_CHARSET); //列名 static final byte[] BAR = \"bar\".getBytes(ConfigConstants.DEFAULT_CHARSET); public static void main(String[] args) throws Exception &#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.createInput(new TableInputFormat&lt;Tuple2&lt;String, String&gt;&gt;() &#123; private Tuple2&lt;String, String&gt; reuse = new Tuple2&lt;String, String&gt;(); @Override protected Scan getScanner() &#123; Scan scan = new Scan(); scan.addColumn(INFO, BAR); return scan; &#125; @Override protected String getTableName() &#123; return HBASE_TABLE_NAME; &#125; @Override protected Tuple2&lt;String, String&gt; mapResultToTuple(Result result) &#123; String key = Bytes.toString(result.getRow()); String val = Bytes.toString(result.getValue(INFO, BAR)); reuse.setField(key, 0); reuse.setField(val, 1); return reuse; &#125; &#125;).filter(new FilterFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public boolean filter(Tuple2&lt;String, String&gt; value) throws Exception &#123; return value.f1.startsWith(\"zhisheng\"); &#125; &#125;).print(); &#125;&#125; 上面代码中将 HBase 中的读取全部读取出来后然后过滤以 zhisheng 开头的 value 数据。读取结果如下图所示： 可以看到输出的结果中已经将以 zhisheng 开头的四条数据都打印出来了。 3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据添加依赖Flink Job 代码3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据读取数据写入数据配置文件3.10.5 项目运行及验证3.10.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/3bimqBM","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Connector —— ElasticSearch 的用法和分析","date":"2021-07-19T16:00:00.000Z","path":"2021/07/20/flink-in-action-3.9/","text":"3.9 Flink Connector —— ElasticSearch 的用法和分析ElasticSearch 现在也是非常火的一门技术，目前很多公司都有使用，本节将介绍 Flink ElasticSearch Connector 的实战使用和可能会遇到的问题。 3.9.1 准备环境和依赖首先准备 ElasticSearch 的环境和项目的环境依赖。 ElasticSearch 安装 因为在 2.1 节中已经讲过 ElasticSearch 的安装，这里就不做过多的重复，需要注意的一点就是 Flink 的 ElasticSearch Connector 是区分版本号的，官方支持的版本如下图所示。 所以添加依赖的时候要区分一下，根据你安装的 ElasticSearch 来选择不一样的版本依赖，另外就是不同版本的 ElasticSearch 还会导致下面的数据写入到 ElasticSearch 中出现一些不同，我们这里使用的版本是 ElasticSearch6，如果你使用的是其他的版本可以参考官网的实现。 添加依赖 因为我们在 2.1 节中安装的 ElasticSearch 版本是 6.3.2 版本的，所有这里引入的依赖就选择 flink-connector-elasticsearch6，具体依赖如下所示。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 上面这个 scala.binary.version 和 flink.version 版本号需要自己在使用的时候根据使用的版本做相应的改变。 3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序准备好环境和相关的依赖后，接下来开始编写 Flink 程序。 ESSinkUtil 工具类，代码如下所示，这个工具类是笔者封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面章节还会再讲其他的配置。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ESSinkUtil &#123; /** * es sink * * @param hosts es hosts * @param bulkFlushMaxActions bulk flush size * @param parallelism 并行数 * @param data 数据 * @param func * @param &lt;T&gt; */ public static &lt;T&gt; void addSink(List&lt;HttpHost&gt; hosts, int bulkFlushMaxActions, int parallelism, SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func) &#123; ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = new ElasticsearchSink.Builder&lt;&gt;(hosts, func); esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions); data.addSink(esSinkBuilder.build()).setParallelism(parallelism); &#125; /** * 解析配置文件的 es hosts * * @param hosts * @return * @throws MalformedURLException */ public static List&lt;HttpHost&gt; getEsAddresses(String hosts) throws MalformedURLException &#123; String[] hostList = hosts.split(\",\"); List&lt;HttpHost&gt; addresses = new ArrayList&lt;&gt;(); for (String host : hostList) &#123; if (host.startsWith(\"http\")) &#123; URL url = new URL(host); addresses.add(new HttpHost(url.getHost(), url.getPort())); &#125; else &#123; String[] parts = host.split(\":\", 2); if (parts.length &gt; 1) &#123; addresses.add(new HttpHost(parts[0], Integer.parseInt(parts[1]))); &#125; else &#123; throw new MalformedURLException(\"invalid elasticsearch hosts format\"); &#125; &#125; &#125; return addresses; &#125;&#125; Flink 程序会读取到 ElasticSearch 的配置，然后将从 Kafka 读取到的数据写入进 ElasticSearch，具体的写入代码如下所示。 123456789101112131415161718192021222324252627public class Sink2ES6Main &#123; public static void main(String[] args) throws Exception &#123; //获取所有参数 final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); //准备好环境 StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); //从kafka读取数据 DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); //从配置文件中读取 es 的地址 List&lt;HttpHost&gt; esAddresses = ESSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS)); //从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒 int bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, 40); //从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积 int sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, 5); //自己再自带的 es sink 上一层封装了下 ESSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data, (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123; requestIndexer.add(Requests.indexRequest() .index(ZHISHENG + \"_\" + metric.getName()) //es 索引名 .type(ZHISHENG) //es type .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); &#125;); env.execute(\"flink learning connectors es6\"); &#125;&#125; 配置文件中包含了 Kafka 和 ElasticSearch 的配置，如下所示，地址都支持集群模式填写，注意用 , 分隔。 12345678910kafka.brokers=localhost:9092kafka.group.id=zhisheng-metrics-group-testkafka.zookeeper.connect=localhost:2181metrics.topic=zhisheng-metricsstream.parallelism=5stream.checkpoint.interval=1000stream.checkpoint.enable=falseelasticsearch.hosts=localhost:9200elasticsearch.bulk.flush.max.actions=40stream.sink.parallelism=5 3.9.3 验证数据是否写入 ElasticSearch？3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题3.9.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/Jeqzfem","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 自定义 Flink Connector","date":"2021-07-18T16:00:00.000Z","path":"2021/07/19/flink-in-action-3.8/","text":"3.8 自定义 Flink Connector在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。 3.8.1 如何自定义 Source Connector？这里就演示一下如何自定义 Source 从 MySQL 中读取数据。 添加依赖 在 pom.xml 中添加 MySQL 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt;&lt;/dependency&gt; 数据库建表 数据库建表的 SQL 语句如下： 12345678DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(25) COLLATE utf8_bin DEFAULT NULL, `password` varchar(25) COLLATE utf8_bin DEFAULT NULL, `age` int(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 数据库插入数据 往新建的数据库表中插入 4 条数据的 SQL 语句如下： 12INSERT INTO `student` VALUES ('1', 'zhisheng01', '123456', '18'), ('2', 'zhisheng02', '123', '17'), ('3', 'zhisheng03', '1234', '18'), ('4', 'zhisheng04', '12345', '16');COMMIT; 新建实体类 对应数据库字段的实体类如下： 123456789@Data@AllArgsConstructor@NoArgsConstructorpublic class Student &#123; public int id; //id public String name; //姓名 public String password; //密码 public int age; //年龄&#125; 自定义 Source 类 SourceFromMySQL 是自定义的 Source 类，该类继承 RichSourceFunction ，实现里面的 open、close、run、cancel 方法，它的作用是读取 MySQL 中的数据，代码如下所示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class SourceFromMySQL extends RichSourceFunction&lt;Student&gt; &#123; PreparedStatement ps; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); connection = getConnection(); String sql = \"select * from Student;\"; ps = this.connection.prepareStatement(sql); &#125; /** * 程序执行完毕就可以进行，关闭连接和释放资源的动作了 * * @throws Exception */ @Override public void close() throws Exception &#123; super.close(); if (connection != null) &#123; //关闭连接和释放资源 connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * DataStream 调用一次 run() 方法用来获取数据 * * @param ctx * @throws Exception */ @Override public void run(SourceContext&lt;Student&gt; ctx) throws Exception &#123; ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) &#123; Student student = new Student( resultSet.getInt(\"id\"), resultSet.getString(\"name\").trim(), resultSet.getString(\"password\").trim(), resultSet.getInt(\"age\")); ctx.collect(student); &#125; &#125; @Override public void cancel() &#123; &#125; private static Connection getConnection() &#123; Connection con = null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8\", \"root\", \"123456\"); &#125; catch (Exception e) &#123; System.out.println(\"mysql get connection has exception , msg = \" + e.getMessage()); &#125; return con; &#125;&#125; Flink 应用程序代码 读取 MySQL 数据的代码完成后，接下来 Flink 主程序的代码就可以直接在 addSource() 方法中构造一个 SourceFromMySQL 对象作为一个参数传入，具体代码如下所示。 123456789public class Main2 &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(new SourceFromMySQL()).print(); env.execute(\"Flink add data sourc\"); &#125;&#125; 运行 Flink 程序，控制台日志中可以看见打印的 student 信息，结果如下图所示。 3.8.2 RichSourceFunction 的用法及源码分析从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，其实也是可以使用 SourceFunction 函数来自定义 Source。 RichSourceFunction 函数比 SourceFunction 多了 open 方法（可以用来初始化）和获取应用上下文的方法，那么来了解一下该类，它的类结构如下图所示。 它是一个抽象类，继承自 AbstractRichFunction，实现了 SourceFunction 接口，其子类有三个，如下图所示，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。 这三个子类的功能如下： MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。 MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。 ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。 除了上面使用 RichSourceFunction 和 SourceFunction 来自定义 Source，还可以继承 RichParallelSourceFunction 抽象类或实现 ParallelSourceFunction 接口来实现自定义 Source 函数。 3.8.3 自定义 Sink Connector下面将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去 工具类写了一个工具类往 Kafka 的 topic 中发送数据。 123456789101112131415161718192021222324252627/** * 往kafka中写数据，可以使用这个main函数进行测试一下 */public class KafkaUtils2 &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"student\"; //kafka topic 需要和 flink 程序用同一个 topic public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 1; i &lt;= 100; i++) &#123; Student student = new Student(i, \"zhisheng\" + i, \"password\" + i, 18 + i); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, JSON.toJSONString(student)); producer.send(record); System.out.println(\"发送数据: \" + JSON.toJSONString(student)); &#125; producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; writeToKafka(); &#125;&#125; SinkToMySQL该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法，在 invoke 方法中将数据插入到 MySQL 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class SinkToMySQL extends RichSinkFunction&lt;Student&gt; &#123; PreparedStatement ps; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); connection = getConnection(); String sql = \"insert into Student(id, name, password, age) values(?, ?, ?, ?);\"; ps = this.connection.prepareStatement(sql); &#125; @Override public void close() throws Exception &#123; super.close(); //关闭连接和释放资源 if (connection != null) &#123; connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * 每条数据的插入都要调用一次 invoke() 方法 * * @param value * @param context * @throws Exception */ @Override public void invoke(Student value, Context context) throws Exception &#123; //组装数据，执行插入操作 ps.setInt(1, value.getId()); ps.setString(2, value.getName()); ps.setString(3, value.getPassword()); ps.setInt(4, value.getAge()); ps.executeUpdate(); &#125; private static Connection getConnection() &#123; Connection con = null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8\", \"root\", \"root123456\"); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \"+ e.getMessage()); &#125; return con; &#125;&#125; Flink 程序这里的 source 是从 Kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 Student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。 1234567891011121314151617181920212223public class Main3 &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; JSON.parseObject(string, Student.class)); //Fastjson 解析字符串成 student 对象 student.addSink(new SinkToMySQL()); //数据 sink 到 mysql env.execute(\"Flink add sink\"); &#125;&#125; 结果3.8.4 RichSinkFunction 的用法及源码分析3.8.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/Y3RBaaQ 批量写 MySQL 可以参考 ：https://t.zsxq.com/FAmYFYJ","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink Connector —— Kafka 的使用和源码分析","date":"2021-07-16T16:00:00.000Z","path":"2021/07/17/flink-in-action-3.7/","text":"3.7 Flink Connector —— Kafka 的使用和源码分析在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去，整个案例的执行流程如下图所示。 3.7.1 准备环境和依赖接下来准备 Kafka 环境的安装和添加相关的依赖。 环境安装和启动如果你已经安装好了 Flink 和 Kafka，那么接下来使用命令运行启动 Flink、Zookepeer、Kafka 就行了。 启动 Flink 的命令如下图所示： 启动 Kafka 的命令如下图所示： 执行命令都启动好了后就可以添加依赖了。 添加 Maven 依赖Flink 里面支持 Kafka 0.8.x 以上的版本，具体采用哪个版本的 Maven 依赖需要根据安装的 Kafka 版本来确定。因为之前我们安装的 Kafka 是 1.1.0 版本，所以这里我们选择的 Kafka Connector 为 flink-connector-kafka-0.11_2.11 （支持 Kafka 0.11.x 版本及以上，该 Connector 支持 Kafka 事务消息传递，所以能保证 Exactly Once）。Flink Kafka Connector 支持的版本如下图所示： 添加如下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; Flink、Kafka、Flink Kafka Connector 三者对应的版本可以根据 官网 的对比来选择。需要注意的是 flink-connector-kafka_2.11 这个版本支持的 Kafka 版本要大于 1.0.0，从 Flink 1.9 版本开始，它使用的是 Kafka 2.2.0 版本的客户端，虽然这些客户端会做向后兼容，但是建议还是按照官网约定的来规范使用 Connector 版本。另外你还要添加的依赖有： 12345678910111213141516171819202122232425262728293031323334&lt;!--flink java--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!--log--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--alibaba fastjson--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.51&lt;/version&gt;&lt;/dependency&gt; 3.7.2 将测试数据发送到 Kafka Topic我们模拟一些测试数据，然后将这些测试数据发到 Kafka Topic 中去，数据的结构如下： 123456789@Data@AllArgsConstructor@NoArgsConstructorpublic class Metric &#123; public String name; //指标名 public long timestamp; //时间戳 public Map&lt;String, Object&gt; fields; //指标含有的属性 public Map&lt;String, String&gt; tags; //指标的标识&#125; 往 kafka 中写数据工具类 KafkaUtils.java，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 往kafka中写数据，可以使用这个main函数进行测试一下 */public class KafkaUtils &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"metric\"; // kafka topic，Flink 程序中需要和这个统一 public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //key 序列化 props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //value 序列化 KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); Metric metric = new Metric(); metric.setTimestamp(System.currentTimeMillis()); metric.setName(\"mem\"); Map&lt;String, String&gt; tags = new HashMap&lt;&gt;(); Map&lt;String, Object&gt; fields = new HashMap&lt;&gt;(); tags.put(\"cluster\", \"zhisheng\"); tags.put(\"host_ip\", \"101.147.022.106\"); fields.put(\"used_percent\", 90d); fields.put(\"max\", 27244873d); fields.put(\"used\", 17244873d); fields.put(\"init\", 27244873d); metric.setTags(tags); metric.setFields(fields); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, JSON.toJSONString(metric)); producer.send(record); System.out.println(\"发送数据: \" + JSON.toJSONString(metric)); producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; while (true) &#123; Thread.sleep(300); writeToKafka(); &#125; &#125;&#125; 运行结果如下图所示： 如果出现如上图标记的，即代表能够不断往 kafka 发送数据的。 3.7.3 Flink 如何消费 Kafka 数据？Flink 消费 Kafka 数据的应用程序如下： 12345678910111213141516171819202122public class Main &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); //key 反序列化 props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); //value 反序列化 DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"metric\", //kafka topic new SimpleStringSchema(), // String 序列化 props)).setParallelism(1); dataStreamSource.print(); //把从 kafka 读取到的数据打印在控制台 env.execute(\"Flink add data source\"); &#125;&#125; 运行结果如下图所示（程序可以不断的消费到 Kafka Topic 中的数据）： 代码分析 使用 FlinkKafkaConsumer011 时传入了三个参数： Kafka topic：这个代表了 Flink 要消费的是 Kafka 哪个 Topic，如果你要同时消费多个 Topic 的话，那么你可以传入一个 Topic List 进去，另外也支持正则表达式匹配 Topic，源码如下图所示。 序列化：上面代码我们使用的是 SimpleStringSchema。 配置属性：将 Kafka 等的一些配置传入 。 前面演示了 Flink 如何消费 Kafak 数据，接下来演示如何把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。 3.7.4 Flink 如何将计算后的数据发送到 Kafka？将 Kafka 集群中 topic 数据写入本地 Kafka 的程序中要填写的配置有消费的 Kafka 集群地址、group.id、将数据写入 Kafka 的集群地址、topic 信息等，将所有的配置提取到配置文件中，如下所示。 1234567891011//其他 Kafka 集群配置kafka.brokers=xxx:9092,xxx:9092,xxx:9092kafka.group.id=metrics-group-testkafka.zookeeper.connect=xxx:2181metrics.topic=xxxstream.parallelism=5kafka.sink.brokers=localhost:9092kafka.sink.topic=metric-teststream.checkpoint.interval=1000stream.checkpoint.enable=falsestream.sink.parallelism=5 目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令： 1bin/kafka-topics.sh --list --zookeeper localhost:2181 执行上面命令后的结果如下图所示： 可以看到本地的 Kafka 是没有任何 topic 的，如果等下程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。 整个 Flink 程序的代码如下： 12345678910111213141516public class Main &#123; public static void main(String[] args) throws Exception&#123; final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); data.addSink(new FlinkKafkaProducer011&lt;Metrics&gt;( parameterTool.get(\"kafka.sink.brokers\"), parameterTool.get(\"kafka.sink.topic\"), new MetricSchema() )).name(\"flink-connectors-kafka\") .setParallelism(parameterTool.getInt(\"stream.sink.parallelism\")); env.execute(\"flink learning connectors kafka\"); &#125;&#125; 启动程序，查看运行结果，不断执行查看 topic 列表的命令，观察是否有新的 topic 出来，结果如下图所示： 执行命令可以查看该 topic 的信息： 1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test 该 topic 信息如下图所示： 前面代码使用的 FlinkKafkaProducer011 只传了三个参数：brokerList、topicId、serializationSchema（序列化），其实是支持传入多个参数的，Flink 中的源码如下图所示。 3.7.5 FlinkKafkaConsumer 源码分析3.7.6 FlinkKafkaProducer 源码分析3.7.7 使用 Flink-connector-kafka 可能会遇到的问题如何消费多个 Kafka Topic想要获取数据的元数据信息多种数据类型序列化失败Kafka 消费 Offset 的选择如何自动发现 Topic 新增的分区并读取数据程序消费 Kafka 的 offset 是如何管理的3.7.8 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/2bmurFy","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 常用的 Source Connector 和 Sink Connector 介绍","date":"2021-07-15T16:00:00.000Z","path":"2021/07/16/flink-in-action-3.6/","text":"3.6 Flink 常用的 Source Connector 和 Sink Connector 介绍通过前面我们可以知道 Flink Job 的大致结构就是 Source ——&gt; Transformation ——&gt; Sink。 那么这个 Source 是什么意思呢？我们下面来看看。 3.6.1 Data Source 简介Data Source 是什么呢？就字面意思其实就可以知道：数据来源。 Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。 Flink 中你可以使用 StreamExecutionEnvironment.addSource(sourceFunction) 来为你的程序添加数据来源。 Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。 那么常用的 Data Source 有哪些呢？ 3.6.2 常用的 Data SourceStreamExecutionEnvironment 中可以使用如下图所示的这些已实现的 Stream Source。 总的来说可以分为集合、文件、Socket、自定义四大类。 基于集合基于集合的有下面五种方法： 1、fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。 2、fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。 3、fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。 12345678StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;Event&gt; input = env.fromElements( new Event(1, \"barfoo\", 1.0), new Event(2, \"start\", 2.0), new Event(3, \"foobar\", 3.0), ...); 4、fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。 5、generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。 基于文件基于文件的有下面三种方法： 1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。 123final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;String&gt; text = env.readTextFile(\"file:///path/to/file\"); 2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。 3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS_ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。 12345final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;MyEvent&gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo); 实现: 在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。 重要注意： 如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。 如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。 基于 SocketsocketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。 12345678StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env .socketTextStream(\"localhost\", 9999) // 监听 localhost 的 9999 端口过来的数据 .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); 自定义addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。 说说上面几种的特点 1、基于集合：有界数据集，更偏向于本地测试用 2、基于文件：适合监听文件修改并读取其内容 3、基于 Socket：监听主机的 host port，从 Socket 中获取数据 4、自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。 123456789StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;KafkaEvent&gt; input = env .addSource( new FlinkKafkaConsumer011&lt;&gt;( parameterTool.getRequired(\"input-topic\"), //从参数中获取传进来的 topic new KafkaEventSchema(), parameterTool.getProperties()) .assignTimestampsAndWatermarks(new CustomWatermarkExtractor())); Flink 目前支持的 Source 如下图所示： 如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。 3.6.3 Data Sink 简介3.6.4 常用的 Data Sink3.6.5 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/FAayJU3","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Watermark 的用法和结合 Window 处理延迟数据","date":"2021-07-14T16:00:00.000Z","path":"2021/07/15/flink-in-action-3.5/","text":"3.5 Watermark 的用法和结合 Window 处理延迟数据在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。 选择 Event Time 与 Process Time 的实际效果如下图所示： 在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实却仍然这么骨感，会因为各种各样的问题（网络的抖动、设备的故障、应用的异常等原因）从而导致如图中曲线一样，Process Time 总是会与 Event Time 有一些延迟。所谓乱序，其实是指 Flink 接收到的事件的先后顺序并不是严格的按照事件的 Event Time 顺序排列的。如下图所示： 然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如： 错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首 设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件 这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。庆幸的是 Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。 3.5.1 Watermark 简介举个例子： 统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。 Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。 下面通过几个图来了解一下 Watermark 是如何工作的！如下图所示，数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，W(4) 和 W(9) 是水印。 经过 Flink 的消费，数据 1、3、2 进入了第一个窗口，然后 7 会进入第二个窗口，接着 3 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 4 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示。 那么接着后面的数据 5 和 6 会进入到第二个窗口里面，数据 9 会进入在第三个窗口里面，如下图所示。 那么当遇到水印 9 时，发现水印比第二个窗口的结束时间 8 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去，如下图所示。 相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。 3.5.2 Flink 中的 Watermark 的设置在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。 12345678910111213141516171819public SingleOutputStreamOperator&lt;T&gt; assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner) &#123; final int inputParallelism = getTransformation().getParallelism(); final AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner); TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = new TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner); return transform(\"Timestamps/Watermarks\", getTransformation().getOutputType(), operator).setParallelism(inputParallelism);&#125;public SingleOutputStreamOperator&lt;T&gt; assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner) &#123; final int inputParallelism = getTransformation().getParallelism(); final AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner); TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = new TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner); return transform(\"Timestamps/Watermarks\", getTransformation().getOutputType(), operator).setParallelism(inputParallelism);&#125; 所以设置 Watermark 是有如下两种方式： AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。 在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。 AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。 在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。 下面再分别详细讲下这两种的实现方式。 3.5.3 Punctuated WatermarkAssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。 那么该怎么利用这个来定义水印生成器呢？ 12345678910111213public class WordPunctuatedWatermark implements AssignerWithPunctuatedWatermarks&lt;Word&gt; &#123; @Nullable @Override public Watermark checkAndGetNextWatermark(Word lastElement, long extractedTimestamp) &#123; return extractedTimestamp % 3 == 0 ? new Watermark(extractedTimestamp) : null; &#125; @Override public long extractTimestamp(Word element, long previousElementTimestamp) &#123; return element.getTimestamp(); &#125;&#125; 需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。 3.5.4 Periodic Watermark通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。 123456789101112131415161718public class WordPeriodicWatermark implements AssignerWithPeriodicWatermarks&lt;Word&gt; &#123; private long currentTimestamp = Long.MIN_VALUE; @Override public long extractTimestamp(Word word, long previousElementTimestamp) &#123; long timestamp = word.getTimestamp(); currentTimestamp = Math.max(timestamp, currentTimestamp); return word.getTimestamp(); &#125; @Nullable @Override public Watermark getCurrentWatermark() &#123; long maxTimeLag = 5000; return new Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag); &#125;&#125; 上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，然后将当前时间戳与事件时间进行比较，取最大值后赋值给当前时间戳 currentTimestamp，然后返回事件时间。getCurrentWatermark() 方法是获取当前的水位线，通过 currentTimestamp - maxTimeLag 得到水印的值，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 long maxTimeLag = 5000; 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。 通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。 AssignerWithPeriodicWatermarks 这个接口有四个实现类，如下图所示： 这四个实现类的功能和使用方式如下： BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下图所示： 你可以像下面一样使用该类来分配时间和生成水印： 12345678//Time.seconds(10) 代表允许延迟的时间大小dataStream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(10)) &#123; //重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法 @Override public long extractTimestamp(Event event) &#123; return event.getTimestamp(); &#125;&#125;) CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。 AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下： 12345678910public final long extractTimestamp(T element, long elementPrevTimestamp) &#123; final long newTimestamp = extractAscendingTimestamp(element); if (newTimestamp &gt;= this.currentTimestamp) &#123; this.currentTimestamp = ne∏wTimestamp; return newTimestamp; &#125; else &#123; violationHandler.handleViolation(newTimestamp, this.currentTimestamp); return newTimestamp; &#125;&#125; IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 System.currentTimeMillis() 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。 注意： 1、使用这种方式周期性生成水印的话，你可以通过 env.getConfig().setAutoWatermarkInterval(...); 来设置生成水印的间隔（每隔 n 毫秒）。 2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义 1234567891011@Overridepublic void run(SourceContext&lt;MyType&gt; ctx) throws Exception &#123; while (/* condition */) &#123; MyType next = getNext(); ctx.collectWithTimestamp(next, next.getEventTimestamp()); if (next.hasWatermarkTime()) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime())); &#125; &#125;&#125; 3.5.5 每个 Kafka 分区的时间戳3.5.6 将 Watermark 与 Window 结合起来处理延迟数据3.5.7 处理延迟数据的三种方法丢弃（默认）allowedLateness 再次指定允许数据延迟的时间sideOutputLateData 收集迟到的数据3.5.8 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/RbufeIA","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 使用 DataStream API 来处理数据","date":"2021-07-13T16:00:00.000Z","path":"2021/07/14/flink-in-action-3.4/","text":"3.4 使用 DataStream API 来处理数据在 3.3 节中讲了数据转换常用的 Operators（算子），然后在 3.2 节中也讲了 Flink 中窗口的概念和原理，那么我们这篇文章再来细讲一下 Flink 中的各种 DataStream API。 我们先来看下源码里面的 DataStream 大概有哪些类呢？如下图所示，展示了 1.9 版本中的 DataStream 类。 可以发现其实还是有很多的类，只有熟练掌握了这些 API，我们才能在做数据转换和计算的时候足够灵活的运用开来（知道何时该选用哪种 DataStream？选用哪个 Function？）。那么我们先从 DataStream 开始吧！ 3.4.1 DataStream 的用法及分析首先我们来看下 DataStream 这个类的定义吧： 12A DataStream represents a stream of elements of the same type. A DataStreamcan be transformed into another DataStream by applying a transformation as DataStream#map or DataStream#filter&#125; 大概意思是：DataStream 表示相同类型的元素组成的数据流，一个数据流可以通过 map/filter 等算子转换成另一个数据流。 然后 DataStream 的类结构图如下图所示： 它的继承类有 KeyedStream、SingleOutputStreamOperator 和 SplitStream。这几个类本文后面都会一一给大家讲清楚。下面我们来看看 DataStream 这个类中的属性和方法吧。 它的属性就只有两个： 123protected final StreamExecutionEnvironment environment;protected final StreamTransformation&lt;T&gt; transformation; 但是它的方法却有很多，并且我们平时写的 Flink Job 几乎离不开这些方法，这也注定了这个类的重要性，所以得好好看下这些方法该如何使用，以及是如何实现的。 union通过合并相同数据类型的数据流，然后创建一个新的数据流，union 方法代码实现如下： 12345678910111213public final DataStream&lt;T&gt; union(DataStream&lt;T&gt;... streams) &#123; List&lt;StreamTransformation&lt;T&gt;&gt; unionedTransforms = new ArrayList&lt;&gt;(); unionedTransforms.add(this.transformation); for (DataStream&lt;T&gt; newStream : streams) &#123; if (!getType().equals(newStream.getType())) &#123; //判断数据类型是否一致 throw new IllegalArgumentException(\"Cannot union streams of different types: \" + getType() + \" and \" + newStream.getType()); &#125; unionedTransforms.add(newStream.getTransformation()); &#125; //构建新的数据流 return new DataStream&lt;&gt;(this.environment, new UnionTransformation&lt;&gt;(unionedTransforms));//通过使用 UnionTransformation 将多个 StreamTransformation 合并起来&#125; 那么我们该如何去使用 union 呢（不止连接一个数据流，也可以连接多个数据流）？ 12345//数据流 1 和 2final DataStream&lt;Integer&gt; stream1 = env.addSource(...);final DataStream&lt;Integer&gt; stream2 = env.addSource(...);//unionstream1.union(stream2) split该方法可以将两个数据流进行拆分，拆分后的数据流变成了 SplitStream（在下文会详细介绍这个类的内部实现），该 split 方法通过传入一个 OutputSelector 参数进行数据选择，方法内部实现就是构造一个 SplitStream 对象然后返回： 123public SplitStream&lt;T&gt; split(OutputSelector&lt;T&gt; outputSelector) &#123; return new SplitStream&lt;&gt;(this, clean(outputSelector));&#125; 然后我们该如何使用这个方法呢？ 1234567891011121314dataStream.split(new OutputSelector&lt;Integer&gt;() &#123; private static final long serialVersionUID = 8354166915727490130L; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; s = new ArrayList&lt;String&gt;(); if (value &gt; 4) &#123; //大于 4 的数据放到 &gt; 这个 tag 里面去 s.add(\"&gt;\"); &#125; else &#123; //小于等于 4 的数据放到 &lt; 这个 tag 里面去 s.add(\"&lt;\"); &#125; return s; &#125;&#125;); 注意：该方法已经不推荐使用了！在 1.7 版本以后建议使用 Side Output 来实现分流操作。 connect通过连接不同或相同数据类型的数据流，然后创建一个新的连接数据流，如果连接的数据流也是一个 DataStream 的话，那么连接后的数据流为 ConnectedStreams（会在下文介绍这个类的具体实现），它的具体实现如下： 123public &lt;R&gt; ConnectedStreams&lt;T, R&gt; connect(DataStream&lt;R&gt; dataStream) &#123; return new ConnectedStreams&lt;&gt;(environment, this, dataStream);&#125; 如果连接的数据流是一个 BroadcastStream（广播数据流），那么连接后的数据流是一个 BroadcastConnectedStream（会在下文详细介绍该类的内部实现），它的具体实现如下： 12345public &lt;R&gt; BroadcastConnectedStream&lt;T, R&gt; connect(BroadcastStream&lt;R&gt; broadcastStream) &#123; return new BroadcastConnectedStream&lt;&gt;( environment, this, Preconditions.checkNotNull(broadcastStream), broadcastStream.getBroadcastStateDescriptor());&#125; 使用如下： 123456789//1、连接 DataStreamDataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(new Tuple2&lt;&gt;(0L, 0L));DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(new Tuple2&lt;&gt;(0L, 0L));ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);//2、连接 BroadcastStreamDataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(new Tuple2&lt;&gt;(0L, 0L));final BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);BroadcastConnectedStream&lt;Tuple2&lt;Long, Long&gt;, String&gt; connect = src1.connect(broadcast); keyBykeyBy 方法是用来将数据进行分组的，通过该方法可以将具有相同 key 的数据划分在一起组成新的数据流，该方法有四种（它们的参数各不一样）： 12345678910111213141516171819202122232425262728293031//1、参数是 KeySelector 对象public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key) &#123; ... return new KeyedStream&lt;&gt;(this, clean(key));//构造 KeyedStream 对象&#125;//2、参数是 KeySelector 对象和 TypeInformation 对象public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key, TypeInformation&lt;K&gt; keyType) &#123; ... return new KeyedStream&lt;&gt;(this, clean(key), keyType);//构造 KeyedStream 对象&#125;//3、参数是 1 至多个字段（用 0、1、2... 表示）public KeyedStream&lt;T, Tuple&gt; keyBy(int... fields) &#123; if (getType() instanceof BasicArrayTypeInfo || getType() instanceof PrimitiveArrayTypeInfo) &#123; return keyBy(KeySelectorUtil.getSelectorForArray(fields, getType())); &#125; else &#123; return keyBy(new Keys.ExpressionKeys&lt;&gt;(fields, getType()));//调用 private 的 keyBy 方法 &#125;&#125;//4、参数是 1 至多个字符串public KeyedStream&lt;T, Tuple&gt; keyBy(String... fields) &#123; return keyBy(new Keys.ExpressionKeys&lt;&gt;(fields, getType()));//调用 private 的 keyBy 方法&#125;//真正调用的方法private KeyedStream&lt;T, Tuple&gt; keyBy(Keys&lt;T&gt; keys) &#123; return new KeyedStream&lt;&gt;(this, clean(KeySelectorUtil.getSelectorForKeys(keys, getType(), getExecutionConfig())));&#125; 如何使用呢： 12345678910111213141516171819202122232425262728293031DataStream&lt;Event&gt; dataStream = env.fromElements( new Event(1, \"zhisheng01\", 1.0), new Event(2, \"zhisheng02\", 2.0), new Event(3, \"zhisheng03\", 2.1), new Event(3, \"zhisheng04\", 3.0), new SubEvent(4, \"zhisheng05\", 4.0, 1.0),);//第1种dataStream.keyBy(new KeySelector&lt;Event, Integer&gt;() &#123; @Override public Integer getKey(Event value) throws Exception &#123; return value.getId(); &#125;&#125;);//第2种dataStream.keyBy(new KeySelector&lt;Event, Integer&gt;() &#123; @Override public Integer getKey(Event value) throws Exception &#123; return value.getId(); &#125;&#125;, Types.STRING);//第3种dataStream.keyBy(0);//第4种dataStream.keyBy(\"zhisheng01\", \"zhisheng02\"); partitionCustom使用自定义分区器在指定的 key 字段上将 DataStream 分区，这个 partitionCustom 有 3 个不同参数的方法，分别要传入的参数有自定义分区 Partitioner 对象、位置、字符和 KeySelector。它们内部也都是调用了私有的 partitionCustom 方法。 broadcastbroadcast 是将数据流进行广播，然后让下游的每个并行 Task 中都可以获取到这份数据流，通常这些数据是一些配置，一般这些配置数据的数据量不能太大，否则资源消耗会比较大。这个 broadcast 方法也有两个，一个是无参数，它返回的数据是 DataStream；另一种的参数是 MapStateDescriptor，它返回的参数是 BroadcastStream（这个也会在下文详细介绍）。 使用方法： 12345678910//1、第一种DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; source = env.addSource(...).broadcast();//2、第二种final MapStateDescriptor&lt;Long, String&gt; utterDescriptor = new MapStateDescriptor&lt;&gt;( \"broadcast-state\", BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO);final DataStream&lt;String&gt; srcTwo = env.fromCollection(expected.values());final BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor); mapmap 方法需要传入的参数是一个 MapFunction，当然传入 RichMapFunction 也是可以的，它返回的是 SingleOutputStreamOperator（这个类在会在下文详细介绍），该 map 方法里面的实现如下： 1234567public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; map(MapFunction&lt;T, R&gt; mapper) &#123; TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(), Utils.getCallLocationName(), true); //调用 transform 方法 return transform(\"Map\", outType, new StreamMap&lt;&gt;(clean(mapper)));&#125; 该方法平时使用的非常频繁，然后我们该如何使用这个方法呢： 12345678dataStream.map(new MapFunction&lt;Integer, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public String map(Integer value) throws Exception &#123; return value.toString(); &#125;&#125;) flatMapflatMap 方法需要传入一个 FlatMapFunction 参数，当然传入 RichFlatMapFunction 也是可以的，如果你的 Flink Job 里面有连续的 filter 和 map 算子在一起，可以考虑使用 flatMap 一个算子来完成两个算子的工作，它返回的是 SingleOutputStreamOperator，该 flatMap 方法里面的实现如下： 12345678public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; flatMap(FlatMapFunction&lt;T, R&gt; flatMapper) &#123; TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper), getType(), Utils.getCallLocationName(), true); //调用 transform 方法 return transform(\"Flat Map\", outType, new StreamFlatMap&lt;&gt;(clean(flatMapper)));&#125; 该方法平时使用的非常频繁，使用方式如下： 123456dataStream.flatMap(new FlatMapFunction&lt;Integer, Integer&gt;() &#123; @Override public void flatMap(Integer value, Collector&lt;Integer&gt; out) throws Exception &#123; out.collect(value); &#125;&#125;) process在输入流上应用给定的 ProcessFunction，从而创建转换后的输出流，通过该方法返回的是 SingleOutputStreamOperator，具体代码实现如下： 1234567891011121314151617public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; process(ProcessFunction&lt;T, R&gt; processFunction) &#123; TypeInformation&lt;R&gt; outType = TypeExtractor.getUnaryOperatorReturnType( processFunction, ProcessFunction.class, 0, 1, TypeExtractor.NO_INDEX, getType(), Utils.getCallLocationName(), true); //调用下面的 process 方法 return process(processFunction, outType);&#125;public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; process( ProcessFunction&lt;T, R&gt; processFunction, TypeInformation&lt;R&gt; outputType) &#123; ProcessOperator&lt;T, R&gt; operator = new ProcessOperator&lt;&gt;(clean(processFunction)); //调用 transform 方法 return transform(\"Process\", outputType, operator);&#125; 使用方法： 1234567891011121314151617181920DataStreamSource&lt;Long&gt; data = env.generateSequence(0, 0);//定义的 ProcessFunctionProcessFunction&lt;Long, Integer&gt; processFunction = new ProcessFunction&lt;Long, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void processElement(Long value, Context ctx, Collector&lt;Integer&gt; out) throws Exception &#123; //具体逻辑 &#125; @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;Integer&gt; out) throws Exception &#123; //具体逻辑 &#125;&#125;;DataStream&lt;Integer&gt; processed = data.keyBy(new IdentityKeySelector&lt;Long&gt;()).process(processFunction); filterfilter 用来过滤数据的，它需要传入一个 FilterFunction，然后返回的数据也是 SingleOutputStreamOperator，该方法的实现是： 123public SingleOutputStreamOperator&lt;T&gt; filter(FilterFunction&lt;T&gt; filter) &#123; return transform(\"Filter\", getType(), new StreamFilter&lt;&gt;(clean(filter)));&#125; 该方法平时使用非常多： 1234567DataStream&lt;String&gt; filter1 = src .filter(new FilterFunction&lt;String&gt;() &#123; @Override public boolean filter(String value) throws Exception &#123; return \"zhisheng\".equals(value); &#125; &#125;) 上面这些方法是平时写代码时用的非常多的方法，我们这里讲解了它们的实现原理和使用方式，当然还有其他方法，比如 assignTimestampsAndWatermarks、join、shuffle、forward、addSink、rebalance、iterate、coGroup、project、timeWindowAll、countWindowAll、windowAll、print 等，这里由于篇幅的问题就不一一展开来讲了。 3.4.2 SingleOutputStreamOperator 的用法及分析SingleOutputStreamOperator 这个类继承自 DataStream，所以 DataStream 中有的方法在这里也都有，那么这里就讲解下额外的方法的作用，如下。 name()：该方法可以设置当前数据流的名称，如果设置了该值，则可以在 Flink UI 上看到该值；uid() 方法可以为算子设置一个指定的 ID，该 ID 有个作用就是如果想从 savepoint 恢复 Job 时是可以根据这个算子的 ID 来恢复到它之前的运行状态； setParallelism() ：该方法是为每个算子单独设置并行度的，这个设置优先于你通过 env 设置的全局并行度； setMaxParallelism() ：该为算子设置最大的并行度； setResources()：该方法有两个（参数不同），设置算子的资源，但是这两个方法对外还没开放（是私有的，暂时功能性还不全）； forceNonParallel()：该方法强行将并行度和最大并行度都设置为 1； setChainingStrategy()：该方法对给定的算子设置 ChainingStrategy； disableChaining()：该这个方法设置后将禁止该算子与其他的算子 chain 在一起； getSideOutput()：该方法通过给定的 OutputTag 参数从 side output 中来筛选出对应的数据流。 3.4.3 KeyedStream 的用法及分析KeyedStream 是 DataStream 在根据 KeySelector 分区后的数据流，DataStream 中常用的方法在 KeyedStream 后也可以用（除了 shuffle、forward 和 keyBy 等分区方法），在该类中的属性分别是 KeySelector 和 TypeInformation。 DataStream 中的窗口方法只有 timeWindowAll、countWindowAll 和 windowAll 这三种全局窗口方法，但是在 KeyedStream 类中的种类就稍微多了些，新增了 timeWindow、countWindow 方法，并且是还支持滑动窗口。 除了窗口方法的新增外，还支持大量的聚合操作方法，比如 reduce、fold、sum、min、max、minBy、maxBy、aggregate 等方法（列举的这几个方法都支持多种参数的）。 最后就是它还有 asQueryableState() 方法，能够将 KeyedStream 发布为可查询的 ValueState 实例。 3.4.4 SplitStream 的用法及分析SplitStream 这个类比较简单，它代表着数据分流后的数据流了，它有一个 select 方法可以选择分流后的哪种数据流了，通常它是结合 split 使用的，对于单次分流来说还挺方便的。但是它是一个被废弃的类（Flink 1.7 后被废弃的，可以看下笔者之前写的一篇文章 Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ ），其实可以用 side output 来代替这种 split，后面文章中我们也会讲通过简单的案例来讲解一下该如何使用 side output 做数据分流操作。 因为这个类的源码比较少，我们可以看下这个类的实现： 12345678910111213141516171819202122232425public class SplitStream&lt;OUT&gt; extends DataStream&lt;OUT&gt; &#123; //构造方法 protected SplitStream(DataStream&lt;OUT&gt; dataStream, OutputSelector&lt;OUT&gt; outputSelector) &#123; super(dataStream.getExecutionEnvironment(), new SplitTransformation&lt;OUT&gt;(dataStream.getTransformation(), outputSelector)); &#125; //选择要输出哪种数据流 public DataStream&lt;OUT&gt; select(String... outputNames) &#123; return selectOutput(outputNames); &#125; //上面那个 public 方法内部调用的就是这个方法，该方法是个 private 方法，对外隐藏了它是如何去找到特定的数据流。 private DataStream&lt;OUT&gt; selectOutput(String[] outputNames) &#123; for (String outName : outputNames) &#123; if (outName == null) &#123; throw new RuntimeException(\"Selected names must not be null\"); &#125; &#125; //构造了一个 SelectTransformation 对象 SelectTransformation&lt;OUT&gt; selectTransform = new SelectTransformation&lt;OUT&gt;(this.getTransformation(), Lists.newArrayList(outputNames)); //构造了一个 DataStream 对象 return new DataStream&lt;OUT&gt;(this.getExecutionEnvironment(), selectTransform); &#125;&#125; 3.4.5 WindowedStream 的用法及分析虽然 WindowedStream 不是继承自 DataStream，并且我们在 3.1 节中也做了一定的讲解，但是当时没讲里面的 Function，所以在这里刚好一起做一个补充。 在 WindowedStream 类中定义的属性有 KeyedStream、WindowAssigner、Trigger、Evictor、allowedLateness 和 lateDataOutputTag。 KeyedStream：代表着数据流，数据分组后再开 Window WindowAssigner：Window 的组件之一 Trigger：Window 的组件之一 Evictor：Window 的组件之一（可选） allowedLateness：用户指定的允许迟到时间长 lateDataOutputTag：数据延迟到达的 Side output，如果延迟数据没有设置任何标记，则会被丢弃 在 3.1 节中我们讲了上面的三个窗口组件 WindowAssigner、Trigger、Evictor，并教大家该如何使用，那么在这篇文章我就不再重复，那么接下来就来分析下其他几个的使用方式和其实现原理。 先来看下 allowedLateness 这个它可以在窗口后指定允许迟到的时间长，使用如下： 123dataStream.keyBy(0) .timeWindow(Time.milliseconds(20)) .allowedLateness(Time.milliseconds(2)) lateDataOutputTag 这个它将延迟到达的数据发送到由给定 OutputTag 标识的 side output（侧输出），当水印经过窗口末尾（并加上了允许的延迟后），数据就被认为是延迟了。 对于 keyed windows 有五个不同参数的 reduce 方法可以使用，如下： 1234567891011121314151617181920212223242526272829//1、参数为 ReduceFunctionpublic SingleOutputStreamOperator&lt;T&gt; reduce(ReduceFunction&lt;T&gt; function) &#123; ... return reduce(function, new PassThroughWindowFunction&lt;K, W, T&gt;());&#125;//2、参数为 ReduceFunction 和 WindowFunctionpublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function) &#123; ... return reduce(reduceFunction, function, resultType);&#125;//3、参数为 ReduceFunction、WindowFunction 和 TypeInformationpublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType) &#123; ... return input.transform(opName, resultType, operator);&#125;//4、参数为 ReduceFunction 和 ProcessWindowFunctionpublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function) &#123; ... return reduce(reduceFunction, function, resultType);&#125;//5、参数为 ReduceFunction、ProcessWindowFunction 和 TypeInformationpublic &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType) &#123; ... return input.transform(opName, resultType, operator);&#125; 除了 reduce 方法，还有六个不同参数的 fold 方法、aggregate 方法；两个不同参数的 apply 方法、process 方法（其中你会发现这两个 apply 方法和 process 方法内部其实都隐式的调用了一个私有的 apply 方法）；其实除了前面说的两个不同参数的 apply 方法外，还有四个其他的 apply 方法，这四个方法也是参数不同，但是其实最终的是利用了 transform 方法；还有的就是一些预定义的聚合方法比如 sum、min、minBy、max、maxBy，它们的方法参数的个数不一致，这些预聚合的方法内部调用的其实都是私有的 aggregate 方法，该方法允许你传入一个 AggregationFunction 参数。我们来看一个具体的实现： 123456789101112131415161718//maxpublic SingleOutputStreamOperator&lt;T&gt; max(String field) &#123; //内部调用私有的的 aggregate 方法 return aggregate(new ComparableAggregator&lt;&gt;(field, input.getType(), AggregationFunction.AggregationType.MAX, false, input.getExecutionConfig()));&#125;//私有的 aggregate 方法private SingleOutputStreamOperator&lt;T&gt; aggregate(AggregationFunction&lt;T&gt; aggregator) &#123; //继续调用的是 reduce 方法 return reduce(aggregator);&#125;//该 reduce 方法内部其实又是调用了其他多个参数的 reduce 方法public SingleOutputStreamOperator&lt;T&gt; reduce(ReduceFunction&lt;T&gt; function) &#123; ... function = input.getExecutionEnvironment().clean(function); return reduce(function, new PassThroughWindowFunction&lt;K, W, T&gt;());&#125; 从上面的方法调用过程，你会发现代码封装的很深，得需要你自己好好跟一下源码才可以了解更深些。 上面讲了这么多方法，你会发现 reduce 方法其实是用的蛮多的之一，那么就来看看该如何使用： 123456789dataStream.keyBy(0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) &#123; return value1; &#125; &#125;) .print(); 3.4.6 AllWindowedStream 的用法及分析前面讲完了 WindowedStream，再来看看这个 AllWindowedStream 你会发现它的实现其实无太大区别，该类中的属性和方法都和前面 WindowedStream 是一样的，然后我们就不再做过多的介绍，直接来看看该如何使用呢？ AllWindowedStream 这种场景下是不需要让数据流做 keyBy 分组操作，直接就进行 windowAll 操作，然后在 windowAll 方法中传入 WindowAssigner 参数对象即可，然后返回的数据结果就是 AllWindowedStream 了，下面使用方式继续执行了 AllWindowedStream 中的 reduce 方法来返回数据： 12345678910dataStream.windowAll(SlidingEventTimeWindows.of(Time.of(1, TimeUnit.SECONDS), Time.of(100, TimeUnit.MILLISECONDS))) .reduce(new RichReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123; private static final long serialVersionUID = -6448847205314995812L; @Override public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return value1; &#125; &#125;); 3.4.7 ConnectedStreams 的用法及分析ConnectedStreams 这个类定义是表示（可能）两个不同数据类型的数据连接流，该场景如果对一个数据流进行操作会直接影响另一个数据流，因此可以通过流连接来共享状态。比较常见的一个例子就是一个数据流（随时间变化的规则数据流）通过连接其他的数据流，这样另一个数据流就可以利用这些连接的规则数据流。 ConnectedStreams 在概念上可以认为和 Union 数据流是一样的。 在 ConnectedStreams 类中有三个属性：environment、inputStream1 和 inputStream2，该类中的方法如下图所示： 在 ConnectedStreams 中可以通过 getFirstInput 获取连接的第一个流、通过 getSecondInput 获取连接的第二个流，同时它还含有六个 keyBy 方法来将连接后的数据流进行分组，这六个 keyBy 方法的参数各有不同。另外它还含有 map、flatMap、process 方法来处理数据（其中 map 和 flatMap 方法的参数分别使用的是 CoMapFunction 和 CoFlatMapFunction），其实如果你细看其方法里面的实现就会发现都是调用的 transform 方法。 上面讲完了 ConnectedStreams 类的基础定义，接下来我们来看下该类如何使用呢？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(new Tuple2&lt;&gt;(0L, 0L)); //流 1DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(new Tuple2&lt;&gt;(0L, 0L)); //流 2ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2); //连接流 1 和流 2//使用连接流的六种 keyBy 方法ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup1 = connected.keyBy(0, 0);ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup2 = connected.keyBy(new int[]&#123;0&#125;, new int[]&#123;0&#125;);ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup3 = connected.keyBy(\"f0\", \"f0\");ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup4 = connected.keyBy(new String[]&#123;\"f0\"&#125;, new String[]&#123;\"f0\"&#125;);ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(new FirstSelector(), new FirstSelector());ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(new FirstSelector(), new FirstSelector(), Types.STRING);//使用连接流的 map 方法connected.map(new CoMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Object&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Object map1(Tuple2&lt;Long, Long&gt; value) &#123; return null; &#125; @Override public Object map2(Tuple2&lt;Long, Long&gt; value) &#123; return null; &#125;&#125;);//使用连接流的 flatMap 方法connected.flatMap(new CoFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123; @Override public void flatMap1(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123;&#125; @Override public void flatMap2(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123;&#125;&#125;).name(\"testCoFlatMap\")//使用连接流的 process 方法connected.process(new CoProcessFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123; @Override public void processElement1(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; if (value.f0 &lt; 3) &#123; out.collect(value); ctx.output(sideOutputTag, \"sideout1-\" + String.valueOf(value)); &#125; &#125; @Override public void processElement2(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; if (value.f0 &gt;= 3) &#123; out.collect(value); ctx.output(sideOutputTag, \"sideout2-\" + String.valueOf(value)); &#125; &#125;&#125;); 3.4.8 BroadcastStream 的用法及分析3.4.9 BroadcastConnectedStream 的用法及分析3.4.10 QueryableStateStream 的用法及分析3.4.11 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/fy3RnMv","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 数据转换必须熟悉的算子（Operator)","date":"2021-07-12T16:00:00.000Z","path":"2021/07/13/flink-in-action-3.3/","text":"3.3 必须熟悉的数据转换 Operator(算子)在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是上图这种模型，有数据源（source），有数据下游（sink），我们写的应用程序多是对数据源过来的数据做一系列操作，总结如下。 1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation: 数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink: 接收器，Sink 是指 Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来。Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 Socket 、自定义的 Sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。 那么本文将给大家介绍的就是 Flink 中的批和流程序常用的算子（Operator）。 3.3.1 DataStream Operator我们先来看看流程序中常用的算子。 MapMap 算子的输入流是 DataStream，经过 Map 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成一个元素，举个例子： 12345678SingleOutputStreamOperator&lt;Employee&gt; map = employeeStream.map(new MapFunction&lt;Employee, Employee&gt;() &#123; @Override public Employee map(Employee employee) throws Exception &#123; employee.salary = employee.salary + 5000; return employee; &#125;&#125;);map.print(); 新的一年给每个员工的工资加 5000。 FlatMapFlatMap 算子的输入流是 DataStream，经过 FlatMap 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成零个、一个或多个元素，举个例子： 123456789SingleOutputStreamOperator&lt;Employee&gt; flatMap = employeeStream.flatMap(new FlatMapFunction&lt;Employee, Employee&gt;() &#123; @Override public void flatMap(Employee employee, Collector&lt;Employee&gt; out) throws Exception &#123; if (employee.salary &gt;= 40000) &#123; out.collect(employee); &#125; &#125;&#125;);flatMap.print(); 将工资大于 40000 的找出来。 FilterKeyByReduceAggregationsWindowWindowAllUnionWindow JoinSplitSelect3.3.2 DataSet OperatorFirst-n3.3.3 流计算与批计算统一的思路加入知识星球可以看到上面文章：https://t.zsxq.com/iYFMZFA 3.3.4 小结与反思本节介绍了在开发 Flink 作业中数据转换常使用的算子（包含流作业和批作业），DataStream API 和 DataSet API 中部分算子名字是一致的，也有不同的地方，最后讲解了下 Flink 社区后面流批统一的思路。 你们公司使用 Flink 是流作业居多还是批作业居多？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 如何使用 Flink Window 及 Window 基本概念与实现原理?","date":"2021-07-11T16:00:00.000Z","path":"2021/07/12/flink-in-action-3.2/","text":"3.2 Flink Window 基础概念与实现原理目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。 对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。 在本节将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。 3.2.1 Window 简介下面我们结合一个现实的例子来说明。 就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？ 假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图所示： 可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下图所示： 这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？ 这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图所示： 第一分钟的数量为 18，第二分钟是 28，第三分钟是 24……这样，1 个小时内会有 60 个 Window。 再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和，如下图所示： 此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。 3.2.2 Window 有什么作用？通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。 3.2.3 Flink 自带的 WindowFlink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window： 以时间驱动的 Time Window 以事件数量驱动的 Count Window 以会话间隔驱动的 Session Window 提供上面三种 Window 机制后，由于某些特殊的需要，DataStream API 也提供了定制化的 Window 操作，供用户自定义 Window。 下面将先围绕上面说的三种 Window 来进行分析并教大家如何使用，然后对其原理分析，最后在解析其源码实现。 3.2.4 Time Window 的用法及源码分析3.2.5 Count Window 的用法及源码分析3.2.6 Session Window 的用法及源码分析3.2.7 如何自定义 Window？3.2.8 Window 源码分析3.2.9 Window 组件之 WindowAssigner 的用法及源码分析3.2.10 Window 组件之 Trigger 的用法及源码分析3.2.11 Window 组件之 Evictor 的用法及源码分析加入知识星球可以看到上面文章：https://t.zsxq.com/qnQRvrf 3.2.12 小结与反思本节从生活案例来分享关于 Window 方面的需求，进而开始介绍 Window 相关的知识，并把 Flink 中常使用的三种窗口都一一做了介绍，并告诉大家如何使用，还分析了其实现原理。最后还对 Window 的内部组件做了详细的分析，为自定义 Window 提供了方法。 不知道你看完本节后对 Window 还有什么疑问吗？你们是根据什么条件来选择使用哪种 Window 的？在使用的过程中有遇到什么问题吗？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析","date":"2021-07-10T16:00:00.000Z","path":"2021/07/11/flink-in-action-3.1/","text":"第三章 —— Flink 中的流计算处理通过第二章的入门案例讲解，相信你已经知道了 Flink 程序的开发过程，本章将带你熟悉 Flink 中的各种特性，比如多种时间语义、丰富的窗口机制、流计算中常见的运算操作符、Watermark 机制、丰富的 Connectors（Kafka、ElasticSearch、Redis、HBase 等） 的使用方式。除了介绍这些知识点的原理之外，笔者还将通过案例来教会大家如何去实战使用，最后还会讲解这些原理的源码实现，希望你可以更深刻的理解这些特性。 3.1 Flink 多种时间语义对比 Flink 在流应用程序中支持不同的 Time 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。下面我们一起来看看这三个 Time。 3.1.1 Processing TimeProcessing Time 是指事件被处理时机器的系统时间。 如果我们 Flink Job 设置的时间策略是 Processing Time 的话，那么后面所有基于时间的操作（如时间窗口）都将会使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。 例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。 Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。 3.1.2 Event TimeEvent Time 是指事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。 完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（事件产生的时间顺序）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。 假设所有数据都已到达，Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，不管它们到达的顺序如何（是否按照事件产生的时间）。 3.1.3 Ingestion TimeIngestion Time 是事件进入 Flink 的时间。 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。 Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，成本可能会高一点，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（只在进入 Flink 的时候分配一次），所以对事件的不同窗口操作将使用相同的时间戳（第一次分配的时间戳），而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。 与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序中不必指定如何生成水印。 在 Flink 中，Ingestion Time 与 Event Time 非常相似，唯一区别就是 Ingestion Time 具有自动分配时间戳和自动生成水印功能。 3.1.4 三种 Time 的对比结果3.1.5 使用场景分析3.1.6 Time 策略设置3.1.7 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/znqnMNB","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 案例2：实时处理 Socket 数据","date":"2021-07-09T16:00:00.000Z","path":"2021/07/10/flink-in-action-2.4/","text":"2.4 案例2：实时处理 Socket 数据在 2.3 节中讲解了 Flink 最简单的 WordCount 程序的创建、运行结果查看和代码分析，本节将继续带大家来看一个入门上手的程序：Flink 处理 Socket 数据。 2.4.1 使用 IDEA 创建项目使用 IDEA 创建新的 module，结构如下： 123456789101112├── pom.xml└── src ├── main │ ├── java │ │ └── com │ │ └── zhisheng │ │ └── socket │ │ └── Main.java │ └── resources │ └── log4j.properties └── test └── java 项目创建好了后，我们下一步开始编写 Flink Socket Job 的代码。 2.4.2 实时处理 Socket 数据应用程序代码实现程序代码如下所示： 123456789101112131415161718192021222324252627282930313233public class Main &#123; public static void main(String[] args) throws Exception &#123; //参数检查 if (args.length != 2) &#123; System.err.println(\"USAGE:\\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;\"); return; &#125; String hostname = args[0]; Integer port = Integer.parseInt(args[1]); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据 DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port); //计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new LineSplitter()) .keyBy(0) .sum(1); sum.print(); env.execute(\"Java WordCount from SocketText\"); &#125; public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; String[] tokens = s.toLowerCase().split(\"\\\\W+\"); for (String token: tokens) &#123; if (token.length() &gt; 0) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; pom.xml 添加 build： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.flink:force-shading&lt;/exclude&gt; &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt; &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt; &lt;exclude&gt;log4j:*&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt; &lt;!--注意：这里一定要换成你自己的 Job main 方法的启动类--&gt; &lt;mainClass&gt;com.zhisheng.socket.Main&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2.4.3 运行实时处理 Socket 数据应用程序下面分别讲解在 IDE 和 Flink UI 上运行作业。 本地 IDE 运行UI 运行 Job2.4.4 实时处理 Socket 数据应用程序代码分析加入知识星球可以看到上面文章：https://t.zsxq.com/VBEQv3F 2.4.5 Flink 中使用 Lambda 表达式因为 Lambda 表达式看起来简洁，所以有时候也是希望在这些 Flink 作业中也可以使用上它，虽然 Flink 中是支持 Lambda，但是个人感觉不太友好。比如上面的应用程序如果将 LineSplitter 该类之间用 Lambda 表达式完成的话则要像下面这样写： 12345678910stream.flatMap((s, collector) -&gt; &#123; for (String token : s.toLowerCase().split(\"\\\\W+\")) &#123; if (token.length() &gt; 0) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125;&#125;) .keyBy(0) .sum(1) .print(); 但是这样写完后，运行作业报错如下： 123456789101112Exception in thread &quot;main&quot; org.apache.flink.api.common.functions.InvalidTypesException: The return type of function &apos;main(LambdaMain.java:34)&apos; could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the &apos;ResultTypeQueryable&apos; interface. at org.apache.flink.api.dag.Transformation.getOutputType(Transformation.java:417) at org.apache.flink.streaming.api.datastream.DataStream.getType(DataStream.java:175) at org.apache.flink.streaming.api.datastream.DataStream.keyBy(DataStream.java:318) at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:41)Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of &apos;Collector&apos; are missing. In many cases lambda methods don&apos;t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the &apos;org.apache.flink.api.common.functions.FlatMapFunction&apos; interface. Otherwise the type has to be specified explicitly using type information. at org.apache.flink.api.java.typeutils.TypeExtractionUtils.validateLambdaType(TypeExtractionUtils.java:350) at org.apache.flink.api.java.typeutils.TypeExtractionUtils.extractTypeFromLambda(TypeExtractionUtils.java:176) at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:571) at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196) at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:611) at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:34) 根据上面的报错信息其实可以知道要怎么解决了，该错误是因为 Flink 在用户自定义的函数中会使用泛型来创建 serializer，当使用匿名函数时，类型信息会被保留。但 Lambda 表达式并不是匿名函数，所以 javac 编译的时候并不会把泛型保存到 class 文件里。解决方法：使用 Flink 提供的 returns 方法来指定 flatMap 的返回类型 12//使用 TupleTypeInfo 来指定 Tuple 的参数类型.returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class)) 在 flatMap 后面加上上面这个 returns 就行了，但是如果算子多了的话，每个都去加一个 returns，其实会很痛苦的，所以通常使用匿名函数或者自定义函数居多。 2.4.5 小结与反思在第一章中介绍了 Flink 的特性，本章主要是让大家能够快速入门，所以在第一节和第二节中分别讲解了 Flink 的环境准备和搭建，在第三节和第四节中通过两个入门的应用程序（WordCount 应用程序和读取 Socket 数据应用程序）让大家可以快速入门 Flink，两个程序都是需要自己动手实操，所以更能加深大家的印象。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 案例1：WordCount 应用程序","date":"2021-07-08T16:00:00.000Z","path":"2021/07/09/flink-in-action-2.3/","text":"2.3 案例1：WordCount 应用程序在 2.2 节中带大家讲解了下 Flink 的环境安装，这篇文章就开始我们的第一个 Flink 案例实战，也方便大家快速开始自己的第一个 Flink 应用。大数据里学习一门技术一般都是从 WordCount 开始入门的，那么笔者还是不打破常规了，所以这篇文章笔者也将带大家通过 WordCount 程序来初步了解 Flink。 2.3.1 使用 Maven 创建项目Flink 支持 Maven 直接构建模版项目，你在终端使用该命令： 1234mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.9.0 在执行的过程中它会提示你输入 groupId、artifactId、和 package 名，你按照要求输入就行，最后就可以成功创建一个项目，如下图所示。 进入到目录你就可以看到已经创建了项目，里面结构如下： 1234567891011121314 zhisheng@zhisheng ~/IdeaProjects/github/Flink-WordCount tree.├── pom.xml└── src └── main ├── java │ └── com │ └── zhisheng │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties6 directories, 4 files 该项目中包含了两个类 BatchJob 和 StreamingJob，另外还有一个 log4j.properties 配置文件，然后你就可以将该项目导入到 IDEA 了。你可以在该目录下执行 mvn clean package 就可以编译该项目，编译成功后在 target 目录下会生成一个 Job 的 Jar 包，但是这个 Job 还不能执行，因为 StreamingJob 这个类中的 main 方法里面只是简单的创建了 StreamExecutionEnvironment 环境，然后就执行 execute 方法，这在 Flink 中是不算一个可执行的 Job 的，因此如果你提交到 Flink UI 上也是会报错的。 如下图所示，上传作业程序打包编译的 Jar 包： 运行报错结果如下图所示： 12Server Response Message:Internal server error. 我们查看 Flink JobManager 的日志，可以看见错误信息如下图所示： 122019-04-26 17:27:33,706 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler - Unhandled exception.org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: No operators defined in streaming topology. Cannot execute. 因为 execute 方法之前我们是需要补充我们 Job 的一些算子操作的，所以报错还是很正常的，本节下面将会提供完整代码。 2.3.2 使用 IDEA 创建项目一般我们项目可能是由多个 Job 组成，并且代码也都是在同一个工程下面进行管理，上面那种创建方式适合单个 Job 执行，但如果在公司多人合作的时候还是得在同一个工程下面创建项目，每个 Flink Job 对应着一个 module，该 module 负责独立的业务逻辑，比如笔者在 GitHub 的 https://github.com/zhisheng17/flink-learning 项目，它的项目结构如下图所示： 接下来我们需要在父工程的 pom.xml 中加入如下属性（含编码、Flink 版本、JDK 版本、Scala 版本、Maven 编译版本）： 1234567891011&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!--Flink 版本--&gt; &lt;flink.version&gt;1.9.0&lt;/flink.version&gt; &lt;!--JDK 版本--&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;!--Scala 2.11 版本--&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt;&lt;/properties&gt; 然后加入依赖： 1234567891011121314151617181920212223242526272829303132&lt;dependencies&gt; &lt;!-- Apache Flink dependencies --&gt; &lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Add logging framework, to produce console output when running in the IDE. --&gt; &lt;!-- These dependencies are excluded from the application JAR by default. --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 上面依赖中 flink-java 和 flink-streaming-java 是我们 Flink 必备的核心依赖，为什么设置 scope 为 provided 呢（默认是 compile）？是因为 Flink 其实在自己的安装目录中 lib 文件夹里的 lib/flink-dist_2.11-1.9.0.jar 已经包含了这些必备的 Jar 了，所以我们在给自己的 Flink Job 添加依赖的时候最后打成的 Jar 包可不希望又将这些重复的依赖打进去。有两个好处： 减小了我们打的 Flink Job Jar 包容量大小 不会因为打入不同版本的 Flink 核心依赖而导致类加载冲突等问题 但是问题又来了，我们需要在 IDEA 中调试运行我们的 Job，如果将 scope 设置为 provided 的话，是会报错的： 123456789101112131415Error: A JNI error has occurred, please check your installation and try againException in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/api/common/ExecutionConfig$GlobalJobParameters at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.privateGetMethodRecursive(Class.java:3048) at java.lang.Class.getMethod0(Class.java:3018) at java.lang.Class.getMethod(Class.java:1784) at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.common.ExecutionConfig$GlobalJobParameters at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 7 more 默认 scope 为 compile 的话，本地调试的话就不会出错了。另外测试到底能够减小多少 Jar 包的大小呢？我这里先写了个 Job 测试。当 scope 为 compile 时，编译后的 target 目录： 12345678zhisheng@zhisheng ~/Flink-WordCount/target  master ●✚ lltotal 94384-rw-r--r-- 1 zhisheng staff 45M 4 26 21:23 Flink-WordCount-1.0-SNAPSHOT.jardrwxr-xr-x 4 zhisheng staff 128B 4 26 21:23 classesdrwxr-xr-x 3 zhisheng staff 96B 4 26 21:23 generated-sourcesdrwxr-xr-x 3 zhisheng staff 96B 4 26 21:23 maven-archiverdrwxr-xr-x 3 zhisheng staff 96B 4 26 21:23 maven-status-rw-r--r-- 1 zhisheng staff 7.2K 4 26 21:23 original-Flink-WordCount-1.0-SNAPSHOT.jar 当 scope 为 provided 时，编译后的 target 目录： 12345678zhisheng@zhisheng ~/Flink-WordCount/target  master ●✚ lltotal 32-rw-r--r-- 1 zhisheng staff 7.5K 4 26 21:27 Flink-WordCount-1.0-SNAPSHOT.jardrwxr-xr-x 4 zhisheng staff 128B 4 26 21:27 classesdrwxr-xr-x 3 zhisheng staff 96B 4 26 21:27 generated-sourcesdrwxr-xr-x 3 zhisheng staff 96B 4 26 21:27 maven-archiverdrwxr-xr-x 3 zhisheng staff 96B 4 26 21:27 maven-status-rw-r--r-- 1 zhisheng staff 7.2K 4 26 21:27 original-Flink-WordCount-1.0-SNAPSHOT.jar 。。。 2.3.3 流计算 WordCount 应用程序代码实现2.3.4 运行流计算 WordCount 应用程序本地 IDE 运行UI 运行 Job2.3.5 流计算 WordCount 应用程序代码分析2.3.6 小结与反思加入知识星球可以看到上面文章：https://t.zsxq.com/Z7EAmq3","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 环境搭建","date":"2021-07-07T16:00:00.000Z","path":"2021/07/08/flink-in-action-2.2/","text":"2.2 Flink 环境搭建 在 2.1 节中已经将 Flink 的准备环境已经讲完了，本章节将带大家正式开始接触 Flink，那么我们得先安装一下 Flink。Flink 是可以在多个平台（Windows、Linux、Mac）上安装的。在开始写本书的时候最新版本是 1.8 版本，但是写到一半后更新到 1.9 了（合并了大量 Blink 的新特性），所以笔者又全部更新版本到 1.9，书籍后面也都是基于最新的版本讲解与演示。 Flink 的官网地址是：https://flink.apache.org/ 2.2.1 Flink 下载与安装Flink 在 Mac、Linux、Window 平台上的安装方式如下。 在 Mac 和 Linux 下安装你可以通过该地址 https://flink.apache.org/downloads.html 下载到最新版本的 Flink。这里我们选择 Apache Flink 1.9.0 for Scala 2.11 版本，点击跳转到了一个镜像下载选择的地址，如下图所示，随便选择哪个就行，只是下载速度不一致而已。 下载完后，你就可以直接解压下载的 Flink 压缩包了。接下来我们可以启动一下 Flink，我们进入到 Flink 的安装目录下执行命令 ./bin/start-cluster.sh 即可，产生的日志如下： 1234zhisheng@zhisheng /usr/local/flink-1.9.0 ./bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host zhisheng.Starting taskexecutor daemon on host zhisheng. 如果你的电脑是 Mac 的话，那么你也可以通过 Homebrew 命令进行安装。先通过命令 brew search flink 查找一下包： 123 zhisheng@zhisheng ~ brew search flink==&gt; Formulaeapache-flink ✔ homebrew/linuxbrew-core/apache-flink 可以发现找得到 Flink 的安装包，但是这样安装的版本可能不是最新的，如果你要安装的话，则使用命令： 1brew install apache-flink 那么它就会开始进行下载并安装好，安装后的目录应该是在 /usr/local/Cellar/apache-flink 下，如下图所示： 你可以通过下面命令检查安装的 Flink 到底是什么版本的： 1flink --version 结果如下： 1Version: 1.9.0, Commit ID: ff472b4 这种的话运行是得进入 /usr/local/Cellar/apache-flink/1.9.0/libexec/bin 目录下执行命令 ./start-cluster.sh 才可以启动 Flink 的。执行命令后的启动日志如下所示： 123Starting cluster.Starting standalonesession daemon on host zhisheng.Starting taskexecutor daemon on host zhisheng. 在 Windows 下安装如果你的电脑系统是 Windows 的话，那么你就直接双击 Flink 安装目录下面 bin 文件夹里面的 start-cluster.bat 就行，同样可以将 Flink 起动成功。 2.2.2 Flink 启动与运行启动成功后的话，我们可以通过访问地址http://localhost:8081/ 查看 UI 长啥样了，如下图所示： 2.2.3 Flink 目录配置文件解读2.2.4 Flink 源码下载2.2.5 Flink 源码编译2.2.6 将 Flink 源码导入到 IDE加入知识星球可以看到上面文章：https://t.zsxq.com/JyRzVnU 2.2.7 小结与反思本节主要讲了 FLink 在不同系统下的安装和运行方法，然后讲了下怎么去下载源码和将源码导入到 IDE 中。不知道你在将源码导入到 IDE 中是否有遇到什么问题呢？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— Flink 环境准备","date":"2021-07-06T16:00:00.000Z","path":"2021/07/07/flink-in-action-2.1/","text":"第二章 —— Flink 入门通过第一章对 Flink 的介绍，相信你对 Flink 的概念和特性有了一定的了解，接下来本章将开始正式进入 Flink 的学习之旅，笔者将带你搭建 Flink 的环境和编写两个案例（WordCount 程序、读取 Socket 数据）来入门 Flink。 2.1 Flink 环境准备 通过前面的章节内容，相信你已经对 Flink 的基础概念等知识已经有一定了解，现在是不是迫切的想把 Flink 给用起来？先别急，我们先把电脑的准备环境给安装好，这样后面才能更愉快地玩耍。 废话不多说了，直奔主题。因为本书后面章节内容会使用 Kafka、MySQL、ElasticSearch 等组件，并且运行 Flink 程序是需要依赖 Java 的，另外就是我们需要使用 IDE 来开发 Flink 应用程序以及使用 Maven 来管理 Flink 应用程序的依赖，所以本节我们提前安装这个几个组件，搭建好本地的环境，后面如果还要安装其他的组件笔者会在对应的章节中补充，如果你的操作系统已经中已经安装过 JDK、Maven、MySQL、IDEA 等，那么你可以跳过对应的内容，直接看你未安装过的。 这里笔者再说下自己电脑的系统环境：macOS High Sierra 10.13.5，后面文章的演示环境不作特别说明的话就是都在这个系统环境中。 2.1.1 JDK 安装与配置虽然现在 JDK 已经更新到 12 了，但是为了稳定我们还是安装 JDK 8，如果没有安装过的话，可以去官网 的下载页面下载对应自己操作系统的最新 JDK8 就行。 Mac 系统的是 jdk-8u211-macosx-x64.dmg 格式、Linux 系统的是 jdk-8u211-linux-x64.tar.gz 格式。Mac 系统安装的话直接双击然后一直按照提示就行了，最后 JDK 的安装目录在 /Library/Java/JavaVirtualMachines/ ，然后在 /etc/hosts 中配置好环境变量（注意：替换你自己电脑本地的路径）。 123export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Homeexport CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:export PATH=$PATH:$JAVA_HOME/bin Linux 系统的话就是在某个目录下直接解压就行了，然后在 /etc/profile 添加一下上面的环境变量（注意：替换你自己电脑的路径）。然后执行 java -version 命令可以查看是否安装成功！ 1234 zhisheng@zhisheng ~ java -versionjava version &quot;1.8.0_152&quot;Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode) 2.1.2 Maven 安装与配置安装好 JDK 后我们就可以安装 Maven 了，我们在官网下载二进制包就行，然后在自己本地软件安装目录解压压缩包就行。接下来你需要配置一下环境变量： 12export M2_HOME=/Users/zhisheng/Documents/maven-3.5.2export PATH=$PATH:$M2_HOME/bin 然后执行命令 mvn -v 可以验证是否安装成功，结果如下： 1234567zhisheng@zhisheng ~ /Users mvn -vApache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)Maven home: /Users/zhisheng/Documents/maven-3.5.2Java version: 1.8.0_152, vendor: Oracle CorporationJava home: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;10.13.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; 2.1.3 IDE 安装与配置安装完 JDK 和 Maven 后，就可以安装 IDE 了，大家可以选择你熟练的 IDE 就行，笔者后面演示的代码都是在 IDEA 中运行的，如果想为了后面不出其他的问题的话，建议尽量和笔者的环境保持一致。 IDEA 官网下载地址：下载页面的地址，下载后可以双击后然后按照提示一步步安装，安装完成后需要在 IDEA 中配置 JDK 路径和 Maven 的路径，后面我们开发也都是靠 Maven 来管理项目的依赖。 2.1.4 MySQL 安装与配置2.1.5 Kafka 安装与配置2.1.6 ElasticSearch 安装与配置加入知识星球可以看到上面文章：https://t.zsxq.com/JyRzVnU 2.1.7 小结与反思本节讲解了下 JDK、Maven、IDE、MySQL、Kafka、ElasticSearch 的安装与配置，因为这些都是后面要用的，所以这里单独抽一篇文章来讲解环境准备的安装步骤，当然这里还并不涉及全，因为后面我们还可能会涉及到 HBase、HDFS 等知识，后面我们用到再看，本书的内容主要讲解 Flink，所以更多的环境准备还是得靠大家自己独立完成。 这里笔者说下笔者自己一般安装环境的选择： xxx 下面章节我们就正式进入 Flink 专题了！","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 大数据计算框架对比","date":"2021-07-05T16:00:00.000Z","path":"2021/07/06/flink-in-action-1.3/","text":"1.3 大数据计算框架对比在 1.2 节中已经跟大家详细介绍了 Flink，那么在本节就主要 Blink、Spark Streaming、Structured Streaming 和 Storm 的区别。 1.3.1 FlinkFlink 是一个针对流数据和批数据分布式处理的引擎，在某些对实时性要求非常高的场景，基本上都是采用 Flink 来作为计算引擎，它不仅可以处理有界的批数据，还可以处理无界的流数据，在 Flink 的设计愿想就是将批处理当成是流处理的一种特例。 如下图所示，在 Flink 的母公司 Data Artisans 被阿里收购之后，阿里也在开始逐步将内部的 Blink 代码开源出来并合并在 Flink 主分支上。 而 Blink 一个很强大的特点就是它的 Table API &amp; SQL 很强大，社区也在 Flink 1.9 版本将 Blink 开源版本大部分代码合进了 Flink 主分支。 1.3.2 BlinkBlink 是早期阿里在 Flink 的基础上开始修改和完善后在内部创建的分支，然后 Blink 目前在阿里服务于阿里集团内部搜索、推荐、广告、菜鸟物流等大量核心实时业务，如下图所示。 Blink 在阿里内部错综复杂的业务场景中锻炼成长着，经历了内部这么多用户的反馈（各种性能、资源使用率、易用性等诸多方面的问题），Blink 都做了针对性的改进。在 Flink Forward China 峰会上，阿里巴巴集团副总裁周靖人宣布 Blink 在 2019 年 1 月正式开源，同时阿里也希望 Blink 开源后能进一步加深与 Flink 社区的联动， Blink 开源地址：https://github.com/apache/flink/tree/blink 开源版本 Blink 的主要功能和优化点： 1、Runtime 层引入 Pluggable Shuffle Architecture，开发者可以根据不同的计算模型或者新硬件的需要实现不同的 shuffle 策略进行适配；为了性能优化，Blink 可以让算子更加灵活的 chain 在一起，避免了不必要的数据传输开销；在 BroadCast Shuffle 模式中，Blink 优化掉了大量的不必要的序列化和反序列化开销；Blink 提供了全新的 JM FailOver 机制，JM 发生错误之后，新的 JM 会重新接管整个 JOB 而不是重启 JOB，从而大大减少了 JM FailOver 对 JOB 的影响；Blink 支持运行在 Kubernetes 上。 2、SQL/Table API 架构上的重构和性能的优化是 Blink 开源版本的一个重大贡献。 3、Hive 的兼容性，可以直接用 Flink SQL 去查询 Hive 的数据，Blink 重构了 Flink catalog 的实现，并且增加了两种 catalog，一个是基于内存存储的 FlinkInMemoryCatalog，另外一个是能够桥接 Hive metaStore 的 HiveCatalog。 4、Zeppelin for Flink 5、Flink Web，更美观的 UI 界面，查看日志和监控 Job 都变得更加方便 对于开源那会看到一个对话让笔者感到很震撼： Blink 开源后，两个开源项目之间的关系会是怎样的？未来 Flink 和 Blink 也会由不同的团队各自维护吗？ Blink 永远不会成为另外一个项目，如果后续进入 Apache 一定是成为 Flink 的一部分 对话详情如下图所示： 在 Blink 开源那会，笔者就将源码自己编译了一份，然后自己在本地一直运行着，感兴趣的可以看看文章 阿里巴巴开源的 Blink 实时计算框架真香 ，你会发现 Blink 的 UI 还是比较美观和实用的。 如果你还对 Blink 有什么疑问，可以看看下面两篇文章： 阿里重磅开源 Blink：为什么我们等了这么久？ 重磅！阿里巴巴 Blink 正式开源，重要优化点解读 1.3.3 SparkApache Spark 是一种包含流处理能力的下一代批处理框架。与 Hadoop 的 MapReduce 引擎基于各种相同原则开发而来的 Spark 主要侧重于通过完善的内存计算和处理优化机制加快批处理工作负载的运行速度。Spark 可作为独立集群部署（需要相应存储层的配合），或可与 Hadoop 集成并取代 MapReduce 引擎。 Spark Streaming 是 Spark API 核心的扩展，可实现实时数据的快速扩展，高吞吐量，容错处理。数据可以从很多来源（如 Kafka、Flume、Kinesis 等）中提取，并且可以通过很多函数来处理这些数据，处理完后的数据可以直接存入数据库或者 Dashboard 等，如下两图所示。 Spark Streaming 的内部实现原理是接收实时输入数据流并将数据分成批处理，然后由 Spark 引擎处理以批量生成最终结果流，也就是常说的 micro-batch 模式，如下图所示。 Spark DStreams DStreams 是 Spark Streaming 提供的基本的抽象，它代表一个连续的数据流。它要么是从源中获取的输入流，要么是输入流通过转换算子生成的处理后的数据流。在内部实现上，DStream 由连续的序列化 RDD 来表示，每个 RDD 含有一段时间间隔内的数据，如下图所示： 任何对 DStreams 的操作都转换成了对 DStreams 隐含的 RDD 的操作。例如 flatMap 操作应用于 lines 这个 DStreams 的每个 RDD，生成 words 这个 DStreams 的 RDD 过程如下图所示： 通过 Spark 引擎计算这些隐含 RDD 的转换算子。DStreams 操作隐藏了大部分的细节，并且为了更便捷，为开发者提供了更高层的 API。 Spark 支持的滑动窗口 它和 Flink 的滑动窗口类似，支持传入两个参数，一个代表窗口长度，一个代表滑动间隔，如下图所示。 Spark 支持更多的 API 因为 Spark 是使用 Scala 开发的居多，所以从官方文档就可以看得到对 Scala 的 API 支持的很好，而 Flink 源码实现主要以 Java 为主，因此也对 Java API 更友好，从两者目前支持的 API 友好程度，应该是 Spark 更好，它目前也支持 Python API，但是 Flink 新版本也在不断的支持 Python API。 Spark 支持更多的 Machine Learning Lib 你可以很轻松的使用 Spark MLlib 提供的机器学习算法，然后将这些这些机器学习算法模型应用在流数据中，目前 Flink Machine Learning 这块的内容还较少，不过阿里宣称会开源些 Flink Machine Learning 算法，保持和 Spark 目前已有的算法一致，我自己在 GitHub 上看到一个阿里开源的仓库，感兴趣的可以看看 flink-ai-extended。 Spark Checkpoint Spark 和 Flink 一样都支持 Checkpoint，但是 Flink 还支持 Savepoint，你可以在停止 Flink 作业的时候使用 Savepoint 将作业的状态保存下来，当作业重启的时候再从 Savepoint 中将停止作业那个时刻的状态恢复起来，保持作业的状态和之前一致。 Spark SQL Spark 除了 DataFrames 和 Datasets 外，也还有 SQL API，这样你就可以通过 SQL 查询数据，另外 Spark SQL 还可以用于从 Hive 中读取数据。 从 Spark 官网也可以看到很多比较好的特性，这里就不一一介绍了，如果对 Spark 感兴趣的话也可以去官网了解一下具体的使用方法和实现原理。 Spark Streaming 优缺点 1、优点 Spark Streaming 内部的实现和调度方式高度依赖 Spark 的 DAG 调度器和 RDD，这就决定了 Spark Streaming 的设计初衷必须是粗粒度方式的，也就无法做到真正的实时处理 Spark Streaming 的粗粒度执行方式使其确保“处理且仅处理一次”的特性，同时也可以更方便地实现容错恢复机制。 由于 Spark Streaming 的 DStream 本质是 RDD 在流式数据上的抽象，因此基于 RDD 的各种操作也有相应的基于 DStream 的版本，这样就大大降低了用户对于新框架的学习成本，在了解 Spark 的情况下用户将很容易使用 Spark Streaming。 2、缺点 Spark Streaming 的粗粒度处理方式也造成了不可避免的数据延迟。在细粒度处理方式下，理想情况下每一条记录都会被实时处理，而在 Spark Streaming 中，数据需要汇总到一定的量后再一次性处理，这就增加了数据处理的延迟，这种延迟是由框架的设计引入的，并不是由网络或其他情况造成的。 使用的是 Processing Time 而不是 Event Time 1.3.4 Structured Streaming1.3.5 StormStorm 核心组件Storm 核心概念Storm 数据处理流程图1.3.6 计算框架对比Flink VS SparkFlink VS Storm全部对比结果加入知识星球可以看到上面文章：https://t.zsxq.com/vVjeMBY 1.3.7 小结与反思因在 1.2 节中已经对 Flink 的特性做了很详细的讲解，所以本节主要介绍其他几种计算框架（Blink、Spark、Spark Streaming、Structured Streaming、Storm），并对比分析了这几种框架的特点与不同。你对这几种计算框架中的哪个最熟悉呢？了解过它们之间的差异吗？你有压测过它们的处理数据的性能吗？ 本章第一节从公司的日常实时计算需求出发，来分析该如何去实现这种实时需求，接着对比了实时计算与离线计算的区别，从而引出了实时计算的优势，接着就在第二节开始介绍本书的重点 —— 实时计算引擎 Flink，把 Flink 的架构、API、特点、优势等方面都做了讲解，在第三节中对比了市面上现有的计算框架，分别对这些框架做了异同点对比，最后还汇总了它们在各个方面的优势和劣势，以供大家公司内部的技术选型。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》—— 彻底了解大数据实时计算框架 Flink","date":"2021-07-04T16:00:00.000Z","path":"2021/07/05/flink-in-action-1.2/","text":"1.2 彻底了解大数据实时计算框架 Flink在 1.1 节中讲解了日常开发常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。 第一代：Hadoop 承载的 MapReduce 第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务 第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表 第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表 或许会有人不同意以上的分类，笔者觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。 本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看 数据集类型 和 数据运算模型 的种类。 数据集类型数据集类型有分无穷和有界数据集： 无穷数据集：无穷的持续集成的数据集合 有界数据集：有限不会改变的数据集合 那么那些常见的无穷数据集有哪些呢？ 用户与客户端的实时交互数据 应用实时产生的日志 金融市场的实时交易记录 … 数据运算模型数据运算模型有分流式处理和批处理： 流式：只要数据一直在产生，计算就持续地进行 批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源 那么我们再来看看 Flink 它是什么呢？ 1.2.1 Flink 简介Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集，总结如下图所示。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。 如下图所示，Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。 1.2.2 Flink 整体架构Flink 整体架构从下至上分为： 部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。 运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。 API：DataStream、DataSet、Table API &amp; SQL。 扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。 整体架构如下图所示： 1.2.3 Flink 的多种方式部署作为一个计算引擎，如果要做的足够完善，除了它自身的各种特点要包含，还得支持各种生态圈，比如部署的情况，Flink 是支持以 Standalone、YARN、Kubernetes、Mesos 等形式部署的，如下图所示。 每种部署方式介绍如下： Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群。 Standalone：在 Flink 目录下执行 bin/start-cluster.sh 脚本则会启动一个 Standalone 模式的集群。 YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下图所示。 Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 GitHub 看到有下面这种运行架构的。 通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。 1.2.4 Flink 分布式运行流程Flink 作业提交架构流程如下图所示： 具体流程介绍如下： Program Code：我们编写的 Flink 应用程序代码 Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 JobManager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户 JobManager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 Checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 Checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; JobManager 包含 Actor system、Scheduler、Check pointing 三个重要的组件 TaskManager：从 JobManager 处接收需要部署的 Task。TaskManager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 TaskManager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 TaskManager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。同一 JVM 中的任务共享 TCP 连接和心跳消息。TaskManager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 1.2.5 Flink APIFlink 提供了不同的抽象级别的 API 以开发流式或批处理应用，如下图所示。 这四种 API 功能分别是： 最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。 DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。 Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。 Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。 Flink 除了 DataStream 和 DataSet API，它还支持 Table API &amp; SQL，Flink 也将通过 SQL 来构建统一的大数据流批处理引擎，因为在公司中通常会有那种每天定时生成报表的需求（批处理的场景，每晚定时跑一遍昨天的数据生成一个结果报表），但是也是会有流处理的场景（比如采用 Flink 来做实时性要求很高的需求），于是慢慢的整个公司的技术选型就变得越来越多了，这样开发人员也就要面临着学习两套不一样的技术框架，运维人员也需要对两种不一样的框架进行环境搭建和作业部署，平时还要维护作业的稳定性。当我们的系统变得越来越复杂了，作业越来越多了，这对于开发人员和运维来说简直就是噩梦，没准哪天凌晨晚上就被生产环境的告警电话给叫醒。所以 Flink 系统能通过 SQL API 来解决批流统一的痛点，这样不管是开发还是运维，他们只需要关注一个计算框架就行，从而减少企业的用人成本和后期开发运维成本。 1.2.6 Flink 程序与数据流结构一个完整的 Flink 应用程序结构如下图所示： 它们的功能分别是： Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。 代码结构如下图所示： 1.2.7 丰富的 Connector通过源码可以发现不同版本的 Kafka、不同版本的 ElasticSearch、Cassandra、HBase、Hive、HDFS、RabbitMQ 都是支持的，除了流应用的 Connector 是支持的，另外还支持 SQL，如下图所示。 再就是要考虑计算的数据来源和数据最终存储，因为 Flink 在大数据领域的的定位就是实时计算，它不做存储（虽然 Flink 中也有 State 去存储状态数据，这里说的存储类似于 MySQL、ElasticSearch 等存储），所以在计算的时候其实你需要考虑的是数据源来自哪里，计算后的结果又存储到哪里去。庆幸的是 Flink 目前已经支持大部分常用的组件了，比如在 Flink 中已经支持了如下这些 Connector： 不同版本的 Kafka 不同版本的 ElasticSearch Redis MySQL Cassandra RabbitMQ HBase HDFS … 这些 Connector 除了支持流作业外，目前还有还有支持 SQL 作业的，除了这些自带的 Connector 外，还可以通过 Flink 提供的接口做自定义 Source 和 Sink（在 3.8 节中）。 1.2.8 事件时间&amp;处理时间语义Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，如下图所示，后面 3.1 节中会很详细的讲解 Flink 中 Time 的概念。 1.2.9 灵活的窗口机制Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window，如下图所示。后面 3.2 节中会很详细的讲解 Flink 中 Window 的概念。 1.2.10 并行执行任务机制Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1： 1.2.11 状态存储和容错Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构： ValueState MapState ListState ReducingState AggregatingState 另外状态存储也支持多种方式： MemoryStateBackend：存储在内存中 FsStateBackend：存储在文件中 RocksDBStateBackend：存储在 RocksDB 中 Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。 1.2.12 自己的内存管理机制Flink 并不是直接把对象存放在堆内存上，而是将对象序列化为固定数量的预先分配的内存段。它采用类似 DBMS 的排序和连接算法，可以直接操作二进制数据，以此将序列化和反序列化开销降到最低。如果需要处理的数据容量超过内存，那么 Flink 的运算符会将部分数据存储到磁盘。Flink 的主动内存管理和操作二进制数据有几个好处： 保证内存可控，可以防止 OutOfMemoryError 减少垃圾收集压力 节省数据的存储空间 高效的二进制操作 Flink 是如何分配内存、将对象进行序列化和反序列化以及对二进制数据进行操作的，可以参考文章 Flink 是如何管理好内存的？ ，该文中讲解了 Flink 的内存管理机制。 1.2.13 多种扩展库Flink 扩展库中含有机器学习、Gelly 图形处理、CEP 复杂事件处理、State Processing API 等，这些扩展库在一些特殊场景下会比较适用，关于这块内容可以在第六章查看。 1.2.14 小结与反思本节在开始介绍 Flink 之前先讲解了下数据集类型和数据运算模型，接着开始介绍 Flink 的各种特性，对于这些特性，你是否有和其他的计算框架做过对比？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《Flink 实战与性能优化》——你的公司是否需要引入实时计算引擎？","date":"2021-07-03T16:00:00.000Z","path":"2021/07/04/flink-in-action-1.1/","text":"第一章 —— 实时计算引擎本章会从公司常见的计算需求去分析该如何实现这些需求，接着会对比分析实时计算与离线计算之间的区别，从而帮大家分析公司是否需要引入实时计算引擎。接着将会详细的介绍目前最火的实时计算引擎 Flink 的特性，让大家知道其优点，最后会对比其他的计算框架，比如 Spark、Storm 等，希望可以从对比结果来分析这几种计算框架的各自优势，从而为你做技术选型提供一点帮助。 1.1 你的公司是否需要引入实时计算引擎 大数据发展至今，数据呈指数倍的增长，对实效性的要求也越来越高，所以你可能接触到的实时计算需求会越来越多。本章节将从实时计算需求开始讲起，然后阐述完成该需求需要做的工作，最后对比实时计算与离线计算。 1.1.1 实时计算需求在公司里面，你可能会收到领导、产品经理或者运营等提出的如下需求： 1234567891011121314151617小田，你看能不能做个监控大屏实时查看促销活动商品总销售额（GMV）？小朱，搞促销活动的时候能不能实时统计下网站的 PV/UV 啊？小鹏，我们现在搞促销活动能不能实时统计销量 Top5 商品啊？小李，怎么回事啊？现在搞促销活动结果服务器宕机了都没告警，能不能加一个？小刘，服务器这会好卡，是不是出了什么问题啊，你看能不能做个监控大屏实时查看机器的运行情况？小赵，我们线上的应用频繁出现 Error 日志，但是只有靠人肉上机器查看才知道情况，能不能在出现错误的时候及时告警通知？小夏，我们 1 元秒杀促销活动中有件商品被某个用户薅了 100 件，怎么都没有风控啊？小宋，你看我们搞促销活动能不能根据每个顾客的浏览记录实时推荐不同的商品啊？…… 那上面这些需求分别对应着什么业务场景呢？我们来总结下，大概如下图所示： 初看这些需求，你是不是感觉实现会比较难？那么接下来我们来分析一下该如何实现这些需求？从这些需求来看，最根本的业务都是需要实时查看数据信息，那么首先我们得想想如何实时去采集数据，然后将采集到的数据进行实时的计算，最后将计算后的结果下发到第三方。从采集到计算再到下发计算结果的整个过程，必须都得是实时的，这样我们看到的数据才是最接近实时的，这样才能够很完美的完成上面的这些实时计算需求。 1.1.2 数据实时采集就上面这些需求，我们知道了需要实时去采集数据，但是针对这些需求，我们到底需要采集些什么数据呢？如下就是我们需要采集的数据： 用户搜索信息 用户浏览商品信息 用户下单订单信息 网站的所有浏览记录 机器 CPU/Mem/IO 信息 应用日志信息 1.1.3 数据实时计算采集后的数据实时上报后，需要做实时的计算，那我们怎么实现计算呢？ 计算所有商品的总销售额 统计单个商品的销量，最后求 Top5 关联用户信息和浏览信息、下单信息 统计网站所有的请求 IP 并统计每个 IP 的请求数量 计算一分钟内机器 CPU/Mem/IO 的平均值、75 分位数值 过滤出 Error 级别的日志信息 1.1.4 数据实时下发实时计算后的数据，需要及时的下发到下游，这里说的下游代表可能是告警方式（邮件、短信、钉钉、微信）、存储（消息队列、DB、文件系统等）。 （1）告警方式（邮件、短信、钉钉、微信） 在计算层会将计算结果与阈值进行比较，超过阈值触发告警，让运维提前收到通知，告警消息如下图所示，这样运维可以及时做好应对措施，减少故障的损失大小。 （2）存储（消息队列、DB、文件系统等） 数据存储后，监控大盘（Dashboard）从存储（ElasticSearch、HBase 等）里面查询对应指标的数据就可以查看实时的监控信息，做到对促销活动的商品销量、销售额，机器 CPU、Mem 等有实时监控，运营、运维、开发、领导都可以实时查看并作出对应的措施。 让运营知道哪些商品是爆款，哪些店铺成交额最多，如下图所示，哪些商品成交额最高，哪些商品浏览量最多； 让运维可以时刻了解机器的运行状况，如下图所示，出现宕机或者其他不稳定情况可以及时处理； 让开发知道自己项目运行的情况，从 Error 日志知道出现了哪些 Bug，如下图所示； 让领导知道这次促销赚了多少 money，如下图所示。 从数据采集到数据计算再到数据下发，如下图所示，整个流程在上面的场景对实时性要求还是很高的，任何一个地方出现问题都将影响最后的效果！ 1.1.5 实时计算场景前面说了这么多场景，这里我们总结一下实时计算常用的场景有哪些呢？比如： 交通信号灯数据 道路上车流量统计（拥堵状况） 公安视频监控 服务器运行状态监控 金融证券公司实时跟踪股市波动，计算风险价值 数据实时 ETL 银行或者支付公司涉及金融盗窃的预警 …… 另外自己还做过调研，实时计算框架的使用场景有如下这些： 业务数据处理，聚合业务数据，统计之类 流量日志 ETL 安防这块，公安视频结构化数据，用 Flink 做图片搜索 风控，主要处理结构化数据 业务告警 动态数据监控 总结一下大概有下面这四类，如下图所示： 这四类分别是： 实时数据存储：实时数据存储的时候做一些微聚合、过滤某些字段、数据脱敏，组建数据仓库，实时 ETL。 实时数据分析：实时数据接入机器学习框架（TensorFlow）或者一些算法进行数据建模、分析，然后动态的给出商品推荐、广告推荐 实时监控告警：金融相关涉及交易、实时风控、车流量预警、服务器监控告警、应用日志告警 实时数据报表：活动营销时销售额/销售量大屏，TopN 商品 说到实时计算，这里不得不讲一下它与传统的离线计算之间的区别！ 1.1.6 离线计算 vs 实时计算再讲离线计算和实时计算这两个区别之前，我们先来看看流处理和批处理。 流处理与批处理流处理是一种重要的大数据处理手段，其主要特点是其处理的数据是源源不断且实时到来的。批处理历史比较悠久，而且使用的场景比较多，其主要操作的是大容量的静态数据集，并在计算过程完成后返回结果。它们之间的区别如下图所示： 看完流处理与批处理这两者的区别之后，我们来抽象一下前面内容的场景需求计算流程（实时计算）如下图所示： 实时计算需要不断的从 MQ 中读取采集的数据，然后处理计算后往 DB 里存储，在计算这层你无法感知到会有多少数据量过来、要做一些简单的操作（过滤、聚合等）、及时将数据下发。然而传统的离线计算却如下图所示： 在计算这层，它从 DB（不限 MySQL，还有其他的存储介质）里面读取数据，该数据一般就是固定的（前一天、前一星期、前一个月），然后再做一些复杂的计算或者统计分析，最后生成可供直观查看的报表（dashboard）。 离线计算的特点离线计算一般有下面这些特点： 数据量大且时间周期长（一天、一星期、一个月、半年、一年） 在大量数据上进行复杂的批量计算操作 数据在计算之前已经固定，不再会发生变化 能够方便的查询批量计算的结果 实时计算的特点在大数据中与离线计算对应的则是实时计算，那么实时计算有什么特点呢？由于应用场景的各不相同，所以这两种计算引擎接收数据的方式也不太一样：离线计算的数据是固定的（不再会发生变化），通常离线计算的任务都是定时的，如：每天晚上 0 点的时候定时计算前一天的数据，生成报表；然而实时计算的数据源却是流式的。 这里我不得不讲讲什么是流式数据呢？我的理解是比如你在淘宝上下单了某个商品或者点击浏览了某件商品，你就会发现你的页面立马就会给你推荐这种商品的广告和类似商品的店铺，这种就是属于实时数据处理然后作出相关推荐，这类数据需要不断的从你在网页上的点击动作中获取数据，之后进行实时分析然后给出推荐。 流式数据的特点流式数据一般有下面这些特点： 数据实时到达 数据到达次序独立，不受应用系统所控制 数据规模大且无法预知容量 原始数据一经处理，除非特意保存，否则不能被再次取出处理，或者再次提取数据代价昂贵 通过上面的内容可以总结实时计算与离线计算的对比如下图所示： 实时计算的优势实时计算一时爽，一直实时计算一直爽，对于持续生成最新数据的场景，采用流数据处理是非常有利的。例如，再监控服务器的一些运行指标的时候，能根据采集上来的实时数据进行判断，当超出一定阈值的时候发出警报，进行提醒作用。再如通过处理流数据生成简单的报告，如五分钟的窗口聚合数据平均值。复杂的事情还有在流数据中进行数据多维度关联、聚合、筛选，从而找到复杂事件中的根因。更为复杂的是做一些复杂的数据分析操作，如应用机器学习算法，然后根据算法处理后的数据结果提取出有效的信息，作出、给出不一样的推荐内容，让不同的人可以看见不同的网页（千人千面）。 1.1.7 实时计算面临的挑战虽然实时计算有这么多好处，但是要使用实时计算也会面临很多挑战，比如下面这些： 数据处理唯一性（如何保证数据只处理一次？至少一次？最多一次？） 数据处理的及时性（采集的实时数据量太大的话可能会导致短时间内处理不过来，如何保证数据能够及时的处理，不出现数据堆积？） 数据处理层和存储层的可扩展性（如何根据采集的实时数据量的大小提供动态扩缩容？） 数据处理层和存储层的容错性（如何保证数据处理层和存储层高可用，出现故障时数据处理层和存储层服务依旧可用？） 因为各种需求，也就造就了现在不断出现实时计算框架，在 1.2 节中将重磅介绍如今最火的实时计算框架 —— Flink，在 1.3 节中会对比介绍 Spark Streaming、Structured Streaming 和 Storm 之间的区别。 1.1.8 小结与反思本节从实时计算的需求作为切入点，然后分析该如何去完成这种实时计算的需求，从而得知整个过程包括数据采集、数据计算、数据存储等，接着总结了实时计算场景的类型。最后开始介绍离线计算与实时计算的区别，并提出了实时计算可能带来的挑战。你们公司有文中所讲的类似需求吗？你是怎么解决的呢？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"基于 Flink 的动态欺诈检测系统(下)","date":"2021-07-02T16:00:00.000Z","path":"2021/07/03/Flink-Fraud-Detection-engine-3/","text":"如何实现呢？ 介绍在本系列的前两篇文章中，我们描述了如何基于动态更新配置（欺诈检测规则）来实现灵活的数据流分区，以及如何利用 Flink 的 Broadcast 机制在运行时在相关算子之间分配处理配置。 直接跟上我们上次讨论端到端解决方案的地方，在本篇文章中，我们将描述如何使用 Flink 的 “瑞士军刀” —— Process Function 来创建一个量身定制的实现，以满足你的流业务逻辑需求。我们的讨论将在欺诈检测引擎的背景下继续进行，我们还将演示如何在 DataStream API 提供的窗口不能满足你的要求的情况下，通过自定义窗口来实现你自己的需求。特别的是，我们将研究在设计需要对单个事件进行低延迟响应的解决方案时可以做出权衡。 本文将描述一些可以独立应用的高级概念，建议你先阅读本系列第一篇和第二篇文章，并阅读其代码实现，以便更容易理解本文。 ProcessFunction 当作 Window低延迟首先来看下我们将要支持的欺诈检测规则类型： “每当同一付款人在 24 小时内支付给同一受益人的款项总额超过 20 万美元时，就会触发警报。” 换句话说，假设现在有一个按照付款人和受益人组成 key 分区的交易数据流，对于每条到来的交易数据流，我们都会统计两个特定参与者之间前 24 小时到现在的付款总额是否超过预定义的阈值。 欺诈检测系统的常见关键要求之一是响应时间短。欺诈行为越早被检测到，阻止就会越及时，带来的负面影响就会越小。这一要求在金融领域尤为突出，因为用于评估欺诈检测系统的任何时间都是用户需要等待响应所花费的时间。处理的迅速性通常成为各种支付系统之间的竞争优势，产生告警的时间限制可能低至 300-500 毫秒。这是从欺诈检测系统接收到金融交易事件的那一刻起，直到下游系统发出告警为止的所有延迟时间限制。 你可能知道，Flink 已经提供了强大的 Window API，这些 API 可以适用于广泛的场景。但是你查看 Flink 所有支持的窗口类型，你会发现没有一个能完全符合我们这个场景的要求 —— 低延迟的计算每条交易数据。Flink 自带的窗口没有可以表达 “从当前事件返回 x 分钟/小时/天” 的语义。在 Window API 中，事件会落到由窗口分配器定义的窗口中，但是他们本身不能单独控制 Window 的创建和计算。如上所述，我们的欺诈检测引擎的目标是在收到新事件后立即对之前的相关数据进行计算。在这种场景下，利用 Flink 自带的 Window API 不清楚是否可行。Window API 提供了一些用于自定义的 Trigger、Evictor 和 Window Assigner，或许它们可能会帮助到我们获得所需的结果。但是，通常情况下很难做到这一点，此外，这种方法不提供对广播状态的访问，这是然后广播状态是实现业务规则动态配置所必须的。 1) 除了会话窗口，它们仅限于基于会话间隙的分配 我们以使用 Flink 的 Window API 中的滑动窗口为例。使用滑动步长为 S 的滑动窗口转化为等于 S/2 的评估延迟的预期值。这意味着你需要定义 600～1000 毫秒的滑动窗口来满足 300～500 毫秒延迟的低延迟要求。Flink 要为每个滑动窗口存储单独的窗口状态，这会导致作业状态非常大，在任何中等高负载的情况下，这种方案都不可行。为了满足需求，我们需要创建自定的低延迟窗口实现，幸运的是，Flink 为我们提供了这样做所需的所有工具，ProcessFunction 是 Flink API 中一个低级但功能强大的类。它有一个简单的约定： 12345678public class SomeProcessFunction extends KeyedProcessFunction&lt;KeyType, InputType, OutputType&gt; &#123; public void processElement(InputType event, Context ctx, Collector&lt;OutputType&gt; out)&#123;&#125; public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;OutputType&gt; out) &#123;&#125; public void open(Configuration parameters)&#123;&#125;&#125; processElement()：接收输入数据，你可以通过调用 out.collect() 为下一个算子生成一个或者多个输出事件来对每个输入作出反应，你可以将数据传递到侧输出或完全忽略特定的输入数据 onTimer()：当之前注册的定时器触发时，Flink 会调用 onTimer()，支持事件时间和处理时间定时器 open()：相当于一个构造函数，它在 TaskManager 的 JVM 内部调用，用于初始化，例如注册 Flink 管理内存，可以在该方法初始化那些没有序列化的字段或者无法从 JobManager JVM 中传递过来的字段。 最重要的是，ProcessFunction 还可以访问由 Flink 处理的容错状态。这种组合，再加上 Flink 的消息处理能力和低延迟的保证，使得构建具有几乎任意复杂业务逻辑的弹性事件驱动应用程序成为可能。这包括创建和处理带有状态的自定义窗口。 实现状态的清除 为了能够处理时间窗口，我们需要在程序内部跟踪属于该窗口的数据。为了确保这些数据是容错的，并且能够在分布式系统中发生故障的情况下恢复，我们应该将其存储在 Flink 管理的状态中。随着时间的推移，我们不需要保留所有以前的交易数据。根据欺诈检测样例规则，所有早于 24 小时的交易数据都变得无关紧要。我们正在查看一个不断移动的数据窗口，其中过期的数据需要不断移出范围（换句话说，从状态中清除）。 我们将使用 MapState 来存储窗口的各个事件。为了有效清理超出范围的事件，我们将使用事件时间戳作为 MapState 的 key。 在一般情况下，我们必须考虑这样一个事实，即可能存在具有完全相同时间戳的不同事件，因此我们将存储集合而不是每个键（时间戳）的单条数据。 1MapState&lt;Long, Set&lt;Transaction&gt;&gt; windowState; 注意⚠️： 当在 KeyedProcessFunction 中使用任何 Flink 管理的状态时，state.value() 调用返回的数据会自动由当前处理的事件的 key 确定范围 - 参见下图。 如果使用 MapState，则适用相同的原则，不同之处在于返回的是 Map 而不是 MyObject。如果你被迫执行类似 mapState.value().get(inputEvent.getKey()) 之类的操作，你可能应该使用 ValueState 而不是 MapState。因为我们想为每个事件 key 存储多个值，所以在我们的例子中，MapState 是正确的选择。 如本系列的第一篇博客所述，我们将根据主动欺诈检测规则中指定的 key 分配数据。多个不同的规则可以基于相同的分组 key。这意味着我们的警报功能可能会接收由相同 key（例如 {payerId=25;beneficiaryId=12}）限定的交易，但注定要根据不同的规则进行计算，这意味着时间窗口的长度可能不同。这就提出了一个问题，即我们如何才能最好地在 KeyedProcessFunction 中存储容错窗口状态。一种方法是为每个规则创建和管理单独的 MapState。然而，这种方法会很浪费——我们会单独保存重叠时间窗口的状态，因此不必要地存储重复的数据。更好的方法是始终存储刚好足够的数据，以便能够估计由相同 key 限定的所有当前活动规则。为了实现这一点，每当添加新规则时，我们将确定其时间窗口是否具有最大跨度，并将其存储在特殊保留的 WIDEST_RULE_KEY 下的广播状态。 123456789101112131415161718@Overridepublic void processBroadcastElement(Rule rule, Context ctx, Collector&lt;Alert&gt; out)&#123; ... updateWidestWindowRule(rule, broadcastState);&#125;private void updateWidestWindowRule(Rule rule, BroadcastState&lt;Integer, Rule&gt; broadcastState)&#123; Rule widestWindowRule = broadcastState.get(WIDEST_RULE_KEY); if (widestWindowRule == null) &#123; broadcastState.put(WIDEST_RULE_KEY, rule); return; &#125; if (widestWindowRule.getWindowMillis() &lt; rule.getWindowMillis()) &#123; broadcastState.put(WIDEST_RULE_KEY, rule); &#125;&#125; 现在让我们更详细地看一下主要方法 processElement() 的实现。 在上一篇博文中，我们描述了 DynamicKeyFunction 如何允许我们根据规则定义中的 groupingKeyNames 参数执行动态数据分区。随后的描述主要围绕 DynamicAlertFunction，它利用了剩余的规则设置。 如博文系列的前几部分所述，我们的警报处理函数接收 Keyed&lt;Transaction, String, Integer&gt; 类型的事件，其中 Transaction 是主要的“包装”事件，String 是 key（payer #x - beneficiary #y)，Integer 是导致调度此事件的规则的 ID。此规则之前存储在广播状态中，必须通过 ID 从该状态中检索。下面是实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142public class DynamicAlertFunction extends KeyedBroadcastProcessFunction&lt; String, Keyed&lt;Transaction, String, Integer&gt;, Rule, Alert&gt; &#123; private transient MapState&lt;Long, Set&lt;Transaction&gt;&gt; windowState; @Override public void processElement( Keyed&lt;Transaction, String, Integer&gt; value, ReadOnlyContext ctx, Collector&lt;Alert&gt; out)&#123; // Add Transaction to state long currentEventTime = value.getWrapped().getEventTime(); // &lt;--- (1) addToStateValuesSet(windowState, currentEventTime, value.getWrapped()); // Calculate the aggregate value Rule rule = ctx.getBroadcastState(Descriptors.rulesDescriptor).get(value.getId()); // &lt;--- (2) Long windowStartTimestampForEvent = rule.getWindowStartTimestampFor(currentEventTime);// &lt;--- (3) SimpleAccumulator&lt;BigDecimal&gt; aggregator = RuleHelper.getAggregator(rule); // &lt;--- (4) for (Long stateEventTime : windowState.keys()) &#123; if (isStateValueInWindow(stateEventTime, windowStartForEvent, currentEventTime)) &#123; aggregateValuesInState(stateEventTime, aggregator, rule); &#125; &#125; // Evaluate the rule and trigger an alert if violated BigDecimal aggregateResult = aggregator.getLocalValue(); // &lt;--- (5) boolean isRuleViolated = rule.apply(aggregateResult); if (isRuleViolated) &#123; long decisionTime = System.currentTimeMillis(); out.collect(new Alert&lt;&gt;(rule.getRuleId(), rule, value.getKey(), decisionTime, value.getWrapped(), aggregateResult)); &#125; // Register timers to ensure state cleanup long cleanupTime = (currentEventTime / 1000) * 1000; // &lt;--- (6) ctx.timerService().registerEventTimeTimer(cleanupTime); &#125; 以下是步骤的详细信息： 1）我们首先将每个新事件添加到我们的窗口状态： 123456789101112static &lt;K, V&gt; Set&lt;V&gt; addToStateValuesSet(MapState&lt;K, Set&lt;V&gt;&gt; mapState, K key, V value) throws Exception &#123; Set&lt;V&gt; valuesSet = mapState.get(key); if (valuesSet != null) &#123; valuesSet.add(value); &#125; else &#123; valuesSet = new HashSet&lt;&gt;(); valuesSet.add(value); &#125; mapState.put(key, valuesSet); return valuesSet;&#125; 2）接下来，我们检索先前广播的规则，需要根据该规则计算传入的交易数据。 3) getWindowStartTimestampFor 确定，给定规则中定义的窗口跨度和当前事件时间戳，然后计算窗口应该跨度多久。 4) 通过迭代所有窗口状态并应用聚合函数来计算聚合值。它可以是平均值、最大值、最小值，或者如本文开头的示例规则中的总和。 1234567891011121314private boolean isStateValueInWindow( Long stateEventTime, Long windowStartForEvent, long currentEventTime) &#123; return stateEventTime &gt;= windowStartForEvent &amp;&amp; stateEventTime &lt;= currentEventTime;&#125;private void aggregateValuesInState( Long stateEventTime, SimpleAccumulator&lt;BigDecimal&gt; aggregator, Rule rule) throws Exception &#123; Set&lt;Transaction&gt; inWindow = windowState.get(stateEventTime); for (Transaction event : inWindow) &#123; BigDecimal aggregatedValue = FieldsExtractor.getBigDecimalByName(rule.getAggregateFieldName(), event); aggregator.add(aggregatedValue); &#125;&#125; 5) 有了聚合值，我们可以将其与规则定义中指定的阈值进行比较，并在必要时发出警报。 6) 最后，我们使用 ctx.timerService().registerEventTimeTimer() 注册一个清理计时器。当它要移出范围时，此计时器将负责删除当前数据。 7) onTimer 方法会触发窗口状态的清理。 如前所述，我们总是在状态中保留尽可能多的事件，以计算具有最宽窗口跨度的活动规则。这意味着在清理过程中，我们只需要删除这个最宽窗口范围之外的状态。 这是清理程序的实现方式： 123456789101112131415161718192021222324252627@Overridepublic void onTimer(final long timestamp, final OnTimerContext ctx, final Collector&lt;Alert&gt; out) throws Exception &#123; Rule widestWindowRule = ctx.getBroadcastState(Descriptors.rulesDescriptor).get(WIDEST_RULE_KEY); Optional&lt;Long&gt; cleanupEventTimeWindow = Optional.ofNullable(widestWindowRule).map(Rule::getWindowMillis); Optional&lt;Long&gt; cleanupEventTimeThreshold = cleanupEventTimeWindow.map(window -&gt; timestamp - window); // Remove events that are older than (timestamp - widestWindowSpan)ms cleanupEventTimeThreshold.ifPresent(this::evictOutOfScopeElementsFromWindow);&#125;private void evictOutOfScopeElementsFromWindow(Long threshold) &#123; try &#123; Iterator&lt;Long&gt; keys = windowState.keys().iterator(); while (keys.hasNext()) &#123; Long stateEventTime = keys.next(); if (stateEventTime &lt; threshold) &#123; keys.remove(); &#125; &#125; &#125; catch (Exception ex) &#123; throw new RuntimeException(ex); &#125;&#125; 以上是实现细节的描述。我们的方法会在新交易数据到达时立即触发对时间窗口的计算。因此，它满足了我们的主要要求——发出警报的低延迟。完整的实现请看 github 上的项目代码 https://github.com/afedulov/fraud-detection-demo。 完善和优化上面描述的方法的优缺点是什么？ 优点： 低延迟能力 具有潜在用例特定优化的定制解决方案 高效的状态重用（具有相同 key 的规则共享状态） 缺点： 无法利用现有 Window API 中潜在的未来优化 无延迟事件处理，可在 Window API 中开箱即用 二次计算复杂度和潜在的大状态 现在让我们看看后两个缺点，看看我们是否可以解决它们。 延迟数据处理延迟数据之前先提出了一个问题 - 在延迟数据到达的情况下重新评估窗口是否仍然有意义？ 如果需要这样做，你需要增加最宽的窗口大小，用来允许容忍最大的数据延迟。这样将避免因延迟数据问题导致触发了不完整的时间窗口数据。 然而，可以说，对于强调低延迟处理的场景，这种延迟触发将毫无意义。在这种情况下，我们可以跟踪到目前为止我们观察到的最新时间戳，对于不会单调增加此值的事件，只需将它们添加到状态并跳过聚合计算和警报触发逻辑。 冗余重复计算和状态大小在我们描述的实现中，我们保存每条数据处于状态中并在每个新数据来时遍历它们并一次又一次地计算聚合。这在重复计算上浪费计算资源方面显然不是最佳的。 保存每个交易数据处于状态的主要原因是什么？存储事件的粒度直接对应于时间窗口计算的精度。因为我们是存储每条明细交易数据，所以一旦它们离开精确的 2592000000 毫秒时间窗口（以毫秒为单位的 30 天），我们就可以精确地移除它们。在这一点上，值得提出一个问题——在估计这么长的时间窗口时，我们真的需要这个毫秒级的精度，还是在特殊情况下可以接受潜在的误报？如果你的用例的答案是不需要这样的精度，那么你可以基于分桶和预聚合实施额外的优化。这种优化的思想可以分解如下： 不是存储每条明细交易数据，而是创建一个父类，该类可以包含单条数据的字段或是根据聚合函数计算处理一批数据后的聚合值。 不要使用以毫秒为单位的时间戳作为 MapState key，而是将它们四舍五入到你愿意接受的粒度级别（例如，一分钟），将数据分桶。 每当计算窗口时，将新的交易数据存储到聚合桶中，而不是为每个数据存储单独的数据点。 状态数据和序列化器为了进一步优化实现，我们可以问自己的另一个问题是获得具有完全相同时间戳的不同事件的可能性有多大。在所描述的实现中，我们展示了通过在 MapState&lt;Long, Set&lt;Transaction&gt;&gt; 中存储每个时间戳的数据集来解决这个问题的一种方法。但是，这种选择对性能的影响可能比预期的要大。原因是 Flink 当前不提供原生 Set 序列化器，而是强制使用效率较低的 Kryo 序列化器（FLINK-16729）。一个有意义的替代策略是假设在正常情况下，没有两个有差异的事件可以具有完全相同的时间戳，并将窗口状态转换为 MapState&lt;Long, Transaction&gt; 类型。你可以使用辅助输出来收集和监控与你的假设相矛盾的任何意外事件。性能优化期间，我通常建议你禁用 Kryo，并通过确保使用更高效的序列化程序来验证你的应用程序可以进一步优化的位置。 你可以通过设置断点并验证返回的 TypeInformation 的类型来快速确定你的类将使用哪个序列化程序。 PojoTypeInfo 表示将使用高效的 Flink POJO 序列化器。 GenericTypeInfo 表示使用了 Kryo 序列化程序。 交易数据修剪：我们可以将单个事件数据减少到仅要用到的字段，而不是存储完整的事件数据，减少数据序列化与反序列化对机器施加额外的压力。这可能需要根据活动规则的配置将单个事件提取需要对字段出来，并将这些字段存储到通用 Map 数据结构中。 虽然这种调整可能会对大对象产生显著的改进，但它不应该是你的首选。 总结本文总结了我们在第一部分中开始的欺诈检测引擎的实现描述。在这篇博文中，我们演示了如何利用 ProcessFunction 来“模拟”具有复杂自定义逻辑的窗口。我们已经讨论了这种方法的优缺点，并详细说明了如何应用自定义场景特定的优化 - 这是 Window API 无法直接实现的。 这篇博文的目的是说明 Apache Flink API 的强大功能和灵活性。它的核心是 Flink 的支柱，作为开发人员，它为你节省了大量的工作，并通过提供以下内容很好地推广到广泛的用例： 分布式集群中的高效数据交换 通过数据分区的水平可扩展性 具有快速本地访问的容错状态 方便处理状态数据，就像使用局部变量一样简单 多线程、并行执行引擎。 ProcessFunction 代码在单线程中运行，无需同步。 Flink 处理所有并行执行方面并正确访问共享状态，而你作为开发人员不必考虑（并发很难）。 所有这些方面都使得使用 Flink 构建应用程序成为可能，这些应用程序远远超出了普通的流 ETL 用例，并且可以实现任意复杂的分布式事件驱动应用程序。使用 Flink，你可以重新思考处理广泛用例的方法，这些用例通常依赖于使用无状态并行执行节点并将状态容错问题“推”到数据库，这种方法通常注定会遇到可扩展性问题面对不断增长的数据量。 本篇文章属于翻译文章，作者：zhisheng 原文地址：http://www.54tianzhisheng.cn/2021/07/03/Flink-Fraud-Detection-engine-3/ 英文作者：alex_fedulov 英文原文地址：https://flink.apache.org/news/2020/07/30/demo-fraud-detection-3.html 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"基于 Flink 的动态欺诈检测系统(中)","date":"2021-01-22T16:00:00.000Z","path":"2021/01/23/Flink-Fraud-Detection-engine-2/","text":"如何实现呢？ 前言在上一篇博客中，我们对欺诈检测引擎的目标和所需要的功能进行了描述，我们还描述了如何基于可修改的规则而不是使用硬编码的 KeysExtractor 实现 Flink 应用程序的数据自定义分区。 我们在上篇博客中特意省略了有关如何初始化应用的规则以及如何在作业运行时更新的细节，在本文我们将详细的介绍这些细节，你将学习如何将上篇博客中描述的数据分区防御与动态配置结合使用，当这两种模式结合使用的时候，可以省去重新编译代码和重新部署 Flink 作业的需要，从而可以应对多种业务场景逻辑修改的情况。 广播规则首先让我们看看预定义的数据处理代码： 12345DataStream&lt;Alert&gt; alerts = transactions .process(new DynamicKeyFunction()) .keyBy((keyed) -&gt; keyed.getKey()); .process(new DynamicAlertFunction()) DynamicKeyFunction 函数提供动态数据分区，同时 DynamicAlertFunction 函数负责执行处理数据的主要逻辑并根据已定义的规则发动告警消息。 在上篇文中中简化了用例，并假定已预先初始化了所应用的规则集数据，并可以通过 DynamicKeyFunction 的 List&lt;Rules&gt; 访问这些规则： 1234567public class DynamicKeyFunction extends ProcessFunction&lt;Transaction, Keyed&lt;Transaction, String, Integer&gt;&gt; &#123; /* Simplified */ List&lt;Rule&gt; rules = /* Rules that are initialized somehow.*/; ...&#125; 显然，在初始化阶段，可以直接在 Flink Job 的代码内部添加规则到此列表（创建List对象；使用它的add方法）。这样做的主要缺点是，每次修改规则后都需要重新编译作业。在真实的欺诈检测系统中，规则会经常更改，因此从业务和运营需求的角度来看，使此方法不可接受，需要一种不同的方法。 接下来，让我们看一下在上一篇文章中介绍的示例规则定义： 上一篇文章介绍了使用 DynamicKeyFunction提取数据含 groupingKeyNames 里面字段组成数据分组 key 的方法。此规则第二部分中的参数由 DynamicAlertFunction 使用：它们定义了所执行操作的实际逻辑及其参数（例如告警触发限制）。这意味着相同的规则必须同时存在于DynamicKeyFunction和DynamicAlertFunction。 下图展示了我们正在构建的系统的最终工作图： 上图的主要模块是： Transaction Source：Flink 作业的 Source 端，它会并行的消费 Kafka 中的金融交易流数据 **Dynamic Key Function：动态的提取数据分区的 key。随后的keyBy函数会将动态的 key 值进行 hash，并在后续运算符的所有并行实例之间相应地对数据进行分区。 Dynamic Alert Function：累积窗口中的数据，并基于该窗口创建告警。 Apache Flink 内部的数据交换上面的作业图还展示了运算符之间的各种数据交换模式，为了了解广播模式是如何工作的，我们先来看一下 Apache Flink 在分布式运行时存在哪些消息传播方法： FORWARD：上图中 Transaction Source 后的 FORWARD 意味着每个 Transaction Source 的并行度实例消费到的数据都将精确的传输到后面的 DynamicKeyFunction 运算符的每个实例。它还表示两个连接的运算符处于相同的并行度，这种模式如下图所示： HASH：DynamicKeyFunction 和 DynamicAlertFunction 之间的 HASH 意味着每条消息都会计算一个哈希值，并且消息会在下一个运算符的所有可用并行度之间均匀分配，这种连接一般是通过 keyBy 算子。 REBALANCE：这种情况下一般是手动的调用 rebalance() 函数或者并行度发生改变导致的，这样会导致数据以循环的方式重新分区，有助于某些情况喜爱的数据倾斜。 BROADCAST：在本文图二中的欺诈检测作业图中包含了一个 Rules Source，它会从 Kafka 中消费规则数据，然后通过 BROADCAST 的通道将规则数据发动到处理实时数据流的算子中去。与在运算符之间传输数据的其他方法（例如 forward、hash、rebalence，这三种仅会将数据发到下游运算符的某个并行度中去）不同，broadcast 可以使得每条消息都会在下游所有的并行度中处理。broadcast 适用于需要影响所有消息处理的任务，而不管消息的 key 或者 Source 的分区是多少。 广播状态为了使用规则数据流，我们需要将其连接到主数据流： 1234567891011121314// Streams setupDataStream&lt;Transaction&gt; transactions = [...]DataStream&lt;Rule&gt; rulesUpdateStream = [...]BroadcastStream&lt;Rule&gt; rulesStream = rulesUpdateStream.broadcast(RULES_STATE_DESCRIPTOR);// Processing pipeline setup DataStream&lt;Alert&gt; alerts = transactions .connect(rulesStream) .process(new DynamicKeyFunction()) .keyBy((keyed) -&gt; keyed.getKey()) .connect(rulesStream) .process(new DynamicAlertFunction()) 如您所见，可以通过调用broadcast方法并指定状态描述符，从任何常规流中创建广播流。在处理主数据流的事件时需要存储和查找广播的数据，因此，Flink 始终根据此状态描述符自动创建相应的广播状态。这与你在使用其他的状态类型不一样，那些是需要在 open 方法里面对其进行初始化。另请注意，广播状态始终是 KV 格式（MapState）。 12public static final MapStateDescriptor&lt;Integer, Rule&gt; RULES_STATE_DESCRIPTOR = new MapStateDescriptor&lt;&gt;(\"rules\", Integer.class, Rule.class); 连接rulesStream后会导致 ProcessFunction 的内部发生某些变化。上一篇文章以稍微简化的方式介绍了ProcessFunction。但是 DynamicKeyFunction实际上是一个BroadcastProcessFunction。 1234567891011public abstract class BroadcastProcessFunction&lt;IN1, IN2, OUT&gt; &#123; public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector&lt;OUT&gt; out) throws Exception;&#125; 不同的是，添加processBroadcastElement 了方法，该方法是用于处理到达的广播规则流。以下新版本的DynamicKeyFunction 函数允许在 processElement 方法里面中动态的修改数据分发的 key 列表： 1234567891011121314151617181920212223242526public class DynamicKeyFunction extends BroadcastProcessFunction&lt;Transaction, Rule, Keyed&lt;Transaction, String, Integer&gt;&gt; &#123; @Override public void processBroadcastElement(Rule rule, Context ctx, Collector&lt;Keyed&lt;Transaction, String, Integer&gt;&gt; out) &#123; BroadcastState&lt;Integer, Rule&gt; broadcastState = ctx.getBroadcastState(RULES_STATE_DESCRIPTOR); broadcastState.put(rule.getRuleId(), rule); &#125; @Override public void processElement(Transaction event, ReadOnlyContext ctx, Collector&lt;Keyed&lt;Transaction, String, Integer&gt;&gt; out)&#123; ReadOnlyBroadcastState&lt;Integer, Rule&gt; rulesState = ctx.getBroadcastState(RULES_STATE_DESCRIPTOR); for (Map.Entry&lt;Integer, Rule&gt; entry : rulesState.immutableEntries()) &#123; final Rule rule = entry.getValue(); out.collect( new Keyed&lt;&gt;( event, KeysExtractor.getKey(rule.getGroupingKeyNames(), event), rule.getRuleId())); &#125; &#125;&#125; 在上面的代码中，processElement()接收金融交易数据，并在 processBroadcastElement() 接收规则更新数据。创建新规则时，将如上面广播流的那张图所示进行分配，并会保存在所有使用 processBroadcastState 运算符的并行实例中。我们使用规则的 ID 作为存储和引用单个规则的 key。我们遍历动态更新的广播状态中的数据，而不是遍历硬编码的 List&lt;Rules&gt; 。 在将规则存储在广播 MapState 中时，DynamicAlertFunction 遵循相同的逻辑。如第 1 部分中所述，通过processElement 方法输入的每条消息均应按照一个特定规则进行处理，并通过 DynamicKeyFunction 对其进行“预标记”并带有相应的ID。我们需要做的就是使用提供的 ID 从 BroadcastState 中检索相应规则，并根据该规则所需的逻辑对其进行处理。在此阶段，我们还将消息添加到内部函数状态，以便在所需的数据时间窗口上执行计算。我们将在下一篇文章中考虑如何实现这一点。 总结在本文，我们继续研究了使用 Apache Flink 构建的欺诈检测系统的用例。我们研究了在并行运算符实例之间分配数据的不同方式，最重要的是广播状态。我们演示了如何通过广播状态提供的功能来组合和增强动态分区。在运行时发送动态更新的功能是 Apache Flink 的强大功能，适用于多种其他使用场景，例如控制状态（清除/插入/修复），运行 A / B 实验或执行 ML 模型系数的更新。 本篇文章属于翻译文章，作者：zhisheng 原文地址：http://www.54tianzhisheng.cn/2021/01/23/Flink-Fraud-Detection-engine-2/ 英文作者：alex_fedulov 英文原文地址：https://flink.apache.org/news/2020/03/24/demo-fraud-detection-2.html 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"基于 Flink 的动态欺诈检测系统(上)","date":"2021-01-21T16:00:00.000Z","path":"2021/01/22/Flink-Fraud-Detection-engine/","text":"如何实现呢？ 前言在本系列博客中，你将学习到三种构建 Flink 应用程序的强大案例： 动态更新应用程序的逻辑 动态的数据分区（shuffle），在作业运行时进行控制 基于自定义窗口逻辑的低延迟告警（不使用 Window API） 这几个案例扩展了使用静态定义的数据流可以实现的功能，并提供了满足复杂业务需求的基础。 动态更新应用程序的逻辑 允许作业在运行时进行更改，不需要将作业停止后修改代码再发布。 动态的数据分区 为运行中的 Flink 作业作业提供了动态地将数据分组（group by）的功能。对于想要构建一个可以动态配置应用逻辑的 Flink 程序，类似功能很常见。 自定义窗口管理 演示了如何在原生的 Window API 不能完全满足你的需求下，去通过最底层的 Process Function API 来完成你的需求。你将学会如何自定义 Window 逻辑来实现低延迟告警以及如何利用定时器（Timer）来限制状态的无限增长。 这几个案例都是建立在 Flink 核心功能的基础上，但是通过官方文档你可能无法立即明白，因为如果没有具体的用例，解释和呈现它们背后的原理其实并不是那么简单。这就是为什么我们将通过一些实际的案例来展示，本案例为 Apache Flink 的一个真实使用场景 —— 欺诈检测引擎。我希望你能从本系列文章中收获到这些强大的功能和方法，然后能应用在你们实际的应用场景中去。 在该系列的第一篇博客中，我们将先来看看这个应用程序的架构、组件和交互。然后我们将深入研究第一个案例的的实现细节 —— 动态数据分区。 你将能够在本地运行完整的欺诈检测演示应用程序，并且可以通过 Github 仓库查看其完整实现代码。 欺诈检测系统演示本次掩饰的欺诈检测引擎的代码是开源的，可以在线获取，要是想在本地运行它，请按照 https://github.com/afedulov/fraud-detection-demo 中的 README 描述的步骤自行进行操作。 你将看到该案例的代码和组件都很全，仅需要通过 docker 和 docker-compose 构建源码。仓库里面包含了下面组件： 含有 Zookeeper 的 Apache Kafka Apache Flink（应用程序） 欺诈检测引擎的 Web 应用 欺诈检测引擎的目标是消费金融交易的实时数据流，然后根据一组检测规则对其进行评估。这些规则会经常更改和调整，在实际的生产系统中，重要的是要能在作业运行的时候去添加和删除规则，而不会因停止和重新启动作业从而造成高昂的代价。 当你在本地运行成功后，你在浏览器中输入 URL 可以看到如下效果： 点击 “Start” 按钮后，你可以在左侧看到系统中流动的金融交易大盘，你可以通过顶部的滑块去控制每秒生成的数据，中间部分用于管理 Flink 用于计算的规则，你可以在这里创建新规则以及发出控制命令，例如清除 Flink 的状态。 现成的演示带有一组预定义的示例规则，你可以点击 Start 按钮，一段时间之后，将观察到 UI 右侧部分中显示的告警，这些告警消息是 Flink 根据预定义的规则评估生成的交易流的结果。 我们的样本欺诈检测系统包含三个主要的组件： 前端（React） 后端（SpringBoot） 欺诈检测 Flink 应用程序 三者之间的组成关系如下图所示： 后端将 REST API 暴露给前端，用于创建/删除规则以及发出用于管理演示执行的控制命令，然后，它会将这些前端操作行为数据发送到 Kafka Topic Control 中。后端还包含了一个交易数据生成器组件，该组件用来模拟交易数据的，然后会将这些交易数据发送到 Kafka Topic Transactions 中，这些数据最后都会被 Flink 应用程序去消费，Flink 程序经过规则计算这些交易数据后生成的告警数据会发送到 Kafka Topic Alerts 中，并通过 Web Sockets 将数据传到前端 UI。 现在你已经熟悉了该欺诈检测引擎的总体结构和布局了，接下来我们详细介绍这个系统里面包含的内容。 数据动态分区如果过去你曾经使用过 Flink DataStream API，那么你肯定很熟悉 keyBy 方法。对数据流中的所有数据按键进行 shuffle，这样具有相同 key 的元素就会被分配到相同的分区。 一般在程序中，数据分区的 keyBy 字段是固定的，由数据内的某些静态字段确定，例如，当构建一个简单的基于窗口的交易流聚合时，我们可能总是按照交易账户 ID 进行分组。 1234DataStream&lt;Transaction&gt; input = // [...]DataStream&lt;...&gt; windowed = input .keyBy(Transaction::getAccountId) .window(/*window specification*/); 这种方法是在广泛的用例中实现水平可伸缩性的主要模块，但是在应用程序试图在运行时提供业务逻辑灵活性的情况下，这还是不够的。为了理解为什么会发生这种情况，让我们首先以功能需求的形式为欺诈检测系统阐明一个现实的样本规则定义： 在一个星期 之内，当 用户 A 累计 向 B 用户支付的金额超过 1000000 美元，则触发一条告警 PS：A 和 B 用字段描述的话分别是 付款人（payer）和受益人（beneficiary） 在上面的规则中，我们可以发现许多参数，我们希望能够在新提交的规则中指定这些参数，甚至可能在运行时进行动态的修改或调整： 聚合的字段（付款金额） 分组字段（付款人和受益人） 聚合函数（求和） 窗口大小（1 星期） 阈值（1000000） 计算符号（大于） 因此，我们将使用以下简单的 JSON 格式来定义上述参数： 12345678910&#123; \"ruleId\": 1, \"ruleState\": \"ACTIVE\", \"groupingKeyNames\": [\"payerId\", \"beneficiaryId\"], \"aggregateFieldName\": \"paymentAmount\", \"aggregatorFunctionType\": \"SUM\", \"limitOperatorType\": \"GREATER\", \"limit\": 1000000, \"windowMinutes\": 10080&#125; 在这一点上，重要的是了解 groupingKeyNames 决定了数据的实际物理分区，所有指定参数（payerId + beneficiaryId）相同的交易数据都会汇总到同一个物理计算 operator 里面去。很明显，如果要实现这样的功能，在 Flink 里面是使用 keyBy 函数来完成。 Flink 官方文档中 keyBy() 函数的大多数示例都是使用硬编码的 KeySelector，它提取特定数据的字段。但是，为了支持所需的灵活性，我们必须根据规则中的规范以更加动态的方式提取它们，为此，我们将不得不使用一个额外的运算符，该运算符会将每条数据分配到正确的聚合实例中。 总体而言，我们的主要处理流程如下所示： 12345DataStream&lt;Alert&gt; alerts = transactions .process(new DynamicKeyFunction()) .keyBy(/* some key selector */); .process(/* actual calculations and alerting */) 先前我们已经建立了每个规则定义一个groupingKeyNames参数，该参数指定将哪些字段组合用于传入事件的分组。每个规则可以使用这些字段的任意组合。同时，每个传入事件都可能需要根据多个规则进行评估。这意味着事件可能需要同时出现在计算 operator 的多个并行实例中，这些实例对应于不同的规则，因此需要进行分叉。确保此类事件的调度能达到 DynamicKeyFunction() 的目的。 DynamicKeyFunction迭代一组已定义的规则，并通过 keyBy()函数提取所有数据所需的分组 key ： 1234567891011121314151617181920212223public class DynamicKeyFunction extends ProcessFunction&lt;Transaction, Keyed&lt;Transaction, String, Integer&gt;&gt; &#123; ... /* Simplified */ List&lt;Rule&gt; rules = /* Rules that are initialized somehow. Details will be discussed in a future blog post. */; @Override public void processElement( Transaction event, Context ctx, Collector&lt;Keyed&lt;Transaction, String, Integer&gt;&gt; out) &#123; for (Rule rule :rules) &#123; out.collect( new Keyed&lt;&gt;( event, KeysExtractor.getKey(rule.getGroupingKeyNames(), event), rule.getRuleId())); &#125; &#125; ...&#125; KeysExtractor.getKey()使用反射从数据中提取groupingKeyNames里面所有所需字段的值，并将它们拼接为字符串，例如&quot;{payerId=25;beneficiaryId=12}&quot;。Flink 将计算该字符串的哈希值，并将此特定组合的数据处理分配给集群中的特定服务器。这样就会跟踪付款人25和受益人12之间的所有交易，并在所需的时间范围内评估定义的规则。 注意，Keyed引入了具有以下签名的包装器类作为输出类型DynamicKeyFunction： 12345678910public class Keyed&lt;IN, KEY, ID&gt; &#123; private IN wrapped; private KEY key; private ID id; ... public KEY getKey()&#123; return key; &#125;&#125; 此 POJO 的字段携带了以下信息：wrapped是原始数据，key是使用 KeysExtractor提取出来的结果，id是导致事件的调度规则的 ID（根据规则特定的分组逻辑）。 这种类型的事件将作为keyBy()函数的输入，并允许使用简单的 lambda 表达式作为KeySelector实现动态数据 shuffle 的最后一步。 12345DataStream&lt;Alert&gt; alerts = transactions .process(new DynamicKeyFunction()) .keyBy((keyed) -&gt; keyed.getKey()); .process(new DynamicAlertFunction()) 通过应用，DynamicKeyFunction我们隐式复制了事件，以便在 Flink 集群中并行的执行每个规则评估。通过这样做，我们获得了一个重要的功能——规则处理的水平可伸缩性。通过向集群添加更多服务器，即增加并行度，我们的系统将能够处理更多规则。实现此功能的代价是数据重复，这可能会成为一个问题，具体取决于一组特定的参数，例如传入数据速率，可用网络带宽，事件有效负载大小等。在实际情况下，可以进行其他优化应用，例如组合计算具有相同groupingKeyNames 的规则，或使用过滤层，将事件中不需要处理特定规则的所有字段删除。 总结在此博客文章中，我们通过查看示例用例（欺诈检测引擎）讨论了如何对 Flink 应用程序进行动态，运行时更改。我们已经描述了总体项目结构及其组件之间的交互，并提供了使用 docker 进行构建和运行演示欺诈检测应用程序。然后，我们展示了将 数据动态分区 ，这是第一个实现灵活的动态配置的代码案例。 为了专注于描述本案例的核心机制，我们将 DSL 和基本规则引擎的复杂性降至最低。在未来，不难想象会添加一些扩展，例如允许使用更复杂的规则定义，包括某些事件的过滤，逻辑规则链接以及其他更高级的功能。 在本系列的第二篇博客中，我们将描述规则如何进入正在运行的欺诈检测引擎。此外，我们将详细介绍引擎的主要处理功能 DynamicAlertFunction() 的实现细节。 在下一篇文章中，我们会教大家如何利用 Apache Flink 的广播流在我们的欺诈检测系统中动态的处理规则。 本篇文章属于翻译文章，作者：zhisheng 原文地址：http://www.54tianzhisheng.cn/2021/01/22/Flink-Fraud-Detection-engine/ 英文作者：alex_fedulov 英文原文地址：https://flink.apache.org/news/2020/01/15/demo-fraud-detection.html 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink Forward Asia 2020 全部 PPT 开放下载","date":"2020-12-20T16:00:00.000Z","path":"2020/12/21/flink-forward-Asia-2020/","text":"Flink Forward Asia 2020 在北京召开的，有主会场和几个分会场（企业实践、Apache Flink 核心技术、开源大数据生态、实时数仓、人工智能），内容涉及很多，可以查看下面图片介绍。 如何获取上面这些 PPT？上面的这些 PPT 本人已经整理好了，你可以扫描下面二维码，关注微信公众号：zhisheng，然后在里面回复关键字: ffa2020 即可获取已放出的 PPT。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink 1.12 Release 文档解读","date":"2020-12-05T16:00:00.000Z","path":"2020/12/06/Flink-1.12/","text":"Flink 1.12 快要发布了，这里提前解读一下 Release 文档 本文的 Release 文档描述了在 Flink 1.11 和 Flink 1.12 之间更改的重要方面，例如配置，行为或依赖项。 如果您打算将 Flink 版本升级到 1.12，请仔细阅读这些说明。 API移除掉 ExecutionConfig 中过期的方法 移除掉了 ExecutionConfig#isLatencyTrackingEnabled 方法, 你可以使用 ExecutionConfig#getLatencyTrackingInterval 方法代替. 移除掉了 ExecutionConfig#enable/disableSysoutLogging、ExecutionConfig#set/isFailTaskOnCheckpointError 过期的方法。 移除掉了 -q CLI 参数。 移除掉过期的 RuntimeContext#getAllAccumulators 方法 过期的 RuntimeContext#getAllAccumulators 方法被移除掉了，请使用 RuntimeContext#getAccumulator 方法作为代替。 由于数据丢失的风险把 CheckpointConfig#setPreferCheckpointForRecovery 方法标为过期 CheckpointConfig#setPreferCheckpointForRecovery 方法标记为过期了, 因为作业在进行恢复时，如果使用较旧的 Checkpoint 状态而不使用新的 Save point 状态数据，可能会导致数据丢失。 FLIP-134: DataStream API 的批处理执行 允许在 KeyedStream.intervalJoin() 的配置时间属性，在 Flink 1.12 之前 KeyedStream.intervalJoin() 算子的时间属性依赖于全局设置的时间属性。在 Flink 1.12 中我们可以在 IntervalJoin 方法后加上 inProcessingTime() 或 inEventTime() ，这样 Join 就不再依赖于全局的时间属性。 在 Flink 1.12 中将 DataStream API 的 timeWindow() 方法标记为过期，请使用 window(WindowAssigner)、TumblingEventTimeWindows、 SlidingEventTimeWindows、TumblingProcessingTimeWindows 或者 SlidingProcessingTimeWindows。 将 StreamExecutionEnvironment.setStreamTimeCharacteristic() 和 TimeCharacteristic 方法标记为过期。在 Flink 1.12 中，默认的时间属性改变成 EventTime 了，于是你不再需要该方法去开启 EventTime 了。在 EventTime 时间属性下，你使用 processing-time 的 windows 和 timers 也都依旧会生效。如果你想禁用水印，请使用 ExecutionConfig.setAutoWatermarkInterval(long) 方法。如果你想使用 IngestionTime，请手动设置适当的 WatermarkStrategy。如果你使用的是基于时间属性更改行为的通用 ‘time window’ 算子(eg: KeyedStream.timeWindow())，请使用等效操作明确的指定处理时间和事件时间。 允许在 CEP PatternStream 上显式配置时间属性在 Flink 1.12 之前，CEP 算子里面的时间依赖于全局配置的时间属性，在 1.12 之后可以在 PatternStream 上使用 inProcessingTime() 或 inEventTime() 方法。 API 清理 移除了 UdfAnalyzer 配置，移除了 ExecutionConfig#get/setCodeAnalysisMode 方法和 SkipCodeAnalysis 类。 移除了过期的 DataStream#split 方法，该方法从很早的版本中已经标记成为过期的了，你可以使用 Side Output 来代替。 移除了过期的 DataStream#fold() 方法和其相关的类，你可以使用更加高性能的 DataStream#reduce。 扩展 CompositeTypeSerializerSnapshot 以允许复合序列化器根据外部配置迁移 不再推荐使用 CompositeTypeSerializerSnapshot 中的 isOuterSnapshotCompatible(TypeSerializer) 方法，推荐使用 OuterSchemaCompatibility#resolveOuterSchemaCompatibility(TypeSerializer) 方法。 将 Scala 版本升级到 2.1.1 Flink 现在依赖 Scala 2.1.1，意味着不再支持 Scala 版本小于 2.11.11。 SQL对 aggregate 函数的 SQL DDL 使用新类型推断 aggregate 函数的 CREATE FUNCTION DDL 现在使用新类型推断，可能有必要将现有实现更新为新的反射类型提取逻辑，将 StreamTableEnvironment.registerFunction 标为过期。 更新解析器模块 FLIP-107 现在 METADATA 属于保留关键字，记得使用反引号转义。 将内部 aggregate 函数更新为新类型 使用 COLLECT 函数的 SQL 查询可能需要更新为新类型的系统。 Connectors 和 Formats移除 Kafka 0.10.x 和 0.11.x Connector 在 Flink 1.12 中，移除掉了 Kafka 0.10.x 和 0.11.x Connector，请使用统一的 Kafka Connector（适用于 0.10.2.x 版本之后的任何 Kafka 集群），你可以参考 Kafka Connector 页面的文档升级到新的 Flink Kafka Connector 版本。 CSV 序列化 Schema 包含行分隔符 csv.line-delimiter 配置已经从 CSV 格式中移除了，因为行分隔符应该由 Connector 定义而不是由 format 定义。如果用户在以前的 Flink 版本中一直使用了该配置，则升级到 Flink 1.12 时，应该删除该配置。 升级 Kafka Schema Registry Client 到 5.5.0 版本 flink-avro-confluent-schema-registry 模块不再在 fat-jar 中提供，你需要显式的在你自己的作业中添加该依赖，SQL-Client 用户可以使用flink-sql-avro-confluent-schema-registry fat jar。 将 Avro 版本从 1.8.2 升级到 1.10.0 版本 flink-avro 模块中的 Avro 版本升级到了 1.10，如果出于某种原因要使用较旧的版本，请在项目中明确降级 Avro 版本。 注意：我们观察到，与 1.8.2 相比，Avro 1.10 版本的性能有所下降，如果你担心性能，并且可以使用较旧版本的 Avro，那么请降级 Avro 版本。 为 SQL Client 打包 flink-avro 模块时会创建一个 uber jar SQL Client jar 会被重命名为 flink-sql-avro-1.12.jar，以前是 flink-avro-1.12-sql-jar.jar，而且不再需要手动添加 Avro 依赖。 Deployment（部署）默认 Log4j 配置了日志大小超过 100MB 滚动 默认的 log4j 配置现在做了变更：除了在 Flink 启动时现有的日志文件滚动外，它们在达到 100MB 大小时也会滚动。Flink 总共保留 10 个日志文件，从而有效地将日志目录的总大小限制为 1GB（每个 Flink 服务记录到该目录）。 默认在 Flink Docker 镜像中使用 jemalloc 在 Flink 的 Docker 镜像中，jemalloc 被用作默认的内存分配器，以减少内存碎片问题。用户可以通过将 disable-jemalloc 标志传递给 docker-entrypoint.sh 脚本来回滚使用 glibc。有关更多详细信息，请参阅 Docker 文档上的 Flink。 升级 Mesos 版本到 1.7 将 Mesos 依赖版本从 1.0.1 版本升级到 1.7.0 版本。 如果 Flink 进程在超时后仍未停止，则发送 SIGKILL 在 Flink 1.12 中，如果 SIGTERM 无法成功关闭 Flink 进程，我们更改了独立脚本的行为以发出 SIGKILL。 介绍非阻塞作业提交 提交工作的语义略有变化，提交调用几乎立即返回，并且作业处于新的 INITIALIZING 状态，当作业处于该状态时，对作业做 Savepoint 或者检索作业详情信息等操作将不可用。 一旦创建了该作业的 JobManager，该作业就处于 CREATED 状态，并且所有的调用均可用。 RuntimeFLIP-141: Intra-Slot Managed Memory 共享 python.fn-execution.buffer.memory.size 和 python.fn-execution.framework.memory.size 的配置已删除，因此不再生效。除此之外，python.fn-execution.memory.managed 默认的值更改为 true， 因此默认情况下 Python workers 将使用托管内存。 FLIP-119 Pipelined Region Scheduling 从 Flink 1.12 开始，将以 pipelined region 为单位进行调度。pipelined region 是一组流水线连接的任务。这意味着，对于包含多个 region 的流作业，在开始部署任务之前，它不再等待所有任务获取 slot。取而代之的是，一旦任何 region 获得了足够的任务 slot 就可以部署它。对于批处理作业，将不会为任务分配 slot，也不会单独部署任务。取而代之的是，一旦某个 region 获得了足够的 slot，则该任务将与所有其他任务一起部署在同一区域中。 可以使用 jobmanager.scheduler.scheduling-strategy：legacy 启用旧的调度程序。 RocksDB optimizeForPointLookup 导致丢失时间窗口 默认情况下，我们会将 RocksDB 的 ReadOptions 的 setTotalOrderSeek 设置为true，以防止用户忘记使用 optimizeForPointLookup。 同时，我们支持通过RocksDBOptionsFactory 自定义 ReadOptions。如果观察到任何性能下降，请将 setTotalOrderSeek 设置为 false（根据我们的测试，这是不可能的）。 自定义 OptionsFactory 设置似乎对 RocksDB 没有影响 过期的 OptionsFactory 和 ConfigurableOptionsFactory 类已移除，请改用 RocksDBOptionsFactory 和 ConfigurableRocksDBOptionsFactory。 如果有任何扩展 DefaultConfigurableOptionsFactory 的类，也请重新编译你的应用程序代码。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"强烈推荐三本 Spark 新书籍","date":"2020-10-17T16:00:00.000Z","path":"2020/10/18/Spark-book/","text":"挺好的三本书 前言看到标题大家可能会想，zhisheng 之前不是一直写 Flink 相关的文章吗？咋开始推荐 Spark 书籍了，这里解释一下，因为本人前段时间接手了公司 Spark 引擎，所以偶尔也会抽空学习一下 Spark，这不看到几本不错的 Spark 书籍，于是想在这里与大家分享一下。 《Stream Processing with Apache Spark》 这本书出版时间是 2019 年 6 月，算是与 《Stream Processing with Apache Flink》是姊妹篇，主要是讲 Spark 的流处理，比如 Structured Streaming 和 Spark Streaming，对 Spark 流处理感兴趣的不可错过该书，虽然现在 Flink 是流处理的 No1，但是并不影响对比着学习他们之间的技术。 《Learning Spark, 2nd Edition》 这本书出版时间是 2020 年 7 月，全书我觉得对于整个 Spark 的体系讲的还是很全的，从概念的介绍，到 API / SQL 的使用，再到如何优化 Spark 作业，接着讲解了 Structured Streaming，然后还讲解了通过 Spark 构建数据湖，并且该章节中还对目前很热门的三大数据湖框架 Apache Hudi / Apache Iceberg / Delta Lake 进行了介绍。接着讲解了 Spark 在机器学习相关场景的水碱和应用，最后介绍了 Spark 3.0 的新特性，也是目前唯一不多介绍 Spark 3.0 版本的书籍之一。 《Spark in Action, 2nd Edition》 本书出版时间是 2020 年 5 月，出版社是 Manning，不同于上面两本书是出版于 O’Reilly。本书内容跟其标题其实还是比较相符的，主讲实战，目录如下。 扫描下面二维码，回复 Spark 可获取本文提及到的三本书","tags":[{"name":"Spark","slug":"Spark","permalink":"http://www.54tianzhisheng.cn/tags/Spark/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"}]},{"title":"用了 Flink History Server，妈妈再也不用担心我的作业半夜挂了","date":"2020-10-12T16:00:00.000Z","path":"2020/10/13/flink-history-server/","text":"保存作业停止之前的信息 前言Flink On YARN 默认作业挂了之后打开的话，是一个如下这样的页面： 对于这种我们页面我们只能查看 JobManager 的日志，不再可以查看作业挂掉之前的运行的 Web UI，很难清楚知道作业在挂的那一刻到底发生了啥？如果我们还没有 Metrics 监控的话，那么完全就只能通过日志去分析和定位问题了，所以如果能还原之前的 Web UI，我们可以通过 UI 发现和定位一些问题。 History Server 介绍那么这里就需要利用 Flink 中的 History Server 来解决这个问题。那么 History Server 是什么呢？ 它可以用来在相应的 Flink 集群关闭后查询已完成作业的统计信息。例如有个批处理作业是凌晨才运行的，并且我们都知道只有当作业处于运行中的状态，才能够查看到相关的日志信息和统计信息。所以如果作业由于异常退出或者处理结果有问题，我们又无法及时查看（凌晨运行的）作业的相关日志信息。那么 History Server 就显得十分重要了，因为通过 History Server 我们才能查询这些已完成作业的统计信息，无论是正常退出还是异常退出。 此外，它对外提供了 REST API，它接受 HTTP 请求并使用 JSON 数据进行响应。Flink 任务停止后，JobManager 会将已经完成任务的统计信息进行存档，History Server 进程则在任务停止后可以对任务统计信息进行查询。比如：最后一次的 Checkpoint、任务运行时的相关配置。 那么如何开启这个呢？你需要在 flink-conf.yml 中配置如下： 1234567891011121314151617181920212223242526#==============================================================================# HistoryServer#==============================================================================# The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)# Directory to upload completed jobs to. Add this directory to the list of# monitored directories of the HistoryServer as well (see below). # flink job 运行完成后的日志存放目录jobmanager.archive.fs.dir: hdfs:///flink/history-log# The address under which the web-based HistoryServer listens.# flink history进程所在的主机#historyserver.web.address: 0.0.0.0# The port under which the web-based HistoryServer listens.# flink history进程的占用端口#historyserver.web.port: 8082# Comma separated list of directories to monitor for completed jobs.# flink history进程的hdfs监控目录historyserver.archive.fs.dir: hdfs:///flink/history-log# Interval in milliseconds for refreshing the monitored directories.# 刷新受监视目录的时间间隔（以毫秒为单位）#historyserver.archive.fs.refresh-interval: 10000 注意： jobmanager.archive.fs.dir 要和 historyserver.archive.fs.dir 配置的路径要一样 执行命令： 1./bin/historyserver.sh start 发现报错如下： 1234567891011121314151617181920212223242526272020-10-13 21:21:01,310 main INFO org.apache.flink.core.fs.FileSystem - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.2020-10-13 21:21:01,336 main INFO org.apache.flink.runtime.security.modules.HadoopModuleFactory - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.2020-10-13 21:21:01,352 main INFO org.apache.flink.runtime.security.modules.JaasModule - Jaas file will be created as /tmp/jaas-354359771751866787.conf.2020-10-13 21:21:01,355 main INFO org.apache.flink.runtime.security.SecurityUtils - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.2020-10-13 21:21:01,363 main WARN org.apache.flink.runtime.webmonitor.history.HistoryServer - Failed to create Path or FileSystem for directory 'hdfs:///flink/history-log'. Directory will not be monitored.org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded. at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:450) at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362) at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298) at org.apache.flink.runtime.webmonitor.history.HistoryServer.&lt;init&gt;(HistoryServer.java:187) at org.apache.flink.runtime.webmonitor.history.HistoryServer.&lt;init&gt;(HistoryServer.java:137) at org.apache.flink.runtime.webmonitor.history.HistoryServer$1.call(HistoryServer.java:122) at org.apache.flink.runtime.webmonitor.history.HistoryServer$1.call(HistoryServer.java:119) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.runtime.webmonitor.history.HistoryServer.main(HistoryServer.java:119)Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not in the classpath/dependencies. at org.apache.flink.core.fs.UnsupportedSchemeFactory.create(UnsupportedSchemeFactory.java:58) at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:446) ... 8 more2020-10-13 21:21:01,367 main ERROR org.apache.flink.runtime.webmonitor.history.HistoryServer - Failed to run HistoryServer.org.apache.flink.util.FlinkException: Failed to validate any of the configured directories to monitor. at org.apache.flink.runtime.webmonitor.history.HistoryServer.&lt;init&gt;(HistoryServer.java:196) at org.apache.flink.runtime.webmonitor.history.HistoryServer.&lt;init&gt;(HistoryServer.java:137) at org.apache.flink.runtime.webmonitor.history.HistoryServer$1.call(HistoryServer.java:122) at org.apache.flink.runtime.webmonitor.history.HistoryServer$1.call(HistoryServer.java:119) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.runtime.webmonitor.history.HistoryServer.main(HistoryServer.java:119) 这个异常的原因是因为 Flink 集群的 CLASS_PATH 下缺少了 HDFS 相关的 jar，我们可以引入 HDFS 的依赖放到 lib 目录下面或者添加 Hadoop 的环境变量。 这里我们在 historyserver.sh 脚本中增加下面脚本，目的就是添加 Hadoop 的环境变量： 123456# export hadoop classpathif [ `command -v hadoop` ];then export HADOOP_CLASSPATH=`hadoop classpath`else echo \"hadoop command not found in path!\"fi 效果添加后再启动脚本则可以运行成功了，打开页面 机器IP:8082 则可以看到历史所有运行完成或者失败的作业列表信息。 点进单个作业可以看到作业挂之前的所有信息，便于我们去查看挂之前作业的运行情况（Exception 信息/Checkpoint 信息/算子的流入和流出数据量信息等） 原理分析再来看看配置的 /flink/history-log/ 目录有什么东西呢？执行下面命令可以查看 1hdfs dfs -ls /flink/history-log/ 其实 history server 会在本地存储已结束 Job 信息，你可以配置 historyserver.web.tmpdir 来决定存储在哪，默认的拼接规则为： 1System.getProperty(\"java.io.tmpdir\") + File.separator + \"flink-web-history-\" + UUID.randomUUID() Linux 系统临时目录为 /tmp，你可以看到源码中 HistoryServerOptions 该类中的可选参数。 12345678/*** The local directory used by the HistoryServer web-frontend.*/public static final ConfigOption&lt;String&gt; HISTORY_SERVER_WEB_DIR = key(\"historyserver.web.tmpdir\") .noDefaultValue() .withDescription(\"This configuration parameter allows defining the Flink web directory to be used by the\" + \" history server web interface. The web interface will copy its static files into the directory.\"); 那么我们找到本地该临时目录，可以观察到里面保存着很多 JS 文件，其实就是我们刚才看到的页面 历史服务存储文件中，存储了用于页面展示的模板配置。历史任务信息存储在 Jobs 路径下，其中包含了已经完成的 Job，每次启动都会从 historyserver.archive.fs.dir 拉取所有的任务元数据信息。 每个任务文件夹中包含我们需要获取的一些信息，通过 REST API 获取时指标时，就是返回这些内容（Checkpoint/Exception 信息等）。 REST API以下是可用且带有示例 JSON 响应的请求列表。所有请求格式样例均为 http://hostname:8082/jobs，下面我们仅列出了 URLs 的 path 部分。 尖括号中的值为变量，例如作业 7684be6004e4e955c2a558a9bc463f65 的 http://hostname:port/jobs/&lt;jobid&gt;/exceptions 请求须写为 http://hostname:port/jobs/7684be6004e4e955c2a558a9bc463f65/exceptions。 /config /jobs/overview /jobs/&lt;jobid&gt; /jobs/&lt;jobid&gt;/vertices /jobs/&lt;jobid&gt;/config /jobs/&lt;jobid&gt;/exceptions /jobs/&lt;jobid&gt;/accumulators /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt; /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/subtasktimes /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/taskmanagers /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/accumulators /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/subtasks/accumulators /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/subtasks/&lt;subtasknum&gt; /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/subtasks/&lt;subtasknum&gt;/attempts/&lt;attempt&gt; /jobs/&lt;jobid&gt;/vertices/&lt;vertexid&gt;/subtasks/&lt;subtasknum&gt;/attempts/&lt;attempt&gt;/accumulators /jobs/&lt;jobid&gt;/plan 总结这样我们就可以开心的去查看作业挂之前的 Web UI 信息了，妈妈在也不用担心我的作业挂了！😁 参考文章 History Server flink历史服务 Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"如何生成 Flink 作业的交互式火焰图？","date":"2020-10-04T16:00:00.000Z","path":"2020/10/05/flink-jvm-profiler/","text":"Flink 作业生成火焰图 前言Flink 是目前最流行的大数据及流式计算框架之一，用户可以使用 Java/Scala/Python 的DataStream 接口或者标准 SQL 语言来快速实现一个分布式高可用的流式应用，通过内部的 Java JIT、off-heap 内存管理等技术优化性能，并且有完整的 Source、Sink、WebUI、Metrics 等功能集成，让 Flink 几乎成为了流式计算的事实标准。 但是当处理海量数据的时候，很容易出现各种异常和性能瓶颈，这时我们需要优化系统性能时，常常需要分析程序运行行为和性能瓶颈。Profiling 技术是一种在应用运行时收集程序相关信息的动态分析手段，常用的 JVM Profiler 可以从多个方面对程序进行动态分析，如 CPU、Memory、Thread、Classes、GC 等，其中 CPU Profiling 的应用最为广泛。CPU Profiling 经常被用于分析代码的执行热点，如“哪个方法占用 CPU 的执行时间最长”、“每个方法占用 CPU 的比例是多少”等等，通过 CPU Profiling 得到上述相关信息后，研发人员就可以轻松针对热点瓶颈进行分析和性能优化，进而突破性能瓶颈，大幅提升系统的吞吐量。 本文介绍我们在做性能优化常用的火焰图以及为如何集成火焰图到通用的 Flink 作业中。 火焰图介绍火焰图是《性能之巅》作者以及 DTrace 等一系列 Linux 系统优化工具作者 Brendan Gregg 大神的作品之一，可以非常清晰地展示应用程序的函数调用栈以及函数调用时间占比，基本原理是通过各种 agent 在程序运行时采样并输出日志，使用 FlameGraph 工具把日志提取出来输出可在浏览器交互式查看的 SVG图片。 Uber 开源了 jvm-profiler 项目，介绍如何为 Spark 应用和 Java 应用添加火焰图支持，但是目前 Flink 社区和 jvm-profiler 官网都还没有相关的使用教程。 实际上基于 JVM 的程序都可以使用这个工具，本文将基于 jvm-profiler 来介绍如何生成 Flink 作业的火焰图。 下载和编译 jvm-profiler123git clone git clone https://github.com/uber-common/jvm-profiler.gitmvn clean install -DskipTests=true -Dcheckstyle.skip -Dfast -T 8C 编译好了之后，将项目 target 目录下的 jvm-profiler-1.0.0.jar 复制一份到 flink 的 lib 目录下面 1cp target/jvm-profiler-1.0.0.jar /usr/local/flink-1.11.1/lib 下载 FlameGraph由于 jvm-profiler 支持生成火焰图需要的日志文件，将日志转化成交互式 SVG 图片还是使用 Brendan Gregg 的FlameGraph 工具。 1git clone https://github.com/brendangregg/FlameGraph.git 下载项目源码即可，后面会使用 flamegraph.pl 工具来生成图片文件。 配置 Flink对于 Flink 应用，我们只需要在 TaskManager 中注入打点的 Java agent 即可，这里测试，我就使用本地 standalone 模式，修改 Flink conf 目录下的 flink-conf.yaml 文件，添加一下如下配置： 1env.java.opts.taskmanager: \"-javaagent:/usr/local/flink-1.11.1/lib/jvm-profiler-1.0.0.jar=sampleInterval=50\" 目前最小的采样间隔就是 50 毫秒，然后启动集群和运行一个 Flink 作业： 12345./bin/start-cluster.sh//运行一个作业./bin/flink run ./examples/streaming/StateMachineExample.jar 运行之后可以看到 TaskManager 的 stdout 里面打印如下： 因为已经注入 Java agent，因此在标准输出中会定期添加火焰图所需要的打点数据，然后使用下面的命令提取相关日志，并且使用 jvm-profiler 和 FlameGraph 提供的工具来生成 SVG 图片文件。 12345678910111213//1、提取 stdout 文件中的相关日志cat log/flink-zhisheng-taskexecutor-0-zhisheng.out | grep \"ConsoleOutputReporter - Stacktrace:\" | awk '&#123;print substr($0,37)&#125;' &gt; stacktrace.json//2、在 jvm-profiler 目录下执行下面命令python ./stackcollapse.py -i /usr/local/flink-1.11.1/stacktrace.json &gt; stacktrace.folded//3、在 FlameGraph 目录下执行下面命令生成 SVG 图片./flamegraph.pl /Users/zhisheng/Documents/github/jvm-profiler/stacktrace.folded &gt; stacktrace.svg 然后用浏览器打开刚才生成的 SVG 图片就可以看到火焰图信息。 总结本文主要目的在于教大家如何利用 jvm-profiler 去生成 Flink 作业的运行火焰图，这样可以在遇到性能瓶颈问题的时候会很方便大家去定位问题，关于如何去读懂生成的火焰图，后面可以再分享系列文章。 参考资料 JVM CPU Profiler技术原理及源码深度解析 jvm-profile 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink 精进学习知识星球内容整理","date":"2020-08-08T16:00:00.000Z","path":"2020/08/09/flink-zsxq/","text":"整理自己发在知识星球和公众号的系列文章，方便查找。 介绍进知识星球的小伙伴有的是刚接触 Flink 的，有的是根本没接触过的，有的是已经用 Flink 很久的，所以很难适合所有的口味。我一向认为对一门技术的学习方式应该是： 了解（知道它的相关介绍、用处） 用（了解常用 API） 用熟（对常用 API 能够用熟来，并了解一些高级 API） 解决问题（根据业务场景遇到的问题能够定位问题并解决） 看源码（深入源码的实现，此种情况主要是兴趣爱好驱动） 这里先把《从 0 到 1 学习 Flink》的系列文章给列出来，我觉得从这个系列文章的顺序来学习起码可以让你先达到第四个步骤，如果有什么疑问或者文章不足之处欢迎指出。 《从 0 到 1 学习 Flink》系列 Flink 从 0 到 1 学习 —— Apache Flink 介绍 Flink 从 0 到 1 学习 —— Flink 架构、原理与部署测试 Flink 从 0 到 1 学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 Flink 从 0 到 1 学习 —— Flink 配置文件详解 Flink 从 0 到 1 学习 —— Flink JobManager 高可用性配置 Flink 从 0 到 1 学习 —— Data Source 介绍 Flink 从 0 到 1 学习 —— 如何自定义 Data Source ？ Flink 从 0 到 1 学习 —— Data Sink 介绍 Flink 从 0 到 1 学习 —— 如何自定义 Data Sink ？ Flink 从 0 到 1 学习 —— Flink Data transformation(转换) Flink 从 0 到 1 学习 —— 介绍Flink中的Stream Windows Flink 从 0 到 1 学习 —— Flink 流计算编程–看看别人怎么用 Session Window Flink 从 0 到 1 学习 —— 这一次带你彻底搞懂 Flink Watermark Flink 从 0 到 1 学习 —— Flink 中几种 Time 详解 Flink 从 0 到 1 学习 —— Flink 项目如何运行？ Flink 从 0 到 1 学习 —— Flink parallelism 和 Slot 介绍 Flink 从 0 到 1 学习 —— Flink 写入数据到 ElasticSearch Flink 从 0 到 1 学习 —— Flink 实时写入数据到 ElasticSearch 性能调优 Flink 从 0 到 1 学习 —— Flink 写入数据到 Kafka Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据批量写入到 MySQL Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ Flink 从 0 到 1 学习 —— 你上传的 jar 包藏到哪里去了? Flink 从 0 到 1 学习 —— Flink 中如何管理配置？ Flink 从 0 到 1 学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 Flink 从 0 到 1 学习 —— 为什么说流处理即未来? Flink 从 0 到 1 学习 —— 流计算框架 Flink 与 Storm 的性能对比 Flink 从 0 到 1 学习 —— Flink Checkpoint 轻量级分布式快照 Flink 从 0 到 1 学习 —— Flink状态管理和容错机制介绍 Flink 从 0 到 1 学习 —— Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 Flink 从 0 到 1 学习 —— 使用 Prometheus Grafana 监控 Flink Flink 从 0 到 1 学习 —— 使用 InflubDB 和 Grafana 监控 Flink JobManager TaskManager 和作业 Flink 从 0 到 1 学习 —— 从0到1搭建一套 Flink 监控系统 Flink 从 0 到 1 学习 —— 详解 Flink Metrics 原理与监控实战 Flink 从 0 到 1 学习 —— Flink 读取 Kafka 商品数据后写入到 Redis Flink 从 0 到 1 学习 —— 一文搞懂 Flink 网络流控与反压机制 Flink 从 0 到 1 学习 —— 一文搞懂Flink内部的Exactly Once和At Least Once Flink 从 0 到 1 学习 —— Flink On K8s Flink 从 0 到 1 学习 —— Apache Flink 是如何管理好内存的? Flink 从 0 到 1 学习 —— Flink 参数配置和常见参数调优 Flink 从 0 到 1 学习 —— Flink 状态生存时间（State TTL）机制的底层实现 Flink 从 0 到 1 学习 —— Flink State 最佳实践 Flink 从 0 到 1 学习 —— Flink 使用大状态时的一点优化 Flink 从 0 到 1 学习 —— Flink 使用 broadcast 实现维表或配置的实时更新 Flink 从 0 到 1 学习 —— Spark/Flink广播实现作业配置动态更新 Flink 从 0 到 1 学习 —— Flink 清理过期 Checkpoint 目录的正确姿势 Flink 从 0 到 1 学习 —— Flink 状态管理与 Checkpoint 机制 Flink 从 0 到 1 学习 —— Flink 能否动态更改 Checkpoint 配置 Flink 从 0 到 1 学习 —— Flink Checkpoint 问题排查实用指南 Flink 从 0 到 1 学习 —— Apache Flink 管理大型状态之增量 Checkpoint 详解 Flink 从 0 到 1 学习 —— 深入理解 Flink 容错机制 Flink 从 0 到 1 学习 —— Flink 使用 connect 实现双流匹配 Flink 从 0 到 1 学习 —— Flink流计算编程–Flink扩容、程序升级前后的思考 Flink 从 0 到 1 学习 —— Flink HDFS Sink 如何保证 exactly-once 语义 Flink 从 0 到 1 学习 —— Flink Connector 深度解析 Flink 从 0 到 1 学习 —— 如何使用 Side Output 来分流？ Flink 从 0 到 1 学习 —— Flink 不可以连续 Split(分流)？ Flink 从 0 到 1 学习 —— Flink 全链路端到端延迟的测量方法 Flink 从 0 到 1 学习 —— Flink on Yarn / K8s 原理剖析及实践 Flink 从 0 到 1 学习 —— 如何使用 Kubernetes 部署 Flink 应用 Flink 从 0 到 1 学习 —— 一张图轻松掌握 Flink on YARN 基础架构与启动流程 Flink 从 0 到 1 学习 —— Flink on YARN 常见问题与排查思路 Flink 从 0 到 1 学习 —— Flink 单并行度内使用多线程来提高作业性能 Flink 从 0 到 1 学习 —— Flink中资源管理机制解读与展望 Flink 从 0 到 1 学习 —— Flink Back Pressure(背压)是怎么实现的？有什么绝妙之处？ Flink SQL 知识星球 Flink 标签所有内容 Java SPI 机制在 Flink SQL 中的应用 Flink 通过 DDL 和 SQL 来实现读取 Kafka 数据并处理后将数据写回 Kafka Flink SQL 实战——读取Kafka数据处理后写入 ElasticSearch 6 和 7 两种版本 Flink 聚合性能优化 – MiniBatch 分析 Flink流计算编程：双流中实现Inner Join、Left Join与Right Join Flink SQL 如何实现数据流的 Join？ 《Flink 各版本功能特性解读》 Apache Flink 1.9 重大特性提前解读 Flink 1.11 日志文件该如何配置？ Flink 1.11 Release 文档解读 Apache Flink 1.10 TaskManager 内存管理优化 Flink 版本升级方案 Flink 1.11 新特性详解:【非对齐】Unaligned Checkpoint 优化高反压 千呼万唤，Apache Flink 1.11.0 新功能正式介绍 重磅！Apache Flink 1.11 会有哪些牛逼的功能 Flink 1.10 新特性研究 修改代码150万行！Apache Flink 1.9.0做了这些重大修改！ 《Flink 在大厂的实践与应用》 OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 360深度实践：Flink与Storm协议级对比 携程——如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 数据仓库、数据库的对比介绍与实时数仓案例分享 基于 Apache Flink 的监控告警系统 文章 基于 Apache Flink 的监控告警系统 视频 如何利用Flink Rest API 监控满足生产环境非常刚需的需求 无流量 Flink 作业告警 Apache Flink 维表关联实战 如何利用 Flink 实时将应用 Error 日志告警？ Flink 流批一体的技术架构以及在阿里 的实践 基于 Flink 搭建实时个性化营销平台？ 基于 Flink 和 Drools 的实时日志处理 新一代大数据实时数据架构到底长啥样 从 Spark Streaming 到 Apache Flink：bilibili 实时平台的架构与实践 日均万亿条数据如何处理？爱奇艺实时计算平台这样做 Flink 流批一体的实践与探索 趣头条基于 Flink+ClickHouse 构建实时数据分析平台 Flink 维表关联多种方案对比 美团点评基于 Flink 的实时数仓平台实践 基于 Apache Flink 的大规模准实时数据分析平台 阿里巴巴 Flink 踩坑经验：如何大幅降低 HDFS 压力？ 58 同城基于 Flink 的千亿级实时计算平台架构实践 基于 Flink 构建关联分析引擎的挑战和实践 滴滴实时计算发展之路及平台架构实践 如何使用 Flink 每天实时处理百亿条日志？ 美团点评基于 Flink 的实时数仓建设实践 基于Kafka+Flink+Redis的电商大屏实时计算案例 Flink 在小红书推荐系统中的应用 Flink 实战 | 贝壳找房基于Flink的实时平台建设 Flink 在趣头条的应用与实践 《Flink 实战与性能优化》专栏部分文章因为这个专栏是一开始自己写的，当时还没有和任何一家公司签协议，所以当时就是想放在知识星球的，后面有公司联系，才有完整的专栏文章诞生出来，否则自己也不知道是否可以坚持写完这个系列，所以后面合作开这个专栏后新写的文章就没放在星球了，因为签了合同的，是不能够在其他平台公开的，这里希望大家可以体谅，但是已经早公开的依旧不会删除掉的，有如下这些文章： 大数据重磅炸弹实时计算框架Flink 《大数据重磅炸弹——实时计算引擎 Flink》开篇词 预备篇： 你公司到底需不需要引入实时计算引擎？ 一文让你彻底了解大数据实时计算框架 Flink 别再傻傻的分不清大数据框架Flink、Blink、Spark Streaming、Structured Streaming和Storm之间的区别了 Flink 环境准备看这一篇就够了 一文讲解从 Flink 环境安装到源码编译运行 通过 WordCount 程序教你快速入门上手 Flink Flink 如何处理 Socket 数据及分析实现过程 Flink job 如何在 Standalone、YARN、Mesos、K8S 上部署运行？ 基础篇 : Flink 数据转换必须熟悉的算子（Operator) Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析 如何使用 Flink Window 及 Window 基本概念与实现原理 如何使用 DataStream API 来处理数据？ Flink WaterMark 详解及结合 WaterMark 处理延迟数据 Flink 常用的 Source 和 Sink Connectors 介绍 Flink 最最最常使用的 Connector —— Kafka 该如何使用？ 如何自定义 Flink Connectors（Source 和 Sink）？ Flink 读取 Kafka 数据后如何批量写入到 MySQL？ 一文了解如何使用 Flink Connectors —— ElasticSearch？ 一文了解如何使用 Flink Connectors —— HBase？ 如何利用 Redis 存储 Flink 计算后的数据？ 《Flink 源码解析文章》 Flink 源码解析 —— 源码编译运行 Flink 源码解析 —— 项目结构一览 Flink 源码解析 —— Flink 源码的结构和其对应的功能点 Flink 源码解析—— local 模式启动流程 Flink 源码解析 —— standalonesession 模式启动流程 Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 Flink 源码解析 —— 如何获取 JobGraph？ Flink 源码解析 —— 如何获取 StreamGraph？ Flink 源码解析 —— Flink JobManager 有什么作用？ Flink 源码解析 —— Flink TaskManager 有什么作用 Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 Flink 源码解析 —— 深度解析 Flink 序列化机制 Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ Flink 源码解析 —— Flink-metrics-core 源码解析 Flink 源码解析 —— Flink-metrics-datadog 源码解析 Flink 源码解析 —— Flink-metrics-dropwizard 源码解析 Flink 源码解析 —— Flink-metrics-graphite 源码解析 Flink 源码解析 —— Flink-metrics-influxdb 源码解析 Flink 源码解析 —— Flink-metrics-jmx 源码解析 Flink 源码解析 —— Flink-metrics-slf4j 源码解析 Flink 源码解析 —— Flink-metrics-statsd 源码解析 Flink 源码解析 —— Flink-metrics-prometheus 源码解析 Flink 源码解析 —— Flink 注解源码解析 Flink 源码解析 —— Flink Metrics 实战 《Flink 自己录制过的视频》 Flink 整合 Apollo 动态更新配置 Flink 整合 Nacos 动态更新配置 Flink 专栏的开篇词 你公司到底需不需要引入实时计算引擎 一文让你彻底了解大数据实时计算框架 Flink 别再傻傻的分不清大数据框架 Flink、Blink、Spark Streaming、Structured Streaming 和 Storm 之间的区别了 Flink环境准备 Flink环境安装 Flink WordCount 程序入门上手及分析实现过程 Flink 如何处理 Socket 数据及分析实现过程 Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析 如何使用 Flink Window 及 Window 基本概念与实现原理 Flink_Window组件深度讲解和如何自定义Window Flink 读取 Kafka 商品数据后写入到 Redis 基于 Apache Flink 的监控告警系统 Flink源码解析01——源码编译运行 Flink源码解析02——源码结构一览 Flink源码解析03——源码阅读规划 Flink源码解析04——flink-example模块源码结构 Flink源码解析05——flink-example模块源码分析 Flink源码解析06——flink-example-streaming 异步IO源码分析 Flink源码解析07——flink-example-streaming SideOutput源码分析 Flink源码解析08——flink-example-streaming Socket源码分析 Flink源码解析09——flink-example-streaming window和join源码分析 Flink源码解析10——flink-example-streaming 源码分析总结 Flink到底是否可以动态更改checkpoint配置 Flink 通过 DDL 和 SQL 来实现读取 Kafka 数据并处理后将数据写回 Kafka Flink SQL 实战——读取Kafka数据处理后写入 ElasticSearch 6 和 7 两种版本 其他资源下载 Flink Forward Asia 2019 的 PPT和视频下载 Flink Forward 2020 PPT 下载 实时计算平台架构（上） 实时计算平台架构（下） 基于Flink实现的商品实时推荐系统 Flink1.8学习路线 Kafka 学习文章和视频 数据分析指南 TimeoutException The heartbeat of TaskManager Flink on RocksDB 参数调优指南 2020最新Java面试题及答案 以业务为核心的中台体系建设 Skip List–跳表(全网最详细的跳表文章没有之一) Stream Processing with Apache Flink 假如我是面试官，我会问你这些问题，请接招 YARN 运行机制分析 企业大数据平台仓库架构建设思路 阿里巴巴开源的 Blink 实时计算框架真香 吐血之作 | 流系统Spark/Flink/Kafka/DataFlow端到端一致性实现对比 另外就是星球里可以向我提问，我看到问题会及时回答的，发现提问的还是比较少，想想当初就该还是要所有的都付费才能进，免费进的就会让你不珍惜自己付出的钱💰，自己也不会持续跟着一直学习下去。后面我会根据提问情况把长期潜水且当初是没付费的移除掉！还有就是群里的一些问题解答会同步到这里沉淀下来！如果你对这些问题还有更好的解答也欢迎提出你的回答，如果觉得棒的话我会进行一定额度的打赏！打赏包括但不限制于： 高质量的问题 学习资料资源分享 问题全面的解答 分享自己的建议 好好做好这几点，肯定会把入知识星球的钱赚到！为什么要做一个这样的 Flink 知识星球？ 帮助他人成长就是自己在成长 主动促使自己去深入这门技术（心里总觉得要对得起付费玩家） 真的想遇到那么一两个人可以一直好好学习下去（学习真特么是孤独的，一个人学习确实遇到的坑很多，效率肯定也低点，如果没有找到的话，那么还是我自己的那句话：坑要自己一个个填，路要自己一步步走！） 一个人走的快一些，一群人走的远一些，欢迎扫码上面的二维码加入知识星球，我们一起向前！","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink 1.11 日志该如何配置？","date":"2020-08-01T16:00:00.000Z","path":"2020/08/02/flink-1.11-log/","text":"Flink 1.11 日志升级到了 Log4j2，并且 Web UI 增强了功能。 Flink 1.11 之前在 Flink 1.11 之前，Flink 使用的日志是 Log4j，配置文件 log4j.properties 中的内容如下： 123456789101112131415161718192021222324# This affects logging for both user code and Flinklog4j.rootLogger=INFO, file# Uncomment this if you want to _only_ change Flink&apos;s logging#log4j.logger.org.apache.flink=INFO# The following lines keep the log level of common libraries/connectors on# log level INFO. The root logger does not override this. You have to manually# change the log levels here.log4j.logger.akka=INFOlog4j.logger.org.apache.kafka=INFOlog4j.logger.org.apache.hadoop=INFOlog4j.logger.org.apache.zookeeper=INFO# Log all infos in the given filelog4j.appender.file=org.apache.log4j.FileAppenderlog4j.appender.file.file=$&#123;log.file&#125;log4j.appender.file.append=falselog4j.appender.file.layout=org.apache.log4j.PatternLayoutlog4j.appender.file.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %-5p %-60c %x - %m%n# Suppress the irrelevant (wrong) warnings from the Netty channel handlerlog4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file 该配置文件会将 JobManager 和 TaskManager 的日志分别打印在不同的文件中，每个文件的日志大小一直会增加，如果想配置日志文件按大小滚动的话可以使用 RollingFileAppender，则要将配置文件改成如下： 12345678910111213141516171819202122232425# This affects logging for both user code and Flinklog4j.rootLogger=INFO, RFA # Uncomment this if you want to _only_ change Flink&apos;s logging#log4j.logger.org.apache.flink=INFO # The following lines keep the log level of common libraries/connectors on# log level INFO. The root logger does not override this. You have to manually# change the log levels here.log4j.logger.akka=INFOlog4j.logger.org.apache.kafka=INFOlog4j.logger.org.apache.hadoop=INFOlog4j.logger.org.apache.zookeeper=INFO log4j.appender.RFA=org.apache.log4j.RollingFileAppenderlog4j.appender.RFA.File=$&#123;log.file&#125;log4j.appender.RFA.MaxFileSize=256MBlog4j.appender.RFA.Append=truelog4j.appender.RFA.MaxBackupIndex=10log4j.appender.RFA.layout=org.apache.log4j.PatternLayoutlog4j.appender.RFA.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %t %-5p %-60c %x - %m%n # Suppress the irrelevant (wrong) warnings from the Netty channel handlerlog4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, RFA 为什么要在生产环境下将日志文件改成按照大小滚动呢？ 无非是在生产情况下，流数据是非常大的，有的时候自己可能会通过 print() 打印出来流数据进来验证结果，有的时候可能是打印的日志记录做 debug 用，然后到生产忘记关了，结果到生产就全部将流数据打印出来，这种情况下，就会导致 TaskManager 的日志文件会非常大，那么我们打开 Web UI 查看可能就会很卡，这也就是为啥我们有时候打开 Web UI 查看日志的时候，非常卡顿，加载不出来的原因了，主要原因就是日志文件太大导致的。 当然有的同学可能会想着将 Flink 作业的日志发到 Kafka 做统一的收集，然后做一些日志分析告警和再消费发到 ElasticSearch 等去做日志搜索，如果是发到 Kafka，可以使用 KafkaLog4jAppender，日志文件配置如下： 123456789101112131415161718192021222324252627282930# This affects logging for both user code and Flinklog4j.rootLogger=INFO, kafka# Uncomment this if you want to _only_ change Flink&apos;s logging#log4j.logger.org.apache.flink=INFO# The following lines keep the log level of common libraries/connectors on# log level INFO. The root logger does not override this. You have to manually# change the log levels here.log4j.logger.akka=INFOlog4j.logger.org.apache.kafka=INFOlog4j.logger.org.apache.hadoop=INFOlog4j.logger.org.apache.zookeeper=INFO# log send to kafkalog4j.appender.kafka=org.apache.kafka.log4jappender.KafkaLog4jAppenderlog4j.appender.kafka.brokerList=localhost:9092log4j.appender.kafka.topic=flink_logslog4j.appender.kafka.compressionType=nonelog4j.appender.kafka.requiredNumAcks=0log4j.appender.kafka.syncSend=falselog4j.appender.kafka.layout=org.apache.log4j.PatternLayoutlog4j.appender.kafka.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %-5p %c&#123;1&#125;:%L - %m%nlog4j.appender.kafka.level=INFO# Suppress the irrelevant (wrong) warnings from the Netty channel handlerlog4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, kafka Flink 1.11在 Flink 1.11 中，将 Log4j 升级到了 Log4j2，可以通过查看 FLINK-15672 可以知道变更的文件很多， 新版本的日志配置文件还是叫 log4j.properties，配置如下所示，不清楚为啥 FLINK-15672 代码提交中改了很多地方的配置文件名为 log4j2.properties，但是 Flink 最后打包的 conf 目录下还是保持和之前一样的文件名，按道理不是也应该进行更改成 log4j2.properties 的吗？ 12345678910111213141516171819202122232425262728293031# This affects logging for both user code and FlinkrootLogger.level = INFOrootLogger.appenderRef.file.ref = MainAppender# Uncomment this if you want to _only_ change Flink&apos;s logging#logger.flink.name = org.apache.flink#logger.flink.level = INFO# The following lines keep the log level of common libraries/connectors on# log level INFO. The root logger does not override this. You have to manually# change the log levels here.logger.akka.name = akkalogger.akka.level = INFOlogger.kafka.name= org.apache.kafkalogger.kafka.level = INFOlogger.hadoop.name = org.apache.hadooplogger.hadoop.level = INFOlogger.zookeeper.name = org.apache.zookeeperlogger.zookeeper.level = INFO# Log all infos in the given fileappender.main.name = MainAppenderappender.main.type = Fileappender.main.append = falseappender.main.fileName = $&#123;sys:log.file&#125;appender.main.layout.type = PatternLayoutappender.main.layout.pattern = %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %-5p %-60c %x - %m%n# Suppress the irrelevant (wrong) warnings from the Netty channel handlerlogger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipelinelogger.netty.level = OFF 默认这个配置也是不会对日志文件进行按照大小滚动的，那么如果我们要保持和之前的效果一样（按照日志大小滚动日志文件）该怎么做呢？你可以更改配置文件的内容如下所示： 12345678910111213141516171819202122232425262728293031323334353637# This affects logging for both user code and FlinkrootLogger.level = INFOrootLogger.appenderRef.rolling.ref = RollingFileAppender# Uncomment this if you want to _only_ change Flink&apos;s logging#logger.flink.name = org.apache.flink#logger.flink.level = INFO# The following lines keep the log level of common libraries/connectors on# log level INFO. The root logger does not override this. You have to manually# change the log levels here.logger.akka.name = akkalogger.akka.level = INFOlogger.kafka.name= org.apache.kafkalogger.kafka.level = INFOlogger.hadoop.name = org.apache.hadooplogger.hadoop.level = INFOlogger.zookeeper.name = org.apache.zookeeperlogger.zookeeper.level = INFO# Log all infos in the given rolling fileappender.rolling.name = RollingFileAppenderappender.rolling.type = RollingFileappender.rolling.append = falseappender.rolling.fileName = $&#123;sys:log.file&#125;appender.rolling.filePattern = $&#123;sys:log.file&#125;.%iappender.rolling.layout.type = PatternLayoutappender.rolling.layout.pattern = %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %-5p %-60c %x - %m%nappender.rolling.policies.type = Policiesappender.rolling.policies.size.type = SizeBasedTriggeringPolicyappender.rolling.policies.size.size = 200MBappender.rolling.strategy.type = DefaultRolloverStrategyappender.rolling.strategy.max = 10# Suppress the irrelevant (wrong) warnings from the Netty channel handlerlogger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipelinelogger.netty.level = OFF 如果你升级了到 1.11 版本，不能继续延用之前 1.11 之前的配置的那种日志文件滚动的配置了，需要做点变更。上面配置的表示日志文件以每隔 200MB 会进行切分，然后日志切分后的文件名是 ${sys:log.file}.%i，以数字结尾。 1.11 进行了配置厚度的效果如下图所示： 从上面图中可以发现 1.11 对于查看日志比之前友好了不少，多个日志文件都有列表展示，而不再是之前只能查看单个日志文件了。 注：为了演示日志文件滚动的效果，测试的时候设置的日志 appender.rolling.policies.size.size 是 1KB 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 1.11 Release 文档解读","date":"2020-06-28T16:00:00.000Z","path":"2020/06/29/flink-1.11/","text":"Flink 1.11 快要发布了，这里提前解读一下 Release 文档 集群和部署 支持 Hadoop 3.0 及更高的版本：Flink 不再提供任何 flink-shaded-hadoop- 依赖。用户可以通过配置 HADOOP_CLASSPATH 环境变量(推荐)或在 lib 文件夹下放入 Hadoop 依赖项。另外 include-hadoop Maven profile 也已经被移除了。 移除了 LegacyScheduler：Flink 不再支持 legacy scheduler，如果你设置了 jobmanager.scheduler: legacy 将不再起作用并且会抛出 IllegalArgumentException 异常，该参数的默认值并且是唯一选项为 ng。 将用户代码的类加载器和 slot 的生命周期进行绑定：只要为单个作业分配了至少一个 slot，TaskManager 就会重新使用用户代码的类加载器。这会稍微改变 Flink 的恢复行为，从而不会重新加载静态字段。这样做的好处是，可以大大减轻对 JVM metaspace 的压力。 slave 文件重命名为 workers：对于 Standalone 模式安装，worker 节点文件不再是 slaves 而是 workers，以前使用 start-cluster.sh 和 stop-cluster.sh 脚本的设置需要重命名该文件。 完善 Flink 和 Docker 的集成： Dockerfiles 文件样例和 build.sh Docker 镜像文件都从 Flink GitHub 仓库中移除了，这些示例社区不再提供，因此 flink-contrib/docker-flink 、 flink-container/docker 和 flink-container/kubernetes 模块都已删除了。目前你可以通过查看 Flink Docker integration 官方文档学会如何使用和自定义 Flink Docker 镜像，文档中包含了 docker run、docker compose、docker swarm 和 standalone Kubernetes。 内存管理 JobManager 使用新的内存模型：可以参考 FLIP-116，介绍了 JobManager 新的内存模型，提供了新的配置选项来控制 JobManager 的进程内存消耗，这种改变会影响 Standalone、YARN、Mesos 和 Active Kubernetes。如果你尝试在不做任何调整的情况下重用以前的Flink 配置，则新的内存模型可能会导致 JVM 的计算内存参数不同，从而导致性能发生变化甚至失败，可以参考 Migrate Job Manager Memory Configuration 文档进行迁移变更。 jobmanager.heap.size 和 jobmanager.heap.mb 配置参数已经过期了，如果这些过期的选项还继续使用的话，为了维持向后兼容性，它们将被解释为以下新选项之一： jobmanager.memory.heap.size：JVM Heap，为了 Standalone 和 Mesos 部署 jobmanager.memory.process.size：进程总内存，为了容器部署（Kubernetes 和 YARN） 下面两个选项已经删除了并且不再起作用了： containerized.heap-cutoff-ratio containerized.heap-cutoff-min JVM 参数，JobManager JVM 进程的 direct 和 metaspace 内存现在通过下面两个参数进行配置： jobmanager.memory.off-heap.size jobmanager.memory.jvm-metaspace.size 如果没有正确配置或存在相应的内存泄漏，这些新的限制可能会产生相应的 OutOfMemoryError 异常，可以参考 OutOfMemoryError 文档进行解决。 移除过期的mesos.resourcemanager.tasks.mem参数 Table API/SQL Blink planner 成为默认的 planner 改变了 Table API 的包结构：由于包 org.apache.flink.table.api.scala/java 中的各种问题，这些包下的所有类都已迁移。 此外，如 Flink 1.9 中所述，scala 表达式已移至 org.apache.flink.table.api 。 如果你之前使用了下面的类： org.apache.flink.table.api.java.StreamTableEnvironment org.apache.flink.table.api.scala.StreamTableEnvironment org.apache.flink.table.api.java.BatchTableEnvironment org.apache.flink.table.api.scala.BatchTableEnvironment 如果你不需要转换成 DataStream 或者从 DataStream 转换，那么你可以使用： org.apache.flink.table.api.TableEnvironment 如果你需要转换成 DataStream/DataSet，或者从 DataStream/DataSet 转换，那么你需要将依赖 imports 改成： org.apache.flink.table.api.bridge.java.StreamTableEnvironment org.apache.flink.table.api.bridge.scala.StreamTableEnvironment org.apache.flink.table.api.bridge.java.BatchTableEnvironment org.apache.flink.table.api.bridge.scala.BatchTableEnvironment 对于 Scala 表达式，使用下面的 import： org.apache.flink.table.api._ instead of org.apache.flink.table.api.bridge.scala._ 如果你使用 Scala 隐式转换成 DataStream/DataSet，或者从 DataStream/DataSet 转换，那么该导入 org.apache.flink.table.api.bridge.scala._ 移除 StreamTableSink 接口中的 emitDataStream 方法：该接口的 emitDataStream 方法将移除 移除 BatchTableSink 中的 emitDataSet 方法：将该接口的 emitDataSet 方法重命名为 consumeDataSet 并且返回 DataSink 纠正 TableEnvironment.execute()和 StreamTableEnvironment.execute() 的执行行为：在早期的版本， TableEnvironment.execute() 和 StreamExecutionEnvironment.execute() 都可以触发 Table 程序和 DataStream 程序。从 Flink 1.11.0 开始，Table 程序只能由 TableEnvironment.execute() 触发。将 Table 程序转换为 DataStream 程序（通过 toAppendStream() 或 toRetractStream() 方法）后，只能由 StreamExecutionEnvironment.execute() 触发它。 纠正 ExecutionEnvironment.execute() 和 BatchTableEnvironment.execute() 的执行行为：在早期的版本中， BatchTableEnvironment.execute() 和 ExecutionEnvironment.execute() 都可以触发 Table 和 DataSet 应用程序（针对老的 planner）。 从 Flink 1.11.0 开始，批处理 Table 程序只能由 BatchEnvironment.execute() 触发。将 Table 程序转换为DataSet 程序（通过 toDataSet() 方法）后，只能由 ExecutionEnvironment.execute() 触发它。 在 Row 类型中添加了更改标志：在 Row 类型中添加了一个更改标志 RowKind 配置 重命名 log4j-yarn-session.properties 和 logback-yarn.xml 配置文件：日志配置文件 log4j-yarn-session.properties 和 logback-yarn.xml 被重命名为 log4j-session.properties 和 logback-session.xml 而且， yarn-session.sh 和 kubernet -session.sh 使用这些日志配置文件。 状态 删除已弃用的后台清理开关： StateTtlConfig#cleanupInBackground 已经被删除，因为在 1.10 中该方法已被弃用，并且默认启用了后台 TTL。 删除禁用 TTL 压缩过滤器的的选项：默认情况下，RocksDB 中的 TTL 压缩过滤器在 1.10 中是启用的，在 1.11+ 中总是启用的。因此，在 1.11 中删除了以下选项和方法： state.backend.rocksdb.ttl.compaction.filter.enabled StateTtlConfig#cleanupInRocksdbCompactFilter() RocksDBStateBackend#isTtlCompactionFilterEnabled RocksDBStateBackend#enableTtlCompactionFilter RocksDBStateBackend#disableTtlCompactionFilter (state_backend.py) is_ttl_compaction_filter_enabled (state_backend.py) enable_ttl_compaction_filter (state_backend.py) disable_ttl_compaction_filter 改变 StateBackendFactory#createFromConfig 的参数类型：从 Flink 1.11 开始， StateBackendFactory 接口中的 createFromConfig方法中的参数变为 ReadableConfig 而不是 Configuration。Configuration 类是 ReadableConfig 接口的实现类，因为它实现了 ReadableConfig 接口，所以自定义 StateBackend 也应该做相应的调整。 删除过期的 OptionsFactory 和 ConfigurableOptionsFactory 类：过期的 OptionsFactory 和 ConfigurableOptionsFactory 类已被删除。请改用 RocksDBOptionsFactory 和 ConfigurableRocksDBOptionsFactory。如果任何类扩展了DefaultConfigurableOptionsFactory，也请重新编译你的应用程序代码。 默认情况下启用 setTotalOrderSeek：从 Flink 1.11 开始，默认情况下，RocksDB 的 ReadOptions 将启用 setTotalOrderSeek 选项。这是为了防止用户忘记使用 optimizeForPointLookup。为了向后兼容，我们支持通过 RocksDBOptionsFactory 自定义 ReadOptions。如果观察到性能下降，请将 setTotalOrderSeek 设置为 false（根据我们的测试，这种情况不应该发生）。 增加 state.backend.fs.memory-threshold 的默认值： state.backend.fs.memory-threshold 的默认值已从 1K 增加到20K，以防止在远程 FS 上为小状态创建太多小文件。对于那些 source 处配置很多并行度或者有状态的算子的作业可能会因此变更而出现 JM OOM 或 RPC message exceeding maximum frame size 的问题。如果遇到此类问题，请手动将配置设置回 1K。 PyFlink 对于不支持的数据类型将抛出异常：可以使用一些参数（例如，精度）来配置数据类型。但是，在以前的版本中，用户提供的精度没有任何效果，会使用该精度的默认值。为了避免混淆，从 Flink 1.11 开始，如果不支持该数据类型，则将引发异常。 更改包括： TimeType 精度只能为 0 VarBinaryType/VarCharType 的长度是 0x7fffffff DecimalType 可选值是 38/18 TimestampType/LocalZonedTimestampType 的精度只能是 3 DayTimeIntervalType 的单位是 SECOND ，fractionalPrecision 精度只能为 3 YearMonthIntervalType 的单位是 MONTH ，yearPrecision 精度只能为 2 CharType/BinaryType/ZonedTimestampType 不支持 监控 将所有的 MetricReporters 转换为 plugins：Flink 的所有 MetricReporters 都已经转换为 plugins，它们不再存放在 lib 目录下（这样做可能会导致依赖冲突），而应该放到 /plugins/&lt;some_directory&gt; 目录下。 改变 DataDog 的 metrics reporter Counter Metrics：现在 DataDog metrics reporter 程序将 Counter 指标上报为报告时间间隔内的事件数，而不是总数，将 Counter 语义与 DataDog 文档保持一致。 切换 Log4j2 为默认的：Flink 现在默认使用 Log4j2，希望恢复到 Log4j1 的用户可以在日志文档中找到操作说明 更改 JobManager API 的日志请求行为：从 JobManager 服务端请求一个不可用的 log 或者 stdout 文件现在会返回 404 状态码，在之前的版本中，会返回 file unavailable 。 移除 lastCheckpointAlignmentBuffered metric：现在 lastCheckpointAlignmentBuffered metric 已经被移除了，因为在发出 Checkpoint barrier 之后，上游的任务不会发送任何数据，直到下游侧完成对齐为止，WebUI 仍然会显示该值，但现在始终为 0。 Connectors 移除 Kafka 0.8/0.9 Connector 移除 ElasticSearch 2.x Connector 移除 KafkaPartitioner 改进的 fallback 文件系统，以只处理特定的文件系统 将 FileSystem#getKind 方法设置过期的 Runtime 流作业在 Checkpoint 同步部分失败时会立即失败：无论配置什么参数，Checkpoint 同步部分中的失败（如算子抛出异常）都将立即使其任务（和作业）失败，从 Flink 1.5 版本开始，可以通过设置 setTolerableCheckpointFailureNumber(...) 或 setFailTaskOnCheckpointError(...) 参数来忽略此类的失败，现在这两个参数只影响异步的失败。 Checkpoint 超时不再被 CheckpointConfig#setTolerableCheckpointFailureNumber 忽略：现在将 Checkpoint 超时视为正常的 Checkpoint 故障，并根据 CheckpointConfig＃setTolerableCheckpointFailureNumber 配置的值进行检查。 各种接口变更 移除过期的 StreamTask#getCheckpointLock() ：在方法在 Flink 1.10 中已经设置过期了，目前不再提供该方法。用户可以使用 MailboxExecutor 来执行需要与任务线程安全的操作。 flink-streaming-java 模块不再依赖 flink-client 模块：从 Flink 1.11.0 开始，flink-streaming-java 模块不再依赖 flink-client 模块，如果你项目依赖于 flink-client 模块，需要显示的添加其为依赖项。 AsyncWaitOperator 是可链接的：默认情况下，将允许 AsyncWaitOperator 与所有算子链接在一起，但带有 SourceFunction 的任务除外。 更改了 ShuffleEnvironment 接口的 createInputGates 和 createResultPartitionWriters 方法的参数类型。 CompositeTypeSerializerSnapshot#isOuterSnapshotCompatible 方法标示过期了。 移除了过期的 TimestampExtractor：可以使用 TimestampAssigner 和 WatermarkStrategies。 将 ListCheckpointed 标示为过期的：可以使用 CheckpointedFunction 作为代替 移除了过期的 state 连接方法：移除了 RuntimeContext#getFoldingState() 、 OperatorStateStore#getSerializableListState() 和 OperatorStateStore#getOperatorState() 连接状态的方法，这意味着在 1.10 运行成功的代码在 1.11 上是运行不了的。 详情参考 https://ci.apache.org/projects/flink/flink-docs-master/release-notes/flink-1.11.html 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Apache Flink 1.10 TaskManager 内存管理优化","date":"2020-05-15T16:00:00.000Z","path":"2020/05/16/flink-taskmanager-memory-model/","text":"Apache Flink 1.10 对 TaskManager 的内存模型和 Flink 应用程序的配置选项进行了重大变更。这些最近引入的更改做到了对内存消耗提供了严格的控制，使得 Flink 在各种部署环境（例如 Kubernetes，Yarn，Mesos）更具有适应能力， 在本文中，我们将介绍 Flink 1.10 中的内存模型、如何设置和管理 Flink 应用程序的内存消耗以及社区在最新的 Apache Flink Release 版本中的变化。 Flink 内存模型的介绍对 Apache Flink 的内存模型有清晰的了解，可以使您更有效地管理各种情况下的资源使用情况。 下图描述了 Flink 中的主要内存组件： TaskManager 进程是一个 JVM 进程，从较高的角度来看，它的内存由 JVM Heap 和 Off-Heap 组成。这些类型的内存由 Flink 直接使用，或由 JVM 用于其特定目的（比如元空间 metaspace）。 Flink 中有两个主要的内存使用者： 用户代码中的作业 task 算子 Flink 框架本身的内部数据结构、网络缓冲区 (Network Buffers)等 请注意，用户代码可以直接访问所有的内存类型：JVM 堆、Direct 和 Native 内存。因此，Flink 不能真正控制其分配和使用。但是，有两种供作业 Task 使用并由 Flink 严格控制的 Off-Heap 内存，它们分别是： Managed Memory (Off-Heap) 网络缓冲区 (Network Buffers) 网络缓冲区 (Network Buffers) 是 JVM Direct 内存的一部分，分配在算子和算子之间用于进行用户数据的交换。 怎么去配置 Flink 的内存在最新 Flink 1.10 版本中，为了提供更好的用户体验，框架提供了内存组件的高级和细粒度调优。在 TaskManager 中设置内存基本上有三种选择。 前两个（也是最简单的）选择是需要你配置以下两个选项之一，以供 TaskManager 的 JVM 进程使用的总内存： Total Process Memory：Flink Java 应用程序（包括用户代码）和 JVM 运行整个进程所消耗的总内存。 Total Flink Memory：仅 Flink Java 应用程序消耗的内存，包括用户代码，但不包括 JVM 为其运行而分配的内存。 如果是以 standalone 模式部署，则建议配置 Total Flink Memory，在这种情况下，显式声明为 Flink 分配多少内存是一种常见的做法，而外部 JVM 开销却很少。 对于在容器化环境（例如 Kubernetes，Yarn 或 Mesos）中部署 Flink 的情况，建议配置 Total Process Memory，因为它表示所请求容器的总内存大小，容器化环境通常严格执行此内存限制。 其余的内存组件将根据其默认值或其他已配置的参数自动进行调整。Flink 还会检查整体一致性。你可以在相应的文档中找到有关不同内存组件的更多信息。 此外，你可以使用 FLIP-49 的配置电子表格尝试不同的配置选项，并根据你的情况检查相应的结果。 如果要从 1.10 之前的 Flink 版本进行迁移，我们建议你遵循 Flink 文档的迁移指南中的步骤。 其他组件在配置 Flink 的内存时，可以使用相应选项的值固定不同内存组件的大小，也可以使用多个选项进行调整。下面我们提供有关内存设置的更多信息。 按比例细分 Total Flink Memory此方法允许按比例细分 Total Flink Memory，其中 Managed Memory（如果未明确设置）和网络缓冲区可以占用一部分。然后，将剩余的内存分配给 Task Heap（如果未明确设置）和其他固定的 JVM Heap 和 Off-Heap 组件。下图是这种设置的示例： 请注意： Flink 会校验分配的 Network Memory 大小在其最小值和最大值之间，否则 Flink 的启动会失败，最大值和最小值的限制具有默认值，这些默认值是可以被相应的配置选项覆盖。 通常，Flink 将配置的占比分数视为提示。在某些情况下，真正分配的值可能与占比分数不匹配。例如，如果将 Total Flink Memory 和 Task Heap 配置为固定值，则 Managed Memory 将获得一定比例的内存，而 Network Memory 将获得可能与该比例不完全匹配的剩余内存。 控制容器内存限制的更多提示堆内存和 direct 内存的使用是由 JVM 管理的。在 Apache Flink 或其用户应用程序中，还有许多其他 native 内存消耗的可能来源，它们不是由 Flink 或 JVM 管理的。通常很难控制它们的限制大小，这会使调试潜在的内存泄漏变得复杂。 如果 Flink 的进程以不受管理的方式分配了过多的内存，则在容器化环境中通常可能导致 TaskManager 容器会被杀死。在这种情况下，可能很难理解哪种类型的内存消耗已超过其限制。 Flink 1.10 引入了一些特定的调整选项，以清楚地表示这些组件。 尽管 Flink 不能始终严格执行严格的限制和界限，但此处的想法是明确计划内存使用情况。 下面我们提供一些示例，说明内存设置如何防止容器超出其内存限制： RocksDB 状态不能太大：RocksDB 状态后端的内存消耗是在 Managed Memory 中解决的。 RocksDB 默认情况下遵守其限制（仅自 Flink 1.10 起）。你可以增加 Managed Memory 的大小以提高 RocksDB 的性能，也可以减小 Managed Memory 的大小以节省资源。 用户代码或其依赖项会消耗大量的 off-heap 内存：调整 Task Off-Heap 选项可以为用户代码或其任何依赖项分配额外的 direct 或 native 内存。Flink 无法控制 native 分配，但它设置了 JVM Direct 内存分配的限制。Direct 内存限制由 JVM 强制执行。 JVM metaspace 需要额外的内存：如果遇到 OutOfMemoryError：Metaspace，Flink 提供了一个增加其限制的选项，并且 JVM 将确保不超过该限制。 JVM 需要更多内部内存：无法直接控制某些类型的 JVM 进程分配，但是 Flink 提供了 JVM 开销选项。这些选项允许声明额外的内存量，这些内存是为这些分配所预期的，并且未被其他选项覆盖。 结论最新的 Flink 版本（Flink 1.10）对 Flink 的内存配置进行了一些重大更改，从而可以比以前更好地管理应用程序内存和调试 Flink。未来 JobManager 的内存模型也会采取类似的更改，可以参考 FLIP-116，因此请继续关注即将发布的新版本中新增的功能。如果你对社区有任何建议或问题，我们建议你注册 Apache Flink 邮件列表并参与其中的讨论。 博客英文地址：https://flink.apache.org/news/2020/04/21/memory-management-improvements-flink-1.10.html作者: Andrey Zagrebin本文翻译作者：zhisheng翻译后首发地址：http://www.54tianzhisheng.cn/2020/05/16/flink-taskmanager-memory-model/ 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink Forward 2020 PPT 下载","date":"2020-05-12T16:00:00.000Z","path":"2020/05/13/flink-forward-2020/","text":"Flink Forward 2020 是在线上举办的一次会议 1、《Keynote:Introducing Stateful Functions 2.0: Stream Processing meets Serverless Applications》Stephan Ewen – Apache Flink PMC,Ververica Co-founder, CTO 讲解嘉宾：李钰（绝顶） – Apache Flink Committer，Apache Flink 1.10 Release Manager，阿里巴巴高级技术专家 2、《Keynote:Stream analytics made real with Pravega and Apache Flink》 Srikanth Satya – VP of Engineering at DellEMC 讲解嘉宾：滕昱 – DellEMC 技术总监 3、《Keynote:Apache Flink – Completing Cloudera’s End to End Streaming Platform》 Marton Balassi – Apache Flink PMC ，Senior Solutions Architect at Cloudera Joe Witt – VP of Engineering at Cloudera 讲解嘉宾：杨克特（鲁尼） – Apache Member, Apache Flink PMC, 阿里巴巴高级技术专家 4、《Keynote:The Evolution of Data Infrastructure at Splunk》Eric Sammer – Distinguished Engineer at Splunk 讲解嘉宾：王治江（淘江） – 阿里巴巴高级技术专家 5、《Flink SQL 之 2020：舍我其谁》Fabian Hueske, &amp; Timo Walther 讲解嘉宾：伍翀（云邪），Apache Flink PMC，阿里巴巴技术专家 6、《微博基于 Flink 的机器学习实践》 分享嘉宾： 于茜，微博机器学习研发中心高级算法工程师。多年来致力于使用 Flink 构建实时数据处理和在线机器学习框架，有丰富的社交媒体应用推荐系统的开发经验。 曹富强，微博机器学习研发中心系统工程师。现负责微博机器学习平台数据计算模块。主要涉及实时计算 Flink，Storm，Spark Streaming，离线计算 Hive，Spark 等。目前专注于 Flink 在微博机器学习场景的应用。 于翔，微博机器学习研发中心算法架构工程师。 7、《Flink’s application at Didi》 分享嘉宾：薛康 – 现任滴滴技术专家，实时计算负责人 8、《Alink：提升基于 Flink 的机器学习平台易用性》 分享嘉宾：杨旭（品数） – 阿里巴巴资深技术专家。 9、《Google: 机器学习工作流的分布式处理》Ahmet Altay &amp; Reza Rokni &amp; Robert Crowe 讲解嘉宾：秦江杰 – Apache Flink PMC，阿里巴巴高级技术专家 10、《Flink + AI Flow：让 AI 易如反掌》 分享嘉宾：秦江杰 – Apache Flink PMC，阿里巴巴高级技术专家 11、《终于等到你：PyFlink + Zeppelin》 分享嘉宾： 孙金城（金竹） – Apache Member，Apache Flink PMC，阿里巴巴高级技术专家 章剑锋（简锋） – Apache Member，Apache Zeppelin PMC，阿里巴巴高级技术专家 12、《Uber ：使用 Flink CEP 进行地理情形检测的实践》Teng (Niel) Hu 讲解嘉宾：付典 – Apache Flink Committer，阿里巴巴技术专家 13、《AWS: 如何在全托管 Apache Flink 服务中提供应用高可用》 Ryan Nienhuis &amp; Tirtha Chatterjee 讲解嘉宾：章剑锋（简锋） – Apache Member，Apache Zeppelin PMC，阿里巴巴高级技术专家 14、《Production-Ready Flink and Hive Integration – what story you can tell now?》 Bowen Li 讲解嘉宾：李锐（天离） – Apache Hive PMC，阿里巴巴技术专家 15、《Data Warehouse, Data Lakes, What’s Next?》Xiaowei Jiang 讲解嘉宾：金晓军（仙隐） – 阿里巴巴高级技术专家 16、《Netflix 的 Flink 自动扩缩容》 Abhay Amin 讲解嘉宾：吕文龙（龙三），阿里巴巴技术专家 17、《Apache Flink 误用之痛》 Konstantin Knauf 讲解嘉宾：孙金城（金竹） – Apache Member，Apache Flink PMC，阿里巴巴高级技术专家 18、《A deep dive into Flink SQL》 分享嘉宾：伍翀（云邪），Apache Flink PMC，阿里巴巴技术专家 19、《Lyft: 基于Flink的准实时海量数据分析平台》 Ying Xu &amp; Kailash Hassan Dayanand 讲解嘉宾：王阳（亦祺），阿里巴巴技术专家 如何获取上面这些 PPT？上面的这些 PPT 本人已经整理好了，你可以扫描下面二维码，关注微信公众号：zhisheng，然后在里面回复关键字: ff2020 即可获取已放出的 PPT。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"如何实时监控 Flink 集群和作业？","date":"2020-05-06T16:00:00.000Z","path":"2020/05/07/flink-job-monitor/","text":"Flink 相关的组件和作业的稳定性通常是比较关键的，所以得需要对它们进行监控，如果有异常，则需要及时告警通知。本章先会教会教会大家如何利用现有 Flink UI 上面的信息去发现和排查问题，会指明一些比较重要和我们非常关心的指标，通过这些指标我们能够立马定位到问题的根本原因。接着笔者会教大家如何去利用现有的 Metrics Reporter 去构建一个 Flink 的监控系统，它可以收集到所有作业的监控指标，并会存储这些监控指标数据，最后还会有一个监控大盘做数据可视化，通过这个大盘可以方便排查问题。 实时监控 Flink 及其作业当将 Flink JobManager、TaskManager 都运行起来了，并且也部署了不少 Flink Job，那么它到底是否还在运行、运行的状态如何、资源 TaskManager 和 Slot 的个数是否足够、Job 内部是否出现异常、计算速度是否跟得上数据生产的速度 等这些问题其实对我们来说是比较关注的，所以就很迫切的需要一个监控系统帮我们把整个 Flink 集群的运行状态给展示出来。通过监控系统我们能够很好的知道 Flink 内部的整个运行状态，然后才能够根据项目生产环境遇到的问题 ‘对症下药’。下面分别来讲下 JobManager、TaskManager、Flink Job 的监控以及最关心的一些监控指标。 监控 JobManager我们知道 JobManager 是 Flink 集群的中控节点，类似于 Apache Storm 的 Nimbus 以及 Apache Spark 的 Driver 的角色。它负责作业的调度、作业 Jar 包的管理、Checkpoint 的协调和发起、与 TaskManager 之间的心跳检查等工作。如果 JobManager 出现问题的话，就会导致作业 UI 信息查看不了，TaskManager 和所有运行的作业都会受到一定的影响，所以这也是为啥在 7.1 节中强调 JobManager 的高可用问题。 在 Flink 自带的 UI 上 JobManager 那个 Tab 展示的其实并没有显示其对应的 Metrics，那么对于 JobManager 来说常见比较关心的监控指标有哪些呢？ 基础指标因为 Flink JobManager 其实也是一个 Java 的应用程序，那么它自然也会有 Java 应用程序的指标，比如内存、CPU、GC、类加载、线程信息等。 内存：内存又分堆内存和非堆内存，在 Flink 中还有 Direct 内存，每种内存又有初始值、使用值、最大值等指标，因为在 JobManager 中的工作其实相当于 TaskManager 来说比较少，也不存储事件数据，所以通常 JobManager 占用的内存不会很多，在 Flink JobManager 中自带的内存 Metrics 指标有： 123456789101112jobmanager_Status_JVM_Memory_Direct_Countjobmanager_Status_JVM_Memory_Direct_MemoryUsedjobmanager_Status_JVM_Memory_Direct_TotalCapacityjobmanager_Status_JVM_Memory_Heap_Committedjobmanager_Status_JVM_Memory_Heap_Maxjobmanager_Status_JVM_Memory_Heap_Usedjobmanager_Status_JVM_Memory_Mapped_Countjobmanager_Status_JVM_Memory_Mapped_MemoryUsedjobmanager_Status_JVM_Memory_Mapped_TotalCapacityjobmanager_Status_JVM_Memory_NonHeap_Committedjobmanager_Status_JVM_Memory_NonHeap_Maxjobmanager_Status_JVM_Memory_NonHeap_Used CPU：JobManager 分配的 CPU 使用情况，如果使用类似 K8S 等资源调度系统，则需要对每个容器进行设置资源，比如 CPU 限制不能超过多少，在 Flink JobManager 中自带的 CPU 指标有： 12jobmanager_Status_JVM_CPU_Loadjobmanager_Status_JVM_CPU_Time GC：GC 信息对于 Java 应用来说是避免不了的，每种 GC 都有时间和次数的指标可以供参考，提供的指标有： 1234jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Countjobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Timejobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Countjobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Time Checkpoint 指标因为 JobManager 负责了作业的 Checkpoint 的协调和发起功能，所以 Checkpoint 相关的指标就有表示 Checkpoint 执行的时间、Checkpoint 的时间长短、完成的 Checkpoint 的次数、Checkpoint 失败的次数、Checkpoint 正在执行 Checkpoint 的个数等，其对应的指标如下： 123456789jobmanager_job_lastCheckpointAlignmentBufferedjobmanager_job_lastCheckpointDurationjobmanager_job_lastCheckpointExternalPathjobmanager_job_lastCheckpointRestoreTimestampjobmanager_job_lastCheckpointSizejobmanager_job_numberOfCompletedCheckpointsjobmanager_job_numberOfFailedCheckpointsjobmanager_job_numberOfInProgressCheckpointsjobmanager_job_totalNumberOfCheckpoints 重要的指标另外还有比较重要的指标就是 Flink UI 上也提供的，类似于 Slot 总共个数、Slot 可使用的个数、TaskManager 的个数（通过查看该值可以知道是否有 TaskManager 发生异常重启）、正在运行的作业数量、作业运行的时间和完成的时间、作业的重启次数，对应的指标如下： 12345678jobmanager_job_uptimejobmanager_numRegisteredTaskManagersjobmanager_numRunningJobsjobmanager_taskSlotsAvailablejobmanager_taskSlotsTotaljobmanager_job_downtimejobmanager_job_fullRestartsjobmanager_job_restartingTime 监控 TaskManagerTaskManager 在 Flink 集群中也是一个个的进程实例，它的数量代表着能够运行作业个数的能力，所有的 Flink 作业最终其实是会在 TaskManager 上运行的，TaskManager 管理着运行在它上面的所有作业的 Task 的整个生命周期，包括了 Task 的启动销毁、内存管理、磁盘 IO、网络传输管理等。 因为所有的 Task 都是运行运行在 TaskManager 上的，有的 Task 可能会做比较复杂的操作或者会存储很多数据在内存中，那么就会消耗很大的资源，所以通常来说 TaskManager 要比 JobManager 消耗的资源要多，但是这个资源具体多少其实也不好预估，所以可能会出现由于分配资源的不合理，导致 TaskManager 出现 OOM 等问题。一旦 TaskManager 因为各种问题导致崩溃重启的话，运行在它上面的 Task 也都会失败，JobManager 与它的通信也会丢失。因为作业出现 failover，所以在重启这段时间它是不会去消费数据的，所以必然就会出现数据消费延迟的问题。对于这种情况那么必然就很需要 TaskManager 的监控信息，这样才能够对整个集群的 TaskManager 做一个提前预警。 那么在 Flink 中自带的 TaskManager Metrics 有哪些呢？主要也是 CPU、类加载、GC、内存、网络等。其实这些信息在 Flink UI 上也是有，如下图所示，不知道读者有没有细心观察过。 在这个 TaskManager 的 Metrics 监控页面通常比较关心的指标有内存相关的，还有就是 GC 的指标，通常一个 TaskManager 出现 OOM 之前会不断的进行 GC，在这个 Metrics 页面它展示了年轻代和老年代的 GC 信息（时间和次数），如下图所示，大家可以细心观察下是否 TaskManager OOM 前老年代和新生代的 GC 次数比较、时间比较长。 在 Flink Reporter 中提供的 TaskManager Metrics 指标如下： 12345678910111213141516171819202122232425taskmanager_Status_JVM_CPU_Loadtaskmanager_Status_JVM_CPU_Timetaskmanager_Status_JVM_ClassLoader_ClassesLoadedtaskmanager_Status_JVM_ClassLoader_ClassesUnloadedtaskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Counttaskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Timetaskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Counttaskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Timetaskmanager_Status_JVM_Memory_Direct_Counttaskmanager_Status_JVM_Memory_Direct_MemoryUsedtaskmanager_Status_JVM_Memory_Direct_TotalCapacitytaskmanager_Status_JVM_Memory_Heap_Committedtaskmanager_Status_JVM_Memory_Heap_Maxtaskmanager_Status_JVM_Memory_Heap_Usedtaskmanager_Status_JVM_Memory_Mapped_Counttaskmanager_Status_JVM_Memory_Mapped_MemoryUsedtaskmanager_Status_JVM_Memory_Mapped_TotalCapacitytaskmanager_Status_JVM_Memory_NonHeap_Committedtaskmanager_Status_JVM_Memory_NonHeap_Maxtaskmanager_Status_JVM_Memory_NonHeap_Usedtaskmanager_Status_JVM_Threads_Counttaskmanager_Status_Network_AvailableMemorySegmentstaskmanager_Status_Network_TotalMemorySegmentstaskmanager_Status_Shuffle_Netty_AvailableMemorySegmentstaskmanager_Status_Shuffle_Netty_TotalMemorySegments 监控 Flink 作业对于运行的作业来说，其实我们会更关心其运行状态，如果没有其对应的一些监控信息，那么对于我们来说这个 Job 就是一个黑盒，完全不知道是否在运行，Job 运行状态是什么、Task 运行状态是什么、是否在消费数据、消费数据是咋样（细分到每个 Task）、消费速度能否跟上生产数据的速度、处理数据的过程中是否有遇到什么错误日志、处理数据是否有出现反压问题等等。 上面列举的这些问题通常来说是比较关心的，那么在 Flink UI 上也是有提供的查看对应的信息的，点开对应的作业就可以查看到作业的执行图，每个 Task 的信息都是会展示出来的，包含了状态、Bytes Received（接收到记录的容量大小）、Records Received（接收到记录的条数）、Bytes Sent（发出去的记录的容量大小）、Records Sent（发出去记录的条数）、异常信息、timeline（作业运行状态的时间线）、Checkpoint 信息，如下图所示。 这些指标也可以通过 Flink 的 Reporter 进行上报存储到第三方的时序数据库，然后通过类似 Grafana 展示出来，如下图所示。通过这些信息大概就可以清楚的知道一个 Job 的整个运行状态，然后根据这些运行状态去分析作业是否有问题。 在流作业中最关键的指标无非是作业的实时性，那么延迟就是衡量作业的是否实时的一个基本参数，但是对于现有的这些信息其实还不知道作业的消费是否有延迟，通常来说可以结合 Kafka 的监控去查看对应消费的 Topic 的 Group 的 Lag 信息，如果 Lag 很大就表明有数据堆积了，另外还有一个办法就是需要自己在作业中自定义 Metrics 做埋点，将算子在处理数据的系统时间与数据自身的 Event Time 做一个差值，求得值就可以知道算子消费的数据是什么时候的了。比如在 1571457964000（2019-10-19 12:06:04）Map 算子消费的数据的事件时间是 1571457604000（2019-10-19 12:00:04），相差了 6 分钟，那么就表明消费延迟了 6 分钟，然后通过 Metrics Reporter 将埋点的 Metrics 信息上传，这样最终就可以获取到作业在每个算子处的消费延迟的时间。 上面的是针对于作业延迟的判断方法，另外像类似于作业反压的情况，在 Flink 的 UI 也会有展示，具体怎么去分析和处理这种问题在 9.1 节中有详细讲解。 根据这些监控信息不仅可以做到提前预警，做好资源的扩容（比如增加容器的数量／内存／CPU／并行度／Slot 个数），也还可以找出作业配置的资源是否有浪费。通常来说一个作业的上线可能是会经过资源的预估，然后才会去申请这个作业要配置多少资源，比如算子要使用多少并行度，最后上线后可以通过完整的运行监控信息查看该作业配置的并行度是否有过多或者配置的内存比较大。比如出现下面这些情况的时候可能就是资源出现浪费了： 作业消费从未发生过延迟，即使在数据流量高峰的时候，也未发生过消费延迟 作业运行所在的 TaskManager 堆内存使用率异常的低 作业运行所在的 TaskManager 的 GC 时间和次数非常规律，没有出现异常的现象，如下图所示。 在 Flink Metrics Reporter 上传的指标中大概有下面这些： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647taskmanager_job_task_Shuffle_Netty_Input_Buffers_outPoolUsagetaskmanager_job_task_Shuffle_Netty_Input_Buffers_outputQueueLengthtaskmanager_job_task_Shuffle_Netty_Output_Buffers_inPoolUsagetaskmanager_job_task_Shuffle_Netty_Output_Buffers_inputExclusiveBuffersUsagetaskmanager_job_task_Shuffle_Netty_Output_Buffers_inputFloatingBuffersUsagetaskmanager_job_task_Shuffle_Netty_Output_Buffers_inputQueueLengthtaskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocaltaskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocalPerSecondtaskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemotetaskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemotePerSecondtaskmanager_job_task_Shuffle_Netty_Output_numBytesInLocaltaskmanager_job_task_Shuffle_Netty_Output_numBytesInLocalPerSecondtaskmanager_job_task_Shuffle_Netty_Output_numBytesInRemotetaskmanager_job_task_Shuffle_Netty_Output_numBytesInRemotePerSecondtaskmanager_job_task_buffers_inPoolUsagetaskmanager_job_task_buffers_inputExclusiveBuffersUsagetaskmanager_job_task_buffers_inputFloatingBuffersUsagetaskmanager_job_task_buffers_inputQueueLengthtaskmanager_job_task_buffers_outPoolUsagetaskmanager_job_task_buffers_outputQueueLengthtaskmanager_job_task_checkpointAlignmentTimetaskmanager_job_task_currentInputWatermarktaskmanager_job_task_numBuffersInLocaltaskmanager_job_task_numBuffersInLocalPerSecondtaskmanager_job_task_numBuffersInRemotetaskmanager_job_task_numBuffersInRemotePerSecondtaskmanager_job_task_numBuffersOuttaskmanager_job_task_numBuffersOutPerSecondtaskmanager_job_task_numBytesIntaskmanager_job_task_numBytesInLocaltaskmanager_job_task_numBytesInLocalPerSecondtaskmanager_job_task_numBytesInPerSecondtaskmanager_job_task_numBytesInRemotetaskmanager_job_task_numBytesInRemotePerSecondtaskmanager_job_task_numBytesOuttaskmanager_job_task_numBytesOutPerSecondtaskmanager_job_task_numRecordsIntaskmanager_job_task_numRecordsInPerSecondtaskmanager_job_task_numRecordsOuttaskmanager_job_task_numRecordsOutPerSecondtaskmanager_job_task_operator_currentInputWatermarktaskmanager_job_task_operator_currentOutputWatermarktaskmanager_job_task_operator_numLateRecordsDroppedtaskmanager_job_task_operator_numRecordsIntaskmanager_job_task_operator_numRecordsInPerSecondtaskmanager_job_task_operator_numRecordsOuttaskmanager_job_task_operator_numRecordsOutPerSecond 最关心的性能指标上面已经提及到 Flink 的 JobManager、TaskManager 和运行的 Flink Job 的监控以及常用的监控信息，这些指标有的是可以直接在 Flink 的 UI 上观察到的，另外 Flink 提供了 Metrics Reporter 进行上报存储到监控系统中去，然后通过可视化的图表进行展示，在 8.2 节中将教大家如何构建一个完整的监控系统。那么有了这么多监控指标，其实哪些是比较重要的呢，比如说这些指标出现异常的时候可以发出告警及时进行通知，这样可以做到预警作用，另外还可以根据这些信息进行作业资源的评估。下面列举一些笔者觉得比较重要的指标： JobManager在 JobManager 中有着该集群中所有的 TaskManager 的个数、Slot 的总个数、Slot 的可用个数、运行的时间、作业的 Checkpoint 情况，笔者觉得这几个指标可以重点关注。 TaskManager 个数：如果出现 TaskManager 突然减少，可能是因为有 TaskManager 挂掉重启，一旦该 TaskManager 之前运行了很多作业，那么重启带来的影响必然是巨大的。 Slot 个数：取决于 TaskManager 的个数，决定了能运行作业的最大并行度，如果资源不够，及时扩容。 作业运行时间：根据作业的运行时间来判断作业是否存活，中途是否掉线过。 Checkpoint 情况：Checkpoint 是 JobManager 发起的，并且关乎到作业的状态是否可以完整的保存。 TaskManager因为所有的作业最终都是运行在 TaskManager 上，所以 TaskManager 的监控指标也是异常的监控，并且作业的复杂度也会影响 TaskManager 的资源使用情况，所以 TaskManager 的基础监控指标比如内存、GC 如果出现异常或者超出设置的阈值则需要立马进行告警通知，防止后面导致大批量的作业出现故障重启。 内存使用率：部分作业的算子会将所有的 State 数据存储在内存中，这样就会导致 TaskManager 的内存使用率会上升，还有就是可以根据该指标看作业的利用率，从而最后来重新划分资源的配置。 GC 情况：分时间和次数，一旦 TaskManager 的内存率很高的时候，必定伴随着频繁的 GC，如果在 GC 的时候没有得到及时的预警，那么将面临 OOM 风险。 Flink Job作业的稳定性和及时性其实就是大家最关心的，常见的指标有：作业的状态、Task 的状态、作业算子的消费速度、作业出现的异常日志。 作业的状态：在 UI 上是可以看到作业的状态信息，常见的状态变更信息如下图所示。 Task 的状态：其实导致作业的状态发生变化的原因通常是由于 Task 的运行状态出现导致，所以也需要对 Task 的运行状态进行监控，Task 的运行状态如下图所示。 作业异常日志：导致 Task 出现状态异常的根因通常是作业中的代码出现各种各样的异常日志，最后可能还会导致作业无限重启，所以作业的异常日志也是需要及时关注。 作业重启次数：当 Task 状态和作业的状态发生变化的时候，如果作业中配置了重启策略或者开启了 Checkpoint 则会进行作业重启的，重启作业的带来的影响也会很多，并且会伴随着一些不确定的因素，最终导致作业一直重启，这样既不能解决问题，还一直在占用着资源的消耗。 算子的消费速度：代表了作业的消费能力，还可以知道作业是否发生延迟，可以包含算子接收的数据量和发出去数据量，从而可以知道在算子处是否有发生数据的丢失。 小结与反思本节讲了 Flink 中常见的监控对象，比如 JobManager、TaskManager 和 Flink Job，对于这几个分别介绍了其内部大概有的监控指标，以及在真实生产环境关心的指标，你是否还有其他的监控指标需要补充呢？ 本节涉及的监控指标对应的含义可以参考官网链接：metrics 本节涉及的监控指标列表地址：flink_monitor_measurements 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"基于 Apache Flink 的实时 Error 日志告警","date":"2020-04-14T16:00:00.000Z","path":"2020/04/15/flink-error-log-alert/","text":"大数据时代，随着公司业务不断的增长，数据量自然也会跟着不断的增长，那么业务应用和集群服务器的的规模也会逐渐扩大，几百台服务器在一般的公司已经是很常见的了。那么将应用服务部署在如此多的服务器上，对开发和运维人员来说都是一个挑战。一个优秀的系统运维平台是需要将部署在这么多服务器上的应用监控信息汇总成一个统一的数据展示平台，方便运维人员做日常的监测、提升运维效率，还可以及时反馈应用的运行状态给应用开发人员。举个例子，应用的运行日志需要按照时间排序做一个展示，并且提供日志下载和日志搜索等服务，这样如果应用出现问题开发人员首先可以根据应用日志的错误信息进行问题的排查。那么该如何实时的将应用的 Error 日志推送给应用开发人员呢，接下来我们将讲解日志的处理方案。 日志处理方案的演进日志处理的方案也是有一个演进的过程，要想弄清楚整个过程，我们先来看下日志的介绍。 什么是日志？日志是带时间戳的基于时间序列的数据，它可以反映系统的运行状态，包括了一些标识信息（应用所在服务器集群名、集群机器 IP、机器设备系统信息、应用名、应用 ID、应用所属项目等） 日志处理方案演进日志处理方案的演进过程： 日志处理 v1.0: 应用日志分布在很多机器上，需要人肉手动去机器查看日志信息。 日志处理 v2.0: 利用离线计算引擎统一的将日志收集，形成一个日志搜索分析平台，提供搜索让用户根据关键字进行搜索和分析，缺点就是及时性比较差。 日志处理 v3.0: 利用 Agent 实时的采集部署在每台机器上的日志，然后统一发到日志收集平台做汇总，并提供实时日志分析和搜索的功能，这样从日志产生到搜索分析出结果只有简短的延迟（在用户容忍时间范围之内），优点是快，但是日志数据量大的情况下带来的挑战也大。 日志采集工具对比上面提到的日志采集，其实现在已经有很多开源的组件支持去采集日志，比如 Logstash、Filebeat、Fluentd、Logagent 等，这里简单做个对比。 LogstashLogstash 是一个开源数据收集引擎，具有实时管道功能。Logstash 可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地。如下图所示，Logstash 将采集到的数据用作分析、监控、告警等。 优势：Logstash 主要的优点就是它的灵活性，它提供很多插件，详细的文档以及直白的配置格式让它可以在多种场景下应用。而且现在 ELK 整个技术栈在很多公司应用的比较多，所以基本上可以在往上找到很多相关的学习资源。 劣势：Logstash 致命的问题是它的性能以及资源消耗(默认的堆大小是 1GB)。尽管它的性能在近几年已经有很大提升，与它的替代者们相比还是要慢很多的，它在大数据量的情况下会是个问题。另一个问题是它目前不支持缓存，目前的典型替代方案是将 Redis 或 Kafka 作为中心缓冲池： Filebeat作为 Beats 家族的一员，Filebeat 是一个轻量级的日志传输工具，它的存在正弥补了 Logstash 的缺点，Filebeat 作为一个轻量级的日志传输工具可以将日志推送到 Kafka、Logstash、ElasticSearch、Redis。它的处理流程如下图所示： 优势：Filebeat 只是一个二进制文件没有任何依赖。它占用资源极少，尽管它还十分年轻，正式因为它简单，所以几乎没有什么可以出错的地方，所以它的可靠性还是很高的。它也为我们提供了很多可以调节的点，例如：它以何种方式搜索新的文件，以及当文件有一段时间没有发生变化时，何时选择关闭文件句柄。 劣势：Filebeat 的应用范围十分有限，所以在某些场景下我们会碰到问题。例如，如果使用 Logstash 作为下游管道，我们同样会遇到性能问题。正因为如此，Filebeat 的范围在扩大。开始时，它只能将日志发送到 Logstash 和 Elasticsearch，而现在它可以将日志发送给 Kafka 和 Redis，在 5.x 版本中，它还具备过滤的能力。 FluentdFluentd 创建的初衷主要是尽可能的使用 JSON 作为日志输出，所以传输工具及其下游的传输线不需要猜测子字符串里面各个字段的类型。这样它为几乎所有的语言都提供库，这也意味着可以将它插入到自定义的程序中。它的处理流程如下图所示： 优势：和多数 Logstash 插件一样，Fluentd 插件是用 Ruby 语言开发的非常易于编写维护。所以它数量很多，几乎所有的源和目标存储都有插件(各个插件的成熟度也不太一样)。这也意味这可以用 Fluentd 来串联所有的东西。 劣势：因为在多数应用场景下得到 Fluentd 结构化的数据，它的灵活性并不好。但是仍然可以通过正则表达式来解析非结构化的数据。尽管性能在大多数场景下都很好，但它并不是最好的，它的缓冲只存在与输出端，单线程核心以及 Ruby GIL 实现的插件意味着它大的节点下性能是受限的。 LogagentLogagent 是 Sematext 提供的传输工具，它用来将日志传输到 Logsene(一个基于 SaaS 平台的 Elasticsearch API)，因为 Logsene 会暴露 Elasticsearch API，所以 Logagent 可以很容易将数据推送到 Elasticsearch 。 优势：可以获取 /var/log 下的所有信息，解析各种格式的日志，可以掩盖敏感的数据信息。它还可以基于 IP 做 GeoIP 丰富地理位置信息。同样，它轻量又快速，可以将其置入任何日志块中。Logagent 有本地缓冲，所以在数据传输目的地不可用时不会丢失日志。 劣势：没有 Logstash 灵活。 日志结构设计前面介绍了日志和对比了常用日志采集工具的优势和劣势，通常在不同环境，不同机器上都会部署日志采集工具，然后采集工具会实时的将新的日志采集发送到下游，因为日志数据量毕竟大，所以建议发到 MQ 中，比如 Kafka，这样再想怎么处理这些日志就会比较灵活。假设我们忽略底层采集具体是哪种，但是规定采集好的日志结构化数据如下： 12345678public class LogEvent &#123; private String type;//日志的类型(应用、容器、...) private Long timestamp;//日志的时间戳 private String level;//日志的级别(debug/info/warn/error) private String message;//日志内容 //日志的标识(应用 ID、应用名、容器 ID、机器 IP、集群名、...) private Map&lt;String, String&gt; tags = new HashMap&lt;&gt;();&#125; 然后上面这种 LogEvent 的数据（假设采集发上来的是这种结构数据的 JSON 串，所以需要在 Flink 中做一个反序列化解析）就会往 Kafka 不断的发送数据，样例数据如下： 123456789101112&#123; \"type\": \"app\", \"timestamp\": 1570941591229, \"level\": \"error\", \"message\": \"Exception in thread \\\"main\\\" java.lang.NoClassDefFoundError: org/apache/flink/api/common/ExecutionConfig$GlobalJobParameters\", \"tags\": &#123; \"cluster_name\": \"zhisheng\", \"app_name\": \"zhisheng\", \"host_ip\": \"127.0.0.1\", \"app_id\": \"21\" &#125;&#125; 那么在 Flink 中如何将应用异常或者错误的日志做实时告警呢？ 异常日志实时告警项目架构整个异常日志实时告警项目的架构如下图所示。 应用日志散列在不同的机器，然后每台机器都有部署采集日志的 Agent（可以是上面的 Filebeat、Logstash 等），这些 Agent 会实时的将分散在不同机器、不同环境的应用日志统一的采集发到 Kafka 集群中，然后告警这边是有一个 Flink 作业去实时的消费 Kafka 数据做一个异常告警计算处理。如果还想做日志的搜索分析，可以起另外一个作业去实时的将 Kafka 的日志数据写入进 ElasticSearch，再通过 Kibana 页面做搜索和分析。 日志数据发送到 Kafka上面已经讲了日志数据 LogEvent 的结构和样例数据，因为要在服务器部署采集工具去采集应用日志数据对于本地测试来说可能稍微复杂，所以在这里就只通过代码模拟构造数据发到 Kafka 去，然后在 Flink 作业中去实时消费 Kafka 中的数据，下面演示构造日志数据发到 Kafka 的工具类，这个工具类主要分两块，构造 LogEvent 数据和发送到 Kafka。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384@Slf4jpublic class BuildLogEventDataUtil &#123; //Kafka broker 和 topic 信息 public static final String BROKER_LIST = \"localhost:9092\"; public static final String LOG_TOPIC = \"zhisheng_log\"; public static void writeDataToKafka() &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", BROKER_LIST); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 0; i &lt; 10000; i++) &#123; //模拟构造 LogEvent 对象 LogEvent logEvent = new LogEvent().builder() .type(\"app\") .timestamp(System.currentTimeMillis()) .level(logLevel()) .message(message(i + 1)) .tags(mapData()) .build();// System.out.println(logEvent); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(LOG_TOPIC, null, null, GsonUtil.toJson(logEvent)); producer.send(record); &#125; producer.flush(); &#125; public static void main(String[] args) &#123; writeDataToKafka(); &#125; public static String message(int i) &#123; return \"这是第 \" + i + \" 行日志！\"; &#125; public static String logLevel() &#123; Random random = new Random(); int number = random.nextInt(4); switch (number) &#123; case 0: return \"debug\"; case 1: return \"info\"; case 2: return \"warn\"; case 3: return \"error\"; default: return \"info\"; &#125; &#125; public static String hostIp() &#123; Random random = new Random(); int number = random.nextInt(4); switch (number) &#123; case 0: return \"121.12.17.10\"; case 1: return \"121.12.17.11\"; case 2: return \"121.12.17.12\"; case 3: return \"121.12.17.13\"; default: return \"121.12.17.10\"; &#125; &#125; public static Map&lt;String, String&gt; mapData() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"app_id\", \"11\"); map.put(\"app_name\", \"zhisheng\"); map.put(\"cluster_name\", \"zhisheng\"); map.put(\"host_ip\", hostIp()); map.put(\"class\", \"BuildLogEventDataUtil\"); map.put(\"method\", \"main\"); map.put(\"line\", String.valueOf(new Random().nextInt(100))); //add more tag return map; &#125;&#125; 如果之前 Kafka 中没有 zhisheng_log 这个 topic，运行这个工具类之后也会自动创建这个 topic 了。 Flink 实时处理日志数据在 3.7 章中已经讲过如何使用 Flink Kafka connector 了，接下来就直接写代码去消费 Kafka 中的日志数据，作业代码如下： 1234567891011121314public class LogEventAlert &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); Properties properties = KafkaConfigUtil.buildKafkaProps(parameterTool); FlinkKafkaConsumer011&lt;LogEvent&gt; consumer = new FlinkKafkaConsumer011&lt;&gt;( parameterTool.get(\"log.topic\"), new LogSchema(), properties); env.addSource(consumer) .print(); env.execute(\"log event alert\"); &#125;&#125; 因为 Kafka 的日志数据是 JSON 的，所以在消费的时候需要额外定义 Schema 来反序列化数据，定义的 LogSchema 如下： 123456789101112131415161718192021222324public class LogSchema implements DeserializationSchema&lt;LogEvent&gt;, SerializationSchema&lt;LogEvent&gt; &#123; private static final Gson gson = new Gson(); @Override public LogEvent deserialize(byte[] bytes) throws IOException &#123; return gson.fromJson(new String(bytes), LogEvent.class); &#125; @Override public boolean isEndOfStream(LogEvent logEvent) &#123; return false; &#125; @Override public byte[] serialize(LogEvent logEvent) &#123; return gson.toJson(logEvent).getBytes(Charset.forName(\"UTF-8\")); &#125; @Override public TypeInformation&lt;LogEvent&gt; getProducedType() &#123; return TypeInformation.of(LogEvent.class); &#125;&#125; 配置文件中设置如下： 123kafka.brokers=localhost:9092kafka.group.id=zhishenglog.topic=zhisheng_log 接下来先启动 Kafka，然后运行 BuildLogEventDataUtil 工具类，往 Kafka 中发送模拟的日志数据，接下来运行 LogEventAlert 类，去消费将 Kafka 中的数据做一个验证，运行结果如下图所示，可以发现有日志数据打印出来了。 处理应用异常日志上面已经能够处理这些日志数据了，但是需求是要将应用的异常日志做告警，所以在消费到所有的数据后需要过滤出异常的日志，比如可以使用 filter 算子进行过滤。 1.filter(logEvent -&gt; \"error\".equals(logEvent.getLevel())) 过滤后只有 error 的日志数据打印出来了，如下图所示： 再将作业打包通过 UI 提交到集群运行的结果如下： 再获取到这些 Error 类型的数据后，就可以根据这个数据构造成一个新的 Event，组装成告警消息，然后在 Sink 处调用下游的通知策略进行告警通知，当然这些告警通知策略可能会很多，然后还有收敛策略。具体的通知策略和收敛策略在这节不做细讲，最后发出的应用异常日志告警消息中会携带一个链接，点击该链接可以跳转到对应的应用异常页面，这样就可以查看应用堆栈的详细日志，更加好定位问题。 小结与反思本节开始讲了日志处理方案的演进，接着分析最新日志方案的实现架构，包含它的日志结构设计和异常日志实时告警的方案，然后通过模拟日志数据发送到 Kafka，Flink 实时去处理这种日志的数据进行告警。 本节涉及的代码地址：flink-learning-monitor-alert 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink 能否动态更改 Checkpoint 配置","date":"2020-02-28T16:00:00.000Z","path":"2020/02/29/flink-nacos-checkpoint/","text":"前段时间在社区邮件中看到有人提问是否可以动态开启 Checkpoint，昨天在钉钉群中又看到有个同学在问能够动态调整 Checkpoint 的时间，其实不仅仅是这些，在社区邮件和群里经常看到有问这块内容的问题，所以可以发现在 Flink 中其实关于 Checkpoint 相关的东西还是非常重要且解决起来比较麻烦，估计应该也困扰了不少人。 不过今天的话题不是在于去讨论 Checkpoint 的机制，因为前面两个问题都涉及到了动态的去配置 Checkpoint 的参数（是否开启和 Checkpoint 的时间间隔），而 zhisheng 我在前面通过两个视频讲解了 Flink 如何与 Apollo 和 Nacos 整合去动态的更改作业配置，所以私底下就有同学找我咨询是否可以动态的更改 Checkpoint 配置，我当时因为知道其实有些参数是一旦初始化了之后是改不了的，但是具体什么参数我也不难全部列举，所以只好回答那位同学说：以自己实测的结果为准哈。 所以这里我就给大家演示一下到底是否可以动态的更改 Checkpoint 配置，请看我在 B 站的视频： https://www.bilibili.com/video/av92655075/ 通过这个视频，虽然我是使用 Flink 和 Nacos 整合的，作业监听到了 Checkpoint 的配置做了修改，但是可以发现其实 Checkpoint 更改后其实是不生效的。 这里仅从个人的思考来解释一下：因为 Flink 是 Lazy Evaluation（延迟执行），当程序的 main 方法执行时，我们创建的 env 会依次进行属性的初始化配置，但是数据源加载数据和数据转换等算子不会立马执行，这些算子操作会被创建并添加到程序的执行计划中去，只有当执行环境 env 的 execute 方法被显示地触发执行时，整个程序才开始执行实际的操作（StreamGraph -&gt; JobGraph -&gt; ExecutionGraph），所以在程序执行 execute 方法后再修改 env 的配置其实就不起作用了。 另外给大家来看下邱从贤(负责 Flink State 相关)对能否动态配置 Checkpoint 的回答： 相关的测试代码在: https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-configration-center/flink-learning-configration-center-nacos 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Nacos","slug":"Nacos","permalink":"http://www.54tianzhisheng.cn/tags/Nacos/"}]},{"title":"Flink 整合 Apollo，动态更新 Flink 作业配置","date":"2020-02-22T16:00:00.000Z","path":"2020/02/23/flink-apollo/","text":"本人自己录的视频，讲解 Flink 整和 Apollo，动态更新作业配置，无需重启作业！ 在上一篇讲解 Flink 与 Nacos 整合的视频 中，讲过了常见的几种更新配置的方法，最常使用的可能就是通过广播流的方式，相信看完上个视频的，估计对整合 Nacos 做动态更新配置应该问题不大，zhisheng 我也觉得稍微简单，尤其 Nacos 搭建安装也比较简单。不知道大家公司有没有使用 Nacos 呢？我知道有的公司使用 Apollo 居多，所以后面就有读者问我能不能出个整合 Apollo 的视频，所以我趁着周末大晚上的时间就开始折腾了一番，本篇文章将给大家讲解与 Apollo 整合，动态的更新 Flink 配置。 Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 因为它的自身架构原因，导致安装可能会比较复杂，需要安装好多个组件，个人觉得比 Nacos 复杂，幸好的是官方的文档比较详细，跟着安装步骤来说还是没有问题的。zhisheng 我是只在自己 Mac 电脑上面安装了一个单机版的，仅为测试使用。 快速上手的请参考该链接 https://github.com/nobodyiam/apollo-build-scripts，这样你就能够在几分钟内在本地环境部署、启动 Apollo 配置中心。另外还提供了 Quick Start 的 Docker 版本，如果你对 Docker 比较熟悉的话，那更方便了。 主要演示流程（安装 Apollo 和整合 Flink），本人录了个视频，更方便大家去实战操作，欢迎观看： 代码地址：https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-configration-center/flink-learning-configration-center-apollo 注意引入 Apollo 的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.ctrip.framework.apollo&lt;/groupId&gt; &lt;artifactId&gt;apollo-client&lt;/artifactId&gt; &lt;version&gt;1.5.1&lt;/version&gt;&lt;/dependency&gt; 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Apollo","slug":"Apollo","permalink":"http://www.54tianzhisheng.cn/tags/Apollo/"}]},{"title":"Flink 整合 Nacos，让 Flink 作业配置动态更新不再是难事","date":"2020-02-21T16:00:00.000Z","path":"2020/02/22/flink-nacos/","text":"本人自己录的视频，讲解 Flink 整和 Nacos，动态更新作业配置，无需重启作业！ 我们知道 Flink 作业的配置一般都是通过在作业启动的时候通过参数传递的，或者通过读取配置文件的参数，在作业启动后初始化了之后如果再想更新作业的配置一般有两种解决方法： 改变启动参数或者改变配置文件，重启作业，让作业能够读取到修改后的配置 通过读取配置流（需要自定义 Source 读取配置），然后流和流连接起来 这两种解决方法一般是使用的比较多，对于第一种方法，zhisheng 我本人其实是不太建议的，重启作业会带来很多影响，Flink 作业完整的重启流程应该是：当作业停掉的时候需要去做一次 Savepoint（相当于把作业的状态做一次完整的快照），启动的时候又需要将作业从 Savepoint 启动，整个流程如果状态比较大的话，做一次 Savepoint 和从 Savepoint 初始化的时间会比较久，然而流处理的场景下通常数据量都是比较大的，那么在这段时间内，可能会造成不少的数据堆积（可能分钟内就上千万或者更多），当作业启动后再去追这千万量级的数据，对作业来说压力自然会增大。 对于第二种方法也是一种用的很多的方式，自己也比较推荐，之前自己在社区直播的时候也有讲过类似的方案，但是今天我准备讲解另一种方法 —— 整合配置中心，没看见有人这么用过，我也算是第一个吃螃蟹的人了！说到配置中心，目前国内有 Apollo 和 Nacos，这里先来讲下和 Nacos 的整合，下面的实战操作请看我录制的视频。 代码地址：https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-configration-center/flink-learning-configration-center-nacos 我本人安装的 Nacos 依赖是阿里的，因为自己本地编译了一份源码，所以可能会有这些依赖在自己本地的 .m2 目录中： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt; &lt;artifactId&gt;nacos-core&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt; &lt;artifactId&gt;nacos-client&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt; &lt;artifactId&gt;nacos-common&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt; 但是有些同学反馈说上面的依赖引入不上，一直下载不了，比如 nacos-core，这里建议去 https://mvnrepository.com/search?q=nacos-core 看一下第一个，然后引用试试。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Nacos","slug":"Nacos","permalink":"http://www.54tianzhisheng.cn/tags/Nacos/"}]},{"title":"Flink 1.10 新特性研究","date":"2020-02-21T16:00:00.000Z","path":"2020/02/22/flink-1.10-release/","text":"Flink 1.10 release 文档描述了一些比较重要的点，比如配置、操作、依赖、1.9 版本和 1.10 版本之间的区别，如果你准备将 Flink 升级到 1.10 版本，建议仔细看完下面的内容。 集群和部署 文件系统需要通过插件的方式加载 Flink 客户端根据配置的类加载策略加载，parent-first 和 child-first 两种方式 允许在所有的 TaskManager 上均匀地分布任务，需要在 flink-conf.yaml 配置文件中配置 cluster.evenly-spread-out-slots: true 参数 高可用存储目录做了修改，在 HA_STORAGE_DIR/HA_CLUSTER_ID 下，HA_STORAGE_DIR 路径通过 high-availability.storageDir 参数配置，HA_CLUSTER_ID 路径通过 high-availability.cluster-id 参数配置 当使用 -yarnship 命令参数时，资源目录和 jar 文件会被添加到 classpath 中 移除了 --yn/--yarncontainer 命令参数 移除了 --yst/--yarnstreaming 命令参数 Flink Mesos 会拒绝掉所有的过期请求 重构了 Flink 的调度程序，其目标是使调度策略在未来可以定制 支持 Java 11，当使用 Java 11 启动 Flink 时，会有些 WARNING 的日志提醒，注意：Cassandra、Hive、HBase 等 connector 没有使用 Java 11 测试过 内存管理 全新的 Task Executor 内存模型，会影响 standalone、YARN、Mesos、K8S 的部署，JobManager 的内存模型没有修改。如果你在没有调整的情况下，重用以前的 Flink 配置，则新的内存模型可能会导致 JVM 的计算内存参数不同，从而导致性能的变化。 以下选项已经删除，不再起作用： 以下选项已经替换成其他的选项： RocksDB State Backend 内存可以控制，用户可以调整 RocksDB 的写/读内存比率 state.backend.rocksdb.memory.write-buffer-ratio（默认情况下 0.5）和为索引/过滤器保留的内存部分 state.backend.rocksdb.memory.high-prio-pool-ratio（默认情况下0.1） 细粒度的算子（Operator）资源管理，配置选项 table.exec.resource.external-buffer-memory，table.exec.resource.hash-agg.memory，table.exec.resource.hash-join.memory，和 table.exec.resource.sort.memory 已被弃用 Table API 和 SQL 将 ANY 类型重命名为 RAW 类型，该标识符 raw 现在是保留关键字，在用作 SQL 字段或函数名称时必须转义 重命名 Table Connector 属性，以便编写 DDL 语句时提供更好的用户体验，比如 Kafka Connector 属性 connector.properties 和 connector.specific-offsets、Elasticsearch Connector 属性 connector.hosts 之前与临时表和视图进行交互的方法已经被弃用，目前使用 createTemporaryView() 移除了 ExternalCatalog API（ExternalCatalog、SchematicDescriptor、MetadataDescriptor、StatisticsDescriptor），建议使用新的 Catalog API 配置 ConfigOptions 如果无法将配置的值解析成所需要的类型，则会抛出 IllegalArgumentException 异常，之前是会返回默认值 增加默认的重启策略延迟时间（fixed-delay 和 failure-rate 已经默认是 1s，之前是 0） 简化集群级别的重启策略配置，现在集群级别的重启策略仅由 restart-strategy 配置和是否开启 Checkpoint 确定 默认情况下禁用内存映射的 BoundedBlockingSubpartition 移除基于未认证的网络流量控制 移除 HighAvailabilityOptions 中的 HA_JOB_DELAY 配置 状态（State） 默认开启 TTL 的状态后台清理 弃用 StateTtlConfig#Builder#cleanupInBackground() 使用 RocksDBStateBackend 时，默认将计时器存储在 RocksDB 中，之前是存储在堆内存（Heap）中 StateTtlConfig#TimeCharacteristic 已经被移除，目前使用 StateTtlConfig#TtlTimeCharacteristic 新增 MapState#isEmpty() 方法来检查 MapState 是否为空，该方法比使用 mapState.keys().iterator().hasNext() 的速度快 40% RocksDB 升级，发布了自己的 FRocksDB（基于 RocksDB 5.17.2 版本），主要是因为高版本的 RocksDB 在某些情况下性能会下降 默认禁用 RocksDB 日志记录，需要启用的话需要利用 RocksDBOptionsFactory 创建 DBOptions 实例，并通过 setInfoLogLevel 方法设置 INFO_LEVEL 优化从 RocksDB Savepoint 恢复的机制，以前如果从包含大型 KV 对的 RocksDB Savepoint 恢复时，用户可能会遇到 OOM。现在引入了可配置的内存限制，RocksDBWriteBatchWrapper 默认值为 2MB。RocksDB的WriteBatch 将在达到内存限制之前刷新。可以在 flink-conf.yml 中修改 state.backend.rocksdb.write-batch-size 配置 PyFlink 不再支持 Python2 监控 InfluxdbReporter 会跳过 Inf 和 NaN（InfluxDB 不支持的类型，比如 Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, Double.NaN） 连接器（Connectors） 改变 Kinesis 连接器的 License 接口更改 ExecutionConfig＃getGlobalJobParameters() 不再返回 null MasterTriggerRestoreHook 中的 triggerCheckpoint 方法必须时非阻塞的 HA 服务的客户端/服务器端分离，HighAvailabilityServices 已分离成客户端 ClientHighAvailabilityServices 和集群端 HighAvailabilityServices HighAvailabilityServices#getWebMonitorLeaderElectionService() 标记过期 LeaderElectionService 接口做了更改 弃用 Checkpoint 锁 弃用 OptionsFactory 和 ConfigurableOptionsFactory 接口 参考：https://github.com/apache/flink/blob/master/docs/release-notes/flink-1.10.zh.md 看了下官方的这份新版本的介绍，感觉还缺少很多新功能的介绍，比如： 在 1.10 版本中把 Blink 版本的哪些功能整合过来了 竟然没有写 Flink 对原生 Kubernetes 的集成 PyFlink 的介绍是认真的吗？ 对 Hive 的生产级别集成，完全没有提及呀 Table API/SQL 优化点讲得不太多 可能因为篇幅的问题，还有很多特性都没有讲解出来，得我们自己去找源码学习！ 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Nacos","slug":"Nacos","permalink":"http://www.54tianzhisheng.cn/tags/Nacos/"}]},{"title":"Flink Checkpoint 问题排查实用指南","date":"2020-02-19T16:00:00.000Z","path":"2020/02/20/flink-checkpoint/","text":"在 Flink 中，状态可靠性保证由 Checkpoint 支持，当作业出现 failover 的情况下，Flink 会从最近成功的 Checkpoint 恢复。 作者：邱从贤（山智）转载自：https://www.jianshu.com/p/fc100f85a0fb 在实际情况中，我们可能会遇到 Checkpoint 失败，或者 Checkpoint 慢的情况，本文会统一聊一聊 Flink 中 Checkpoint 异常的情况（包括失败和慢），以及可能的原因和排查思路。 1. Checkpoint 流程简介首先我们需要了解 Flink 中 Checkpoint 的整个流程是怎样的，在了解整个流程之后，我们才能在出问题的时候，更好的进行定位分析。 从上图我们可以知道，Flink 的 Checkpoint 包括如下几个部分： JM trigger checkpoint Source 收到 trigger checkpoint 的 PRC，自己开始做 snapshot，并往下游发送 barrier 下游接收 barrier（需要 barrier 都到齐才会开始做 checkpoint） Task 开始同步阶段 snapshot Task 开始异步阶段 snapshot Task snapshot 完成，汇报给 JM 上面的任何一个步骤不成功，整个 checkpoint 都会失败。 2 Checkpoint 异常情况排查2.1 Checkpoint 失败可以在 Checkpoint 界面看到如下图所示，下图中 Checkpoint 10423 失败了。 点击 Checkpoint 10423 的详情，我们可以看到类系下图所示的表格（下图中将 operator 名字截取掉了）。 上图中我们看到三行，表示三个 operator，其中每一列的含义分别如下： 其中 Acknowledged 一列表示有多少个 subtask 对这个 Checkpoint 进行了 ack，从图中我们可以知道第三个 operator 总共有 5 个 subtask，但是只有 4 个进行了 ack； 第二列 Latest Acknowledgement 表示该 operator 的所有 subtask 最后 ack 的时间； End to End Duration 表示整个 operator 的所有 subtask 中完成 snapshot 的最长时间； State Size 表示当前 Checkpoint 的 state 大小 – 主要这里如果是增量 checkpoint 的话，则表示增量大小； Buffered During Alignment 表示在 barrier 对齐阶段积攒了多少数据，如果这个数据过大也间接表示对齐比较慢）； Checkpoint 失败大致分为两种情况：Checkpoint Decline 和 Checkpoint Expire。 2.1.1 Checkpoint Decline我们能从 jobmanager.log 中看到类似下面的日志 Decline checkpoint 10423 by task 0b60f08bf8984085b59f8d9bc74ce2e1 of job 85d268e6fbc19411185f7e4868a44178. 其中10423 是 checkpointID，0b60f08bf8984085b59f8d9bc74ce2e1 是 execution id，85d268e6fbc19411185f7e4868a44178 是 job id，我们可以在 jobmanager.log 中查找 execution id，找到被调度到哪个 taskmanager 上，类似如下所示： 122019-09-02 16:26:20,972 INFO [jobmanager-future-thread-61] org.apache.flink.runtime.executiongraph.ExecutionGraph - XXXXXXXXXXX (100/289) (87b751b1fd90e32af55f02bb2f9a9892) switched from SCHEDULED to DEPLOYING.2019-09-02 16:26:20,972 INFO [jobmanager-future-thread-61] org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying XXXXXXXXXXX (100/289) (attempt #0) to slot container_e24_1566836790522_8088_04_013155_1 on hostnameABCDE 从上面的日志我们知道该 execution 被调度到 hostnameABCDE 的 container_e24_1566836790522_8088_04_013155_1 slot 上，接下来我们就可以到 container container_e24_1566836790522_8088_04_013155 的 taskmanager.log 中查找 Checkpoint 失败的具体原因了。 另外对于 Checkpoint Decline 的情况，有一种情况我们在这里单独抽取出来进行介绍：Checkpoint Cancel。 当前 Flink 中如果较小的 Checkpoint 还没有对齐的情况下，收到了更大的 Checkpoint，则会把较小的 Checkpoint 给取消掉。我们可以看到类似下面的日志： 1$taskNameWithSubTaskAndID: Received checkpoint barrier for checkpoint 20 before completing current checkpoint 19. Skipping current checkpoint. 这个日志表示，当前 Checkpoint 19 还在对齐阶段，我们收到了 Checkpoint 20 的 barrier。然后会逐级通知到下游的 task checkpoint 19 被取消了，同时也会通知 JM 当前 Checkpoint 被 decline 掉了。 在下游 task 收到被 cancelBarrier 的时候，会打印类似如下的日志： 123456789101112DEBUG$taskNameWithSubTaskAndID: Checkpoint 19 canceled, aborting alignment.或者DEBUG$taskNameWithSubTaskAndID: Checkpoint 19 canceled, skipping alignment.或者WARN$taskNameWithSubTaskAndID: Received cancellation barrier for checkpoint 20 before completing current checkpoint 19. Skipping current checkpoint. 上面三种日志都表示当前 task 接收到上游发送过来的 barrierCancel 消息，从而取消了对应的 Checkpoint。 2.1.2 Checkpoint Expire如果 Checkpoint 做的非常慢，超过了 timeout 还没有完成，则整个 Checkpoint 也会失败。当一个 Checkpoint 由于超时而失败是，会在 jobmanager.log 中看到如下的日志： 1Checkpoint 1 of job 85d268e6fbc19411185f7e4868a44178 expired before completing. 表示 Chekpoint 1 由于超时而失败，这个时候可以可以看这个日志后面是否有类似下面的日志： 1Received late message for now expired checkpoint attempt 1 from 0b60f08bf8984085b59f8d9bc74ce2e1 of job 85d268e6fbc19411185f7e4868a44178. 可以按照 2.1.1 中的方法找到对应的 taskmanager.log 查看具体信息。 下面的日志如果是 DEBUG 的话，我们会在开始处标记 DEBUG 我们按照下面的日志把 TM 端的 snapshot 分为三个阶段，开始做 snapshot 前，同步阶段，异步阶段： 12DEBUGStarting checkpoint (6751) CHECKPOINT on task taskNameWithSubtasks (4/4) 这个日志表示 TM 端 barrier 对齐后，准备开始做 Checkpoint。 123DEBUG2019-08-06 13:43:02,613 DEBUG org.apache.flink.runtime.state.AbstractSnapshotStrategy - DefaultOperatorStateBackend snapshot (FsCheckpointStorageLocation &#123;fileSystem=org.apache.flink.core.fs.SafetyNetWrapperFileSystem@70442baf, checkpointDirectory=xxxxxxxx, sharedStateDirectory=xxxxxxxx, taskOwnedStateDirectory=xxxxxx, metadataFilePath=xxxxxx, reference=(default), fileStateSizeThreshold=1024&#125;, synchronous part) in thread Thread[Async calls on Source: xxxxxx_source -&gt; Filter (27/70),5,Flink Task Threads] took 0 ms. 上面的日志表示当前这个 backend 的同步阶段完成，共使用了 0 ms。 12DEBUGDefaultOperatorStateBackend snapshot (FsCheckpointStorageLocation &#123;fileSystem=org.apache.flink.core.fs.SafetyNetWrapperFileSystem@7908affe, checkpointDirectory=xxxxxx, sharedStateDirectory=xxxxx, taskOwnedStateDirectory=xxxxx, metadataFilePath=xxxxxx, reference=(default), fileStateSizeThreshold=1024&#125;, asynchronous part) in thread Thread[pool-48-thread-14,5,Flink Task Threads] took 369 ms 上面的日志表示异步阶段完成，异步阶段使用了 369 ms 在现有的日志情况下，我们通过上面三个日志，定位 snapshot 是开始晚，同步阶段做的慢，还是异步阶段做的慢。然后再按照情况继续进一步排查问题。 2.2 Checkpoint 慢在 2.1 节中，我们介绍了 Checkpoint 失败的排查思路，本节会分情况介绍 Checkpoint 慢的情况。 Checkpoint 慢的情况如下：比如 Checkpoint interval 1 分钟，超时 10 分钟，Checkpoint 经常需要做 9 分钟（我们希望 1 分钟左右就能够做完），而且我们预期 state size 不是非常大。 对于 Checkpoint 慢的情况，我们可以按照下面的顺序逐一检查。 2.2.0 Source Trigger Checkpoint 慢这个一般发生较少，但是也有可能，因为 source 做 snapshot 并往下游发送 barrier 的时候，需要抢锁（这个现在社区正在进行用 mailBox 的方式替代当前抢锁的方式，详情参考[1])。如果一直抢不到锁的话，则可能导致 Checkpoint 一直得不到机会进行。如果在 Source 所在的 taskmanager.log 中找不到开始做 Checkpoint 的 log，则可以考虑是否属于这种情况，可以通过 jstack 进行进一步确认锁的持有情况。 2.2.1 使用增量 Checkpoint现在 Flink 中 Checkpoint 有两种模式，全量 Checkpoint 和 增量 Checkpoint，其中全量 Checkpoint 会把当前的 state 全部备份一次到持久化存储，而增量 Checkpoint，则只备份上一次 Checkpoint 中不存在的 state，因此增量 Checkpoint 每次上传的内容会相对更好，在速度上会有更大的优势。 现在 Flink 中仅在 RocksDBStateBackend 中支持增量 Checkpoint，如果你已经使用 RocksDBStateBackend，可以通过开启增量 Checkpoint 来加速，具体的可以参考 [2]。 2.2.2 作业存在反压或者数据倾斜我们知道 task 仅在接受到所有的 barrier 之后才会进行 snapshot，如果作业存在反压，或者有数据倾斜，则会导致全部的 channel 或者某些 channel 的 barrier 发送慢，从而整体影响 Checkpoint 的时间，这两个可以通过如下的页面进行检查： 上图中我们选择了一个 task，查看所有 subtask 的反压情况，发现都是 high，表示反压情况严重，这种情况下会导致下游接收 barrier 比较晚。 上图中我们选择其中一个 operator，点击所有的 subtask，然后按照 Records Received/Bytes Received/TPS 从大到小进行排序，能看到前面几个 subtask 会比其他的 subtask 要处理的数据多。 如果存在反压或者数据倾斜的情况，我们需要首先解决反压或者数据倾斜问题之后，再查看 Checkpoint 的时间是否符合预期。 2.2.2 Barrier 对齐慢从前面我们知道 Checkpoint 在 task 端分为 barrier 对齐（收齐所有上游发送过来的 barrier），然后开始同步阶段，再做异步阶段。如果 barrier 一直对不齐的话，就不会开始做 snapshot。 barrier 对齐之后会有如下日志打印： 12DEBUGStarting checkpoint (6751) CHECKPOINT on task taskNameWithSubtasks (4/4) 如果 taskmanager.log 中没有这个日志，则表示 barrier 一直没有对齐，接下来我们需要了解哪些上游的 barrier 没有发送下来，如果你使用 At Least Once 的话，可以观察下面的日志： 12DEBUGReceived barrier for checkpoint 96508 from channel 5 表示该 task 收到了 channel 5 来的 barrier，然后看对应 Checkpoint，再查看还剩哪些上游的 barrier 没有接受到，对于 ExactlyOnce 暂时没有类似的日志，可以考虑自己添加，或者 jmap 查看。 2.2.3 主线程太忙，导致没机会做 snapshot在 task 端，所有的处理都是单线程的，数据处理和 barrier 处理都由主线程处理，如果主线程在处理太慢（比如使用 RocksDBBackend，state 操作慢导致整体处理慢），导致 barrier 处理的慢，也会影响整体 Checkpoint 的进度，在这一步我们需要能够查看某个 PID 对应 hotmethod，这里推荐两个方法： 多次连续 jstack，查看一直处于 RUNNABLE 状态的线程有哪些； 使用工具 AsyncProfile dump 一份火焰图，查看占用 CPU 最多的栈； 如果有其他更方便的方法当然更好，也欢迎推荐。 2.2.4 同步阶段做的慢同步阶段一般不会太慢，但是如果我们通过日志发现同步阶段比较慢的话，对于非 RocksDBBackend 我们可以考虑查看是否开启了异步 snapshot，如果开启了异步 snapshot 还是慢，需要看整个 JVM 在干嘛，也可以使用前一节中的工具。对于 RocksDBBackend 来说，我们可以用 iostate 查看磁盘的压力如何，另外可以查看 tm 端 RocksDB 的 log 的日志如何，查看其中 SNAPSHOT 的时间总共开销多少。 RocksDB 开始 snapshot 的日志如下： 12019/09/10-14:22:55.734684 7fef66ffd700 [utilities/checkpoint/checkpoint_impl.cc:83] Started the snapshot process -- creating snapshot in directory /tmp/flink-io-87c360ce-0b98-48f4-9629-2cf0528d5d53/XXXXXXXXXXX/chk-92729 snapshot 结束的日志如下： 12019/09/10-14:22:56.001275 7fef66ffd700 [utilities/checkpoint/checkpoint_impl.cc:145] Snapshot DONE. All is good #####2.2.6 异步阶段做的慢 对于异步阶段来说，tm 端主要将 state 备份到持久化存储上，对于非 RocksDBBackend 来说，主要瓶颈来自于网络，这个阶段可以考虑观察网络的 metric，或者对应机器上能够观察到网络流量的情况（比如 iftop)。 对于 RocksDB 来说，则需要从本地读取文件，写入到远程的持久化存储上，所以不仅需要考虑网络的瓶颈，还需要考虑本地磁盘的性能。另外对于 RocksDBBackend 来说，如果觉得网络流量不是瓶颈，但是上传比较慢的话，还可以尝试考虑开启多线程上传功能[3]。 3 总结在第二部分内容中，我们介绍了官方编译的包的情况下排查一些 Checkpoint 异常情况的主要场景，以及相应的排查方法，如果排查了上面所有的情况，还是没有发现瓶颈所在，则可以考虑添加更详细的日志，逐步将范围缩小，然后最终定位原因。 上文提到的一些 DEBUG 日志，如果 flink dist 包是自己编译的话，则建议将 Checkpoint 整个步骤内的一些 DEBUG 改为 INFO，能够通过日志了解整个 Checkpoint 的整体阶段，什么时候完成了什么阶段，也在 Checkpoint 异常的时候，快速知道每个阶段都消耗了多少时间。 参考内容[1]、Change threading-model in StreamTask to a mailbox-based approach[2]、增量 checkpoint 原理介绍[3]、RocksDBStateBackend 多线程上传 State 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Apache Flink 1.10.0 重磅发布，新特性解读","date":"2020-02-10T16:00:00.000Z","path":"2020/02/11/flink-1.10/","text":"Apache Flink 社区迎来了激动人心的两位数位版本号，Flink 1.10.0 正式宣告发布！ 作为 Flink 社区迄今为止规模最大的一次版本升级，Flink 1.10 容纳了超过 200 位贡献者对超过 1200 个 issue 的开发实现，包含对 Flink 作业的整体性能及稳定性的显著优化、对原生 Kubernetes 的初步集成以及对 Python 支持（PyFlink）的重大优化。 Flink 1.10 同时还标志着对 Blink 的整合宣告完成，随着对 Hive 的生产级别集成及对 TPC-DS 的全面覆盖，Flink 在增强流式 SQL 处理能力的同时也具备了成熟的批处理能力。本篇博客将对此次版本升级中的主要新特性及优化、值得注意的重要变化以及使用新版本的预期效果逐一进行介绍。 新版本的二进制发布包和源码包已经可以在最新的 Flink 官网下载页面找到。更多细节请参考完整的版本更新日志以及最新的用户文档。 新特性及优化内存管理及配置优化Flink 目前的 TaskExecutor 内存模型存在着一些缺陷，导致优化资源利用率比较困难，例如： 流和批处理内存占用的配置模型不同； 流处理中的 RocksDB state backend 需要依赖用户进行复杂的配置。 为了让内存配置变的对于用户更加清晰、直观，Flink 1.10 对 TaskExecutor 的内存模型和配置逻辑进行了较大的改动 FLIP-49。这些改动使得 Flink 能够更好地适配所有部署环境（例如 Kubernetes, Yarn, Mesos），让用户能够更加严格的控制其内存开销。 Managed 内存扩展Managed 内存的范围有所扩展，还涵盖了 RocksDB state backend 使用的内存。尽管批处理作业既可以使用堆内内存也可以使用堆外内存，使用 RocksDB state backend 的流处理作业却只能利用堆外内存。因此为了让用户执行流和批处理作业时无需更改集群的配置，我们规定从现在起 managed 内存只能在堆外。 简化 RocksDB 配置此前，配置像 RocksDB 这样的堆外 state backend 需要进行大量的手动调试，例如减小 JVM 堆空间、设置 Flink 使用堆外内存等。现在，Flink 的开箱配置即可支持这一切，且只需要简单地改变 managed 内存的大小即可调整 RocksDB state backend 的内存预算。 另一个重要的优化是，Flink 现在可以限制 RocksDB 的 native 内存占用（FLINK-7289），以避免超过总的内存预算——这对于 Kubernetes 等容器化部署环境尤为重要。关于如何开启、调试该特性，请参考 RocksDB 调试。 注：FLIP-49 改变了集群的资源配置过程，因此从以前的 Flink 版本升级时可能需要对集群配置进行调整。详细的变更日志及调试指南请参考文档 统一的作业提交逻辑在此之前，提交作业是由执行环境负责的，且与不同的部署目标（例如 Yarn, Kubernetes, Mesos）紧密相关。这导致用户需要针对不同环境保留多套配置，增加了管理的成本。 在 Flink 1.10 中，作业提交逻辑被抽象到了通用的 Executor 接口（FLIP-73）。新增加的 ExecutorCLI （FLIP-81）引入了为任意执行目标指定配置参数的统一方法。此外，随着引入 JobClient（FLINK-74）负责获取 JobExecutionResult，获取作业执行结果的逻辑也得以与作业提交解耦。 上述改变向用户提供了统一的 Flink 入口，使得在 Apache Beam 或 Zeppelin notebooks 等下游框架中以编程方式使用 Flink 变的更加容易。对于需要在多种不同环境使用 Flink 的用户而言，新的基于配置的执行过程同样显著降低了冗余代码量以及维护开销。 原生 Kubernetes 集成（Beta）对于想要在容器化环境中尝试 Flink 的用户来说，想要在 Kubernetes 上部署和管理一个 Flink standalone 集群，首先需要对容器、算子及像 kubectl 这样的环境工具有所了解。 在 Flink 1.10 中，我们推出了初步的支持 session 模式的主动 Kubernetes 集成（FLINK-9953）。其中，“主动”指 Flink ResourceManager (K8sResMngr) 原生地与 Kubernetes 通信，像 Flink 在 Yarn 和 Mesos 上一样按需申请 pod。用户可以利用 namespace，在多租户环境中以较少的资源开销启动 Flink。这需要用户提前配置好 RBAC 角色和有足够权限的服务账号。 正如在统一的作业提交逻辑一节中提到的，Flink 1.10 将命令行参数映射到了统一的配置。因此，用户可以参阅 Kubernetes 配置选项，在命令行中使用以下命令向 Kubernetes 提交 Flink 作业。 1./bin/flink run -d -e kubernetes-session -Dkubernetes.cluster-id=&lt;ClusterId&gt; examples/streaming/WindowJoin.jar 如果你希望第一时间尝试这一特性，欢迎参考相关文档、试用并与社区分享你的反馈意见。 Table API/SQL: 生产可用的 Hive 集成Flink 1.9 推出了预览版的 Hive 集成。该版本允许用户使用 SQL DDL 将 Flink 特有的元数据持久化到 Hive Metastore、调用 Hive 中定义的 UDF 以及读、写 Hive 中的表。Flink 1.10 进一步开发和完善了这一特性，带来了全面兼容 Hive 主要版本的生产可用的 Hive 集成。 Batch SQL 原生分区支持此前，Flink 只支持写入未分区的 Hive 表。在 Flink 1.10 中，Flink SQL 扩展支持了 INSERT OVERWRITE 和 PARTITION 的语法（FLIP-63），允许用户写入 Hive 中的静态和动态分区。 写入静态分区 1INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; 写入动态分区 1INSERT &#123; INTO | OVERWRITE &#125; TABLE tablename1 select_statement1 FROM from_statement; 对分区表的全面支持，使得用户在读取数据时能够受益于分区剪枝，减少了需要扫描的数据量，从而大幅提升了这些操作的性能。 其他优化除了分区剪枝，Flink 1.10 的 Hive 集成还引入了许多数据读取方面的优化，例如： 投影下推：Flink 采用了投影下推技术，通过在扫描表时忽略不必要的域，最小化 Flink 和 Hive 表之间的数据传输量。这一优化在表的列数较多时尤为有效。 LIMIT 下推：对于包含 LIMIT 语句的查询，Flink 在所有可能的地方限制返回的数据条数，以降低通过网络传输的数据量。 读取数据时的 ORC 向量化： 为了提高读取 ORC 文件的性能，对于 Hive 2.0.0 及以上版本以及非复合数据类型的列，Flink 现在默认使用原生的 ORC 向量化读取器。 将可插拔模块作为 Flink 内置对象（Beta）Flink 1.10 在 Flink table 核心引入了通用的可插拔模块机制，目前主要应用于系统内置函数（FLIP-68）。通过模块，用户可以扩展 Flink 的系统对象，例如像使用 Flink 系统函数一样使用 Hive 内置函数。新版本中包含一个预先实现好的 HiveModule，能够支持多个 Hive 版本，当然用户也可以选择编写自己的可插拔模块。 其他 Table API/SQL 优化SQL DDL 中的 watermark 和计算列Flink 1.10 在 SQL DDL 中增加了针对流处理定义时间属性及产生 watermark 的语法扩展（FLIP-66）。这使得用户可以在用 DDL 语句创建的表上进行基于时间的操作（例如窗口）以及定义 watermark 策略。 1234567CREATE TABLE table_name (WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;) WITH (...) 其他 SQL DDL 扩展Flink 现在严格区分临时/持久、系统/目录函数（FLIP-57）。这不仅消除了函数引用中的歧义，还带来了确定的函数解析顺序（例如，当存在命名冲突时，比起目录函数、持久函数 Flink 会优先使用系统函数、临时函数）。 在 FLIP-57 的基础上，我们扩展了 SQL DDL 的语法，支持创建目录函数、临时函数以及临时系统函数（FLIP-79）： 12345CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION[IF NOT EXISTS] [catalog_name.][db_name.]function_nameAS identifier [LANGUAGE JAVA|SCALA] 关于目前完整的 Flink SQL DDL 支持，请参考最新的文档。 注：为了今后正确地处理和保证元对象（表、视图、函数）上的行为一致性，Flink 废弃了 Table API 中的部分对象申明方法，以使留下的方法更加接近标准的 SQL DDL（FLIP-64）。 批处理完整的 TPC-DS 覆盖TPC-DS 是广泛使用的业界标准决策支持 benchmark，用于衡量基于 SQL 的数据处理引擎性能。Flink 1.10 端到端地支持所有 TPC-DS 查询（FLINK-11491），标志着 Flink SQL 引擎已经具备满足现代数据仓库及其他类似的处理需求的能力。 PyFlink: 支持原生用户自定义函数（UDF）作为 Flink 全面支持 Python 的第一步，在之前版本中我们发布了预览版的 PyFlink。在新版本中，我们专注于让用户在 Table API/SQL 中注册并使用自定义函数（UDF，另 UDTF / UDAF 规划中）（FLIP-58）。 如果你对这一特性的底层实现（基于 Apache Beam 的可移植框架）感兴趣，请参考 FLIP-58 的 Architecture 章节以及 FLIP-78。这些数据结构为支持 Pandas 以及今后将 PyFlink 引入到 DataStream API 奠定了基础。 从 Flink 1.10 开始，用户只要执行以下命令就可以轻松地通过 pip 安装 PyFlink： 1pip install apache-flink 重要变更 FLINK-10725：Flink 现在可以使用 Java 11 编译和运行。 FLINK-15495：SQL 客户端现在默认使用 Blink planner，向用户提供最新的特性及优化。Table API 同样计划在下个版本中从旧的 planner 切换到 Blink planner，我们建议用户现在就开始尝试和熟悉 Blink planner。 FLINK-13025：新的 Elasticsearch sink connector 全面支持 Elasticsearch 7.x 版本。 FLINK-15115：Kafka 0.8 和 0.9 的 connector 已被标记为废弃并不再主动支持。如果你还在使用这些版本或有其他相关问题，请通过 @dev 邮件列表联系我们。 FLINK-14516：非基于信用的网络流控制已被移除，同时移除的还有配置项“taskmanager.network.credit.model”。今后，Flink 将总是使用基于信用的网络流控制。 FLINK-12122：在 Flink 1.5.0 中，FLIP-6 改变了 slot 在 TaskManager 之间的分布方式。要想使用此前的调度策略，既尽可能将负载分散到所有当前可用的 TaskManager，用户可以在 flink-conf.yaml 中设置 “cluster.evenly-spread-out-slots: true”。 FLINK-11956：s3-hadoop 和 s3-presto 文件系统不再使用类重定位加载方式，而是使用插件方式加载，同时无缝集成所有认证提供者。我们强烈建议其他文件系统也只使用插件加载方式，并将陆续移除重定位加载方式。 Flink 1.9 推出了新的 Web UI，同时保留了原来的 Web UI 以备不时之需。截至目前，我们没有收到关于新的 UI 存在问题的反馈，因此社区投票决定在 Flink 1.10 中移除旧的 Web UI。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"实时计算","slug":"实时计算","permalink":"http://www.54tianzhisheng.cn/tags/实时计算/"}]},{"title":"Flink 视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新","date":"2019-12-30T16:00:00.000Z","path":"2019/12/31/Flink-resources/","text":"Flink 专栏首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 专栏大纲： Flink 学习项目代码https://github.com/zhisheng17/flink-learning 麻烦路过的各位亲给这个项目点个 star，太不易了，写了这么多，算是对我坚持下来的一种鼓励吧！ 本项目结构 How to buildMaybe your Maven conf file settings.xml mirrors can add aliyun central mirror : 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/central&lt;/url&gt;&lt;/mirror&gt; then you can run the following command : 1mvn clean package -Dmaven.test.skip=true you can see following result if build success. Change2019/09/06 将该项目的 Flink 版本升级到 1.9.0，有一些变动，Flink 1.8.0 版本的代码经群里讨论保存在分支 feature/flink-1.8.0 以便部分同学需要。 2019/06/08 新增 Flink 四本电子书籍的 PDF，在 books 目录下： Introduction_to_Apache_Flink_book.pdf 这本书比较薄，处于介绍阶段，国内有这本的翻译书籍 Learning Apache Flink.pdf 这本书比较基础，初学的话可以多看看 Stream Processing with Apache Flink.pdf 这本书是 Flink PMC 写的 Streaming System.pdf 这本书评价不是一般的高 2019/06/09 新增流处理引擎相关的 Paper，在 paper 目录下： 流处理引擎相关的 Paper 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 Flink 源码项目结构 学习资料另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：yuanblog_tzs，然后回复关键字：Flink 即可无条件获取到，转载请联系本人获取授权，违者必究。 更多私密资料请加入知识星球！ 有人要问知识星球里面更新什么内容？值得加入吗？ 目前知识星球内已更新的系列文章： 大数据重磅炸弹1、《大数据重磅炸弹——实时计算引擎 Flink》开篇词 2、、你公司到底需不需要引入实时计算引擎？ 3、一文让你彻底了解大数据实时计算框架 Flink ​ 4、别再傻傻的分不清大数据框架Flink、Blink、Spark Streaming、Structured Streaming和Storm之间的区别了​ 5、Flink 环境准备看这一篇就够了 6、一文讲解从 Flink 环境安装到源码编译运行 7、通过 WordCount 程序教你快速入门上手 Flink ​ 8、Flink 如何处理 Socket 数据及分析实现过程 9、Flink job 如何在 Standalone、YARN、Mesos、K8S 上部署运行？ 10、Flink 数据转换必须熟悉的算子（Operator） 11、Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析 12、如何使用 Flink Window 及 Window 基本概念与实现原理 13、如何使用 DataStream API 来处理数据？ 14、Flink WaterMark 详解及结合 WaterMark 处理延迟数据 源码系列1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalonesession 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 除了《从1到100深入学习Flink》源码学习这个系列文章，《从0到1学习Flink》的案例文章也会优先在知识星球更新，让大家先通过一些 demo 学习 Flink，再去深入源码学习！ 如果学习 Flink 的过程中，遇到什么问题，可以在里面提问，我会优先解答，这里做个抱歉，自己平时工作也挺忙，微信的问题不能做全部做一些解答，但肯定会优先回复给知识星球的付费用户的，庆幸的是现在星球里的活跃氛围还是可以的，有不少问题通过提问和解答的方式沉淀了下来。 1、为何我使用 ValueState 保存状态 Job 恢复是状态没恢复？ 2、flink中watermark究竟是如何生成的，生成的规则是什么，怎么用来处理乱序数据 3、消费kafka数据的时候，如果遇到了脏数据，或者是不符合规则的数据等等怎么处理呢？ 4、在Kafka 集群中怎么指定读取/写入数据到指定broker或从指定broker的offset开始消费？ 5、Flink能通过oozie或者azkaban提交吗？ 6、jobmanager挂掉后，提交的job怎么不经过手动重新提交执行？ 7、使用flink-web-ui提交作业并执行 但是/opt/flink/log目录下没有日志文件 请问关于flink的日志（包括jobmanager、taskmanager、每个job自己的日志默认分别存在哪个目录 ）需要怎么配置？ 8、通过flink 仪表盘提交的jar 是存储在哪个目录下？ 9、从Kafka消费数据进行etl清洗，把结果写入hdfs映射成hive表，压缩格式、hive直接能够读取flink写出的文件、按照文件大小或者时间滚动生成文件 10、flink jar包上传至集群上运行，挂掉后，挂掉期间kafka中未被消费的数据，在重新启动程序后，是自动从checkpoint获取挂掉之前的kafka offset位置，自动消费之前的数据进行处理，还是需要某些手动的操作呢？ 11、flink 启动时不自动创建 上传jar的路径，能指定一个创建好的目录吗 12、Flink sink to es 集群上报 slot 不够，单机跑是好的，为什么？ 13、Fllink to elasticsearch如何创建索引文档期时间戳？ 14、blink有没有api文档或者demo，是否建议blink用于生产环境。 15、flink的Python api怎样？bug多吗？ 16、Flink VS Spark Streaming VS Storm VS Kafka Stream 17、你们做实时大屏的技术架构是什么样子的？flume→kafka→flink→redis，然后后端去redis里面捞数据，酱紫可行吗？ 18、做一个统计指标的时候，需要在Flink的计算过程中多次读写redis，感觉好怪，星主有没有好的方案？ 19、Flink 使用场景大分析，列举了很多的常用场景，可以好好参考一下 20、将kafka中数据sink到mysql时，metadata的数据为空，导入mysql数据不成功？？？ 21、使用了ValueState来保存中间状态，在运行时中间状态保存正常，但是在手动停止后，再重新运行，发现中间状态值没有了，之前出现的键值是从0开始计数的，这是为什么？是需要实现CheckpointedFunction吗？ 22、flink on yarn jobmanager的HA需要怎么配置。还是说yarn给管理了 23、有两个数据流就行connect，其中一个是实时数据流（kafka 读取)，另一个是配置流。由于配置流是从关系型数据库中读取，速度较慢，导致实时数据流流入数据的时候，配置信息还未发送，这样会导致有些实时数据读取不到配置信息。目前采取的措施是在connect方法后的flatmap的实现的在open 方法中，提前加载一次配置信息，感觉这种实现方式不友好，请问还有其他的实现方式吗？ 24、Flink能通过oozie或者azkaban提交吗？ 25、不采用yarm部署flink，还有其他的方案吗？ 主要想解决服务器重启后，flink服务怎么自动拉起？ jobmanager挂掉后，提交的job怎么不经过手动重新提交执行？ 26、在一个 Job 里将同份数据昨晚清洗操作后，sink 到后端多个地方（看业务需求），如何保持一致性？（一个sink出错，另外的也保证不能插入） 27、flink sql任务在某个特定阶段会发生tm和jm丢失心跳，是不是由于gc时间过长呢， 28、有这样一个需求，统计用户近两周进入产品详情页的来源（1首页大搜索，2产品频道搜索，3其他），为php后端提供数据支持，该信息在端上报事件中，php直接获取有点困难。 我现在的解决方案 通过flink滚动窗口（半小时），统计用户半小时内3个来源pv，然后按照日期序列化，直接写mysql。php从数据库中解析出来，再去统计近两周占比。 问题1，这个需求适合用flink去做吗？ 问题2，我的方案总感觉怪怪的，有没有好的方案？ 29、一个task slot 只能同时运行一个任务还是多个任务呢？如果task slot运行的任务比较大，会出现OOM的情况吗？ 30、你们怎么对线上flink做监控的，如果整个程序失败了怎么自动重启等等 31、flink cep规则动态解析有接触吗？有没有成型的框架？ 32、每一个Window都有一个watermark吗？window是怎么根据watermark进行触发或者销毁的？ 33、 CheckPoint与SavePoint的区别是什么？ 34、flink可以在算子中共享状态吗？或者大佬你有什么方法可以共享状态的呢？ 35、运行几分钟就报了，看taskmager日志，报的是 failed elasticsearch bulk request null，可是我代码里面已经做过空值判断了呀 而且也过滤掉了，flink版本1.7.2 es版本6.3.1 36、这种情况，我们调并行度 还是配置参数好 37、大家都用jdbc写，各种数据库增删查改拼sql有没有觉得很累，ps.set代码一大堆，还要计算每个参数的位置 38、关于datasource的配置，每个taskmanager对应一个datasource?还是每个slot? 实际运行下来，每个slot中datasorce线程池只要设置1就行了，多了也用不到? 39、kafka现在每天出现数据丢失，现在小批量数据，一天200W左右, kafka版本为 1.0.0，集群总共7个节点，TOPIC有十六个分区，单条报文1.5k左右 40、根据key.hash的绝对值 对并发度求模，进行分组，假设10各并发度，实际只有8个分区有处理数据，有2个始终不处理，还有一个分区处理的数据是其他的三倍，如截图 41、flink每7小时不知道在处理什么， CPU 负载 每7小时，有一次高峰，5分钟内平均负载超过0.8，如截图 42、有没有Flink写的项目推荐？我想看到用Flink写的整体项目是怎么组织的，不单单是一个单例子 43、Flink 源码的结构图 44、我想根据不同业务表（case when）进行不同的redis sink（hash ，set），我要如何操作？ 45、这个需要清理什么数据呀，我把hdfs里面的已经清理了 启动还是报这个 46、 在流处理系统，在机器发生故障恢复之后，什么情况消息最多会被处理一次？什么情况消息最少会被处理一次呢？ 47、我检查点都调到5分钟了，这是什么问题 48、reduce方法后 那个交易时间 怎么不是最新的，是第一次进入的那个时间， 49、Flink on Yarn 模式，用yarn session脚本启动的时候，我在后台没有看到到Jobmanager，TaskManager，ApplicationMaster这几个进程，想请问一下这是什么原因呢？因为之前看官网的时候，说Jobmanager就是一个jvm进程，Taskmanage也是一个JVM进程 50、Flink on Yarn的时候得指定 多少个TaskManager和每个TaskManager slot去运行任务，这样做感觉不太合理，因为用户也不知道需要多少个TaskManager适合，Flink 有动态启动TaskManager的机制吗。 51、参考这个例子，Flink 零基础实战教程：如何计算实时热门商品 | Jark’s Blog， 窗口聚合的时候，用keywindow，用的是timeWindowAll，然后在aggregate的时候用aggregate(new CustomAggregateFunction(), new CustomWindowFunction())，打印结果后，发现窗口中一直使用的重复的数据，统计的结果也不变，去掉CustomWindowFunction()就正常了 ？ 非常奇怪 52、用户进入产品预定页面（端埋点上报），并填写了一些信息（端埋点上报），但半小时内并没有产生任何订单，然后给该类用户发送一个push。 1. 这种需求适合用flink去做吗？2. 如果适合，说下大概的思路 53、业务场景是实时获取数据存redis，请问我要如何按天、按周、按月分别存入redis里？（比方说过了一天自动换一个位置存redis） 54、有人 AggregatingState 的例子吗, 感觉官方的例子和 官网的不太一样? 55、flink-jdbc这个jar有吗？怎么没找到啊？1.8.0的没找到，1.6.2的有 56、现有个关于savepoint的问题，操作流程为，取消任务时设置保存点，更新任务，从保存点启动任务；现在遇到个问题，假设我中间某个算子重写，原先通过state编写，有用定时器，现在更改后，采用窗口，反正就是实现方式完全不一样；从保存点启动就会一直报错，重启，原先的保存点不能还原，此时就会有很多数据重复等各种问题，如何才能保证数据不丢失，不重复等，恢复到停止的时候，现在想到的是记下kafka的偏移量，再做处理，貌似也不是很好弄，有什么解决办法吗 57、需要在flink计算app页面访问时长，消费Kafka计算后输出到Kafka。第一条log需要等待第二条log的时间戳计算访问时长。我想问的是，flink是分布式的，那么它能否保证执行的顺序性？后来的数据有没有可能先被执行？ 58、我公司想做实时大屏，现有技术是将业务所需指标实时用spark拉到redis里存着，然后再用一条spark streaming流计算简单乘除运算，指标包含了各月份的比较。请问我该如何用flink简化上述流程？ 59、flink on yarn 方式，这样理解不知道对不对，yarn-session这个脚本其实就是准备yarn环境的，执行run任务的时候，根据yarn-session初始化的yarnDescription 把 flink 任务的jobGraph提交到yarn上去执行 60、同样的代码逻辑写在单独的main函数中就可以成功的消费kafka ，写在一个spring boot的程序中，接受外部请求，然后执行相同的逻辑就不能消费kafka。你遇到过吗？能给一些查问题的建议，或者在哪里打个断点，能看到为什么消费不到kafka的消息呢？ 61、请问下flink可以实现一个流中同时存在订单表和订单商品表的数据 两者是一对多的关系 能实现得到 以订单表为主 一个订单多个商品 这种需求嘛 62、在用中间状态的时候，如果中间一些信息保存在state中，有没有必要在redis中再保存一份，来做第三方的存储。 63、能否出一期flink state的文章。什么场景下用什么样的state？如，最简单的，实时累加update到state。 64、flink的双流join博主有使用的经验吗？会有什么常见的问题吗 65、窗口触发的条件问题 66、flink 定时任务怎么做？有相关的demo么？ 67、流式处理过程中数据的一致性如何保证或者如何检测 68、重启flink单机集群，还报job not found 异常。 69、kafka的数据是用 org.apache.kafka.common.serialization.ByteArraySerialize序列化的，flink这边消费的时候怎么通过FlinkKafkaConsumer创建DataStream？ 70、现在公司有一个需求，一些用户的支付日志，通过sls收集，要把这些日志处理后，结果写入到MySQL，关键这些日志可能连着来好几条才是一个用户的，因为发起请求，响应等每个环节都有相应的日志，这几条日志综合处理才能得到最终的结果，请问博主有什么好的方法没有？ 71、flink 支持hadoop 主备么？ hadoop主节点挂了 flink 会切换到hadoop 备用节点？ 72、请教大家: 实际 flink 开发中用 scala 多还是 java多些？ 刚入手 flink 大数据 scala 需要深入学习么？ 73、我使用的是flink是1.7.2最近用了split的方式分流，但是底层的SplitStream上却标注为Deprecated，请问是官方不推荐使用分流的方式吗？ 74、KeyBy 的正确理解，和数据倾斜问题的解释 75、用flink时，遇到个问题 checkpoint大概有2G左右， 有背压时，flink会重启有遇到过这个问题吗 76、flink使用yarn-session方式部署，如何保证yarn-session的稳定性，如果yarn-session挂了，需要重新部署一个yarn-session，如何恢复之前yarn-session上的job呢，之前的checkpoint还能使用吗？ 77、我想请教一下关于sink的问题。我现在的需求是从Kafka消费Json数据，这个Json数据字段可能会增加，然后将拿到的json数据以parquet的格式存入hdfs。现在我可以拿到json数据的schema，但是在保存parquet文件的时候不知道怎么处理。一是flink没有专门的format parquet，二是对于可变字段的Json怎么处理成parquet比较合适？ 78、flink如何在较大的数据量中做去重计算。 79、flink能在没有数据的时候也定时执行算子吗？ 80、使用rocksdb状态后端，自定义pojo怎么实现序列化和反序列化的，有相关demo么？ 81、check point 老是失败，是不是自定义的pojo问题？到本地可以，到hdfs就不行，网上也有很多类似的问题 都没有一个很好的解释和解决方案 82、cep规则如图，当start事件进入时，时间00:00:15，而后进入end事件，时间00:00:40。我发现规则无法命中。请问within 是从start事件开始计时？还是跟window一样根据系统时间划分的？如果是后者，请问怎么配置才能从start开始计时？ 83、Flink聚合结果直接写Mysql的幂等性设计问题 84、Flink job打开了checkpoint，用的rocksdb，通过观察hdfs上checkpoint目录，为啥算副本总量会暴增爆减 85、Flink 提交任务的 jar包可以指定路径为 HDFS 上的吗 86、在flink web Ui上提交的任务，设置的并行度为2，flink是stand alone部署的。两个任务都正常的运行了几天了，今天有个地方逻辑需要修改，于是将任务cancel掉(在命令行cancel也试了)，结果taskmanger挂掉了一个节点。后来用其他任务试了，也同样会导致节点挂掉 87、一个配置动态更新的问题折腾好久（配置用个静态的map变量存着，有个线程定时去数据库捞数据然后存在这个map里面更新一把），本地 idea 调试没问题，集群部署就一直报 空指针异常。下游的算子使用这个静态变量map去get key在集群模式下会出现这个空指针异常，估计就是拿不到 map 88、批量写入MySQL，完成HBase批量写入 89、用flink清洗数据，其中要访问redis，根据redis的结果来决定是否把数据传递到下流，这有可能实现吗？ 90、监控页面流处理的时候这个发送和接收字节为0。 91、sink到MySQL，如果直接用idea的话可以运行，并且成功，大大的代码上面用的FlinkKafkaConsumer010，而我的Flink版本为1.7，kafka版本为2.12，所以当我用FlinkKafkaConsumer010就有问题，于是改为 FlinkKafkaConsumer就可以直接在idea完成sink到MySQL，但是为何当我把该程序打成Jar包，去运行的时候，就是报FlinkKafkaConsumer找不到呢 92、SocketTextStreamWordCount中输入中文统计不出来，请问这个怎么解决，我猜测应该是需要修改一下代码，应该是这个例子默认统计英文 93、 Flink 应用程序本地 ide 里面运行的时候并行度是怎么算的？ 94、 请问下flink中对于窗口的全量聚合有apply和process两种 他们有啥区别呢 95、不知道大大熟悉Hbase不，我想直接在Hbase中查询某一列数据，因为有重复数据，所以想使用distinct统计实际数据量，请问Hbase中有没有类似于sql的distinct关键字。如果没有，想实现这种可以不？ 96、 来分析一下现在Flink,Kafka方面的就业形势，以及准备就业该如何准备的这方面内容呢？ 97、 大佬知道flink的dataStream可以转换为dataSet吗？因为数据需要11分钟一个批次计算五六个指标，并且涉及好几步reduce，计算的指标之间有联系，用Stream卡住了。 98、1.如何在同一窗口内实现多次的聚合，比如像spark中的这样2.多个实时流的jion可以用window来处理一批次的数据吗？ 99、写的批处理的功能，现在本机跑是没问题的，就是在linux集群上出现了问题，就是不知道如果通过本地调用远程jar包然后传参数和拿到结果参数返回本机 100、我用standalone开启一个flink集群，上传flink官方用例Socket Window WordCount做测试，开启两个parallelism能正常运行，但是开启4个parallelism后出现错误 101、 有使用AssignerWithPunctuatedWatermarks 的案例Demo吗？网上找了都是AssignerWithPeriodicWatermarks的，不知道具体怎么使用？ 102、 有一个datastream(从文件读取的)，然后我用flink sql进行计算，这个sql是一个加总的运算，然后通过retractStreamTableSink可以把文件做sql的结果输出到文件吗？这个输出到文件的接口是用什么呢？ 103、 为啥split这个流设置为过期的 104、 需要使用flink table的水印机制控制时间的乱序问题，这种场景下我就使用水印+窗口了，我现在写的demo遇到了问题，就是在把触发计算的窗口table（WindowedTable）转换成table进行sql操作时发现窗口中的数据还是乱序的，是不是flink table的WindowedTable不支持水印窗口转table-sql的功能 105、 Flink 对 SQL 的重视性 106、 flink job打开了checkpoint，任务跑了几个小时后就出现下面的错，截图是打出来的日志，有个OOM，又遇到过的没？ 107、 本地测试是有数据的，之前该任务放在集群也是有数据的，可能提交过多次，现在读不到数据了 group id 也换过了， 只能重启集群解决么？ 108、使用flink清洗数据存到es中，直接在flatmap中对处理出来的数据用es自己的ClientInterface类直接将数据存入es当中，不走sink，这样的处理逻辑是不是会有问题。 108、 flink从kafka拿数据（即增量数据）与存量数据进行内存聚合的需求，现在有一个方案就是程序启动的时候先用flink table将存量数据加载到内存中创建table中，然后将stream的增量数据与table的数据进行关联聚合后输出结束，不知道这种方案可行么。目前个人认为有两个主要问题：1是增量数据stream转化成append table后不知道能与存量的table关联聚合不，2是聚合后输出的结果数据是否过于频繁造成网络传输压力过大 109、 设置时间时间特性有什么区别呢, 分别在什么场景下使用呢?两种设置时间延迟有什么区别呢 , 分别在什么场景下使用 110、 flink从rabbitmq中读取数据，设置了rabbitmq的CorrelationDataId和checkpoint为EXACTLY_ONCE；如果flink完成一次checkpoint后，在这次checkpoint之前消费的数据都会从mq中删除。如果某次flink停机更新，那就会出现mq中的一些数据消费但是处于Unacked状态。在flink又重新开启后这批数据又会重新消费。那这样是不是就不能保证EXACTLY_ONCE了 111、1. 在Flink checkpoint 中, 像 operator的状态信息 是在设置了checkpoint 之后自动的进行快照吗 ?2. 上面这个和我们手动存储的 Keyed State 进行快照(这个应该是增量快照) 112、现在有个实时商品数，交易额这种统计需求，打算用 flink从kafka读取binglog日志进行计算，但binglog涉及到insert和update这种操作时 怎么处理才能统计准确，避免那种重复计算的问题？ 113、我这边用flink做实时监控，功能很简单，就是每条消息做keyby然后三分钟窗口，然后做些去重操作，触发阈值则报警，现在问题是同一个时间窗口同一个人的告警会触发两次，集群是三台机器，standalone cluster，初步结果是三个算子里有两个收到了同样的数据 114、在使用WaterMark的时候，默认是每200ms去设置一次watermark，那么每个taskmanager之间，由于得到的数据不同，所以往往产生的最大的watermark不同。 那么这个时候，是各个taskmanager广播这个watermark，得到全局的最大的watermark，还是说各个taskmanager都各自用自己的watermark。主要没看到广播watermark的源码。不知道是自己观察不仔细还是就是没有广播这个变量。 115、现在遇到一个需求，需要在job内部定时去读取redis的信息，想请教flink能实现像普通程序那样的定时任务吗？ 116、有个触发事件开始聚合，等到数量足够，或者超时则sink推mq 环境 flink 1.6 用了mapState 记录触发事件 1 数据足够这个OK 2 超时state ttl 1.6支持，但是问题来了，如何在超时时候增加自定义处理？ 117、请问impala这种mpp架构的sql引擎，为什么稳定性比较差呢？ 118、watermark跟并行度相关不是，过于全局了，期望是keyby之后再针对每个keyed stream 打watermark，这个有什么好的实践呢？ 119、请问如果把一个文件的内容读取成datastream和dataset，有什么区别吗？？他们都是一条数据一条数据的被读取吗？ 120、有没有kylin相关的资料，或者调优的经验？ 121、flink先从jdbc读取配置表到流中，另外从kafka中新增或者修改这个配置，这个场景怎么把两个流一份配置流？我用的connect,接着发不成广播变量，再和实体流合并，但在合并时报Exception in thread “main” java.lang.IllegalArgumentException 122、Flink exactly-once，kafka版本为0.11.0 ，sink基于FlinkKafkaProducer011 每五分钟一次checkpoint，但是checkpoint开始后系统直接卡死，at-lease-once 一分钟能完成的checkpoint， 现在十分钟无法完成没进度还是0， 不知道哪里卡住了 123、flink的状态是默认存在于内存的(也可以设置为rocksdb或hdfs)，而checkpoint里面是定时存放某个时刻的状态信息，可以设置hdfs或rocksdb是这样理解的吗？ 124、Flink异步IO中，下图这两种有什么区别？为啥要加 CompletableFuture.supplyAsync，不太明白？ 125、flink的状态是默认存在于内存的(也可以设置为rocksdb或hdfs)，而checkpoint里面是定时存放某个时刻的状态信息，可以设置hdfs或rocksdb是这样理解的吗？ 126、有个计算场景，从kafka消费两个数据源，两个数据结构都有时间段概念，计算需要做的是匹配两个时间段，匹配到了，就生成一条新的记录。请问使用哪个工具更合适，flink table还是cep？请大神指点一下 我这边之前的做法，将两个数据流转为table.两个table over window后join成新的表。结果job跑一会就oom. 127、一个互联网公司，或者一个业务系统，如果想做一个全面的监控要怎么做？有什么成熟的方案可以参考交流吗？有什么有什么度量指标吗？ 128、怎么深入学习flink,或者其他大数据组件，能为未来秋招找一份大数据相关（计算方向）的工作增加自己的竞争力？ 129、oppo的实时数仓，其中明细层和汇总层都在kafka中，他们的关系库的实时数据也抽取到kafka的ods，那么在构建数仓的，需要join 三四个大业务表，业务表会变化，那么是大的业务表是从kafka的ods读取吗？实时数仓，多个大表join可以吗 130、Tuple类型有什么方法转换成json字符串吗？现在的场景是，结果在存储到sink中时希望存的是json字符串，这样应用程序获取数据比较好转换一点。如果Tuple不好转换json字符串，那么应该以什么数据格式存储到sink中 140、端到端的数据保证，是否意味着中间处理程序中断，也不会造成该批次处理失败的消息丢失，处理程序重新启动之后，会再次处理上次未处理的消息 141、关于flink datastream window相关的。比如我现在使用滚动窗口，统计一周内去重用户指标，按照正常watermark触发计算，需要等到当前周的window到达window的endtime时，才会触发，这样指标一周后才能产出结果。我能不能实现一小时触发一次计算，每次统计截止到当前时间，window中所有到达元素的去重数量。 142、FLIP-16 Loop Fault Tolerance 是讲现在的checkpoint机制无法在stream loop的时候容错吗？现在这个问题解决了没有呀？ 143、现在的需求是，统计各个key的今日累计值，一分钟输出一次。如，各个用户今日累计点击次数。这种需求用datastream还是table API方便点？ 144、本地idea可以跑的工程，放在standalone集群上，总报错，报错截图如下，大佬请问这是啥原因 145、比如现在用k8s起了一个flink集群，这时候数据源kafka或者hdfs会在同一个集群上吗，还是会单独再起一个hdfs/kafka集群 146、flink kafka sink 的FlinkFixedPartitioner 分配策略，在并行度小于topic的partitions时，一个并行实例固定的写消息到固定的一个partition，那么就有一些partition没数据写进去？ 147、基于事件时间，每五分钟一个窗口，五秒钟滑动一次，同时watermark的时间同样是基于事件事件时间的，延迟设为1分钟，假如数据流从12：00开始，如果12：07-12：09期间没有产生任何一条数据，即在12：07-12：09这段间的数据流情况为···· （12：07:00，xxx）,(12:09:00,xxx)······，那么窗口[12:02:05-12:07:05]，[12:02:10-12:07:10]等几个窗口的计算是否意味着只有等到，12：09：00的数据到达之后才会触发 148、使用flink1.7，当消费到某条消息(protobuf格式)，报Caused by: org.apache.kafka.common.KafkaException: Record batch for partition Notify-18 at offset 1803009 is invalid, cause: Record is corrupt 这个异常。 如何设置跳过已损坏的消息继续消费下一条来保证业务不终断？ 我看了官网kafka connectors那里，说在DeserializationSchema.deserialize(…)方法中返回null，flink就会跳过这条消息，然而依旧报这个异常 149、是否可以抽空总结一篇Flink 的 watermark 的原理案例？一直没搞明白基于事件时间处理时的数据乱序和数据迟到底咋回事 150、flink中rpc通信的原理，与几个类的讲解，有没有系统详细的文章样，如有求分享，谢谢 151、Flink中如何使用基于事件时间处理，但是又不使用Watermarks? 我在会话窗口中使用遇到一些问题，图一是基于处理时间的，测试结果session是基于keyby(用户)的，图二是基于事件时间的，不知道是我用法不对还是怎么的，测试结果发现并不是基于keyby(用户的)，而是全局的session。不知道怎么修改？ 152、flink实时计算平台，yarn模式日志收集怎么做，为什么会checkpoint失败，报警处理，后需要做什么吗？job监控怎么做 153、有flink与jstorm的在不同应用场景下, 性能比较的数据吗? 从网络上能找大部分都是flink与storm的比较. 在jstorm官网上有一份比较的图表, 感觉参考意义不大, 应该是比较早的flink版本. 154、为什么使用SessionWindows.withGap窗口的话，State存不了东西呀，每次加1 ，拿出来都是null, 我换成 TimeWindow就没问题。 155、请问一下，flink datastream流处理怎么统计去重指标？ 官方文档中只看到批处理有distinct概念。 156、好全的一篇文章，对比分析 Flink，Spark Streaming，Storm 框架 157、关于 structured_streaming 的 paper 158、zookeeper集群切换领导了，flink集群项目重启了就没有数据的输入和输出了，这个该从哪方面入手解决？ 159、我想请教下datastream怎么和静态数据join呢 160、时钟问题导致收到了明天的数据，这时候有什么比较好的处理方法？看到有人设置一个最大的跳跃阈值，如果当前数据时间 - 历史最大时间 超过阈值就不更新。如何合理的设计水印，有没有一些经验呢？ 161、大佬们flink怎么定时查询数据库？ 162、现在我们公司有个想法，就是提供一个页面，在页面上选择source sink 填写上sql语句，然后后台生成一个flink的作业，然后提交到集群。功能有点类似于华为的数据中台，就是页面傻瓜式操作。后台能自动根据相应配置得到结果。请问拘你的了解，可以实现吗？如何实现？有什么好的思路。现在我无从下手 163、请教一下 flink on yarn 的 ha机制 164、在一般的流处理以及cep, 都可以对于eventtime设置watermark, 有时可能需要设置相对大一点的值, 这内存压力就比较大, 有没有办法不应用jvm中的内存, 而用堆外内存, 或者其他缓存, 最好有cache机制, 这样可以应对大流量的峰值. 165、请教一个flink sql的问题。我有两个聚合后的流表A和B，A和Bjoin得到C表。在设置state TTL 的时候是直接对C表设置还是，对A表和B表设置比较好？ 166、spark改写为flink，会不会很复杂，还有这两者在SQL方面的支持差别大吗？ 167、请问flink allowedLateness导致窗口被多次fire，最终数据重复消费，这种问题怎么处理，数据是写到es中 168、设置taskmanager.numberOfTaskSlots: 4的时候没有问题，但是cpu没有压上去，只用了30%左右，于是设置了taskmanager.numberOfTaskSlots: 8，但是就报错误找不到其中一个自定义的类，然后kafka数据就不消费了。为什么？cpu到多少合适？slot是不是和cpu数量一致是最佳配置？kafka分区数多少合适，是不是和slot,parallesim一致最佳？ 169、需求是根据每条日志切分出需要9个字段，有五个指标再根据9个字段的不同组合去做计算。 第一个方法是：我目前做法是切分的9个字段开5分钟大小1分钟计算一次的滑动窗口窗口，进行一次reduce去重，然后再map取出需要的字段，然后过滤再开5分钟大小1分钟计算一次的滑动窗口窗口进行计算保存结果，这个思路遇到的问题是上一个滑动窗口会每一分钟会计算5分钟数据，到第二个窗口划定的5分钟范围的数据会有好多重复，这个思路会造成数据重复。 第二个方法是：切分的9个字段开5分钟大小1分钟计算一次的滑动窗口窗口，再pross方法里完成所有的过滤，聚合计算，但是再高峰期每分钟400万条数据，这个思路担心在高峰期flink计算不过来 170、a,b,c三个表，a和c有eventtime，a和c直接join可以，a和b join后再和c join 就会报错，这是怎么回事呢 171、自定义的source是这样的（图一所示） 使用的时候是这样的（图二所示），为什么无论 sum.print().setParallelism(2)（图2所示）的并行度设置成几最后结果都是这样的 172、刚接触flink，如有问的不合适的地方，请见谅。 1、为什么说flink是有状态的计算？ 2、这个状态是什么？3、状态存在哪里 173、这边用flink 1.8.1的版本，采用flink on yarn，hadoop版本2.6.0。代码是一个简单的滚动窗口统计函数，但启动的时候报错，如下图片。 （2）然后我把flink版本换成1.7.1，重新提交到2.6.0的yarn平台，就能正常运行了。 （3）我们测试集群hadoop版本是3.0，我用flink 1.8.1版本将这个程序再次打包，提交到3.0版本的yarn平台，也能正常运行。 貌似是flink 1.8.1版本与yarn 2.6.0版本不兼容造成的这个问题 174、StateBackend我使用的是MemoryStateBackend， State是怎么释放内存的，例如我在函数中用ValueState存储了历史状态信息。但是历史状态数据我没有手动释放，那么程序会自动释放么？还是一直驻留在内存中 175、请问老师是否可以提供一些Apachebeam的学习资料 谢谢 176、flink 的 DataSet或者DataStream支持索引查询以及删除吗，像spark rdd，如果不支持的话，该转换成什么 177、关于flink的状态，能否把它当做数据库使用，类似于内存数据库，在处理过程中存业务数据。如果是数据库可以算是分布式数据库吗?是不是使用rocksdb这种存储方式才算是?支持的单库大小是不是只是跟本地机器的磁盘大小相关?如果使用硬盘存储会不会效率性能有影响 178、我这边做了个http sink，想要批量发送数据，不过现在只能用数量控制发送，但最后的几个记录没法触发发送动作，想问下有没有什么办法 179、请问下如何做定时去重计数，就是根据时间分窗口，窗口内根据id去重计数得出结果，多谢。试了不少办法，没有简单直接办法 180、我有个job使用了elastic search sink. 设置了批量5000一写入，但是看es监控显示每秒只能插入500条。是不是bulkprocessor的currentrequest为0有关 181、有docker部署flink的资料吗 182、在说明KeyBy的StreamGraph执行过程时，keyBy的ID为啥是6？ 根据前面说，ID是一个静态变量，每取一次就递增1，我觉得应该是3啊，是我理解错了吗 183、有没计划出Execution Graph的远码解析 184、可以分享下物理执行图怎样划分task，以及task如何执行，还有他们之间数据如何传递这块代码嘛？ 185、Flink源码和这个学习项目的结构图 186、请问flink1.8，如何做到动态加载外部udf-jar包呢？ 187、同一个Task Manager中不同的Slot是怎么交互的，比如：source处理完要传递给map的时候，如果在不同的Slot中，他们的内存是相互隔离，是怎么交互的呢？ 我猜是通过序列化和反序列化对象，并且通过网络来进行交互的 188、你们有没有这种业务场景。flink从kafka里面取数据，每一条数据里面有mongdb表A的id,这时我会在map的时候采用flink的异步IO连接A表，然后查询出A表的字段1，再根据该字段1又需要异步IO去B表查询字段2，然后又根据字段2去C表查询字段3…..像这样的业务场景，如果多来几种逻辑，我应该用什么方案最好呢 189、今天本地运行flink程序，消费socket中的数据，连续只能消费两条，第三条flink就消费不了了 190、源数据经过过滤后分成了两条流，然后再分别提取事件时间和水印，做时间窗口，我测试时一条流没有数据，另一条的数据看日志到了窗口操作那边就没走下去，貌似窗口一直没有等到触发 191、有做flink cep的吗，有资料没？ 192、麻烦问一下 BucketingSink跨集群写，如果任务运行在hadoop A集群，从kafka读取数据处理后写到Hadoo B集群，即使把core-site.xml和hdfs-site.xml拷贝到代码resources下，路径使用hdfs://hadoopB/xxx，会提示ava.lang.RuntimeException: Error while creating FileSystem when initializing the state of the BucketingSink.，跨集群写这个问题 flink不支持吗？ 193、想咨询下，如何对flink中的datastream和dataset进行数据采样 194、一个flink作业经常发生oom，可能是什么原因导致的。 处理流程只有15+字段的解析，redis数据读取等操作，TM配置10g。 业务会在夜间刷数据，qps能打到2500左右~ 195、我看到flink 1.8的状态过期仅支持Processing Time，那么如果我使用的是Event time那么状态就不会过期吗 196、请问我想每隔一小时统计一个属性从当天零点到当前时间的平均值，这样的时间窗该如何定义？ 197、flink任务里面反序列化一个类，报ClassNotFoundException，可是包里面是有这个类的，有遇到这种情况吗？ 198、在构造StreamGraph，类似PartitionTransformmation 这种类型的 transform，为什么要添加成一个虚拟节点，而不是一个实际的物理节点呢？ 199、flink消费kafka的数据写入到hdfs中，我采用了BucketingSink 这个sink将operator出来的数据写入到hdfs文件上，并通过在hive中建外部表来查询这个。但现在有个问题，处于in-progress的文件，hive是无法识别出来该文件中的数据，可我想能在hive中实时查询进来的数据，且不想产生很多的小文件，这个该如何处理呢 200、采用Flink单机集群模式一个jobmanager和两个taskmanager，机器是单机是24核，现在做个简单的功能从kafka的一个topic转满足条件的消息到另一个topic，topic的分区是30，我设置了程序默认并发为30，现在每秒消费2w多数据，不够快，请问可以怎么提高job的性能呢？ 201、Flink Metric 源码分析 202、请问怎么理解官网的这段话？按官网的例子，难道只keyby之后才有keyed state，才能托管Flink存储状态么？source和map如果没有自定义operator state的话，状态是不会被保存的？ 203、想用Flink做业务监控告警，并要能够支持动态添加CEP规则，问下可以直接使用Flink CEP还是siddhi CEP? 有没有相关的资料学习下？谢谢！ 204、请问一下，有没有关于水印，触发器的Java方面的demo啊 205、老师，最近我们线上偶尔出现这种情况，就是40个并行度，其他有一个并行度CheckPoint一直失败，其他39个并行度都是毫秒级别就可以CheckPoint成功，这个怎么定位问题呢？还有个问题 CheckPoint的时间分为三部分 Checkpoint Duration (Async）和 Checkpoint Duration (Sync），还有个 end to end 减去同步和异步的时间，这三部分 分别指代哪块？如果发现这三者中的任意一个步骤时间长，该怎么去优化 206、我这边有个场景很依赖消费出来的数据的顺序。在源头侧做了很多处理，将kafka修改成一个分区等等很多尝试，最后消费出来的还是乱序的。能不能在flink消费的时候做处理，来保证处理的数据的顺序。 207、有一个类似于实时计算今天的pv，uv需求，采用source-&gt;keyby-&gt;window-&gt;trigger-&gt;process后，在process里采用ValueState计算uv ,问题是 这个window内一天的所有数据是都会缓存到flink嘛？ 一天的数据量如果大点，这样实现就有问题了， 这个有其他的实现思路嘛？ 208、Flink 注解源码解析 209、如何监控 Flink 的 TaskManager 和 JobManager 210、问下，在真实流计算过程中，并行度的设置，是与 kafka topic的partition数一样的吗？ 211、Flink的日志 如果自己做平台封装在自己的界面中 请问job Manger 和 taskManger 还有用户自己的程序日志 怎么获取呢 有api还是自己需要利用flume 采集到ELK？ 212、我想问下一般用Flink统计pv uv是怎么做的？uv存到redis? 每个uv都存到redis，会不会撑爆？ 213、Flink的Checkpoint 机制，在有多个source的时候，barrier n 的流将被暂时搁置，从其他流接收的记录将不会被处理，但是会放进一个输入缓存input buffer。如果被缓存的record大小超出了input buffer会怎么样？不可能一直缓存下去吧，如果其中某一条就一直没数据的话，整个过程岂不是卡死了？ 214、公司想实时展示订单数据，汇总金额，并需要和前端交互，实时生成数据需要告诉前端，展示成折线图，这种场景的技术选型是如何呢？包括数据的存储，临时汇总数据的存储，何种形式告诉前端 215、请问下checkpoint中存储了哪些东西？ 216、我这边有个需求是实时计算当前车辆与前车距离，用经纬度求距离。大概6000台车，10秒一条经纬度数据。gps流与自己join的地方在进行checkpoint的时候特别缓，每次要好几分钟。checkpoint 状态后端是rocksDB。有什么比较好的方案吗？自己实现一个类似last_value的函数取车辆最新的经纬再join，或者弄个10秒的滑动窗口输出车辆最新的经纬度再进行join，这样可行吗？ 217、flink在启动的时候能不能指定一个时间点从kafka里面恢复数据呢 218、我们线上有个问题，很多业务都去读某个hive表，但是当这个hive表正在写数据的时候，偶尔出现过 读到表里数据为空的情况，这个问题怎么解决呢？ 219、使用 InfluxDB 和 Grafana 搭建监控 Flink 的平台 220、flink消费kafka两个不同的topic,然后进行join操作，如果使用事件时间，两个topic都要设置watermaker吗，如果只设置了topic A的watermaker,topic B的不设置会有什么影响吗？ 221、请教一个问题，我的Flink程序运行一段时间就会报这个错误，定位好多天都没有定位到。checkpoint 时间是5秒，20秒都不行。Caused by: java.io.IOException: Could not flush and close the file system output stream to hdfs://HDFSaaaa/flink/PointWideTable_OffTest_Test2/1eb66edcfccce6124c3b2d6ae402ec39/chk-355/1005127c-cee3-4099-8b61-aef819d72404 in order to obtain the stream state handle 222、Flink的反压机制相比于Storm的反压机制有什么优势呢？问题2: Flink的某一个节点发生故障，是否会影响其他节点的正常工作？还是会通过Checkpoint容错机制吗把任务转移到其他节点去运行呢？ 223、我在验证checkpoint的时候遇到给问题，不管是key state 还是operator state，默认和指定uid是可以的恢复state数据的，当指定uidHash时候无法恢复state数据，麻烦大家给解答一样。我操作state是实现了CheckpointedFunction接口，覆写snapshotState和initializeState，再这两个方法里操作的，然后让程序定时抛出异常，观察发现指定uidHash后snapshotState()方法里context.isRestored()为false，不太明白具体是什么原因 224、kafka 中的每条数据需要和 es 中的所有数据(动态增加)关联，关联之后会做一些额外的操作，这个有什么比较可行的方案？ 225、flink消费kafka数据，设置1分钟checkpoint一次，假如第一次checkpoint完成以后，还没等到下一次checkpoint，程序就挂了，kafka offset还是第一次checkpoint记录的offset,那么下次重新启动程序，岂不是多消费数据了？那flink的 exactly one消费语义是怎么样的？ 226、程序频繁发生Heartbeat of TaskManager with id container_e36_1564049750010_5829_01_000024 timed out. 心跳超时，一天大概10次左右。是内存没给够吗？还是网络波动引起的 227、有没有性能优化方面的指导文章？ 228、flink消费kafka是如何监控消费是否正常的，有啥好办法？ 229、我按照官方的wordcount案例写了一个例子，然后在main函数中起了一个线程，原本是准备定时去更新某些配置，准备测试一下是否可行，所以直接在线程函数中打印一条语句测试是否可行。现在测试的结果是不可行，貌似这个线程根本就没有执行，请问这是什么原因呢？ 按照理解，JobClient中不是反射类执行main函数吗， 执行main函数的时候为什么没有执行这个线程的打印函数呢？ 230、请问我想保留最近多个完成的checkpoint数据，是通过设置 state.checkpoints.num-retained 吗？要怎么使用？ 231、有没有etl实时数仓相关案例么？比如二十张事实表流join 232、为什么我扔到flink 的stream job，立刻就finished 233、有没有在flink上机器学习算法的一些例子啊，除了官网提供的flink exampke里的和flink ml里已有的 234、如果我想扩展sql的关键词，比如添加一些数据支持，有什么思路，现在想的感觉都要改calcite（刚碰flink感觉难度太大了） 235、我想实现统计每5秒中每个类型的次数，这个现在不输出，问题出在哪儿啊 236、我用flink往hbase里写数据，有那种直接批量写hfile的方式的demo没 237、请问怎么监控Kafka消费是否延迟，是否出现消息积压？你有demo吗？这种是用Springboot自己写一个监控，还是咋整啊？ 238、请问有计算pv uv的例子吗 239、通过控制流动态修改window算子窗口类型和长度要怎么写 240、flink的远程调试能出一版么？网上资料坑的多 241、企业里，Flink开发，java用得多，还是scala用得多？ 242、flink的任务运行在yarn的环境上，在yarn的resourcemanager在进行主备切换时，所有的flink任务都失败了，而MR的任务可以正常运行。报错信息如下：AM is not registered for known application attempt: appattempt_1565306391442_89321_000001 or RM had restarted after AM registered . AM should re-register 请问这是什么原因，该如何处理呢？ 243、请教一个分布式问题，比如在Flink的多个TaskManager上统计指标count，TM1有两条数据，TM2有一条数据，程序是怎么计算出来是3呢？原理是怎么样的 244、现在公司部分sql查询oracle数据特别的慢，因为查询条件很多想问一下有什么方法，例如基于大数据组件可以加快查询速度的吗？ 245、想咨询下有没有做过flink同步配置做自定义计算的系统？或者有没有什么好的建议？业务诉求是希望业务用户可以自助配置计算规则做流式计算 246、我这边有个实时同步数据的任务，白天运行的时候一直是正常的，一到凌晨2点多之后就没有数据sink进mysql。晚上会有一些离线任务和一些dataX任务同步数据到mysql。但是任务一切都是正常的，ck也很快20ms，数据也是正常消费。看了yarn上的日志，没有任何error。自定义的sink里面也设置了日志打印，但是log里没有。这种如何快速定位问题。 247、有没有flink处理异常数据的案例资料 248、flink中如何传递一个全局变量 249、台4核16G的Flink taskmanager配一个单独的Yarn需要一台啥样的服务器？其他功能都不需要就一个调度的东西？ 250、side-output 的分享 251、使用 InfluxDB + Grafana 监控flink能否配置告警。是不是prometheus更强大点？ 252、我们线上遇到一个问题，带状态的算子没有指定 uid，现在代码必须改，那个带状态的算子 不能正常恢复了，有解吗？通过某种方式能获取到系统之前自动生成的uid吗？ 253、tableEnv.registerDataStream(“Orders”, ds, “user, product, amount, proctime.proctime, rowtime.rowtime”);请问像这样把流注册成表的时候，这两个rowtime分别是什么意思 254、我想问一下 flink on yarn session 模式下提交任务官网给的例子是 flink run -c xxx.MainClass job.jar 这里是怎么知道 yarn 上的哪个是 flink 的 appid 呢？ 255、Flink Netty Connector 这个有详细的使用例子？ 通过Netty建立的source能直接回复消息吗？还是只能被动接受消息？ 256、请问flink sqlclient 提交的作业可以用于生产环境吗？ 257、flink批处理写回mysql是否没法用tableEnv.sqlUpdate(“insert into t2 select * from t1”)？作为sink表的t2要如何注册？查跟jdbc相关的就两个TableSink，JDBCAppendTableSink用于BatchTableSink，JDBCUpertTablSink用于StreamTableSink。前者只接受insert into values语法。所以我是先通过select from查询获取到DataSet再JDBCAppendTableSink.emitDataSet(ds)实现的，但这样达不到sql rule any目标 258、请问在stream模式下，flink的计算结果在不落库的情况下，可以通过什么restful api获取计算结果吗 259、现在我有场景，需要把一定的消息发送给kafka topic指定的partition，该怎么搞？ 260、请问我的job作业在idea上运行正常 提交到生产集群里提示Caused by: java.lang.NoSuchMethodError: org.apache.flink.api.java.ClosureCleaner.clean(Ljava/lang/Object;Z)V请问如何解决 261、遇到一个很奇怪的问题，在使用streamingSQL时，发现timestamp在datastream的时候还是正常的，在注册成表print出来的时候就少了八小时，大佬知道是什么原因么？ 262、请问将flink的产生的一些记录日志异步到kafka中，需要如何配置，配置后必须要重启集群才会生效吗 263、星主你好，问下flink1.9对维表join的支持怎么样了？有文档吗 264、请问下 flink slq： SELECT city_name as city_name, count(1) as total, max(create_time) as create_time FROM * 。代码里面设置窗口为： retractStream.timeWindowAll(Time.minutes(5))一个global窗口，数据写入hdfs 结果数据重复 ，存在两条完全重复的数据如下 常州、2283、 1566230703）：请问这是为什么 265、我用rocksdb存储checkpoint，线上运行一段时间发展checkpoint占用空间越来越大，我是直接存本地磁盘上的，怎么样能让它自动清理呢？ 266、flink应该在哪个用户下启动呢，是root的还是在其他的用户呢 267、link可以读取lzo的文件吗 268、怎么快速从es里面便利数据？我们公司现在所有的数据都存在Es里面的;我发现每次从里面scan数据的时候特别慢;你那有没有什么好的办法？ 269、如果想让数据按照其中一个假如f0进行分区，然后每一个分区做处理的时候并行度都是1怎么设置呢 270、近在写算子的过程中,使用scala语言写flink比较快,而且在process算子中实现ontime方式时,可以使用scala中的listbuff来输出一个top3的记录;那么到了java中,只能用ArrayList将flink中的ListState使用get()方法取出之后放在ArrayList吗? 271、请问老师能否出一些1.9版本维表join的例子 包括async和维表缓存？ 272、flink kaka source设置为从组内消费，有个问题是第一次启动任务，我发现kafka中的历史数据不会被消费，而是从当前的数据开始消费，而第二次启动的时候才会从组的offset开始消费，有什么办法可以让第一次启动任务的时候可以消费kafka中的历史数据吗 273、1.使用flink定时处理离线数据，有时间戳字段，如何求出每分钟的最大值，类似于流处理窗口那样，2如果想自己实现批流统一，有什么好的合并方向吗？比如想让流处理使用批处理的一个算子。 274、flink怎么实现流式数据批量对待？流的数据是自定义的source，读取的redis多个Hash表，需要控制批次的概念 275、有人说不推荐在一个task中开多个线程，这个你怎么看？ 276、想做一个运行在hbase+es架构上的sql查询方案，flink sql能做吗，或者有没有其他的解决方案或者思路？ 277、正在紧急做第一个用到Flink的项目，咨询一下，Flink 1.8.1写入ES7就是用自带的Sink吗？有没有例子分享一下，我搜到的都是写ES6的。这种要求我知道不适合提，主要是急，自己试几下没成功。T T 278、手动停止任务后，已经保存了最近一次保存点，任务重新启动后，如何使用上一次检查点？ 279、批处理使用流环境（为了使用窗口），那如何确定批处理结束，就是我的任务可以知道批文件读取完事，并且处理完数据后关闭任务，如果不能，那批处理如何实现窗口功能 280、如果限制只能在window 内进行去重，数据量还比较大，有什么好的方法吗？ 281、端到端exactly once有没有出文章 282、流怎么动态加？，流怎么动态删除？，参数怎么动态修改 （广播 283、自定义的source数据源实现了有批次的概念，然后Flink将这个一个批次流注册为多个表join操作，有办法知道这个sql什么时候计算完成了？ 284、编译 Flink 报错，群主遇到过没，什么原因 285、[我现在是flink on yarn用zookeeper做HA现在在zk里查看检查点信息，为什么里面的文件是ip，而不是路径呢？我该如何拿到那个路径。 - 排除rest api 方式获取，因为任务关了restapi就没了 -排除history server，有点不好用](https://t.zsxq.com/nufIaey) 286、在使用streamfilesink消费kafka之后进行hdfs写入的时候，当直接关闭flink程序的时候，下次再启动程序消费写入hdfs的时候，文件又是从part-0-0开始，这样就跟原来写入的冲突了，该文件就一直处于ingress状态。 287、现在有一个实时数据分析的需求，数据量不大，但要求sink到mysql，因为是实时更新的，我现在能想到的处理方法就是每次插入一条数据的时候，先从mysql读数据，如果有这条，就执行update，没有的话就insert，但是这样的话每写一条数据就有两次交互了。想问一下老师有没有更好的办法，或者flink有没有内置的api可以执行这种不确定是更新还是插入的操作 288、Flink设置了checkpoint，job manage会定期删除check point数据，但是task manage不删除，这个是什么原因 289、请教一下使用rocksdb作为statebackend ，在哪里可以监控rocksdb io 内存指标呢 290、状态的使用场景，以及用法能出个文章不，这块不太了解 291、请问一下 Flink 1.9 SQL API中distinct count 是如何实现高效的流式去重的？ 292、在算子内如何获取当前算子并行度以及当前是第几个task 293、有没有flink1.9结合hive的demo。kafka到hive 294、能给讲讲apache calcite吗 295、请问一下像这种窗口操作，怎么保证程序异常重启后保持数据的状态呢？ 296、请问一下，我在使用kafkasource的时候，把接过来的Jsonstr转化成自定义的一个类型，用的是gson. fromJson（jsonstr,classOf[Entity]）报图片上的错误了，不知道怎么解决，在不转直接打印的情况下是没问题的 297、DataStream读数据库的表，做多表join，能设置时间窗口么，一天去刷一次。流程序会一直拉数据，数据库扛不住了 298、请问一下flink支持多路径通配读取吗？例如路径：s3n://pekdc2-deeplink-01/Kinesis/firehose/2019/07/03// ，通配读取找不到路径。是否需要特殊设置 299、flink yarn环境部署 但是把容器的url地址删除。就会跳转到的hadoop的首页。怎么屏蔽hadoop的yarn首页地址呢？要不暴露这个地址用户能看到所有任务很危险 300、flink sql怎么写一个流，每秒输出当前时间呢 301、因为想通过sql弄一个数据流。哈哈 另外想问一个问题，我把全局设置为根据处理时间的时间窗口，那么我在processAllWindowFunction里面要怎么知道进来的每个元素的处理时间是多少呢？这个元素进入这个时间窗口的依据是什么 302、如何实现一个设备上报的数据存储到同一个hdfs文件中？ 303、我自己写的kafka生产者测试，数据格式十分简单（key,i）key是一个固定的不变的字符串，i是自增的，flink consumer这边我开了checkpoint. 并且是exactly once，然后程序很简单，就是flink读取kafka的数据然后直接打印出来，我发现比如我看到打印到key，10的时候我直接关掉程序，然后重新启动程序，按理来说应当是从上次的offset继续消费，也就是key,11，但实际上我看到的可能是从key，9开始，然后依次递增，这是是不是说明是重复消费了，那exactly one需要怎么样去保障？ 304、假设有一个数据源在源源不断的产生数据，到Flink的反压来到source端的时候，由于Flink处理数据的速度跟不上数据源产生数据的速度， 问题1: 这个时候在Flink的source端会怎么处理呢？是将处理不完的数据丢弃还是进行缓存呢？ 问题2: 如果是缓存，怎么进行缓存呢？ 305、一个stream 在sink多个时，这多个sink是串行 还是并行的。 306、我想在流上做一个窗口，触发窗口的条件是固定的时间间隔或者数据量达到预切值，两个条件只要有一个满足就触发，除了重写trigger在，还有什么别的方法吗？ 307、使用rocksdb作为状态后端，对于使用sql方式对时间字段进行group by，以达到去窗口化，但是这样没办法对之前的数据清理，导致磁盘空间很大，对于这种非编码方式，有什么办法设置ttl，清理以前的数据吗 308、请问什么时间窗为什么会有TimeWindow{start=362160000, end=362220000} 和 TimeWindow{start=1568025300000, end=1568025360000}这两种形式，我都用的是一分钟的TumblingEventTimeWindows，为什么会出现不同的情况？ 309、比如我统计一天的订单量。但是某个数据延迟一天才到达。比如2019.08.01这一天订单量应该是1000，但是有个100的单据迟到了，在2019.08.02才到达，那么导致2019.08.01这一天统计的是900.后面怎么纠正这个错误的结果呢 310、flink streaming 模式下只使用堆内内存么 311、如果考虑到集群的迁移，状态能迁移吗 312、我们现在有一个业务场景，数据上报的值是这样的格式（时间，累加值），我们需要这样的格式数据（时间，当前值）。当前值=累加值-前一个数据的累加值。flink如何做到呢，有考虑过state机制，但是服务宕机后，state就被清空了 313、Flink On k8s 与 Flink on Yarn相比的优缺点是什么？那个更适合在生产环境中使用呢 314、有没有datahub链接flink的 连接器呀 315、单点resourcemanager 挂了，对任务会产生什么影响呢 316、flink监控binlog,跟另一张维表做join后，sink到MySQL的最终表。对于最终表的增删改操作，需要定义不同的sink么？ 317、请问窗口是在什么时候合并的呢？例如：数据进入windowoperator的processElement，如果不是sessionwindow，是否会进行窗口合并呢？ 318、Flink中一条流能参与多路计算，并多处输出吗？他们之前会不会相互影响？ 319、keyBy算子定义是将一个流拆分成不相交的分区，每个分区包含具有相同的key的元素。我不明白的地方是: keyBy怎么设置分区数，是给这个算子设置并行度吗？ 分区数和slot数量是什么关系？ 320、动态cep-pattern，能否详细说下？滴滴方案未公布，您贴出来的几张图片是基于1.7的。或者有什么想法也可以讲解下，谢谢了 321、问题1：使用常驻型session ./bin/yarn-session.sh -n 10 -s 3 -d启动，这个时候分配的资源是yarn 队列里面的, flink提交任务 flink run xx.jar, 其余机器是怎样获取到flink需要运行时的环境的，因为我只在集群的一台机器上有flink 安装包。 322、flink task manager中slot间的内存隔离，cpu隔离是怎么实现的？flink 设计slot的概念有什么意义，为什么不像spark executor那样，内部没有做隔离？ 323、spark和kafka集成，direct模式，spark的一个分区对应kafka的一个主题的一个分区。那flink和kafka集成的时候，怎么消费kafka的数据，假设kafka某个主题5个partition 324、./bin/flink run -m yarn-cluster 执行的flink job ，作业自己打印的日志通过yarn application的log查看不了，只有集群自身的日志，程序中logger.info打印日志存放在哪，还是我打包的方式问题，打日志用的是slf4j。 325、在物联网平台中，需要对每个key下的数据做越限判断，由于每个key的越限值是不同的，越限值配置在实时数据库中。 若将越限值加载到state中，由于key的量很大（大概3亿左右），会导致state太大，可能造成内存溢出。若在处理数据时从实时数据库中读取越限值，由于网络IO开销，可能造成实时性下降。请问该如何处理？谢谢 326、如果我一个flink程序有多个window操作，时间戳和watermark是不是每个window都需要分配，还有就是事件时间是不是一定要在数据源中就存在某个字段 327、有没有flink1.9刚支持的用ddl链接kafka并写入hbase的资料，我们公司想把离线的数仓逐渐转成实时的，写sql对于我们来说上手更快一些，就想找一些这方面的资料学习一下。 328、flink1.9 进行了数据类型的转化时发生了不匹配的问题， 目前使用的Type被弃用，推荐使用是datatypes 类型，但是之前使用的Type类型的方法 对应的schema typeinformation 目前跟datatypes的返回值不对应，请问下 该怎么去调整适配？ 329、link中处理数据其中一条出了异常都会导致整个job挂掉?有没有方法(除了异常捕获)让这条数据记录错误日志就行 下面的数据接着处理呢? 粗略看过一些容错处理，是关于程度挂了重启后从检查点拉取数据，但是如果这条数据本身就问提(特别生产上，这样就导致job直接挂了，影响有点大)，那应该怎么过滤掉这条问题数据呢(异常捕获是最后的方法 330、我在一个做日报的统计中使用rabbitmq做数据源，为什么rabbitmq中的数据一直处于unacked状态，每分钟触发一次窗口计算，并驱逐计算过的元素，我在测试环境数据都能ack,但是一到生产环境就不行了，也没有报错，有可能是哪里出了问题啊 331、我们目前数据流向是这样的，kafka source ，etl，redis sink 。这样chk 是否可以保证端到端语义呢？ 332、1.在通过 yarn-session 提交 flink job 的时候。flink-core, flink-clients, flink-scala, flink-streaming-scala, scala-library, flink-connector-kafka-0.10 那些应该写 provided scope，那些应该写 compile scope，才是正确、避免依赖冲突的姿势？ 2.flink-dist_2.11-1.8.0.jar 究竟包含了哪些依赖？（这个文件打包方式不同于 springboot，无法清楚看到有哪些 jar 依赖） 333、Flink 中使用 count window 会有这样的问题就是，最后有部分数据一直没有达到 count 的值，然后窗口就一直不触发，这里看到个思路，可以将 time window + count window 组合起来 334、flink流处理时，注册一个流数据为Table后，该流的历史数据也会一直在Table里面么？为什么每次来新数据，历史处理过得数据会重新被执行？ 335、available是变化数据，除了最新的数据被插入数据库，之前处理过数据又重新执行了几次 336、这里两天在研究flink的广播变量，发现一个问题，DataSet数据集中获取广播变量，获取的内存地址是一样的（一台机器维护一个广播数据集）。在DataStream中获取广播变量就成了一个task维护一个数据集。（可能是我使用方式有问题） 所以想请教下星主，DataStream中获取一个画面变量可以如DataSet中一台机器维护一个数据吗？ 337、Flink程序开启checkpoint 机制后，用yarn命令多次killed以后，ckeckpoint目录下有多个job id，再次开辟资源重新启动程序，程序如何找到上一次jobid目录下，而不是找到其他的jobid目录下？默认是最后一个还是需要制定特定的jobid？ 338、发展昨天的数据重复插入问题，是把kafka里进来的数据流registerDataStream注册为Table做join时，打印表的长度发现，数据会一直往表里追加，怎样才能来一条处理一条，不往上追加呀 339、flink1.9 sql 有没有类似分区表那样的处理方式呢？我们现在有一个业务是1个source，但是要分别计算5分钟，10分钟，15分钟的数据。 340、我刚弄了个服务器，在启动基础的命令时候发现task没有启动起来，导致web页是三个0，我看了log也没有报错信息，请问您知道可能是什么问题吗？ 241、我自定义了个 Sink extends RichSinkFunction，有了 field： private transient Object lock; 这个 lock 我直接初始化 private transient Object lock = new Object(); 就不行，在 invoke 里 使用lock时空指针，如果lock在 自定义 Sink 的 构造器初始化也不行。但是在 open 方法里初始化就可以，为什么？能解释一下 执行原理吗？如果一个slot 运行着5个 sink实例，那么 这个sink对象会new 5个还是1个？ 342、请问Kafka的broker 个数怎么估算？ 343、flink on yarn如何远程调试 344、目前有个需求：就是源数据是dataA、dataB、DataC通过kafka三个topic获取，然后进行合并。 但是有有几个问题，目前不知道怎么解决： dataA=”id:10001,info:,date:2019-08-01 12:23:33,entry1:1,entryInfo1:“ dataB=”id:10001,org:,entry:1” dataC=”id:10001,location:“ (1) 如何将三个流合并？ (1) 数据中dataA是有时间的，但是dataB和dataC中都没有时间戳，那么如何解决eventTime及迟到乱序的问题？帮忙看下，谢谢 345、我flink从kafka读json数据，在反序列化后中文部分变成了一串问号，请问如何做才能使中文正常 346、我有好几个Flink程序（独立jar），在线业务数据分析时都会用到同样的一批MySQL中的配置数据(5千多条)，现在的实现方法是每一个程序都是独立把这些配置数据装到内存中，便于快速使用，但现在感觉有些浪费资源和结构不够美观，请问这类情况有什么其他的解决方案吗？谢谢 347、Flink checkpoint 选 RocksDBStateBackend 还是 FsStatebackEnd ，我们目前是任务执行一段时间之后 任务就会被卡死。 348、flink on k8s的高可用、扩缩容这块目前还有哪些问题？ 349、有个问题问一下，是这样的现在Kafka4个分区每秒钟生产4000多到5000条日志数据，但是在消费者FLINK这边接收我只开了4个solt接收，这边只是接收后做切分存储，现在出现了延迟现象，我不清楚是我这边处切分慢了还是Flink接收kafka的数据慢了？Flink UI界面显示这两个背压高 等等等，还有很多，复制粘贴的我手累啊 😂 另外里面还会及时分享 Flink 的一些最新的资料（包括数据、视频、PPT、优秀博客，持续更新，保证全网最全，因为我知道 Flink 目前的资料还不多） 关于自己对 Flink 学习的一些想法和建议 Flink 全网最全资料获取，持续更新，点击可以获取 再就是星球用户给我提的一点要求：不定期分享一些自己遇到的 Flink 项目的实战，生产项目遇到的问题，是如何解决的等经验之谈！ 1、如何查看自己的 Job 执行计划并获取执行计划图 2、当实时告警遇到 Kafka 千万数据量堆积该咋办？ 3、如何在流数据中比两个数据的大小？多种解决方法 4、kafka 系列文章 5、Flink环境部署、应用配置及运行应用程序 6、监控平台该有架构是长这样子的 7、《大数据“重磅炸弹”——实时计算框架 Flink》专栏系列文章目录大纲 8、《大数据“重磅炸弹”——实时计算框架 Flink》Chat 付费文章 9、Apache Flink 是如何管理好内存的？ 10、Flink On K8s 11、Flink-metrics-core 12、Flink-metrics-datadog 13、Flink-metrics-dropwizard 14、Flink-metrics-graphite 15、Flink-metrics-influxdb 16、Flink-metrics-jmx 17、Flink-metrics-slf4j 18、Flink-metrics-statsd 19、Flink-metrics-prometheus 20、Flink 注解源码解析 21、使用 InfluxDB 和 Grafana 搭建监控 Flink 的平台 22、一文搞懂Flink内部的Exactly Once和At Least Once 23、一文让你彻底了解大数据实时计算框架 Flink 当然，除了更新 Flink 相关的东西外，我还会更新一些大数据相关的东西，因为我个人之前不是大数据开发，所以现在也要狂补些知识！总之，希望进来的童鞋们一起共同进步！ 1、Java 核心知识点整理.pdf 2、假如我是面试官，我会问你这些问题 3、Kafka 系列文章和学习视频 4、重新定义 Flink 第二期 pdf 5、GitChat Flink 文章答疑记录 6、Java 并发课程要掌握的知识点 7、Lightweight Asynchronous Snapshots for Distributed Dataflows 8、Apache Flink™- Stream and Batch Processing in a Single Engine 9、Flink状态管理与容错机制 10、Flink 流批一体的技术架构以及在阿里的实践 11、Flink Checkpoint-\u0007\b轻量级分布式快照 12、Flink 流批一体的技术架构以及在阿里的实践 13、Stream Processing with Apache Flink pdf 14、Flink 结合机器学习算法的监控平台实践 15、《大数据重磅炸弹-实时计算Flink》预备篇——大数据实时计算介绍及其常用使用场景 pdf 和视频 16、《大数据重磅炸弹-实时计算Flink》开篇词 pdf 和视频 17、四本 Flink 书 18、流处理系统 的相关 paper 19、Apache Flink 1.9 特性解读 20、打造基于Flink Table API的机器学习生态 21、基于Flink on Kubernetes的大数据平台 22、基于Apache Flink的高性能机器学习算法库 23、Apache Flink在快手的应用与实践 24、Apache Flink-1.9与Hive的兼容性 25、打造基于Flink Table API的机器学习生态 26、流处理系统的相关 paper","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"美团点评基于 Flink 的实时数仓平台实践","date":"2019-12-29T16:00:00.000Z","path":"2019/12/30/flink-meituan-real-time-warehouse/","text":"数据仓库的建设是“数据智能”必不可少的一环，也是大规模数据应用中必然面临的挑战，而 Flink 实时数仓在数据链路中扮演着极为重要的角色。本文中，美团点评高级技术专家鲁昊为大家分享了美团点评基于 Apache Flink 的实时数仓平台实践。 本文授权转自社区公众号，原文地址 目录： 一、美团点评实时计算演进美团点评实时计算演进历程在 2016 年，美团点评就已经基于 Storm 实时计算引擎实现了初步的平台化。2017 年初，我们引入了 Spark Streaming 用于特定场景的支持，主要是在数据同步场景方面的尝试。在 2017 年底，美团点评实时计算平台引入了 Flink。相比于 Storm 和 Spark Streaming，Flink 在很多方面都具有优势。这个阶段我们进行了深度的平台化，主要关注点是安全、稳定和易用。从 19 年开始，我们致力于建设包括实时数仓、机器学习等特定场景的解决方案来为业务提供更好的支持。 实时计算平台目前，美团点评的实时计算平台日活跃作业数量为万级，高峰时作业处理的消息量达到每秒 1.5 亿条，而机器规模也已经达到了几千台，并且有几千位用户正在使用实时计算服务。 实时计算平台架构如下图所示的是美团点评实时计算平台的架构。 最底层是收集层，这一层负责收集用户的实时数据，包括 Binlog、后端服务日志以及 IoT 数据，经过日志收集团队和 DB 收集团队的处理，数据将会被收集到 Kafka 中。这些数据不只是参与实时计算，也会参与离线计算。 收集层之上是存储层，这一层除了使用 Kafka 做消息通道之外，还会基于 HDFS 做状态数据存储以及基于 HBase 做维度数据的存储。 存储层之上是引擎层，包括 Storm 和 Flink。实时计算平台会在引擎层为用户提供一些框架的封装以及公共包和组件的支持。 在引擎层之上就是平台层了，平台层从数据、任务和资源三个视角去管理。 架构的最上层是应用层，包括了实时数仓、机器学习、数据同步以及事件驱动应用等。 本次分享主要介绍实时数仓方面的建设情况。 从功能角度来看，美团点评的实时计算平台主要包括作业和资源管理两个方面的功能。其中，作业部分包括作业配置、作业发布以及作业状态三个方面的功能。 在作业配置方面，则包括作业设置、运行时设置以及拓扑结构设置； 在作业发布方面，则包括版本管理、编译/发布/回滚等； 作业状态则包括运行时状态、自定义指标和报警以及命令/运行时日志等。 在资源管理方面，则为用户提供了多租户资源隔离以及资源交付和部署的能力。 业务数仓实践 流量 前面提到，现在的美团点评实时计算平台更多地会关注在安全、易用和稳定方面，而应用上很大的一个场景就是业务数仓。接下来会为大家分享几个业务数仓的例子。 第一个例子是流量，流量数仓是流量类业务的基础服务，从业务通道而言，会有不同通道的埋点和不同页面的埋点数据，通过日志收集通道会进行基础明细层的拆分，按照业务维度划分不同的业务通道，如美团通道、外卖通道等。 基于业务通道还会进行一次更加细粒度的拆分，比如曝光日志、猜你喜欢、推荐等。以上这些包括两种使用方式，一种是以流的方式提供下游其他业务方使用，另外一方面就是做一些流量方面的实时分析。 下图中右边是流量数仓的架构图，自下向上分为四层，分别是 SDK 层，包括了前端、小程序以及 APP 的埋点；其上是收集层，埋点日志落地到 Nginx，通过日志收集通道收到 Kafka 中。在计算层，流量团队基于 Storm 能力实现了上层的 SQL 封装，并实现了 SQL 动态更新的特性，在 SQL 变更时不必重启作业。 广告实时效果 这里再举一个基于流量数仓的例子-广告实时效果验证。下图中左侧是广告实时效果的对比图。广告的打点一般分为请求（PV）打点、SPV（Server PV）打点、CPV（Client PV）曝光打点和 CPV 点击打点，在所有打点中都会包含一个流量的 requestID 和命中的实验路径。根据 requestID 和命中的实验路径可以将所有的日志进行 join，得到一个 request 中需要的所有数据，然后将数据存入 Durid 中进行分析，支持实际 CTR、预估 CTR 等效果验证。 即时配送 这里列举的另外一个业务数仓实践的例子是即时配送。实时数据在即时配送的运营策略上发挥了重要作用。以送达时间预估为例，交付时间衡量的是骑手送餐的交付难度，整个履约时间分为了多个时间段，配送数仓会基于 Storm 做特征数据的清洗、提取，供算法团队进行训练并得到时间预估的结果。这个过程涉及到商家、骑手以及用户的多方参与，数据的特征会非常多，数据量也会非常大。 总结 业务实时数仓大致分为三类场景：流量类、业务类和特征类，这三种场景各有不同。 在数据模型上，流量类是扁平化的宽表，业务数仓更多是基于范式的建模，特征数据是 KV 存储。 从数据来源区分，流量数仓的数据来源一般是日志数据；业务数仓的数据来源是业务 binlog 数据；特征数仓的数据来源则多种多样。 从数据量而言，流量和特征数仓都是海量数据，每天百亿级以上，而业务数仓的数据量一般每天百万到千万级。 从数据更新频率而言，流量数据极少更新，则业务和特征数据更新较多。流量数据一般关注时序和趋势，业务数据和特征数据关注状态变更。 在数据准确性上，流量数据要求较低，而业务数据和特征数据要求较高。 在模型调整频率上，业务数据调整频率较高，流量数据和特征数据调整频率较低。 二、基于 Flink 的实时数仓平台上面为大家介绍了实时数仓的业务场景，接下来为大家介绍实时数仓的演进过程和美团点评的实时数仓平台建设思路。 传统数仓模型为了更有效地组织和管理数据，数仓建设往往会进行数据分层，一般自下而上分为四层：ODS（操作数据层）、DWD（数据明细层）、DWS（汇总层）和应用层。即时查询主要通过 Presto、Hive 和 Spark 实现。 实时数仓模型实时数仓的分层方式一般也遵守传统数据仓库模型，也分为了 ODS 操作数据集、DWD 明细层和 DWS 汇总层以及应用层。但实时数仓模型的处理的方式却和传统数仓有所差别，如明细层和汇总层的数据一般会放在 Kafka 上，维度数据一般考虑到性能问题则会放在 HBase 或者 Tair 等 KV 存储上，即席查询则可以使用 Flink 完成。 准实时数仓模型在以上两种数仓模型之外，我们发现业务方在实践过程中还有一种准实时数仓模型，其特点是不完全基于流去做，而是将明细层数据导入到 OLAP 存储中，基于 OLAP 的计算能力去做汇总并进行进一步的加工。 实时数仓和传统数仓的对比实时数仓和传统数仓的对比主要可以从四个方面考虑： 第一个是分层方式，离线数仓为了考虑到效率问题，一般会采取空间换时间的方式，层级划分会比较多；则实时数仓考虑到实时性问题，一般分层会比较少，另外也减少了中间流程出错的可能性。 第二个是事实数据存储方面，离线数仓会基于 HDFS，实时数仓则会基于消息队列（如 Kafka）。 第三个是维度数据存储，实时数仓会将数据放在 KV 存储上面。 第四个是数据加工过程，离线数仓一般以 Hive、Spark 等批处理为主，而实时数仓则是基于实时计算引擎如 Storm、Flink 等，以流处理为主。 实时数仓建设方案对比下图中对于实时数仓的两种建设方式，即准实时数仓和实时数仓两种方式进行了对比。它们的实现方式分别是基于 OLAP 引擎和流计算引擎，实时度则分别是分钟和秒级。 在调度开销方面，准实时数仓是批处理过程，因此仍然需要调度系统支持，虽然调度开销比离线数仓少一些，但是依然存在，而实时数仓却没有调度开销。 在业务灵活性方面，因为准实时数仓基于 OLAP 引擎实现，灵活性优于基于流计算的方式。 在对数据晚到的容忍度方面，因为准实时数仓可以基于一个周期内的数据进行全量计算，因此对于数据晚到的容忍度也是比较高的，而实时数仓使用的是增量计算，对于数据晚到的容忍度更低一些。 在扩展性方面，因为准实时数仓的计算和存储是一体的，因此相比于实时数仓，扩展性更弱一些。 在适用场景方面，准实时数仓主要用于有实时性要求但不太高、数据量不大以及多表关联复杂和业务变更频繁的场景，如交易类型的实时分析，实时数仓则更适用于实时性要求高、数据量大的场景，如实时特征、流量分发以及流量类型实时分析。 总结一下，基于 OLAP 引擎的建设方式是数据量不太大，业务流量不太高情况下为了提高时效性和开发效率的一个折中方案，从未来的发展趋势来看，基于流计算的实时数仓更具有发展前景。 一站式解决方案从业务实践过程中，我们看到了业务建设实时数仓的共同需求，包括发现不同业务的元数据是割裂的，业务开发也倾向于使用 SQL 方式同时开发离线数仓和实时数仓，需要更多的运维工具支持。因此我们规划了一站式解决方案，希望能够将整个流程贯通。 这里的一站式解决方案主要为用户提供了数据开发工作平台、元数据管理。同时我们考虑到业务从生产到应用过程中的问题，我们 OLAP 生产平台，从建模方式、生产任务管理和资源方面解决 OLAP 生产问题。左侧是我们已经具备数据安全体系、资源体系和数据治理，这些是离线数仓和实时数仓可以共用的。 为何选择 Flink？实时数仓平台建设之所以选择 Flink 是基于以下四个方面的考虑，这也是实时数仓方面关注的比较核心的问题。 第一个是状态管理，实时数仓里面会进行很多的聚合计算，这些都需要对于状态进行访问和管理，Flink 在这方面比较成熟。 第二个是表义能力，Flink 提供极为丰富的多层次 API，包括 Stream API、Table API 以及 Flink SQL。 第三个是生态完善，实时数仓的用途广泛，用户对于多种存储有访问需求，Flink 对于这方面的支持也比较完善。 最后一点就是 Flink 提供了流批统一的可能性。 实时数仓平台 建设思路 实时数仓平台的建设思路从外到内分为了四个层次，我们认为平台应该做的事情是为用户提供抽象的表达能力，分别是消息表达、数据表达、计算表达以及流和批统一。 实时数仓平台架构 如下图所示的是美团点评的实时数仓平台架构，从下往上看，资源层和存储层复用了实时计算平台的能力，在引擎层则会基于 Flink Streaming 实现一些扩展能力，包括对 UDF 的集成和 Connector 的集成。再往上是基于 Flink SQL 独立出来的 SQL 层，主要负责解析、校验和优化。在这之上是平台层，包括开发工作台、元数据、UDF 平台以及 OLAP 平台。最上层则是平台所支持的实时数仓的应用，包括实时报表、实时 OLAP、实时 Dashboard 和实时特征等。 消息表达-数据接入 在消息表达层面，因为 Binlog、埋点日志、后端日志以及 IoT 数据等的数据格式是不一致的，因此美团点评的实时数仓平台提供数据接入的流程，能够帮助大家把数据同步到 ODS 层。这里主要实现了两件事情，分别是统一消息协议和屏蔽处理细节。 如下图左侧是接入过程的一个例子，对于 Binlog 类型数据，实时数仓平台还为大家提供了分库分表的支持，能够将属于同一个业务的不同的分库分表数据根据业务规则收集到同一个 ODS 表中去。 计算表达-扩展 DDL 美团点评实时数仓平台基于 Flink 扩展了 DDL，这部分工作的主要目的是建设元数据体系，打通内部的主流实时存储，包括 KV 数据、OLAP 数据等。由于开发工作台和元数据体系是打通的，因此很多数据的细节并不需要大家在 DDL 中明确地声明出来，只需要在声明中写上数据的名字，和运行时的一些设置，比如 MQ 从最新消费还是最旧消费或者从某个时间戳消费即可，其他的数据访问方式是一致的。 计算表达-UDF 平台 对于 UDF 平台而言，需要从三个层面考虑： 首先是数据安全性。之前的数仓建设过程中，用户可以上传 Jar 包去直接引用 UDF，这样做是有危险性存在的，并且我们无法知道数据的流向。从数据安全的角度来考虑，平台会进行代码审计和血缘关系分析，对于历史风险组件或者存在问题的组件可以进行组件收敛。 第二个层面，在数据安全基础上我们还会关注 UDF 的运行质量，平台将会为用户提供模板、用例以及测试的管理，为用户屏蔽编译打包、Jar 包管理的过程，并且会在 UDF 模板中进行指标日志的埋点和异常处理。 第三个层面是 UDF 的复用能力，因为一个业务方开发的 UDF，其他业务方很可能也会使用，但是升级过程中可能会带来不兼容的问题，因此，平台为业务提供了项目管理、函数管理和版本管理的能力。 UDF 的应用其实非常广泛，UDF 平台并不是只支持实时数仓，也会同时支持离线数仓、机器学习以及查询服务等应用场景。下图中右侧展示的是 UDF 的使用案例，左图是 UDF 的开发流程，用户只需要关心注册流程，接下来的编译打包、测试以及上传等都由平台完成；右图是 UDF 的使用流程中，用户只需要声明 UDF，平台会进行解析校验、路径获取以及在作业提交的时候进行集成。 实时数仓平台-Web IDE 最后介绍一下实时数仓平台的开发工作台，以 Web IDE 的形式集成了模型、作业以及 UDF 的管理，用户可以在 Web IDE 上以 SQL 方式开发。平台会对 SQL 做一些版本的管理，并且支持用户回退到已部署成功的版本上去。 三、未来发展与思考资源自动调优从整个实时计算角度来考虑，目前美团点评的实时计算平台的节点数已经达到了几千台，未来很可能会达到上万台，因此资源优化这件事情很快就会被提上日程。由于业务本身的流量存在高峰和低谷，对于一个实时任务来说，可能在高峰时需要很多资源，但是在低谷时并不需要那么多资源。 另外一方面，波峰本身也是会发生变化的，有可能随着业务的上涨使得原来分配的资源数量不够用。因此，资源自动调优有两个含义，一个是指能够适配作业的高峰流量上涨，自动适配 Max 值；另外一个含义是指使得作业能够在高峰过去之后自动适应流量减少，能够快速缩容。我们可以通过每个任务甚至是算子的历史运行情况，拟合得到算子、流量与资源的关系函数，在流量变化时同步调整资源量。 以上是资源优化的思路，除此之外还需要考虑当资源完成优化之后应该如何利用。为了保证可用性，实时和离线任务一般会分开部署，否则带宽、IO 都可能被离线计算打满导致实时任务延迟。而从资源使用率角度出发，则需要考虑实时和离线的混合部署，或者以流的方式来处理一些实时性要求并不是非常高的任务。这就要求更细粒度的资源隔离和更快的资源释放。 推动实时数仓建设方式升级实时数仓的建设一般分为几个步骤： 首先，业务提出需求，后续会进行设计建模、业务逻辑开发和底层技术实现。美团点评的实时数仓建设思路是将技术实现统一表达，让业务关注逻辑开发，而逻辑开发也可以基于配置化手段实现自动构建。 再上一层是可以根据业务需求实现智能建模，将设计建模过程实现自动化。 目前，美团点评的实时数仓平台建设工作还集中在统一表达的层次，距离理想状态仍然有比较长的一段路要走。 Flink Forward Asia 2019 PPT 下载链接给你们准备好啦！公众号(zhisheng)里面回复 ffa 即可下载，也可以扫描下面的二维码关注，回复 ffa 即可下载。 更多 Flink 博客Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 本文的项目代码在 https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"基于 Apache Flink 的监控告警系统","date":"2019-12-22T16:00:00.000Z","path":"2019/12/23/flink-monitor-alert/","text":"本人在 Flink 社区钉钉群直播的视频 监控告警 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"监控告警","slug":"监控告警","permalink":"http://www.54tianzhisheng.cn/tags/监控告警/"}]},{"title":"基于 Apache Flink 的大规模准实时数据分析平台","date":"2019-12-09T16:00:00.000Z","path":"2019/12/10/flink-real-time-data-analysis-platform/","text":"本文来自 Flink Forward Asia 2019 Lyft 公司的分享，作者是徐赢和高立，感谢！ 授权转载自社区公众号：原文地址 一、Lyft 的流数据与场景 关于 LyftLyft 是位于北美的一个共享交通平台，和大家所熟知的 Uber 和国内的滴滴类似，Lyft 也为民众提供共享出行的服务。Lyft 的宗旨是提供世界最好的交通方案来改善人们的生活。 Lyft 的流数据场景Lyft 的流数据可以大致分为三类，秒级别、分钟级别和不高于 5 分钟级别。分钟级别流数据中，自适应定价系统、欺诈和异常检测系统是最常用的，此外还有 Lyft 最新研发的机器学习特征工程。不高于 5 分钟级别的场景则包括准实时数据交互查询相关的系统。 Lyft 数据分析平台架构如下图所示的是 Lyft 之前的数据分析平台架构。Lyft 的大部分流数据都是来自于事件，而事件产生的来源主要有两种，分别是手机 APP 和后端服务，比如乘客、司机、支付以及保险等服务都会产生各种各样的事件，而这些事件都需要实时响应。 在分析平台这部分，事件会流向 AWS 的 Kinesis 上面，这里的 Kinesis 与 Apache Kafka 非常类似，是一种 AWS 上专有的 PubSub 服务，而这些数据流都会量化成文件，这些文件则都会存储在 AWS 的 S3 上面，并且很多批处理任务都会弹出一些数据子集。在分析系统方面，Lyft 使用的是开源社区中比较活跃的 presto 查询引擎。Lyft 数据分析平台的用户主要有四种，即数据工程师、数据分析师以及机器学习专家和深度学习专家，他们往往都是通过分析引擎实现与数据的交互。 既往平台的问题Lyft 之所以要基于 Apache Flink 实现大规模准实时数据分析平台，是因为以往的平台存在一些问题。比如较高的延迟，导入数据无法满足准实时查询的要求；并且基于 Kinesis Client Library 的流式数据导入性能不足；导入数据存在太多小文件导致下游操作性能不足；数据 ETL 大多是高延迟多日多步的架构；此外，以往的平台对于嵌套数据提供的支持也不足。 二、准实时数据分析平台和架构 准实时平台架构在新的准实时平台架构中，Lyft 采用 Flink 实现流数据持久化。Lyft 使用云端存储，而使用 Flink 直接向云端写一种叫做 Parquet 的数据格式，Parquet 是一种列数据存储格式，能够有效地支持交互式数据查询。Lyft 在 Parquet 原始数据上架构实时数仓，实时数仓的结构被存储在 Hive 的 Table 里面，Hive Table 的 metadata 存储在 Hive metastore 里面。 平台会对于原始数据做多级的非阻塞 ETL 加工，每一级都是非阻塞的(nonblocking)，主要是压缩和去重的操作，从而得到更高质量的数据。平台主要使用 Apache Airflow 对于 ETL 操作进行调度。所有的 Parquet 格式的原始数据都可以被 presto 查询，交互式查询的结果将能够以 BI 模型的方式显示给用户。 平台设计Lyft 基于 Apache Flink 实现的大规模准实时数据分析平台具有几个特点： 首先，平台借助 Flink 实现高速有效的流数据接入，使得云上集群规模缩减为原来的十分之一，因此大大降低了运维成本。 其次，Parquet 格式的数据支持交互式查询，当用户仅对于某几个列数据感兴趣时可以通过分区和选择列的方式过滤不必要的数据，从而提升查询的性能。 再次，基于 AWS 的云端存储，平台的数据无需特殊存储形式。 之后，多级 ETL 进程能够确保更好的性能和数据质量。 最后，还能够兼顾性能容错及可演进性。 平台特征及应用Lyft 准实时数据分析平台需要每天处理千亿级事件，能够做到数据延迟小于 5 分钟，而链路中使用的组件确保了数据完整性，同时基于 ETL 去冗余操作实现了数据单一性保证。 数据科学家和数据工程师在建模时会需要进行自发的交互式查询，此外，平台也会提供实时机器学习模型正确性预警，以及实时数据面板来监控供需市场健康状况。 基于 Flink 的准实时数据导入下图可以看到当事件到达 Kinesis 之后就会被存储成为 EventBatch。通过 Flink-Kinesis 连接器可以将事件提取出来并送到 FlatMap 和 Record Counter 上面，FlatMap 将事件打撒并送到下游的 Global Record Aggregator 和 Tagging Partitioning 上面，每当做 CheckPoint 时会关闭文件并做一个持久化操作，针对于 StreamingFileSink 的特征，平台设置了每三分钟做一次 CheckPoint 操作，这样可以保证当事件进入 Kinesis 连接器之后在三分钟之内就能够持久化。 以上的方式会造成太多数量的小文件问题，因为数据链路支持成千上万种文件，因此使用了 Subtasks 记录本地事件权重，并通过全局记录聚合器来计算事件全局权重并广播到下游去。而 Operator 接收到事件权重之后将会将事件分配给 Sink。 ETL 多级压缩和去重上述的数据链路也会做 ETL 多级压缩和去重工作，主要是 Parquet 原始数据会经过每小时的智能压缩去重的 ETL 工作，产生更大的 Parquet File。同理，对于小时级别压缩去重不够的文件，每天还会再进行一次压缩去重。对于新产生的数据会有一个原子性的分区交换，也就是说当产生新的数据之后，ETL Job 会让 Hive metastore 里的表分区指向新的数据和分区。这里的过程使用了启发性算法来分析哪些事件必须要经过压缩和去重以及压缩去重的时间间隔级别。此外，为了满足隐私和合规的要求，一些 ETL 数据会被保存数以年计的时间。 三、平台性能及容错深入分析 事件时间驱动的分区感测Flink 和 ETL 是通过事件时间驱动的分区感测实现同步的。S3 采用的是比较常见的分区格式，最后的分区是由时间戳决定的，时间戳则是基于 EventTime 的，这样的好处在于能够带来 Flink 和 ETL 共同的时间源，这样有助于同步操作。此外，基于事件时间能够使得一些回填操作和主操作实现类似的结果。Flink 处理完每个小时的事件后会向事件分区写入一个 Success 文件，这代表该小时的事件已经处理完毕，ETL 可以对于该小时的文件进行操作了。 Flink 本身的水印并不能直接用到 Lyft 的应用场景当中，主要是因为当 Flink 处理完时间戳并不意味着它已经被持久化到存储当中，此时就需要引入分区水印的概念，这样一来每个 Sink Source 就能够知道当前写入的分区，并且维护一个分区 ID，并且通过 Global State Aggregator 聚合每个分区的信息。每个 Subtasks 能够知道全局的信息，并将水印定义为分区时间戳中最小的一个。 ETL 主要有两个特点，分别是及时性和去重，而 ETL 的主要功能在于去重和压缩，最重要的是在非阻塞的情况下就进行去重。前面也提到 Smart ETL，所谓 Smart 就是智能感知，需要两个相应的信息来引导 Global State Aggregator，分别是分区完整性标识 SuccessFile，在每个分区还有几个相应的 States 统计信息能够告诉下游的 ETL 怎样去重和压缩以及操作的频率和范围。 Schema 演进的挑战ETL 除了去重和压缩的挑战之外，还经常会遇到 Schema 的演化挑战。Schema 演化的挑战分为三个方面，即不同引擎的数据类型、嵌套结构的演变、数据类型演变对去重逻辑的影响。 S3 深入分析Lyft 的数据存储系统其实可以认为是数据湖，对于 S3 而言，Lyft 也有一些性能的优化考量。S3 本身内部也是有分区的，为了使其具有并行的读写性能，添加了 S3 的熵数前缀，在分区里面也增加了标记文件，这两种做法能够极大地降低 S3 的 IO 性能的影响。标识符对于能否触发 ETL 操作会产生影响，与此同时也是对于 presto 的集成，能够让 presto 决定什么情况下能够扫描多少个文件。 Parquet 优化方案Lyft 的准实时数据分析平台在 Parquet 方面做了很多优化，比如文件数据值大小范围统计信息、文件系统统计信息、基于主键数据值的排序加快 presto 的查询速度以及二级索引的生成。 基于数据回填的平台容错机制如下两个图所示的是 Lyft 准实时数据分析平台的基于数据回填的平台容错机制。对于 Flink 而言，因为平台的要求是达到准实时，而 Flink 的 Job 出现失效的时候可能会超过一定的时间，当 Job 重新开始之后就会形成两个数据流，主数据流总是从最新的数据开始往下执行，附加数据流则可以回溯到之前中断的位置进行执行直到中断结束的位置。这样的好处是既能保证主数据流的准实时特性，同时通过回填数据流保证数据的完整性。 对于 ETL 而言，基于数据回填的平台容错机制则表现在 Airflow 的幂等调度系统、原子压缩和 HMS 交换操作、分区自建自修复体系和 Schema 整合。 四、总结与未来展望 体验与经验教训利用 Flink 能够准实时注入 Parquet 数据，使得交互式查询体验为可能。同时，Flink 在 Lyft 中的应用很多地方也需要提高，虽然 Flink 在大多数情况的延时都能够得到保证，但是重启和部署的时候仍然可能造成分钟级别的延时，这会对于 SLO 产生一定影响。 此外，Lyft 目前做的一件事情就是改善部署系统使其能够支持 Kubernetes，并且使得其能够接近 0 宕机时间的效果。因为 Lyft 准实时数据分析平台在云端运行，因此在将数据上传到 S3 的时候会产生一些随机的网络情况，造成 Sink Subtasks 的停滞，进而造成整个 Flink Job 的停滞。而通过引入一些 Time Out 机制来检测 Sink Subtasks 的停滞，使得整个 Flink Job 能够顺利运行下去。 ETL 分区感应能够降低成本和延迟，成功文件则能够表示什么时候处理完成。此外，S3 文件布局对性能提升的影响还是非常大的，目前而言引入熵数还属于经验总结，后续 Lyft 也会对于这些进行总结分析并且公开。因为使用 Parquet 数据，因此对于 Schema 的兼容性要求就非常高，如果引入了不兼容事件则会使得下游的 ETL 瘫痪，因此 Lyft 已经做到的就是在数据链路上游对于 Schema 的兼容性进行检查，检测并拒绝用户提交不兼容的 Schema。 未来展望Lyft 对于准实时数据分析平台也有一些设想。 首先，Lyft 希望将 Flink 部署在 Kubernetes 集群环境下运行，使得 Kubernetes 能够管理这些 Flink Job，同时也能够充分利用 Kubernetes 集群的高可扩展性。 其次，Lyft 也希望实现通用的流数据导入框架，准实时数据分析平台不仅仅支持事件，也能够支持数据库以及服务日志等数据。 再次，Lyft 希望平台能够实现 ETL 智能压缩以及事件驱动 ETL，使得回填等事件能够自动触发相应的 ETL 过程，实现和以前的数据的合并，同时将延时数据导入来对于 ETL 过程进行更新。 最后，Lyft 还希望准实时数据分析平台能够实现存储过程的改进以及查询优化，借助 Parquet 的统计数据来改善 presto 的查询性能，借助表格管理相关的开源软件对存储管理进行性能改善，同时实现更多的功能。 Flink Forward Asia 2019 PPT 下载链接给你们准备好啦！公众号(zhisheng)里面回复 ffa 即可下载，也可以扫描下面的二维码关注，回复 ffa 即可下载。 更多 Flink 博客Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 本文的项目代码在 https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Forward Asia 2019 PPT 下载","date":"2019-12-06T16:00:00.000Z","path":"2019/12/07/Flink_Forward_Asia_2019/","text":"Flink Forward Asia 2019 在北京召开的，有主会场和几个分会场（企业实践、Apache Flink 核心技术、开源大数据生态、实时数仓、人工智能），内容涉及很多，可以查看下面的 PPT。 主会场1、《Stateful Functions: Building general-purpose Applications and Services on Apache Flink》 2、《Apache Flink Heading Towards A Unified Engine》 3、《Storage Reimagined for a Streaming World》 4、《Lyft 基于 Apache Flink 的大规模准实时数据分析平台》 企业实践1、《Apache Flink 在字节跳动的实践与优化》 2、《Apache Flink在快手实时多维分析场景的应用》 3、《bilibili 实时平台的架构与实践》 4、《Apache Flink 资源动态调整及其实践》 5、《Apache Flink在滴滴的应用与实践》 6、《Apache Flink 在网易的实践》 7、《Apache Flink 在中国农业银行的探索和实践》 8、《基于 Apache Flink 的爱奇艺实时计算平台建设实践》 9、《实时计算在贝壳的实践》 10、《基于 Apache Flink 构建 CEP（Complex Event Process）引擎的挑战和实践》 Apache Flink 核心技术1、《Pluggable Shuffle Service and Unaligned Checkpoint》 2、《漂移计算 – 跨 DC 跨数据源的高性能 SQL 引擎》 3、《New Source API – Make it Easy! 》 4、《Stateful Functions: Unlocking the next wave of applications with Stream Processing》 5、《Apache Flink新场景——OLAP引擎》 6、《New Feature and Improvements on State Backends in Flink 1.10》 7、《阿里巴巴在 Apache Flink 大规模持久化存储的实践之道》 8、《Using Apache Flink as a Unified Data Processing Platform》 9、《深入探索 Apache Flink SQL 流批统一的查询引擎与最佳实践》 10、《Apache Flink 流批一体的资源管理与任务调度》 开源大数据生态1、《YuniKorn 对 Apache Flink on K8s 的调度优化》 2、《流处理基准测试》 3、《Apache Flink and the Apache Way》 4、《Delivering stream data reliably with Pravega》 5、《Deep dive into Pyflink &amp; integration with Zeppelin》 6、《Apache Flink 与 Apache Hive 的集成》 7、《趣头条基于 Apache Flink+ClickHouse 构建实时数据分析平台》 8、《基于 Apache Flink 的边缘流式计算》 9、《基于 Apache Pulsar 和 Apache Flink 进行批流一体的弹性数据处理》 10、《The integretion of Apache Flink SQL and Apache Calcite》 实时数仓1、《美团点评基于 Apache Flink 的实时数仓平台实践》 2、《小米流式平台架构演进与实践》 3、《Netflix：Evolving Keystone to an Open Collaborative Real-time ETL Platform》 4、《菜鸟供应链实时数据技术架构的演进》 5、《OPPO 基于 Apache Flink 的实时数仓实践》 人工智能1、《Deep Learning On Apache Flink》 2、《在 Apache Flink 上使用 Analytics-Zoo 进行大数据分析与深度学习模型推理的架构与实践》 3、《携程实时智能检测平台实践》 4、《基于Apache Flink的机器学习算法平台实践与开源》 5、《Apache Flink AI生态系统工作》 如何获取上面这些 PPT？上面的这些 PPT 本人已经整理好了，你可以关注微信公众号：zhisheng，然后在里面回复关键字: ffa 即可获取已放出的 PPT。 另外你也可以加我微信：yuanblog_tzs，探讨技术！ 更多 Flink 的文章博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"阿里巴巴 Flink 踩坑经验：如何大幅降低 HDFS 压力？","date":"2019-11-29T16:00:00.000Z","path":"2019/11/30/flink-checkpoint-hdfs/","text":"众所周知，Flink 是当前最为广泛使用的计算引擎之一，它使用 Checkpoint 机制进行容错处理 [1]，Checkpoint 会将状态快照备份到分布式存储系统，供后续恢复使用。在 Alibaba 内部，我们使用的存储主要是 HDFS，当同一个集群的 Job 到达一定数量后，会对 HDFS 造成非常大的压力，本文将介绍一种大幅度降低 HDFS 压力的方法——小文件合并。 本文转自：https://www.infoq.cn/article/OLlJNzQpTOHfyrgOG8xq 作者：邱从贤 背景不管使用 FsStateBackend、RocksDBStateBackend 还是 NiagaraStateBackend，Flink 在进行 Checkpoint 的时候，TM 会将状态快照写到分布式文件系统中，然后将文件句柄发给 JM，JM 完成全局 checkpoint 快照的存储，如下图所示。 对于全量 Checkpoint 来说，TM 将每个 Checkpoint 内部的数据都写到同一个文件，而对于 RocksDBStateBackend/NiagaraStateBackend 的增量 Checkpoint [2] 来说，则会将每个 sst 文件写到一个分布式系统的文件内。当作业量很大，且作业的并发很大时，则会对底层 HDFS 形成非常大的压力：1）大量的 RPC 请求会影响 RPC 的响应时间（如下图所示）；2）大量文件对 NameNode 内存造成很大压力。 在 Flink 中曾经尝试使用 ByteStreamStateHandle 来解决小文件多的问题 [3]，将小于一定阈值的 state 直接发送到 JM，由 JM 统一写到分布式文件中，从而避免在 TM 端生成小文件。但是这个方案有一定的局限性，阈值设置太小，还会有很多小文件生成，阈值设置太大，则会导致 JM 内存消耗太多有 OOM 的风险。 1 小文件合并方案针对上面的问题我们提出一种解决方案——小文件合并。 在原来的实现中，每个 sst 文件会打开一个 CheckpointOutputStream，每个 CheckpointOutputStream 对应一个 FSDataOutputStream，将本地文件写往一个分布式文件，然后关闭 FSDataOutputStream，生成一个 StateHandle。如下图所示： 小文件合并则会重用打开的 FSDataOutputStream，直至文件大小达到预设的阈值为止，换句话说多个 sst 文件会重用同一个 DFS 上的文件，每个 sst 文件占用 DFS 文件中的一部分，最终多个 StateHandle 共用一个物理文件，如下图所示。 在接下来的章节中我们会描述实现的细节，其中需要重点考虑的地方包括： 并发 Checkpoint 的支持 Flink 天生支持并发 Checkpoint，小文件合并方案则会将多个文件写往同一个分布式存储文件中，如果考虑不当，数据会写串或者损坏，因此我们需要有一种机制保证该方案的正确性，详细描述参考 2.1 节。 防止误删文件 我们使用引用计数来记录文件的使用情况，仅通过文件引用计数是否降为 0 进行判断删除，则可能误删文件，如何保证文件不会被错误删除，我们将会在 2.2 节进行阐述。 降低空间放大 使用小文件合并之后，只要文件中还有一个 statehandle 被使用，整个分布式文件就不能被删除，因此会占用更多的空间，我们在 2.3 节描述了解决该问题的详细方案。 异常处理 我们将在 2.4 节阐述如何处理异常情况，包括 JM 异常和 TM 异常的情况。 2.5 节中会详细描述在 Checkpoint 被取消或者失败后，如何取消 TM 端的 Snapshot，如果不取消 TM 端的 Snapshot，则会导致 TM 端实际运行的 Snapshot 比正常的多。 在第 3 节中阐述了小文件合并方案与现有方案的兼容性；第 4 节则会描述小文件合并方案的优势和不足；最后在第 5 节我们展示在生产环境下取得的效果。 2 设计实现本节中我们会详细描述整个小文件合并的细节，以及其中的设计要点。 这里我们大致回忆一下 TM 端 Snapshot 的过程： TM 端 barrier 对齐 TM Snapshot 同步操作 TM Snapshot 异步操作 其中上传 sst 文件到分布式存储系统在上面的第三步，同一个 Checkpoint 内的文件顺序上传，多个 Checkpoint 的文件上传可能同时进行。 2.1 并发 Checkpoint 支持Flink 天生支持并发 Checkpoint，因此小文件合并方案也需要能够支持并发 Checkpoint，如果不同 Checkpoint 的 sst 文件同时写往一个分布式文件，则会导致文件内容损坏，后续无法从该文件进行 restore。 在 FLINK-11937[4] 的提案中，我们会将每个 Checkpoint 的 state 文件写到同一个 HDFS 文件，不同 Checkpoint 的 state 写到不同的 HDFS 文件 – 换句话说，HDFS 文件不跨 Checkpoint 共用，从而避免了多个客户端同时写入同一个文件的情况。 后续我们会继续推进跨 Checkpoint 共用文件的方案，当然在跨 Checkpoint 共用文件的方案中，并行的 Checkpoint 也会写往不同的 HDFS 文件。 2.2 防止误删文件复用底层文件之后，我们使用引用计数追踪文件的使用情况，在文件引用数降为 0 的情况下删除文件。但是在某些情况下，文件引用数为 0 的时候，并不代表文件不会被继续使用，可能导致文件误删。下面我们会详细描述开启并发 Checkpoint 后可能导致文件误删的情况，以及解决方案。 以下图为例，maxConcurrentlyCheckpoint = 2 上图中共有 3 个 Checkpoint，其中 chk-1 已经完成，chk-2 和 chk-3 都基于 chk-1 进行，chk-2 在 chk-3 前完成，chk-3 在注册 4.sst 的时候发现，发现 4.sst 在 chk-2 中已经注册过，会重用 chk-2 中 4.sst 对应的 stateHandle，然后取消 chk-3 中的 4.sst 的注册，并且删除 stateHandle，在处理完 chk-3 中 4.sst 之后，该 stateHandle 对应的分布式文件的引用计数为 0，如果我们这个时候删除分布式文件，则会同时删除 5.sst 对应的内容，导致后续无法从 chk-3 恢复。 这里的问题是如何在 stateHandle 对应的分布式文件引用计数降为 0 的时候正确判断是否还会继续引用该文件，因此在整个 Checkpoint 完成处理之后再判断某个分布式文件能否删除，如果真个 Checkpoint 完成发现文件没有被引用，则可以安全删除，否则不进行删除。 2.3 降低空间放大使用小文件合并方案后，每个 sst 文件对应分布式文件中的一个 segment，如下图所示： 文件仅能在所有 segment 都不再使用时进行删除，上图中有 4 个 segment，仅 segment-4 被使用，但是整个文件都不能删除，其中 segment[1-3] 的空间被浪费掉了，从实际生产环境中的数据可知，整体的空间放大率（实际占用的空间 / 真实有用的空间）在 1.3 - 1.6 之间。 为了解决空间放大的问题，在 TM 端起异步线程对放大率超过阈值的文件进行压缩。而且仅对已经关闭的文件进行压缩。 整个压缩的流程如下所示： 计算每个文件的放大率 如果放大率较小则直接跳到步骤 7 如果文件 A 的放大率超过阈值，则生成一个对应的新文件 A‘（如果这个过程中创建文件失败，则由 TM 负责清理工作） 记录 A 与 A’ 的映射关系 在下一次 Checkpoint X 往 JM 发送落在文件 A 中的 StateHandle 时，则使用 A` 中的信息生成一个新的 StateHandle 发送给 JM Checkpoint X 完成后，我们增加 A‘ 的引用计数，减少 A 的引用计数，在引用计数降为 0 后将文件 A 删除（如果 JM 增加了 A’ 的引用，然后出现异常，则会从上次成功的 Checkpoint 重新构建整个引用计数器） 文件压缩完成 2.4 异常情况处理在 Checkpoint 的过程中，主要有两种异常：JM 异常和 TM 异常，我们将分情况阐述。 2.4.1 JM 异常JM 端主要记录 StateHandle 以及文件的引用计数，引用计数相关数据不需要持久化到外存中，因此不需要特殊的处理，也不需要考虑 transaction 等相关操作，如果 JM 发送 failover，则可以直接从最近一次 complete Checkpoint 恢复，并重建引用计数即可。 2.4.2 TM 异常TM 异常可以分为两种：1）该文件在之前 Checkpoint 中已经汇报过给 JM；2）文件尚未汇报过给 JM，我们会分情况阐述。 文件已经汇报过给 JM 文件汇报过给 JM，因此在 JM 端有文件的引用计数，文件的删除由 JM 控制，当文件的引用计数变为 0 之后，JM 将删除该文件。 文件尚未汇报给 JM 该文件暂时尚未汇报过给 JM，该文件不再被使用，也不会被 JM 感知，成为孤儿文件。这种情况暂时有外围工具统一进行清理。 2.5 取消 TM 端 snapshot像前面章节所说，我们需要在 Checkpoint 超时 / 失败时，取消 TM 端的 snapshot，而 Flink 则没有相应的通知机制，现在 FLINK-8871[5] 在追踪相应的优化，我们在内部增加了相关实现，当 Checkpoint 失败时会发送 RPC 数据给 TM，TM 端接受到相应的 RPC 消息后，会取消相应的 snapshot。 3 兼容性小文件合并功能支持从之前的版本无缝迁移过来。从之前的 Checkpoint restore 的的步骤如下： 每个 TM 分到自己需要 restore 的 state handle TM 从远程下载 state handle 对应的数据 从本地进行恢复 小文件合并主要影响的是第 2 步，从远程下载对应数据的时候对不同的 StateHandle 进行适配，因此不影响整体的兼容性。 4 优势和不足 优势：大幅度降低 HDFS 的压力：包括 RPC 压力以及 NameNode 内存的压力 不足：不支持 State 多线程上传的功能（State 上传暂时不是 Checkpoint 的瓶颈） 5 线上环境的结果在该方案上线后，对 Namenode 的压力大幅降低，下面的截图来自线上生产集群，从数据来看，文件创建和关闭的 RPC 有明显下降，RPC 的响应时间也有大幅度降低，确保顺利度过双十一。 参考文献[1] https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/checkpoints.html [2] https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html [3] https://www.slideshare.net/dataArtisans/stephan-ewen-experiences-running-flink-at-very-large-scale [4] https://issues.apache.org/jira/browse/FLINK-11937 [5] https://issues.apache.org/jira/browse/FLINK-8871 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"58 同城基于 Flink 的千亿级实时计算平台架构实践","date":"2019-11-29T16:00:00.000Z","path":"2019/11/30/flink-in-58/","text":"58 同城作为覆盖生活全领域的服务平台，业务覆盖招聘、房产、汽车、金融、二手及本地服务等各个方面。丰富的业务线和庞大的用户数每天产生海量用户数据需要实时化的计算分析，实时计算平台定位于为集团海量数据提供高效、稳定、分布式实时计算的基础服务。本文主要介绍 58 同城基于 Flink 打造的一站式实时计算平台 Wstream。 本文转自：dbaplus 社群公众号 作者：冯海涛 / 万石康，负责 58 同城实时计算平台建设。 实时计算场景和很多互联网公司一样，实时计算在 58 拥有丰富的场景需求，主要包括以下几类： 实时数据 ETL：实时消费 Kafka 数据进行清洗、转换、结构化处理用于下游计算处理。 实时数仓：实时化数据计算，仓库模型加工和存储。实时分析业务及用户各类指标，让运营更加实时化。 实时监控：对系统和用户行为进行实时检测和分析，如业务指标实时监控，运维线上稳定性监控，金融风控等。 实时分析：特征平台，用户画像，实时个性化推荐等。 平台演进 在实时计算平台建设过程中，主要是跟进开源社区发展以及实际业务需求，计算框架经历了 Storm 到 Spark Streaming 到 Flink 的发展，同时建设一站式实时计算平台，旨在提升用户实时计算需求开发上线管理监控效率，优化平台管理。 实时计算引擎前期基于 Storm 和 Spark Streaming 构建, 很多情况下并不能很好的满足业务需求，如商业部门基于 Spark Streaming 构建的特征平台希望将计算延迟由分钟级降低到秒级，提升用户体验，运维监控平台基于 Storm 分析公司全量 nginx 日志对线上业务进行监控，需要秒级甚至毫秒级别的延迟，Storm 的吞吐能力成为瓶颈。 同时随着实时需求不断增加，场景更加丰富，在追求任务高吞吐低延迟的基础上，对计算过程中间状态管理，灵活窗口支持，以及 exactly once 语义保障的诉求越来越多。Apache Flink 开源之后，支持高吞吐低延迟的架构设计以及高可用的稳定性，同时拥有实时计算场景一系列特性以及支持实时 Sql 模型，使我们决定采用 Flink 作为新一代实时计算平台的计算引擎。 平台规模 实时计算平台当前主要基于 Storm/Spark Streaming/Flink，集群共计 500 多台机器，每天处理数据量 6000 亿 +，其中 Flink 经过近一年的建设，任务占比已经达到 50%。 Flink 稳定性Flink 作为实时计算集群，可用性要求远高于离线计算集群。为保障集群可用性，平台主要采用任务隔离以及高可用集群架构保障稳定性。 任务隔离在应用层面主要基于业务线以及场景进行机器隔离，队列资源分配管理，避免集群抖动造成全局影响。 集群架构Flink 集群采用了 ON YARN 模式独立部署，为减少集群维护工作量，底层 HDFS 利用公司统一 HDFS Federation 架构下建立独立的 namespace，减少 Flink 任务在 checkpoint 采用 hdfs/rocksdb 作为状态存储后端场景下由于 hdfs 抖动出现频繁异常失败。 在资源隔离层面，引入 Node Label 机制实现重要任务运行在独立机器，不同计算性质任务运行在合适的机器下，最大化机器资源的利用率。同时在 YARN 资源隔离基础上增加 Cgroup 进行物理 cpu 隔离，减少任务间抢占影响，保障任务运行稳定性。 平台化管理Wstream 是一套基于 Apache Flink 构建的一站式、高性能实时大数据处理平台。提供 SQL 化流式数据分析能力，大幅降低数据实时分析门槛，支持通过 DDL 实现 source/sink 以及维表，支持 UDF/UDAF/UDTF，为用户提供更强大的数据实时处理能力。支持多样式应用构建方式 FlinkJar/Stream SQL/Flink-Storm，以满足不同用户的开发需求，同时通过调试，监控，诊断，探查结果等辅助手段完善任务生命周期管理。 ###流式 sql 能力建设Stream SQL 是平台为了打造 sql 化实时计算能力，减小实时计算开发门槛，基于开源的 Flink，对底层 sql 模块进行扩展实现以下功能： 支持自定义 DDL 语法（包括源表, 输出表, 维表） 支持自定义 UDF/UDTF/UDAF 语法 实现了流与维表的 join, 双流 join 在支持大数据开源组件的同时，也打通了公司主流的实时存储平台。同时为用户提供基于 Sql client 的 cli 方式以及在 Wstream 集成了对实时 sql 能力的支持，为用户提供在线开发调试 sql 任务的编辑器，同时支持代码高亮，智能提示，语法校验及运行时校验，尽可能避免用户提交到集群的任务出现异常。 另外也为用户提供了向导化配置方式，解决用户定义 table 需要了解复杂的参数设置，用户只需关心业务逻辑处理，像开发离线 Hive 一样使用 sql 开发实时任务。 Storm 任务迁移 Flink在完善 Flink 平台建设的同时，我们也启动 Storm 任务迁移 Flink 计划，旨在提升实时计算平台整体效率，减少机器成本和运维成本。Flink-Storm 作为官方提供 Flink 兼容 Storm 程序为我们实现无缝迁移提供了可行性，但是作为 beta 版本，在实际使用过程中存在很多无法满足现实场景的情况，因此我们进行了大量改进，主要包括实现 Storm 任务 on yarn ，迁移之后任务 at least once 语义保障，兼容 Storm 的 tick tuple 机制等等。 通过对 Fink-Storm 的优化，在无需用户修改代码的基础上，我们已经顺利完成多个 Storm 版本集群任务迁移和集群下线，在保障实时性及吞吐量的基础上可以节约计算资源 40% 以上，同时借助 yarn 统一管理实时计算平台无需维护多套 Storm 集群，整体提升了平台资源利用率，减轻平台运维工作量。 任务诊断指标监控Flink webUI 提供了大量的运行时信息供用户了解任务当前运行状况，但是存在无法获取历史 metrics 的问题导致用户无法了解任务历史运行状态，因此我们采用了 Flink 原生支持的 Prometheus 进行实时指标采集和存储。 Prometheus 是一个开源的监控和报警系统，通过 pushgateway 的方式实时上报 metrics，Prometheus 集群采用 Fedration 部署模式，meta 节点定时抓取所有子节点指标进行汇总，方便统一数据源提供给 Grafana 进行可视化以及告警配置。 任务延迟吞吐能力和延迟作为衡量实时任务性能最重要的指标，我们经常需要通过这两个指标来调整任务并发度和资源配置。Flink Metrics 提供 latencyTrackingInterval 参数启用任务延迟跟踪，打开会显著影响集群和任务性能，官方高度建议只在 debug 下使用。 在实践场景下，Flink 任务数据源基本都是 Kafka，因此我们采用 topic 消费堆积作为衡量任务延迟的指标，监控模块实时通过 Flink rest 获取任务正在消费 topic 的 offset，同时通过 Kafka JMX 获取对应 topic 的 logsize，采用 logsize– offset 作为 topic 的堆积。 日志检索Flink 作为分布式计算引擎，所有任务会由 YARN 统一调度到任意的计算节点，因此任务的运行日志会分布在不同的机器，用户定位日志困难。我们通过调整 log4j 日志框架默认机制，按天切分任务日志，定期清理过期日志，避免异常任务频繁写满磁盘导致计算节点不可用的情况，同时在所有计算节点部署 agent 实时采集日志，汇聚写入 Kafka，通过日志分发平台实时将数据分发到 ES，方便用户进行日志检索和定位问题。 Flink 优化在实际使用过程中，我们也针对业务场景进行了一些优化和扩展，主要包括： 1）Storm 任务需要 Storm 引擎提供 ack 机制保障消息传递 at least once 语义，迁移到 Flink 无法使用 ack 机制，我们通过定制 KafakSpout 实现 checkpoint 相关接口，通过 Flink checkpoint 机制实现消息传递不丢失。另外 Flink-Storm 默认只能支持 standalone 的提交方式，我们通过实现 yarn client 相关接口增加了 storm on yarn 的支持。 2）Flink 1.6 推荐的是一个 TaskManager 对应一个 slot 的使用方式，在申请资源的时候根据最大并发度申请对应数量的 TaskManger，这样导致的问题就是在任务设置 task slots 之后需要申请的资源大于实际资源。 我们通过在 ResoureManager 请求资源管理器 SlotManager 的时候增加 TaskManagerSlot 相关信息，用于维护申请到的待分配 TaskManager 和 slot，之后对于 SlotRequests 请求不是直接申请 TaskManager，而是先从 SlotManager 申请是否有足够 slot，没有才会启动新的 TaskManger, 这样就实现了申请资源等于实际消耗资源，避免任务在资源足够的情况下无法启动。 3）Kafak Connector 改造，增加自动换行支持，另外针对 08source 无法设置 client.id，通过将 client.id 生成机制优化成更有标识意义的 id，便于 Kafka 层面管控。 4）Flink 提交任务无法支持第三方依赖 jar 包和配置文件供 TaskManager 使用，我们通过修改 flink 启动脚本，增加相关参数支持外部传输文件，之后在任务启动过程中通过将对应的 jar 包和文件加入 classpath，借助 yarn 的文件管理机制实现类似 spark 对应的使用方式，方便用户使用。 5）业务场景存在大量实时写入 hdfs 需求，Flink 自带 BucketingSink 默认只支持 string 和 avro 格式，我们在此基础上同时支持了 LZO 及 Parquet 格式写入，极大提升数据写入性能。 后续规划实时计算平台当前正在进行 Storm 任务迁移 Flink 集群，目前已经基本完成，大幅提升了平台资源利用率和计算效率。后续将继续调研完善 Flink 相关能力，推动 Flink 在更多的实时场景下的应用，包括实时规则引擎，实时机器学习等。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"基于 Flink 构建关联分析引擎的挑战和实践","date":"2019-11-29T16:00:00.000Z","path":"2019/11/30/flink-aqniu/","text":"如何构建流式关联分析引擎？ 本文转载自奇安信官网，作者：奇安信集团高级研发总监韩鹏 原文地址：https://www.aqniu.com/tools-tech/59894.html 随着云计算、大数据等新一代IT技术在各行业的深入应用，政企机构IT规模和复杂程度不断提高，网络流量、日志等各类数据规模大幅提升。与此同时，网络攻防日益激烈，网络安全威胁逐渐凸显出来，这对于SOC/SIEM产品的性能提出了一个很大的挑战。因此，奇安信独立研发了国内首款流式分布式关联分析引擎Sabre，搭载于公司旗下态势感知与安全运营平台（下文简称NGSOC），从而大幅提升NGSOC的数据分析能力和网络安全检测能力。 本文将从技术研发的角度，全面阐述Sabre的由来。 1.Sabre是什么？Sabre是奇安信研发的新一代流式分布式关联分析引擎，是CEP（Complex Event Processing，复杂事件处理）技术在大数据领域的一个具体实现。奇安信研发关联引擎已有数年历史，中间经历了三次主要的技术演进，在2015年之前，奇安信使用的是基于开源CEP软件Esper研发的关联引擎，由于一些架构和设计上的问题，整体性能不是非常理想，也不支持多机扩展；在2016-2017年，用C++开发了一个高性能引擎，代号Dolphin，可以在单机上实现很高的性能；在2018年，从技术上全面转向Flink框架，极大增强了系统的可扩展性，推出了Sabre引擎作为NGSOC的核心检测引擎。 Sabre应用于奇安信的态势感知与安全运营平台（NGSOC）产品中，NGSOC主要服务于中大型政企客户，目前已经成功应用于200+大型政企机构，在国内安全管理平台市场占有率第一，其中搭载的Sabre引擎提供了核心的安全检测能力。和很多互联网公司内部自建数据处理平台不同的是，Sabre更注重的是技术的工程化交付，因此在设计和实现上和一般基于Flink的业务系统相比会有较大差异。 2.为什么要开发Sabre？随着网络应用规模和复杂度的不断提高，网络中传输的数据量急剧上升，网络攻防对抗日趋激烈，企业内部新的安全问题开始显现，实时关联分析引擎，作为NGSOC检测体系中的核心组件，也遇到了越来越多的挑战： (1) 性能优化问题。主要针对随着新型攻击的不断出现，关联分析规则规模不断上升导致的性能问题。传统开源关联引擎往往加载几十条规则即达到了性能瓶颈，而NGSOC的应用场景中，关联引擎需要支撑规模上千的关联规则。在有限的硬件资源条件下，如何避免系统整体性能随规则条数上升而发生线性下降，成为关联引擎的一个主要挑战。 (2) 规则的语义扩展问题。在网络安全事件井喷式发生的今天，安全需求迅速扩展。为了能够在有限时间内对特定语义的快速支持，关联引擎的整体架构必须异常灵活，才能适应未来安全分析场景的各种需求，而基于开源关联引擎实现的产品会在激烈的需求变化时遇到很多问题。 (3) 系统扩展性问题。主要指分布式环境下节点的扩展。随着企业网络流量和业务资产的不断扩容，NGSOC的系统处理能力必须能随企业业务规模的不断扩张而动态扩展。未来的分布式关联分析引擎需要支持数百节点的规模，以能够与现有的大数据平台无缝集成。 与Storm、Spark Streaming等流式计算框架相比，Flink具有编程接口丰富、自带多种Window算子、支持Exactly-Once、高性能分布式检查点、批流计算模式统一等优点。且Flink发展较为迅速，开源社区极为活跃，是目前最具发展潜力的流式计算框架，是未来实时计算执牛耳者。由于Flink为事件驱动的实时关联分析引擎在底层框架上提供了有力支持，因此奇安信的下一代关联分析引擎Sabre是基于Flink流式计算框架实现的。 在选择了Flink之后，发现Flink开源方案直接应用于安全检测领域，仍有很大的技术障碍。 和互联网企业内部使用的大型集群相比，NGSOC面向的企业级应用集群规模较小，硬件资源受限，且客户的定制需求较多，导致安全监测的规则要求更严格，引擎发布成本较高。但是，现有的Flink开源解决方案，或者需要根据业务需求进行改造，或者性能较差，均不能较好地解决上述问题。首先，原生Flink只提供了函数式编程模式，即需要直接编写复合特定业务需求的固定程序代码，由此导致开发测试周期较长，不便于动态更新规则，可复用性较弱，且不能从全局语义层面进行优化，性能较差。其次，Flink-CEP仅是一个受限的序列算子，在运行时需要将所有数据传输到CEP算子，然后在CEP算子中串行执行各个条件语句。这种汇集到单点的运行模式，较多的冗余数据需要执行条件匹配，产生了不必要的网络负载，而且降低了CPU利用率。再次，还存在一些非官方开源的轻量级CEP引擎，比如Flink-siddhi，功能简单，不是一个完整的解决方案。 面向企业级的网络安全监测引擎具有一些特定需求，当前解决方案对此支持较差。比如，现实情况，客户对算子实例和Taskmanager概念较为模糊，真正关心的运行状态的基本单位是规则。而Flink监控页面显示的是算子实例及Taskmanager进程整体内存的运行状态，而在网络安全监控的业务场景中，对运行状态和资源的监控均需要细化到规则层面。其次，在算子层面，Flink原生Window算子，没有较好的资源（CPU/内存）保护机制，且存在大量重复告警，不符合网络安全监测领域的业务需求。再次，Flink缺乏一些必要算子，例如不支持“不发生算子”。一个较为常见的应用场景，某条规则指定在较长时间内没收到某台服务器的系统日志，则认为此台服务器发生了异常，需要及时通知用户。 综上所述，现有解决方案应用于网络安全监测领域均会遇到问题，由此奇安信集团基于Flink构建了一种全新的CEP引擎。 3.Sabre如何处理数据？ 上图为NGSOC的数据处理架构图，展示了整个系统的数据流。自下而上，NGSOC的数据处理过程由四部分组成，其核心是由“流式分布式关联分析引擎Sabre”构成的数据处理层PROCESS，且Sabre运行的硬件环境是由多个节点组成的分布式集群。右侧的规则配置管理模块供专业的安全人员使用，可通过类Visio图的界面较为友好便捷地配置规则；规则管理模块具有添加、删除、编辑和查找规则的功能，并可批量启动/停用多个规则，规则管理模块会将处于启动状态的有效规则统一发送给Sabre引擎。最上方的绿色部分为结果处理层RESULT，Sabre会将处理结果“告警”或“关联事件”发送给下级响应模块，实现响应联动、分析调查及追踪溯源等功能。最下方的蓝色部分为日志采集层COLLECT，主要有“网络流量日志采集器”、“设备及系统日志采集器”和“其他类型的日志采集器（比如：防火墙、入侵检测系统IDS、入侵防护系统IPS、高级威胁监测系统APT等等）”三大类。中间部分为日志解析层PARSE，网络流量日志和系统安全日志格式多种多样，须将上述两类原始日志数据格式化，而其他类型的日志（比如：威胁情报、漏洞、资产）本身即为格式化数据，最终所有格式化数据均需统一存储到高性能消息队列Kafka。 4.Sabre的关键技术(1) 系统架构 上图为Sabre系统整体架构图。Sabre整体架构包含三大核心模块，中间是Sabre-server，左侧是配置端，右侧是Sabre运行端。核心数据流存在两条主线，红线表示规则的提交、编译、发布和运行流程。绿线表示状态监控的生成、收集、统计和展示流程。如图所示，此架构与Hive极为相似，是一种通用的大数据OLAP系统架构。下面详细介绍三大核心模块和两大核心数据流。 首先，通过规则配置端创建规则，采用性能保护配置端修改性能保护策略，然后将任务所属的规则文件和性能保护策略文件一并推送到Sabre-server提供的REST接口，该接口会调用文件解析及优化方法构建规则有向无环图。接着执行词法语法分析方法，将规则有向无环图中各个节点的EPL转换为与其对应的AST（Abstract Syntax Tree，抽象语法树），再将AST翻译为任务java代码。最后调用maven命令打包java代码为任务jar包，并将任务jar包及基础运行库一并提交到Flink-on-YARN集群。 Flink有多种运行模式（例如 standalone Flink cluster、Flink cluster on YARN、Flink job on YARN等），Sabre采用了“Flink job on YARN”模式，在奇安信NGSOC应用的特定场景下，采用YARN可统一维护硬件资源，并且使用 Flink job on YARN 可与Hadoop平台进行无缝对接，以此实现了很好的任务间资源隔离。 在Sabre任务执行过程中，Kafka数据源向引擎提供原始事件。引擎处理结果分为回注事件和告警事件两类。告警事件会直接输出到目的Kafka，供下级应用消费。回注事件表示一条规则的处理结果可直接回注到下级规则，作为下级规则的数据源事件，由此可实现规则的相互引用。 绿线流程表示任务执行过程中会定时输出节点的运行监控消息到Sabre-server的监控消息缓存器，然后监控消息统计器再汇总各个规则实例的运行监控消息，统计为整条规则的运行监控状态，最后通过Sabre-server提供的REST接口推送给规则监控端。 (2) 功能设计 算子的设计和实现是构建CEP的重要组成部分。 上图展示了Flink和Sabre算子的比较关系。包含三列：Flink原生算子、Sabre算子、两者之间的比较结果（相同、实现、优化、新增）。Sabre共有13种完全自研的核心算子，其中Datasource、CustomKafkaSink和CustomDatabase按照 Flink接口要求做了具体实现，Filter、Key、Join和Aggregation按照Flink原有算子的语义做了重新实现，CustomWindow和Sequence在Flink原有算子语义的基础上做了优化实现。 由于Flink原有FilterFunction算子只能简单返回布尔值，以致输出结果的控制能力较差，而重新实现的Filter算子可同时执行多种业务逻辑，将一个“原始事件”输出一个或多个“处理事件”。Sabre还实现了一种针对窗口的全局触发器Trigger，Trigger能够将多个子计算性算子组合为复杂表达式，并实现了具有GroupBy/Distinct功能的Key算子以适配此Trigger算子。众所周知，Join和Aggregation的时间范围由Window限定，而Flink原有Window算子不适合网络安全监测需求，为此Sabre设计了一种“自定义Window算子”，且重新实现了与“自定义Window算子”相匹配的Join和Aggregation算子。 上图展示了Sabre算子间的关联关系。序列Sequence、聚合Aggregation、不发生NotOccur、流式机器学习StreamML和连接Join均属于Window执行时间包含的计算性算子。蓝色虚线表示引用动态数据，紫色虚线表示Filter无须经过Window可直连输出组件。 如上图所示，为满足复杂场景需求，一种规则的输出可直接作为另一种规则的输入。通过这种规则拆分的方式，能分层构造较为复杂的“多级规则”。如：上面的“暴力探测”规则结果可以直接回注到下面的“登陆成功 ”规则，而无须额外的通信组件。 (3) 性能优化 因为采用了Flink作为底层运行组件，所以Sabre具有与Flink等同的执行性能。并且，针对网络安全监测领域的特定需求，还做了如下的性能优化工作： 1）全局组件（数据源、动态表）引用优化。由于Kafka类型的数据源topic有限，而规则数量可动态扩展，导致多个规则会有极大概率共用同一个数据源，根据EPL语义等价原则合并相同的数据源，进而可以减少数据输入总量及线程总数。 2）全新的匹配引擎。序列Sequence算子采用了新颖的流式状态机引擎，复用了状态机缓存的状态，提升了匹配速度。类似优化还包含大规模IP匹配引擎和大规模串匹配引擎。 3）表计算表达式优化。对于规则中引用的动态表，会根据表达式的具体特性构建其对应的最优计算数据结构，以避免扫描全表数据，进而确保了执行的时间复杂度为常量值。 4）自定义流式Window算子。采用“时间槽”技术实现了乱序纠正功能，并具有可以实时输出无重复、无遗漏告警的特性。 5）字段自动推导，优化事件结构。根据规则前后逻辑关系，推导出规则中标注使用的原始日志相关字段，无须输出所有字段，以此优化输出事件结构，减少输出事件大小。 6）数据分区自动推导，优化流拓扑。由于功能需要，Window往往会缓存大量数据，以致消耗较多内存。通过对全局窗口Hash优化，避免所有全局窗口都分配到同一个Taskmanager进程，由此提高了引擎整体内存的利用率。 (4) 机器学习 机器学习在网络异常检测上已经越来越重要，为适应实时检测的需求，Sabre没有使用Flink MachineLearning，而是引入了自研的流式机器学习算子StreamML。Flink MachineLearning是一种基于批模式DataSetApi实现的机器学习函数库，而StreamML是一种流式的机器学习算子，其目的是为了满足网络安全监测的特定需求。与阿里巴巴开源的Alink相比，StreamML允许机器学习算法工程师通过配置规则的方式即可快速验证算法模型，无需编写任何程序代码。并且，流式机器学习算子StreamML实现了“模型训练/更新”与“模型使用”统一的理念。其核心功能是通过算法、技术及模型实现数据训练及对新数据检测。该流式机器学习算子StreamML引入的输入有三类，分别是：事件流、检测对象和对象属性；输出也包含三类，分别是：事件、告警和预警。 流式机器学习算子StreamML的组件栈包含三部分，从下往上依次为：机器学习方法、应用场景和产品业务。通过基本的机器学习算法（比如：统计学习算法、序列分析算法、聚类分析算法），流式机器学习算子StreamML可满足具体特定的安全监测应用场景（比如：行为特征异常检测、时间序列异常检测、群组聚类分析），进而为用户提供可理解的产品业务（比如：基线、用户及实体行为分析UEBA）。 行为特征异常检测：根据采集的样本数据（长时间）对统计分析对象建立行为基线，并以此基线为准，检测发现偏离正常行为模式的行为。例如：该用户通常从哪里发起连接？哪个运营商？哪个国家？哪个地区？这个用户行为异常在组织内是否为常见异常？ 时间序列异常检测：根据某一个或多个统计属性，判断按时间顺序排列的数值序列是否异常，由此通过监测指标变化来发现安全事件。例如：监测某网站每小时的访问量以防止DDOS攻击；建模每个账号传输文件大小的平均值，检测出传输文件大小的平均值离群的账号。 群组聚类分析：对数据的特征属性间潜在相关性进行挖掘，将具有类似特征值的数据进行分组聚类。例如：该用户是否拥有任何特殊特征？可执行权限/特权用户？基于执行的操作命令和可访问的实体，来识别IT管理员、DBA和其它高权限用户。 5.Sabre如何快速适配复杂的客户环境?由于客户规模较大，项目种类较多，部署环境较为复杂，或者存在多种Yarn集群版本，或者Sabre需作为单一Flink应用发布到客户已部署的Flink集群。如何节省成本及提高实施效率，快速适配上述复杂的部署环境是个亟需解决的问题，为此Sabre的设计原则是仅采用Flink的分布式计算能力，业务代码尽可能减少对API层的依赖，以便于兼容多种Flink版本。如图所示，Deploy、Core、APIs、Libraries四层是大家熟知的Flink基本的组件栈。Sabre对API层的依赖降到了最低，只引用了DataStream、KeyedStream和SplitStream三种数据流API。函数依赖则包括DataStream的assignTimestamps、flatMap、union、keyBy、split、process、addSink等函数，KeyedStream最基础的process函数，以及SplitStream的select函数。由于依赖的Flink API较少，Sabre可以很容易适配到各个Flink版本，从而具有良好的Flink版本兼容性。 6.如何保障Sabre稳定运行？为减少引擎的维护成本，需要保障引擎在超限数据量的条件下亦然能够稳定运行，Sabre主要做了两个优化：流量控制和自我保护。 为了增强Sabre引擎的健壮性，避免因规则配置错误，导致生成大量无效告警，在输出端做了流量控制，以更好地保护下级应用。当下级抗压能力较弱时（例如数据库），整个系统会做输出降级。 另一个问题是，跑在JVM上的程序，经常会遇到由于长时间 Full GC导致OOM的错误，并且此时CPU占用率往往非常高，Flink同样存在上述问题。自我保护功能采用了同时兼顾“Window隶属规则的优先级”及“Window引用规则数量”两个条件的加权算法，以此根据全局规则语义实现自动推导Window优先级，并根据此优先级确定各个Window的自我保护顺序。实时监控CPU及内存占用，当超过一定阈值时，智能优化事件分布，以防出现CPU长期过高或内存使用率过大而导致的OOM问题。 目前，基于Flink构建的Sabre引擎还在继续开发新的功能，并会持续优化引擎性能。未来将总结凝练项目中的优秀实践，并及时回馈给Apache Flink社区。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《大数据实时计算引擎 Flink 实战与性能优化》目录大纲","date":"2019-11-23T16:00:00.000Z","path":"2019/11/24/flink-in-action-directory/","text":"基于 Flink 1.9 讲解的书籍目录大纲，含 Flink 入门、概念、原理、实战、性能调优、源码解析等内容。涉及 Flink Connector、Metrics、Library、DataStream API、Table API &amp; SQL 等内容的学习案例，还有 Flink 落地应用的大型项目案例分享。 书籍和专栏同时在进行，扫码下面专栏二维码可以订阅专栏，提前查看书籍内容。 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 书籍目录大纲1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274281预备篇第一章——实时计算引擎 1.1你的公司是否需要引入实时计算引擎 1.1.1 实时计算需求 1.1.2 数据实时采集 1.1.3 数据实时计算 1.1.4 数据实时下发 1.1.5 实时计算场景 1.1.6 离线计算 vs 实时计算 1.1.7 实时计算面临的挑战 1.1.8 小结与反思 1.2彻底了解大数据实时计算框架 Flink 1.2.1 Flink 简介 1.2.2 Flink 整体架构 1.2.3 Flink 的多种方式部署 1.2.4 Flink 分布式运行流程 1.2.5 Flink API 1.2.6 Flink 程序与数据流结构 1.2.7 丰富的 Connector 1.2.8 事件时间&amp;处理时间语义 1.2.9 灵活的窗口机制 1.2.10 并行执行任务机制 1.2.11 状态存储和容错 1.2.12 自己的内存管理机制 1.2.13 多种扩展库 1.2.14 小结与反思 1.3大数据计算框架对比 1.3.1 Flink 1.3.2 Blink 1.3.3 Spark 1.3.4 Spark Streaming 1.3.5 Structured Streaming 1.3.6 Flink VS Spark 1.3.7 Storm 1.3.8 Flink VS Storm 1.3.9 全部对比结果 1.3.10 小结与反思 1.4总结2第二章——Flink 入门 2.1Flink 环境准备 2.1.1 JDK 安装与配置 2.1.2 Maven 安装与配置 2.1.3 IDE 安装与配置 2.1.4 MySQL 安装与配置 2.1.5 Kafka 安装与配置 2.1.6 ElasticSearch 安装与配置 2.1.7 小结与反思 2.2Flink 环境搭建 2.2.1 Flink 下载与安装 2.2.2 Flink 启动与运行 2.2.3 Flink 目录配置文件解读 2.2.4 Flink 源码下载 2.2.5 Flink 源码编译 2.2.6 将 Flink 源码导入到 IDE 2.2.7 小结与反思 2.3案例1：WordCount 应用程序 2.3.1 使用 Maven 创建项目 2.3.2 使用 IDEA 创建项目 2.3.3 流计算 WordCount 应用程序代码实现 2.3.4 运行流计算 WordCount 应用程序 2.3.5 流计算 WordCount 应用程序代码分析 2.3.6 小结与反思 2.4案例2：实时处理 Socket 数据 2.4.1 使用 IDEA 创建项目 2.4.2 实时处理 Socket 数据应用程序代码实现 2.4.3 运行实时处理 Socket 数据应用程序 2.4.4 实时处理 Socket 数据应用程序代码分析 2.4.5 Flink 中使用 Lambda 表达式 2.4.5 小结与反思 2.5总结 2基础篇3第三章——Flink 中的流计算处理 3.1Flink 多种时间语义对比 3.1.1 Processing Time 3.1.2 Event Time 3.1.3 Ingestion Time 3.1.4 三种 Time 的对比结果 3.1.5 使用场景分析 3.1.6 Time 策略设置 3.1.7 小结与反思 3.2Flink Window 基础概念与实现原理 3.2.1 Window 简介 3.2.2 Window 有什么作用？ 3.2.3 Flink 自带的 Window 3.2.4 Time Window 的用法及源码分析 3.2.5 Count Window 的用法及源码分析 3.2.6 Session Window 的用法及源码分析 3.2.7 如何自定义 Window？ 3.2.8 Window 源码分析 3.2.9 Window 组件之 WindowAssigner 的用法及源码分析 3.2.10 Window 组件之 Trigger 的用法及源码分析 3.2.11 Window 组件之 Evictor 的用法及源码分析 3.2.12 小结与反思 3.3必须熟悉的数据转换 Operator(算子) 3.3.1 DataStream Operator 3.3.2 DataSet Operator 3.3.3 流计算与批计算统一的思路 3.3.4 小结与反思 3.4使用 DataStream API 来处理数据 3.4.1 DataStream 的用法及分析 3.4.2 SingleOutputStreamOperator 的用法及分析 3.4.3 KeyedStream 的用法及分析 3.4.4 SplitStream 的用法及分析 3.4.5 WindowedStream 的用法及分析 3.4.6 AllWindowedStream 的用法及分析 3.4.7 ConnectedStreams 的用法及分析 3.4.8 BroadcastStream 的用法及分析 3.4.9 BroadcastConnectedStream 的用法及分析 3.4.10 QueryableStateStream 的用法及分析 3.4.11 小结与反思 3.5Watermark 的用法和结合 Window 处理延迟数据 3.5.1 Watermark 简介 3.5.2 Flink 中的 Watermark 的设置 3.5.3 Punctuated Watermark 3.5.4 Periodic Watermark 3.5.5 每个 Kafka 分区的时间戳 3.5.6 将 Watermark 与 Window 结合起来处理延迟数据 3.5.7 处理延迟数据的三种方法 3.5.8 小结与反思 3.6Flink 常用的 Source Connector 和 Sink Connector 介绍 3.6.1 Data Source 简介 3.6.2 常用的 Data Source 3.6.3 Data Sink 简介 3.6.4 常用的 Data Sink 3.6.5 小结与反思 3.7Flink Connector —— Kafka 的使用和源码分析 3.7.1 准备环境和依赖 3.7.2 将测试数据发送到 Kafka Topic 3.7.3 Flink 如何消费 Kafka 数据？ 3.7.4 Flink 如何将计算后的数据发送到 Kafka？ 3.7.5 FlinkKafkaConsumer 源码分析 3.7.6 FlinkKafkaProducer 源码分析 3.7.7 使用 Flink-connector-kafka 可能会遇到的问题 3.7.8 小结与反思 3.8自定义 Flink Connector 3.8.1 自定义 Source Connector 3.8.2 RichSourceFunction 的用法及源码分析 3.8.3 自定义 Sink Connector 3.8.4 RichSinkFunction 的用法及源码分析 3.8.5 小结与反思 3.9Flink Connector —— ElasticSearch 的用法和分析 3.9.1 准备环境和依赖 3.9.2 使用 Flink 将数据写入到 ElasticSearch 应用程序 3.9.3 验证数据是否写入 ElasticSearch？ 3.9.4 如何保证在海量数据实时写入下 ElasticSearch 的稳定性？ 3.9.5 使用 Flink-connector-elasticsearch 可能会遇到的问题 3.9.6 小结与反思 3.10Flink Connector —— HBase 的用法 3.10.1 准备环境和依赖 3.10.2 Flink 使用 TableInputFormat 读取 HBase 批量数据 3.10.3 Flink 使用 TableOutputFormat 向 HBase 写入数据 3.10.4 Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据 3.10.5 项目运行及验证 3.10.6 小结与反思 3.11Flink Connector —— Redis 的用法 3.11.1 安装 Redis 3.11.2 将商品数据发送到 Kafka 3.11.3 Flink 消费 Kafka 中的商品数据 3.11.4 Redis Connector 简介 3.11.5 Flink 写入数据到 Redis 3.11.6 项目运行及验证 3.11.7 小结与反思 3.12使用 Side Output 分流 3.12.1 使用 Filter 分流 3.12.2 使用 Split 分流 3.12.3 使用 Side Output 分流 3.12.4 小结与反思 3.13总结 3进阶篇4第四章——Flink 中的状态及容错机制 4.1深度讲解 Flink 中的状态 4.1.1 为什么需要 State？ 4.1.2 State 的种类 4.1.3 Keyed State 4.1.4 Operator State 4.1.5 Raw and Managed State 4.1.6 如何使用托管的 Keyed State 4.1.7 State TTL(存活时间) 4.1.8 如何使用托管的 Operator State 4.1.9 Stateful Source Functions 4.1.10 Broadcast State 4.1.11 Queryable State 4.1.12 小结与反思 4.2Flink 状态后端存储 4.2.1 State Backends 4.2.2 MemoryStateBackend 的用法及分析 4.2.3 FsStateBackend 的用法及分析 4.2.4 RocksDBStateBackend 的用法及分析 4.2.5 如何选择状态后端存储？ 4.2.6 小结与反思 4.3Flink Checkpoint 和 Savepoint 的区别及其配置使用 4.3.1 Checkpoint 简介及使用 4.3.2 Savepoint 简介及使用 4.3.3 Savepoint 与 Checkpoint 的区别 4.3.4 Checkpoint 流程 4.3.5 如何从 Checkpoint 中恢复状态 4.3.6 如何从 Savepoint 中恢复状态 4.3.6 小结与反思 4.4总结 5第五章——Table API &amp; SQL 5.1Flink Table &amp; SQL 概念与通用 API 5.1.1 新增 Blink SQL 查询处理器 5.1.2 为什么选择 Table API &amp; SQL？ 5.1.3 Flink Table 项目模块 5.1.4 两种 planner 之间的区别 5.1.5 添加项目依赖 5.1.6 创建一个 TableEnvironment 5.1.7 Table API &amp; SQL 应用程序的结构 5.1.8 Catalog 中注册 Table 5.1.9 注册外部的 Catalog 5.1.10 查询 Table 5.1.11 提交 Table 5.1.12 翻译并执行查询 5.1.13 小结与反思 5.2Flink Table API &amp; SQL 功能 5.2.1 Flink Table 和 SQL 与 DataStream 和 DataSet 集成 5.2.2 查询优化 5.2.3 数据类型 5.2.4 时间属性 5.2.5 SQL Connector 5.2.6 SQL Client 5.2.7 Hive 5.2.8 小结与反思 5.3总结 6第六章——扩展库 6.1Flink CEP 简介及其使用场景 6.1.1 CEP 简介 6.1.2 规则引擎对比 6.1.3 Flink CEP 简介 6.1.4 Flink CEP 动态更新规则 6.1.5 Flink CEP 使用场景分析 6.1.6 小结与反思 6.2使用 Flink CEP 处理复杂事件 6.2.1 准备依赖 6.2.2 Flink CEP 入门应用程序 6.2.3 Pattern API 6.2.4 检测 Pattern 6.2.5 CEP 时间属性 6.2.6 小结与反思 6.3Flink 扩展库——State Processor API 6.3.1 State Processor API 简介 6.3.2 在 Flink 1.9 之前是如何处理状态的？ 6.3.3 使用 State Processor API 读写作业状态 6.3.4 使用 DataSet 读取作业状态 6.3.5 为什么要使用 DataSet API？ 6.3.6 小结与反思 6.4Flink 扩展库——Machine Learning 6.4.1 Flink-ML 简介 6.4.2 使用 Flink-ML 6.4.3 使用 Flink-ML Pipeline 6.4.4 小结与反思 6.5Flink 扩展库——Gelly 6.5.1 Gelly 简介 6.5.2 使用 Gelly 6.5.3 Gelly API 6.5.4 小结与反思 6.6 总结4高级篇7第七章——Flink 作业环境部署 7.1Flink 配置详解及如何配置高可用？ 7.1.1 Flink 配置详解 7.1.2 Log 的配置 7.1.3 如何配置 JobManager 高可用？ 7.1.4 小结与反思 7.2Flink 作业如何在 Standalone、YARN、Mesos、K8S 上部署运行？ 7.2.1 Standalone 7.2.2 YARN 7.2.3 Mesos 7.3.4 Kubernetes 7.2.5 小结与反思 7.3总结 8第八章——Flink 监控 8.1实时监控 Flink 及其作业 8.1.1 监控 JobManager 8.1.2 监控 TaskManager 8.1.3 监控 Flink 作业 8.1.4 最关心的性能指标 8.1.5 小结与反思 8.2搭建一套 Flink 监控系统 8.2.1 利用 API 获取监控数据 8.2.2 Metrics 类型简介 8.2.3 利用 JMXReporter 获取监控数据 8.2.4 利用 PrometheusReporter 获取监控数据 8.2.5 利用 PrometheusPushGatewayReporter 获取监控数据 8.2.6 利用 InfluxDBReporter 获取监控数据 8.2.7 安装 InfluxDB 和 Grafana 8.2.8 配置 Grafana 展示监控数据 8.2.9 小结与反思 8.3总结9第九章——Flink 性能调优 9.1如何处理 Flink Job Backpressure （反压）问题？ 9.1.1 Flink 流处理为什么需要网络流控 9.1.2 Flink 1.5 之前的网络流控机制 9.1.3 基于 Credit 的反压机制 9.1.4 定位产生反压的位置 9.1.5 分析和处理反压问题 9.1.6 小结与反思 9.2如何查看 Flink 作业执行计划？ 9.2.1 如何获取执行计划 JSON？ 9.2.2 生成执行计划图 9.2.3 深入探究 Flink 作业执行计划 9.2.4 Flink 中算子 chain 起来的条件 9.2.5 如何禁止 Operator chain？ 9.2.6 小结与反思 9.3Flink Parallelism 和 Slot 深度理解 9.3.1 Parallelism 简介 9.3.2 如何设置 Parallelism？ 9.3.3 Slot 简介 9.3.4 Slot 和 Parallelism 的关系 9.3.5 可能会遇到 Slot 和 Parallelism 的问题 9.3.6 小结与反思 9.4如何合理的设置 Flink 作业并行度？ 9.4.1 Source 端并行度的配置 9.4.2 中间 Operator 并行度的配置 9.4.3 Sink 端并行度的配置 9.4.4 Operator Chain 9.4.5 小结与反思 9.5Flink 中如何保证 Exactly Once？ 9.5.1 Flink 内部如何保证 Exactly Once？ 9.5.2 端对端如何保证 Exactly Once？ 9.5.3 分析 FlinkKafkaConsumer 的设计思想 9.5.4 小结与反思 9.6如何处理 Flink 中数据倾斜问题？ 9.6.1 数据倾斜简介 9.6.2 判断是否存在数据倾斜 9.6.3 分析和解决数据倾斜问题 9.6.4 小结与反思 9.7总结10第十章——Flink 最佳实践 10.1如何设置 Flink Job RestartStrategy（重启策略）？ 10.1.1 常见错误导致 Flink 作业重启 10.1.2 RestartStrategy 简介 10.1.3 为什么需要 RestartStrategy？ 10.1.4 如何配置 RestartStrategy？ 10.1.5 RestartStrategy 源码分析 10.1.6 Failover Strategies（故障恢复策略） 10.1.7 小结与反思 10.2如何使用 Flink ParameterTool 读取配置？ 10.2.1 Flink Job 配置 10.2.2 ParameterTool 管理配置 10.2.3 ParameterTool 源码分析 10.2.4 小结与反思 10.3总结 5实战篇11第十一章——Flink 实战 11.1如何统计网站各页面一天内的 PV 和 UV？ 11.1.1 统计网站各页面一天内的 PV 11.1.2 统计网站各页面一天内 UV 的三种方案 11.1.3 小结与反思 11.2如何使用 Flink ProcessFunction 处理宕机告警? 11.2.1 ProcessFunction 简介 11.2.2 CoProcessFunction 简介 11.2.3 Timer 简介 11.2.4 如果利用 ProcessFunction 处理宕机告警？ 11.2.5 小结与反思 11.3如何利用 Async I／O 读取告警规则？ 11.3.1 为什么需要 Async I/O？ 11.3.2 Async I/O API 11.3.3 利用 Async I/O 读取告警规则需求分析 11.3.4 如何使用 Async I/O 读取告警规则数据 11.3.5 小结与反思 11.4如何利用广播变量动态更新告警规则？ 11.4.1 BroadcastVariable 简介 11.4.2 如何使用 BroadcastVariable ？ 11.4.3 利用广播变量动态更新告警规则数据需求分析 11.4.4 读取告警规则数据 11.4.5 监控数据连接规则数据 11.4.6 小结与反思 11.5如何实时将应用 Error 日志告警？ 11.5.1 日志处理方案的演进 11.5.2 日志采集工具对比 11.5.3 日志结构设计 11.5.4 异常日志实时告警项目架构 11.5.5 日志数据发送到 Kafka 11.5.6 Flink 实时处理日志数据 11.5.7 处理应用异常日志 11.5.8 小结与反思 11.6总结 6案例篇12第十二章——Flink 案例 12.1基于 Flink 实时处理海量日志 12.2.1 实时处理海量日志需求分析 12.2.2 实时处理海量日志架构设计 12.2.3 日志实时采集 12.2.4 日志格式统一 12.2.5 日志实时清洗 12.2.6 日志实时告警 12.2.7 日志实时存储 12.2.8 日志实时展示 12.2.9 小结与反思 12.2基于 Flink 的百亿数据实时去重 12.2.1 去重的通用解决方案 12.2.2 使用 BloomFilter 实现去重 12.2.3 使用 HBase 维护全局 Set 实现去重 12.2.4 使用 Flink 的 KeyedState 实现去重 12.2.5 使用 RocksDBStateBackend 的优化方法 12.2.6 小结与反思 12.3基于 Flink 的实时监控告警系统 12.3.1 监控系统的诉求 12.3.2 监控系统包含的内容 12.3.3 Metrics／Trace／Log 数据实时采集 12.3.4 消息队列如何撑住高峰流量 12.3.5 指标数据实时计算 12.3.6 提供及时且准确的根因分析告警 12.3.7 AIOps 智能运维道路探索 12.3.8 如何保障高峰流量实时写入存储系统的稳定性 12.3.9 监控数据使用可视化图表展示 12.3.10 小结与反思 12.4总结","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"数据仓库简介、发展、架构演进、实时数仓建设、与离线数仓对比","date":"2019-11-22T16:00:00.000Z","path":"2019/11/23/real-time-warehouse/","text":"数据仓库也是公司数据发展到一定规模后必然会提供的一种基础服务，数据仓库的建设也是“数据智能”中必不可少的一环。本文将从数据仓库的简介、经历了怎样的发展、如何建设、架构演变、应用案例以及实时数仓与离线数仓的对比六个方面全面分享关于数仓的详细内容。 本文作者：郭华（付空） 原地地址：https://ververica.cn/developers/how-to-do-real-time-counting/ 1. 数据仓库简介数据仓库是一个面向主题的（Subject Oriented）、集成的（Integrate）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策。 数据仓库是伴随着企业信息化发展起来的，在企业信息化的过程中，随着信息化工具的升级和新工具的应用，数据量变的越来越大，数据格式越来越多，决策要求越来越苛刻，数据仓库技术也在不停的发展。 数据仓库的趋势： 实时数据仓库以满足实时化&amp;自动化决策需求； 大数据&amp;数据湖以支持大量&amp;复杂数据类型（文本、图像、视频、音频）； 2. 数据仓库的发展数据仓库有两个环节：数据仓库的构建与数据仓库的应用。 早期数据仓库构建主要指的是把企业的业务数据库如 ERP、CRM、SCM 等数据按照决策分析的要求建模并汇总到数据仓库引擎中，其应用以报表为主，目的是支持管理层和业务人员决策（中长期策略型决策）。 随着业务和环境的发展，这两方面都在发生着剧烈变化。 随着IT技术走向互联网、移动化，数据源变得越来越丰富，在原来业务数据库的基础上出现了非结构化数据，比如网站 log，IoT 设备数据，APP 埋点数据等，这些数据量比以往结构化的数据大了几个量级，对 ETL 过程、存储都提出了更高的要求； 互联网的在线特性也将业务需求推向了实时化，随时根据当前客户行为而调整策略变得越来越常见，比如大促过程中库存管理，运营管理等（即既有中远期策略型，也有短期操作型）；同时公司业务互联网化之后导致同时服务的客户剧增，有些情况人工难以完全处理，这就需要机器自动决策。比如欺诈检测和用户审核。 总结来看，对数据仓库的需求可以抽象成两方面：实时产生结果、处理和保存大量异构数据。 注：这里不讨论数据湖技术。 3. 数据仓库建设方法论3.1 面向主题从公司业务出发，是分析的宏观领域，比如供应商主题、商品主题、客户主题和仓库主题 3.2 为多维数据分析服务数据报表；数据立方体，上卷、下钻、切片、旋转等分析功能。 3.3 反范式数据模型以事实表和维度表组成的星型数据模型 4. 数据仓库架构的演变数据仓库概念是 Inmon 于 1990 年提出并给出了完整的建设方法。随着互联网时代来临，数据量暴增，开始使用大数据工具来替代经典数仓中的传统工具。此时仅仅是工具的取代，架构上并没有根本的区别，可以把这个架构叫做离线大数据架构。 后来随着业务实时性要求的不断提高，人们开始在离线大数据架构基础上加了一个加速层，使用流处理技术直接完成那些实时性要求较高的指标计算，这便是 Lambda 架构。 再后来，实时的业务越来越多，事件化的数据源也越来越多，实时处理从次要部分变成了主要部分，架构也做了相应调整，出现了以实时事件处理为核心的 Kappa 架构。 4.1 离线大数据架构数据源通过离线的方式导入到离线数仓中。下游应用根据业务需求选择直接读取 DM 或加一层数据服务，比如 MySQL 或 Redis。数据仓库从模型层面分为三层： ODS，操作数据层，保存原始数据； DWD，数据仓库明细层，根据主题定义好事实与维度表，保存最细粒度的事实数据； DM，数据集市/轻度汇总层，在 DWD 层的基础之上根据不同的业务需求做轻度汇总； 典型的数仓存储是 HDFS/Hive，ETL 可以是 MapReduce 脚本或 HiveSQL。 4.2 Lambda 架构随着大数据应用的发展，人们逐渐对系统的实时性提出了要求，为了计算一些实时指标，就在原来离线数仓的基础上增加了一个实时计算的链路，并对数据源做流式改造（即把数据发送到消息队列），实时计算去订阅消息队列，直接完成指标增量的计算，推送到下游的数据服务中去，由数据服务层完成离线&amp;实时结果的合并。 注：流处理计算的指标批处理依然计算，最终以批处理为准，即每次批处理计算后会覆盖流处理的结果。（这仅仅是流处理引擎不完善做的折中） Lambda 架构问题： 同样的需求需要开发两套一样的代码：这是 Lambda 架构最大的问题，两套代码不仅仅意味着开发困难（同样的需求，一个在批处理引擎上实现，一个在流处理引擎上实现，还要分别构造数据测试保证两者结果一致），后期维护更加困难，比如需求变更后需要分别更改两套代码，独立测试结果，且两个作业需要同步上线。 资源占用增多：同样的逻辑计算两次，整体资源占用会增多（多出实时计算这部分 4.3 Kappa 架构Lambda 架构虽然满足了实时的需求，但带来了更多的开发与运维工作，其架构背景是流处理引擎还不完善，流处理的结果只作为临时的、近似的值提供参考。后来随着 Flink 等流处理引擎的出现，流处理技术很成熟了，这时为了解决两套代码的问题，LickedIn 的 Jay Kreps 提出了 Kappa 架构。 Kappa 架构可以认为是 Lambda 架构的简化版（只要移除 lambda 架构中的批处理部分即可）。 在 Kappa 架构中，需求修改或历史数据重新处理都通过上游重放完成。 Kappa 架构最大的问题是流式重新处理历史的吞吐能力会低于批处理，但这个可以通过增加计算资源来弥补。 Kappa 架构的重新处理过程： 重新处理是人们对 Kappa 架构最担心的点，但实际上并不复杂： 选择一个具有重放功能的、能够保存历史数据并支持多消费者的消息队列，根据需求设置历史数据保存的时长，比如 Kafka，可以保存全部历史数据。 当某个或某些指标有重新处理的需求时，按照新逻辑写一个新作业，然后从上游消息队列的最开始重新消费，把结果写到一个新的下游表中。 当新作业赶上进度后，应用切换数据源，读取 2 中产生的新结果表。 停止老的作业，删除老的结果表。 4.4 Lambda 架构与 Kappa 架构的对比 在真实的场景中，很多时候并不是完全规范的 Lambda 架构或 Kappa 架构，可以是两者的混合，比如大部分实时指标使用 Kappa 架构完成计算，少量关键指标（比如金额相关）使用 Lambda 架构用批处理重新计算，增加一次校对过程。 Kappa 架构并不是中间结果完全不落地，现在很多大数据系统都需要支持机器学习（离线训练），所以实时中间结果需要落地对应的存储引擎供机器学习使用，另外有时候还需要对明细数据查询，这种场景也需要把实时明细层写出到对应的引擎中。参考后面的案例。 另外，随着数据多样性的发展，数据仓库这种提前规定 schema 的模式显得越来难以支持灵活的探索&amp;分析需求，这时候便出现了一种数据湖技术，即把原始数据全部缓存到某个大数据存储上，后续分析时再根据需求去解析原始数据。简单的说，数据仓库模式是 schema on write，数据湖模式是 schema on read。 5. 实时数仓案例菜鸟仓配实时数据仓库本案例参考自菜鸟仓配团队的分享，涉及全局设计、数据模型、数据保障等几个方面。 注：特别感谢缘桥同学的无私分享。 5.1 整体设计整体设计如下图，基于业务系统的数据，数据模型采用中间层的设计理念，建设仓配实时数仓；计算引擎，选择更易用、性能表现更佳的实时计算作为主要的计算引擎；数据服务，选择天工数据服务中间件，避免直连数据库，且基于天工可以做到主备链路灵活配置秒级切换；数据应用，围绕大促全链路，从活动计划、活动备货、活动直播、活动售后、活动复盘五个维度，建设仓配大促数据体系。 5.2 数据模型不管是从计算成本，还是从易用性，还是从复用性，还是从一致性等等，我们都必须避免烟囱式的开发模式，而是以中间层的方式建设仓配实时数仓。与离线中间层基本一致，我们将实时中间层分为两层。 第一层 DWD 公共实时明细层 实时计算订阅业务数据消息队列，然后通过数据清洗、多数据源 join、流式数据与离线维度信息等的组合，将一些相同粒度的业务系统、维表中的维度属性全部关联到一起，增加数据易用性和复用性，得到最终的实时明细数据。这部分数据有两个分支，一部分直接落地到 ADS，供实时明细查询使用，一部分再发送到消息队列中，供下层计算使用； 第二层 DWS 公共实时汇总层 以数据域+业务域的理念建设公共汇总层，与离线数仓不同的是，这里汇总层分为轻度汇总层和高度汇总层，并同时产出，轻度汇总层写入 ADS，用于前端产品复杂的 olap 查询场景，满足自助分析和产出报表的需求；高度汇总层写入 Hbase，用于前端比较简单的 kv 查询场景，提升查询性能，比如实时大屏等； 注： ADS 是一款提供 OLAP 分析服务的引擎。开源提供类似功能的有，Elastic Search、Kylin、Druid 等； 案例中选择把数据写入到 Hbase 供 KV 查询，也可根据情况选择其他引擎，比如数据量不多，查询压力也不大的话，可以用 MySQL； 因主题建模与业务关系较大，这里不做描述； 5.3 数据保障阿里巴巴每年都有双十一等大促，大促期间流量与数据量都会暴增。实时系统要保证实时性，相对离线系统对数据量要更敏感，对稳定性要求更高。所以为了应对这种场景，还需要在这种场景下做两种准备： 大促前的系统压测； 大促中的主备链路保障； 菜鸟双11「仓储配送数据实时化」详情了解：https://yq.aliyun.com/articles/658787 6. 实时数仓与离线数仓的对比在看过前面的叙述与菜鸟案例之后，我们看一下实时数仓与离线数仓在几方面的对比： 首先，从架构上，实时数仓与离线数仓有比较明显的区别，实时数仓以 Kappa 架构为主，而离线数仓以传统大数据架构为主。Lambda 架构可以认为是两者的中间态。 其次，从建设方法上，实时数仓和离线数仓基本还是沿用传统的数仓主题建模理论，产出事实宽表。另外实时数仓中实时流数据的 join 有隐藏时间语义，在建设中需注意。 最后，从数据保障看，实时数仓因为要保证实时性，所以对数据量的变化较为敏感。在大促等场景下需要提前做好压测和主备保障工作，这是与离线数据的一个较为明显的区别。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"详解 Flink Metrics 原理与监控实战","date":"2019-11-22T16:00:00.000Z","path":"2019/11/23/flink-metrics/","text":"本文主要讲解 Metrics、如何使用 Metrics 分析问题并解决问题，并对 Metrics 监控实战进行解释说明。 本文作者：Apache Flink Contributor 刘彪 什么是 Metrics？Flink 提供的 Metrics 可以在 Flink 内部收集一些指标，通过这些指标让开发人员更好地理解作业或集群的状态。由于集群运行后很难发现内部的实际状况，跑得慢或快，是否异常等，开发人员无法实时查看所有的 Task 日志，比如作业很大或者有很多作业的情况下，该如何处理？此时 Metrics 可以很好的帮助开发人员了解作业的当前状况。 Metric TypesMetrics 的类型如下： 首先，常用的如 Counter，写过 mapreduce 作业的开发人员就应该很熟悉 Counter，其实含义都是一样的，就是对一个计数器进行累加，即对于多条数据和多兆数据一直往上加的过程。 第二，Gauge，Gauge 是最简单的 Metrics，它反映一个值。比如要看现在 Java heap 内存用了多少，就可以每次实时的暴露一个 Gauge，Gauge 当前的值就是heap使用的量。 第三，Meter，Meter 是指统计吞吐量和单位时间内发生“事件”的次数。它相当于求一种速率，即事件次数除以使用的时间。 第四，Histogram，Histogram 比较复杂，也并不常用，Histogram 用于统计一些数据的分布，比如说 Quantile、Mean、StdDev、Max、Min 等。 Metric GroupMetric 在 Flink 内部有多层结构，以 Group 的方式组织，它并不是一个扁平化的结构，Metric Group + Metric Name 是 Metrics 的唯一标识。 Metric Group 的层级有 TaskManagerMetricGroup 和TaskManagerJobMetricGroup，每个 Job 具体到某一个 task 的 group，task 又分为 TaskIOMetricGroup 和 OperatorMetricGroup。Operator 下面也有 IO 统计和一些 Metrics，整个层级大概如下图所示。Metrics 不会影响系统，它处在不同的组中，并且 Flink支持自己去加 Group，可以有自己的层级。 123456789•TaskManagerMetricGroup •TaskManagerJobMetricGroup •TaskMetricGroup •TaskIOMetricGroup •OperatorMetricGroup •$&#123;User-defined Group&#125; / $&#123;User-defined Metrics&#125; •OperatorIOMetricGroup•JobManagerMetricGroup •JobManagerJobMetricGroup JobManagerMetricGroup 相对简单，相当于 Master，它的层级也相对较少。 Metrics 定义还是比较简单的，即指标的信息可以自己收集，自己统计，在外部系统能够看到 Metrics 的信息，并能够对其进行聚合计算。 如何使用 Metrics？System MetricsSystem Metrics，将整个集群的状态已经涵盖得非常详细。具体包括以下方面： Master 级别和 Work 级别的 JVM 参数，如 load 和 time；其 Memory 划分也很详细，包括 heap 的使用情况、non-heap 的使用情况、direct 的使用情况，以及 mapped 的使用情况；Threads 可以看到具体有多少线程；还有非常实用的 Garbage Collection。 Network 使用比较广泛，当需要解决一些性能问题的时候，Network 非常实用。Flink 不只是网络传输，还是一个有向无环图的结构，可以看到它的每个上下游都是一种简单的生产者消费者模型。Flink 通过网络相当于标准的生产者和消费者中间通过有限长度的队列模型。如果想要评估定位性能，中间队列会迅速缩小问题的范围，能够很快的找到问题瓶颈。 123456789101112•CPU•Memory•Threads•Garbage Collection•Network•Classloader•Cluster•Availability•Checkpointing•StateBackend•IO•详见: [https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html#system-metrics](https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html) 运维集群的人会比较关心 Cluster 的相关信息，如果作业太大，则需要非常关注 Checkpointing，它有可能会在一些常规的指标上无法体现出潜在问题。比如 Checkpointing 长时间没有工作，数据流看起来没有延迟，此时可能会出现作业一切正常的假象。另外，如果进行了一轮 failover 重启之后，因为 Checkpointing 长时间没有工作，有可能会回滚到很长一段时间之前的状态，整个作业可能就直接废掉了。 RocksDB 是生产环境当中比较常用的 state backend 实现，如果数据量足够大，就需要多关注 RocksDB 的 Metrics，因为它随着数据量的增大，性能可能会下降。 User-defined Metrics除了系统的 Metrics 之外，Flink 支持自定义 Metrics ，即 User-defined Metrics。上文说的都是系统框架方面，对于自己的业务逻辑也可以用 Metrics 来暴露一些指标，以便进行监控。 User-defined Metrics 现在提及的都是 datastream 的 API，table、sql 可能需要 context 协助，但如果写 UDF，它们其实是大同小异的。 Datastream 的 API 是继承 RichFunction ，继承 RichFunction 才可以有 Metrics 的接口。然后通过 RichFunction 会带来一个 getRuntimeContext().getMetricGroup().addGroup(…) 的方法，这里就是 User-defined Metrics 的入口。通过这种方式，可以自定义 user-defined Metric Group。如果想定义具体的 Metrics，同样需要用getRuntimeContext().getMetricGroup().counter/gauge/meter/histogram(…) 方法，它会有相应的构造函数，可以定义到自己的 Metrics 类型中。 123继承 RichFunction •Register user-defined Metric Group: getRuntimeContext().getMetricGroup().addGroup(…) •Register user-defined Metric: getRuntimeContext().getMetricGroup().counter/gauge/meter/histogram(…) User-defined Metrics Example下面通过一段简单的例子说明如何使用 Metrics。比如，定义了一个 Counter 传一个 name，Counter 默认的类型是 single counter（Flink 内置的一个实现），可以对 Counter 进行 inc（）操作，并在代码里面直接获取。 Meter 也是这样，Flink 有一个内置的实现是 Meterview，因为 Meter 是多长时间内发生事件的记录，所以它是要有一个多长时间的窗口。平常用 Meter 时直接 markEvent()，相当于加一个事件不停地打点，最后用 getrate（） 的方法直接把这一段时间发生的事件除一下给算出来。 Gauge 就比较简单了，把当前的时间打出来，用 Lambda 表达式直接把 System::currentTimeMillis 打进去就可以，相当于每次调用的时候都会去真正调一下系统当天时间进行计算。 Histogram 稍微复杂一点，Flink 中代码提供了两种实现，在此取一其中个实现，仍然需要一个窗口大小，更新的时候可以给它一个值。 这些 Metrics 一般都不是线程安全的。如果想要用多线程，就需要加同步，更多详情请参考下面链接。 12345678•Counter processedCount = getRuntimeContext().getMetricGroup().counter(&quot;processed_count&quot;); processedCount.inc();•Meter processRate = getRuntimeContext().getMetricGroup().meter(&quot;rate&quot;, new MeterView(60)); processRate.markEvent();•getRuntimeContext().getMetricGroup().gauge(&quot;current_timestamp&quot;, System::currentTimeMillis);•Histogram histogram = getRuntimeContext().getMetricGroup().histogram(&quot;histogram&quot;, new DescriptiveStatisticsHistogram(1000)); histogram.update(1024);•[https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html#metric-types] 获取 Metrics获取 Metrics 有三种方法，首先可以在 WebUI 上看到；其次可以通过 RESTful API 获取，RESTful API 对程序比较友好，比如写自动化脚本或程序，自动化运维和测试，通过 RESTful API 解析返回的 Json 格式对程序比较友好；最后，还可以通过 Metric Reporter 获取，监控主要使用 Metric Reporter 功能。 获取 Metrics 的方式在物理架构上是怎样实现的？ 了解背景和原理会对使用有更深刻的理解。WebUI 和 RESTful API 是通过中心化节点定期查询把各个组件中的 Metrics 拉上来的实现方式。其中，fetch 不一定是实时更新的，默认为 10 秒，所以有可能在 WebUI 和 RESTful API 中刷新的数据不是实时想要得到的数据；此外，fetch 有可能不同步，比如两个组件，一边在加另一边没有动，可能是由于某种原因超时没有拉过来，这样是无法更新相关值的，它是 try best 的操作，所以有时我们看到的指标有可能会延迟，或许等待后相关值就更新了。 红色的路径通过 MetricFetcher，会有一个中心化的节点把它们聚合在一起展示。而 MetricReporter 不一样，每一个单独的点直接汇报，它没有中心化节点帮助做聚合。如果想要聚合，需要在第三方系统中进行，比如常见的 TSDB 系统。当然，不是中心化结构也是它的好处，它可以免去中心化节点带来的问题，比如内存放不下等，MetricReporter 把原始数据直接 Reporter 出来，用原始数据做处理会有更强大的功能。 Metric ReporterFlink 内置了很多 Reporter，对外部系统的技术选型可以参考，比如 JMX 是 java 自带的技术，不严格属于第三方。还有 InfluxDB、Prometheus、Slf4j（直接打 log 里）等，调试时候很好用，可以直接看 logger，Flink 本身自带日志系统，会打到 Flink 框架包里面去。详见：详见：https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html#reporter 12345678//配置metrics.reporters: your_monitor,jmxmetrics.reporter.jmx.class: org.apache.flink.metrics.jmx.JMXReportermetrics.reporter.jmx.port: 1025-10000metrics.reporter.your_monitor.class: com.your_company.YourMonitorClassmetrics.reporter.your_monitor.interval: 10 SECONDSmetrics.reporter.your_monitor.config.a: your_a_valuemetrics.reporter.your_monitor.config.b: your_b_value Metric Reporter 是如何配置的？如上所示，首先 Metrics Reporters 的名字用逗号分隔，然后通过 metrics.reporter.jmx.class 的 classname 反射找 reporter，还需要拿到 metrics.reporter.jmx.port 的配置，比如像第三方系统通过网络发送的比较多。但要知道往哪里发，ip 地址、port 信息是比较常见的。此外还有 metrics.reporter.your_monitor.class 是必须要有的，可以自己定义间隔时间，Flink 可以解析，不需要自行去读，并且还可以写自己的 config。 实战：利用 Metrics 监控常用 Metrics 做自动化运维和性能分析。 自动化运维 自动化运维怎么做？ 首先，收集一些关键的 Metrics 作为决策依据，利用 Metric Reporter 收集 Metrics 到存储/分析系统 (例如 TSDB)，或者直接通过 RESTful API 获取。有了数据之后，可以定制监控规则，关注关键指标，Failover、Checkpoint,、业务 Delay 信息。定制规则用途最广的是可以用来报警，省去很多人工的工作，并且可以定制 failover 多少次时需要人为介入。当出现问题时，有钉钉报警、邮件报警、短信报警、电话报警等通知工具。自动化运维的优势是可以通过大盘、报表的形式清晰的查看数据，通过大盘时刻了解作业总体信息，通过报表分析优化。 性能分析性能分析一般遵循如下的流程： 首先从发现问题开始，如果有 Metrics 系统，再配上监控报警，就可以很快定位问题。然后对问题进行剖析，大盘看问题会比较方便，通过具体的 System Metrics 分析，缩小范围，验证假设，找到瓶颈，进而分析原因，从业务逻辑、JVM、 操作系统、State、数据分布等多维度进行分析；如果还不能找到问题原因，就只能借助 profiling 工具了。 实战：“我的任务慢，怎么办”“任务慢，怎么办？”可以称之为无法解答的终极问题之一。 其原因在于这种问题是系统框架问题，比如看医生时告诉医生身体不舒服，然后就让医生下结论。而通常医生需要通过一系列的检查来缩小范围，确定问题。同理，任务慢的问题也需要经过多轮剖析才能得到明确的答案。 除了不熟悉 Flink 机制以外，大多数人的问题是对于整个系统跑起来是黑盒，根本不知道系统在如何运行，缺少信息，无法了解系统状态。此时，一个有效的策略是求助 Metrics 来了解系统内部的状况，下面通过一些具体的例子来说明。 发现问题 比如下图 failover 指标，线上有一个不是 0，其它都是 0，此时就发现问题了。 再比如下图 Input 指标正常都在四、五百万，突然跌成 0，这里也存在问题。 业务延时问题如下图，比如处理到的数据跟当前时间比对，发现处理的数据是一小时前的数据，平时都是处理一秒之前的数据，这也是有问题的。 缩小范围，定位瓶颈 当出现一个地方比较慢，但是不知道哪里慢时，如下图红色部分，OUTQ 并发值已经达到 100% 了，其它都还比较正常，甚至优秀。到这里生产者消费者模型出现了问题，生产者 INQ 是满的，消费者 OUT_Q 也是满的，从图中看出节点 4 已经很慢了，节点 1 产生的数据节点 4 处理不过来，而节点 5 的性能都很正常，说明节点 1 和节点 4 之间的队列已经堵了，这样我们就可以重点查看节点 1 和节点 4，缩小了问题范围。 500 个 InBps 都具有 256 个 PARALLEL ，这么多个点不可能一一去看，因此需要在聚合时把 index 是第几个并发做一个标签。聚合按着标签进行划分，看哪一个并发是 100%。在图中可以划分出最高的两个线，即线 324 和线 115，这样就又进一步的缩小了范围。 利用 Metrics 缩小范围的方式如下图所示，就是用 Checkpoint Alignment 进行对齐，进而缩小范围，但这种方法用的较少。 多维度分析 分析任务有时候为什么特别慢呢？ 当定位到某一个 Task 处理特别慢时，需要对慢的因素做出分析。分析任务慢的因素是有优先级的，可以从上向下查，由业务方面向底层系统。因为大部分问题都出现在业务维度上，比如查看业务维度的影响可以有以下几个方面，并发度是否合理、数据波峰波谷、数据倾斜；其次依次从 Garbage Collection、Checkpoint Alignment、State Backend 性能角度进行分析；最后从系统性能角度进行分析，比如 CPU、内存、Swap、Disk IO、吞吐量、容量、Network IO、带宽等。 Q &amp; AQ：Metrics 是系统内部的监控，那是否可以作为 Flink 日志分析的输出？ 可以，但是没有必要，都用 Flink 去处理其他系统的日志了，输出或报警直接当做 sink 输出就好了。因为 Metrics 是统计内部状态，你这是处理正常输入数据，直接输出就可以了。 Q：Reporter 是有专门的线程吗？ 每个 Reporter 都有自己单独的线程。在 Flink 的内部，线程其实还是挺多的，如果跑一个作业，直接到 TaskManager 上，jstack 就能看到线程的详情。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"《大数据实时计算引擎 Flink 实战与性能优化》新专栏","date":"2019-11-14T16:00:00.000Z","path":"2019/11/15/flink-in-action/","text":"基于 Flink 1.9 讲解的专栏，涉及入门、概念、原理、实战、性能调优、系统案例的讲解。 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 专栏亮点 全网首个使用最新版本 Flink 1.9 进行内容讲解（该版本更新很大，架构功能都有更新），领跑于目前市面上常见的 Flink 1.7 版本的教学课程。 包含大量的实战案例和代码去讲解原理，有助于读者一边学习一边敲代码，达到更快，更深刻的学习境界。目前市面上的书籍没有任何实战的内容，还只是讲解纯概念和翻译官网。 在专栏高级篇中，根据 Flink 常见的项目问题提供了排查和解决的思维方法，并通过这些问题探究了为什么会出现这类问题。 在实战和案例篇，围绕大厂公司的经典需求进行分析，包括架构设计、每个环节的操作、代码实现都有一一讲解。 为什么要学习 Flink？随着大数据的不断发展，对数据的及时性要求越来越高，实时场景需求也变得越来越多，主要分下面几大类： 为了满足这些实时场景的需求，衍生出不少计算引擎框架。现有市面上的大数据计算引擎的对比如下图所示： 可以发现无论从 Flink 的架构设计上，还是从其功能完整性和易用性来讲都是领先的，再加上 Flink 是阿里巴巴主推的计算引擎框架，所以从去年开始就越来越火了！ 目前，阿里巴巴、腾讯、美团、华为、滴滴出行、携程、饿了么、爱奇艺、有赞、唯品会等大厂都已经将 Flink 实践于公司大型项目中，带起了一波 Flink 风潮，势必也会让 Flink 人才市场产生供不应求的招聘现象。 专栏内容 预备篇介绍实时计算常见的使用场景，讲解 Flink 的特性，并且对比了 Spark Streaming、Structured Streaming 和 Storm 等大数据处理引擎，然后准备环境并通过两个 Flink 应用程序带大家上手 Flink。 基础篇深入讲解 Flink 中 Time、Window、Watermark、Connector 原理，并有大量文章篇幅（含详细代码）讲解如何去使用这些 Connector（比如 Kafka、ElasticSearch、HBase、Redis、MySQL 等），并且会讲解使用过程中可能会遇到的坑，还教大家如何去自定义 Connector。 进阶篇讲解 Flink 中 State、Checkpoint、Savepoint、内存管理机制、CEP、Table／SQL API、Machine Learning 、Gelly。在这篇中不仅只讲概念，还会讲解如何去使用 State、如何配置 Checkpoint、Checkpoint 的流程和如何利用 CEP 处理复杂事件。 高级篇重点介绍 Flink 作业上线后的监控运维：如何保证高可用、如何定位和排查反压问题、如何合理的设置作业的并行度、如何保证 Exactly Once、如何处理数据倾斜问题、如何调优整个作业的执行效率、如何监控 Flink 及其作业？ 实战篇教大家如何分析实时计算场景的需求，并使用 Flink 里面的技术去实现这些需求，比如实时统计 PV／UV、实时统计商品销售额 TopK、应用 Error 日志实时告警、机器宕机告警。这些需求如何使用 Flink 实现的都会提供完整的代码供大家参考，通过这些需求你可以学到 ProcessFunction、Async I／O、广播变量等知识的使用方式。 系统案例篇讲解大型流量下的真实案例：如何去实时处理海量日志（错误日志实时告警／日志实时 ETL／日志实时展示／日志实时搜索）、基于 Flink 的百亿数据实时去重实践（从去重的通用解决方案 –&gt; 使用 BloomFilter 来实现去重 –&gt; 使用 Flink 的 KeyedState 实现去重）。 多图讲解 Flink 知识点 你将获得什么 掌握 Flink 与其他计算框架的区别 掌握 Flink Time／Window／Watermark／Connectors 概念和实现原理 掌握 Flink State／Checkpoint／Savepoint 状态与容错 熟练使用 DataStream／DataSet／Table／SQL API 开发 Flink 作业 掌握 Flink 作业部署／运维／监控／性能调优 学会如何分析并完成实时计算需求 获得大型高并发流量系统案例实战项目经验 适宜人群 Flink 爱好者 实时计算开发工程师 大数据开发工程师 计算机专业研究生 有实时计算场景场景的 Java 开发工程师","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"一文彻底搞懂 Flink 网络流控与反压机制","date":"2019-08-25T16:00:00.000Z","path":"2019/08/26/flink-back-pressure/","text":"看完本文，你能get到以下知识 Flink 流处理为什么需要网络流控？ Flink V1.5 版之前网络流控介绍 Flink V1.5 版之前的反压策略存在的问题 Credit的反压策略实现原理，Credit是如何解决 Flink 1.5 之前的问题？ 对比spark，都说flink延迟低，来一条处理一条，真是这样吗？其实Flink内部也有Buffer机制，Buffer机制具体是如何实现的？ Flink 如何在吞吐量和延迟之间做权衡？ 后续相关博客 Flink 反压相关 Metrics 介绍 基于 Flink 的流控机制和反压如何定位 Flink 任务的瓶颈。或者说，如果一个平时正常的 Flink 任务突然出现延迟了，怎么来定位问题？到底是 Kafka 读取数据慢，还是中间某个计算环节比较消耗资源使得变慢，还是由于最后的写入外部存储时比较慢？ Flink 流处理为什么需要网络流控？分析一个简单的 Flink 流任务，下图是一个简单的Flink流任务执行图：任务首先从 Kafka 中读取数据、 map 算子对数据进行转换、keyBy 按照指定 key 对数据进行分区（相同 key 的数据经过 keyBy 后分到同一个 subtask 实例中），keyBy 后对数据接着进行 map 转换，然后使用 Sink 将数据输出到外部存储。 众所周知，在大数据处理中，无论是批处理还是流处理，单点处理的性能总是有限的，我们的单个 Job 一般会运行在多个节点上，多个节点共同配合来提升整个系统的处理性能。图中，任务被切分成 4 个可独立执行的 subtask（ A0、A1、B0、B1），在数据处理过程中，就会存在 shuffle（数据传输）的过程。例如，subtask A0 处理完的数据经过 keyBy 后发送到 subtask B0、B1 所在节点去处理。 那么问题来了，下图中，上游 Producer 向下游 Consumer 发送数据，在发送端和接受端都有相应的 Send Buffer 和 Receive Buffer，但是上游 Producer 生成数据的速率比下游 Consumer 消费数据的速率快。Producer 生产数据 2MB/s， Consumer 消费数据 1MB/s，Receive Buffer 只有 5MB，所以过了5秒后，接收端的 Receive Buffer 满了。（可以把下图中的 Producer 当做上面案例中的 subtask A0，把下图中的 Consumer 当做上面案例中的 subtask B0） 下游接收区的 Receive Buffer 有限，如果上游一直有源源不断的数据，那么将会面临着以下两个情况： 下游消费者会丢弃新到达的数据，因为下游消费者的缓冲区放不下 为了不丢弃数据，所以下游消费者的 Receive Buffer 持续扩张，最后耗尽消费者的内存，OOM，程序挂掉 常识告诉我们，这两种情况在生产环境都是不能接受的，第一种会把数据丢弃、第二种会把我们的应用程序挂掉。所以，该问题的解决方案不应该是下游 Receive Buffer 一直累积数据，而是上游 Producer 发现下游 Consumer 处理比较慢之后，应该在 Producer 端做出限流的策略，防止在下游 Consumer 端无限制的数据堆积。 那上游 Producer 端该如何做限流呢？可以采用下图所示静态限流的策略： 静态限速的思想就是，提前已知下游 Consumer 的消费速率，然后通过在上游 Producer 端使用类似令牌桶的思想，限制 Producer 端生产数据的速率，从而控制上游 Producer 端向下游 Consumer 端发送数据的速率。但是静态限速会存在问题： 通常无法事先预估下游 Consumer 端能承受的最大速率 就算通过某种方式预估出下游 Consumer 端能承受的最大速率，下游应用程序也可能会因为网络抖动、 CPU 共享竞争、内存紧张、IO阻塞等原因造成下游应用程序的吞吐量降低，然后又会出现上面所说的下游接收区的 Receive Buffer 有限，上游一直有源源不断的数据发送到下游的问题，还是会造成下游要么丢数据，要么为了不丢数据 buffer 不断扩充导致下游 OOM的问题 综上所述，我们发现了，上游 Producer 端必须有一个限流的策略，且静态限流是不可靠的，于是就需要一个动态限流的策略。可以采用下图动态反馈所示： 下游 Consumer 端会频繁地向上游 Producer 端进行动态反馈，告诉 Producer 下游 Consumer 的负载能力，从而 Producer 端动态调整向下游 Consumer 发送数据的速率实现 Producer 端的动态限流。当 Consumer 端处理较慢时，Consumer 将负载反馈到 Producer 端，Producer端会根据反馈适当降低 Producer 自身从上游或者 Source 端读数据的速率来降低向下游 Consumer 发送数据的速率。当 Consumer 处理负载能力提升后，又及时向 Producer 端反馈，Producer 会通过提升从上游或 Source 端读数据的速率来提升向下游发送数据的速率。通过这个动态反馈来提升整个系统的吞吐量。 补充一点，如下图所示，假如我们的 Job 分为 Task A、B、C，Task A 是 Source Task、Task B 处理数据、Task C 是 Sink Task。假如 Task C 由于各种原因吞吐量降低，会将负载信息反馈给 Task B，Task B 会降低向 Task C 发送数据的速率，此时如果 Task B 如果还是一直从 Task A 读取数据，那么按照同样的道理，数据会把 Task B 的 Send Buffer 和 Receive Buffer 撑爆，又会出现上面描述的问题。所以，当 Task B 的 Send Buffer 和 Receive Buffer 被用完后，Task B 会用同样的原理将负载信息反馈给 Task A，Task A 收到 Task B 的负载信息后，会降低 给 Task B 发送数据的速率，以此类推。 上面这个流程，就是 Flink 动态限流（反压机制）的简单描述。我们可以看到 Flink 的反压其实是从下游往上游传播的，一直往上传播到 Source Task 后，Source Task 最终会降低从 Source 端读取数据的速率。如果下游 Task C 的负载能力提升后，会及时反馈给 Task B，于是 Task B 会提升往 Task C 发送数据的速率，Task B 又将负载提升的信息反馈给 Task A，Task A 就会提升从 Source 端读取数据的速率，从而提升整个系统的负载能力。 读到这里，我们应该知道 Flink 为什么需要一个网络流控机制了，并且知道 Flink 的网络流控机制必须是一个动态反馈的过程。但是还有以下几个问题： 数据具体是怎么从上游 Producer 端发送到下游 Consumer 端的？ Flink 的动态限流具体是怎么实现的？下游的负载能力和压力是如何传递给上游的？ 我们带着这两个问题，学习下面的 Flink 网络流控与反压机制 Flink V1.5 版之前网络流控介绍在 Flink V1.5 版之前，其实 Flink 并没有刻意做上述所说的动态反馈。那么问题来了，没有做上述的动态反馈机制，Flink 难道不怕数据丢失或者上游和下游的一些 Buffer 把内存撑爆吗？当然不怕了，因为 Flink 已经依赖其他机制来实现了所谓的动态反馈。其实很简单，让我们继续往下看。 如下图所示，对于一个 Flink 任务，动态反馈可以抽象成以下两个阶段： 跨 Task，动态反馈如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer 当下游 Task C 的 Receive Buffer 满了，如何告诉上游 Task B 应该降低数据发送速率 当下游 Task C 的 Receive Buffer 空了，如何告诉上游 Task B 应该提升数据发送速率 注：这里又分了两种情况，Task B 和 Task C 可能在同一台节点上运行，也有可能不在同一个台节点运行 Task B 和 Task C 在同一台节点上运行指的是：一台节点运行了一个或多个 TaskManager，包含了多个 Slot，Task B 和 Task C 都运行在这台节点上，且 Task B 是 Task C 的上游，给 Task C 发送数据。此时 Task B 给 Task C 发送数据实际上是同一个 JVM 内的数据发送，所以不存在网络通信 Task B 和 Task C 不在同一台节点上运行指的是：Task B 和 Task C 运行在不同的 TaskManager 中，且 Task B 是 Task C 的上游，给 Task C 发送数据。此时 Task B 给 Task C 发送数据是跨节点的，所以会存在网络通信 Task 内，动态反馈如何从内部的 Send Buffer 反馈给内部的 Receive Buffer 当 Task B 的 Send Buffer 满了，如何告诉 Task B 内部的 Receive Buffer 下游 Send Buffer 满了、下游处理性能不行了？因为要让 Task B 的 Receive Buffer 感受到压力，才能把下游的压力传递到 Task A 当 Task B 的 Send Buffer 空了，如何告诉 Task B 内部的 Receive Buffer 下游 Send Buffer 空了，下游处理性能很强，上游加快处理数据吧 跨 TaskManager，反压如何向上游传播先了解一下 Flink 的 TaskManager 之间网络传输的数据流向： 图中，我们可以看到 TaskManager A 给 TaskManager B 发送数据，TaskManager A 做为 Producer，TaskManager B 做为 Consumer。Producer 端的 Operator 实例会产生数据，最后通过网络发送给 Consumer 端的 Operator 实例。Producer 端 Operator 实例生产的数据首先缓存到 TaskManager 内部的 NetWork Buffer。NetWork 依赖 Netty 来做通信，Producer 端的 Netty 内部有 ChannelOutbound Buffer，Consumer 端的 Netty 内部有 ChannelInbound Buffer。Netty 最终还是要通过 Socket 发送网络请求，Socket 这一层也会有 Buffer，Producer 端有 Send Buffer，Consumer 端有 Receive Buffer。 总结一下，现在有两个 TaskManager A、B，TaskManager A 中 Producer Operator 处理完的数据由 TaskManager B 中 Consumer Operator 处理。那么 Producer Operator 处理完的数据是怎么到达 Consumer Operator 的？首先 Producer Operator 从自己的上游或者外部数据源读取到数据后，对一条条的数据进行处理，处理完的数据首先输出到 Producer Operator 对应的 NetWork Buffer 中。Buffer 写满或者超时后，就会触发将 NetWork Buffer 中的数据拷贝到 Producer 端 Netty 的 ChannelOutbound Buffer，之后又把数据拷贝到 Socket 的 Send Buffer 中，这里有一个从用户态拷贝到内核态的过程，最后通过 Socket 发送网络请求，把 Send Buffer 中的数据发送到 Consumer 端的 Receive Buffer。数据到达 Consumer 端后，再依次从 Socket 的 Receive Buffer 拷贝到 Netty 的 ChannelInbound Buffer，再拷贝到 Consumer Operator 的 NetWork Buffer，最后 Consumer Operator 就可以读到数据进行处理了。这就是两个 TaskManager 之间的数据传输过程，我们可以看到发送方和接收方各有三层的 Buffer。 了解了数据传输流程，我们再具体了解一下跨 TaskManager 的反压过程，如下图所示，Producer 端生产数据速率为 2，Consumer 消费数据速率为 1。持续下去，下游消费较慢，Buffer 容量又是有限的，那 Flink 反压是怎么做的？ 上面介绍后，我们知道每个 Operator 计算数据时，输出和输入都有对应的 NetWork Buffer，这个 NetWork Buffer 对应到 Flink 就是图中所示的 ResultSubPartition 和 InputChannel。ResultSubPartition 和 InputChannel 都是向 LocalBufferPool 申请 Buffer 空间，然后 LocalBufferPool 再向 NetWork BufferPool 申请内存空间。这里，NetWork BufferPool 是 TaskManager 内所有 Task 共享的 BufferPool，TaskManager 初始化时就会向堆外内存申请 NetWork BufferPool。LocalBufferPool 是每个 Task 自己的 BufferPool，假如一个 TaskManager 内运行着 5 个 Task，那么就会有 5 个 LocalBufferPool，但 TaskManager 内永远只有一个 NetWork BufferPool。Netty 的 Buffer 也是初始化时直接向堆外内存申请内存空间。虽然可以申请，但是必须明白内存申请肯定是有限制的，不可能无限制的申请，我们在启动任务时可以指定该任务最多可能申请多大的内存空间用于 NetWork Buffer。 我们继续分析我们的场景， Producer 端生产数据速率为2，Consumer 端消费数据速率为1。数据从 Task A 的 ResultSubPartition 按照上面的流程最后传输到 Task B 的 InputChannel 供 Task B 读取并计算。持续一段时间后，由于 Task B 消费比较慢，导致 InputChannel 被占满了，所以 InputChannel 向 LocalBufferPool 申请新的 Buffer 空间，LocalBufferPool 分配给 InputChannel 一些 Buffer。 再持续一段时间后，InputChannel 重复向 LocalBufferPool 申请 Buffer 空间，导致 LocalBufferPool 也满了，所以 LocalBufferPool 向 NetWork BufferPool 申请 Buffer 空间，NetWork BufferPool 给 LocalBufferPool 分配 Buffer。 再持续下去，NetWork BufferPool 满了，或者说 NetWork BufferPool 不能把自己的 Buffer 全分配给 Task B 对应的 LocalBufferPool ，因为 TaskManager 上一般会运行了多个 Task，每个 Task 只能使用 NetWork BufferPool 中的一部分。所以，可以认为 Task B 把自己可以使用的 InputChannel 、 LocalBufferPool 和 NetWork BufferPool 都用完了。此时 Netty 还想把数据写入到 InputChannel，但是发现 InputChannel 满了，所以 Socket 层会把 Netty 的 autoRead disable，Netty 不会再从 Socket 中去读消息。可以看到下图中多个 ❌，表示 Buffer 已满，数据已经不能往下游写了，发生了阻塞。 由于 Netty 不从 Socket 的 Receive Buffer 读数据了，所以很快 Socket 的 Receive Buffer 就会变满，TCP 的 Socket 通信有动态反馈的流控机制，会把容量为0的消息反馈给上游发送端，所以上游的 Socket 就不会往下游再发送数据 。 Task A 持续生产数据，发送端 Socket 的 Send Buffer 很快被打满，所以 Task A 端的 Netty 也会停止往 Socket 写数据。 接下来，数据会在 Netty 的 Buffer 中缓存数据，但 Netty 的 Buffer 是无界的。但可以设置 Netty 的高水位，即：设置一个 Netty 中 Buffer 的上限。所以每次 ResultSubPartition 向 Netty 中写数据时，都会检测 Netty 是否已经到达高水位，如果达到高水位就不会再往 Netty 中写数据，防止 Netty 的 Buffer 无限制的增长。 接下来，数据会在 Task A 的 ResultSubPartition 中累积，ResultSubPartition 满了后，会向 LocalBufferPool 申请新的 Buffer 空间，LocalBufferPool 分配给 ResultSubPartition 一些 Buffer。 持续下去 LocalBufferPool 也会用完，LocalBufferPool 再向 NetWork BufferPool 申请 Buffer。 然后 NetWork BufferPool 也会用完，或者说 NetWork BufferPool 不能把自己的 Buffer 全分配给 Task A 对应的 LocalBufferPool ，因为 TaskManager 上一般会运行了多个 Task，每个 Task 只能使用 NetWork BufferPool 中的一部分。此时，Task A 已经申请不到任何的 Buffer 了，Task A 的 Record Writer 输出就被 wait ，Task A 不再生产数据。 通过上述的这个流程，来动态反馈，保障各个 Buffer 都不会因为数据太多导致内存溢出。上面描述了整个阻塞的流程，当下游 Task B 持续消费，Buffer 的可用容量会增加，所有被阻塞的数据通道会被一个个打开，之后 Task A 又可以开始正常的生产数据了。 之前介绍，Task 之间的数据传输可能存在上游的 Task A 和下游的 Task B 运行在同一台节点的情况，整个流程与上述类似，只不过由于 Task A 和 B 运行在同一个 JVM，所以不需要网络传输的环节，Task B 的 InputChannel 会直接从 Task A 的 ResultSubPartition 读取数据。 Task 内部，反压如何向上游传播假如 Task A 的下游所有 Buffer 都占满了，那么 Task A 的 Record Writer 会被 block，Task A 的 Record Reader、Operator、Record Writer 都属于同一个线程，所以 Task A 的 Record Reader 也会被 block。 然后可以把这里的 Task A 类比成上面所说的 Task B，Task A 上游持续高速率发送数据到 Task A 就会导致可用的 InputChannel、 LocalBufferPool 和 NetWork BufferPool 都会被用完。然后 Netty 、Socket 同理将压力传输到 Task A 的上游。 假设 Task A 的上游是 Task X，那么 Task A 将压力反馈给 Task X 的过程与 Task B 将压力反馈给 Task A 的过程是一样的。整个 Flink 的反压是从下游往上游传播的，一直传播到 Source Task，Source Task 有压力后，会降低从外部组件中读取数据的速率，例如：Source Task 会降低从 Kafka 中读取数据的速率，来降低整个 Flink Job 中缓存的数据，从而降低负载。 所以得出的结论是：Flink 1.5之前并没有特殊的机制来处理反压，因为 Flink 中的数据传输相当于已经提供了应对反压的机制。 Flink V1.5 版之前的反压策略存在的问题看着挺完美的反压机制，其实是有问题的。如下图所示，我们的任务有4个 SubTask，SubTask A 是 SubTask B的上游，即 SubTask A 给 SubTask B 发送数据。Job 运行在两个 TaskManager中， TaskManager 1 运行着 SubTask A.1 和 SubTask A.2， TaskManager 2 运行着 SubTask B.3 和 SubTask B.4。现在假如由于CPU共享或者内存紧张或者磁盘IO瓶颈造成 SubTask B.4 遇到瓶颈、处理速率有所下降，但是上游源源不断地生产数据，所以导致 SubTask A.2 与 SubTask B.4 产生反压。 这里需要明确一点：不同 Job 之间的每个（远程）网络连接将在 Flink 的网络堆栈中获得自己的TCP通道。 但是，如果同一 Task 的不同 SubTask 被安排到同一个TaskManager，则它们与其他 TaskManager 的网络连接将被多路复用并共享一个TCP信道以减少资源使用。例如，图中的 A.1 -&gt; B.3、A.1 -&gt; B.4、A.2 -&gt; B.3、A.2 -&gt; B.4 这四条将会多路复用共享一个 TCP 信道。 现在 SubTask B.3 并没有压力，从上面跨 TaskManager 的反压流程，我们知道当上图中 SubTask A.2 与 SubTask B.4 产生反压时，会把 TaskManager1 端该任务对应 Socket 的 Send Buffer 和 TaskManager2 端该任务对应 Socket 的 Receive Buffer 占满，多路复用的 TCP 通道已经被占住了，会导致 SubTask A.1 和 SubTask A.2 要发送给 SubTask B.3 的数据全被阻塞了，从而导致本来没有压力的 SubTask B.3 现在接收不到数据了。所以，Flink 1.5 版之前的反压机制会存在当一个 Task 出现反压时，可能导致其他正常的 Task 接收不到数据。 Credit的反压策略实现原理Flink 1.5 之后，为了解决上述所描述的问题，引入了基于 Credit 的反压机制。如下图所示，反压机制作用于 Flink 的应用层，即在 ResultSubPartition 和 InputChannel 这一层引入了反压机制。每次上游 SubTask A.2 给下游 SubTask B.4 发送数据时，会把 Buffer 中的数据和上游 ResultSubPartition 堆积的数据量 Backlog size发给下游，下游会接收上游发来的数据，并向上游反馈目前下游现在的 Credit 值，Credit 值表示目前下游可以接收上游的 Buffer 量，1 个Buffer 等价于 1 个 Credit 。 例如，上游 SubTask A.2 发送完数据后，还有 5 个 Buffer 被积压，那么会把发送数据和 Backlog size = 5 一块发送给下游 SubTask B.4，下游接受到数据后，知道上游积压了 5 个Buffer，于是向 Buffer Pool 申请 Buffer，由于容量有限，下游 InputChannel 目前仅有 2 个 Buffer 空间，所以，SubTask B.4 会向上游 SubTask A.2 反馈 Channel Credit = 2。然后上游下一次最多只给下游发送 2 个 Buffer 的数据，这样每次上游发送的数据都是下游 InputChannel 的 Buffer 可以承受的数据量，所以通过这种反馈策略，保证了不会在公用的 Netty 和 TCP 这一层数据堆积而影响其他 SubTask 通信。 ResultSubPartition 会把 buffer 和 backlog size 同时发送给下游，下游向上游反馈 credit。再用一个案例来详细地描述一下整个过程。 Task A 向 Task B 发送了数据 和 backlog size =3，下游 InputChannel 接受完 后，发现上游目前积压了 3 条数据，但是自己的缓冲区不够，于是向 LocalBufferPool 申请 buffer 空间，申请成功后，向上游反馈 credit = 3，表示下游目前可以接受 3 条记录（实际上是以 Buffer 为单位，而不是记录数，Flink 将真实记录序列化后的二进制数据放到 Buffer 中），然后上游下次最多发送 3 条数据给下游。 持续下去，上游生产数据速率比下游消费速率快，所以 LocalBufferPool 和 NetWork BufferPool 都会被申请完，下游的 InputChannel 没有可用的缓冲区了，所以会向上游反馈 credit = 0，然后上游就不会发送数据到 Netty。所以基于 Credit 的反压策略不会导致 Netty 和 Socket 的数据积压。当然上游也不会一直不发送数据到下游，上游会定期地仅发送 backlog size 给下游，直到下游反馈 credit &gt; 0 时，上游就会继续发送真正的数据到下游了。 基于 Credit 的反压机制还带来了一个优势：由于我们在发送方和接收方之间缓存较少的数据，可能会更早地将反压反馈给上游，缓冲更多数据只是把数据缓冲在内存中，并没有提高处理性能。 Flink 如何在吞吐量和延迟之间做权衡？Flink 天然支持流式处理，即每来一条数据就能处理一条，而不是像 Spark Streaming 一样，完全是微批处理。但是为了提高吞吐量，默认使用的 Flink 并不是每来一条数据就处理一条。那这个到底是怎么控制的呢？ 我们分析了上述的网络传输后，知道每个 SubTask 输出的数据并不是直接输出到下游，而是在 ResultSubPartition 中有一个 Buffer 用来缓存一批数据后，再 Flush 到 Netty 发送到下游 SubTask。那到底哪些情况会触发 Buffer Flush 到 Netty 呢？ Buffer 变满时 Buffer timeout 时 特殊事件来临时，例如：CheckPoint 的 barrier 来临时 Flink 在数据传输时，会把数据序列化成二进制然后写到 Buffer 中，当 Buffer 满了，需要 Flush（默认为32KiB，通过taskmanager.memory.segment-size设置）。但是当流量低峰或者测试环节，可能1分钟都没有 32 KB的数据，就会导致1分钟内的数据都积攒在 Buffer 中不会发送到下游 Task 去处理，从而导致数据出现延迟，这并不是我们想看到的。所以 Flink 有一个 Buffer timeout 的策略，意思是当数据量比较少，Buffer 一直没有变满时，后台的 Output flusher 线程会强制地将 Buffer 中的数据 Flush 到下游。Flink 中默认 timeout 时间是 100ms，即：Buffer 中的数据要么变满时 Flush，要么最多等 100ms 也会 Flush 来保证数据不会出现很大的延迟。当然这个可以通过 env.setBufferTimeout(timeoutMillis) 来控制超时时间。 timeoutMillis &gt; 0 表示最长等待 timeoutMillis 时间，就会flush timeoutMillis = 0 表示每条数据都会触发 flush，直接将数据发送到下游，相当于没有Buffer了(避免设置为0，可能导致性能下降) timeoutMillis = -1 表示只有等到 buffer满了或 CheckPoint的时候，才会flush。相当于取消了 timeout 策略 严格来讲，Output flusher 不提供任何保证——它只向 Netty 发送通知，而 Netty 线程会按照能力与意愿进行处理。这也意味着如果存在反压，则 Output flusher 是无效的。言外之意，如果反压很严重，下游 Buffer 都满了，当然不能强制一直往下游发数据。 一些特殊的消息如果通过 RecordWriter 发送，也会触发立即 Flush 缓存的数据。其中最重要的消息包括 Checkpoint barrier 以及 end-of-partition 事件，这些事件应该尽快被发送，而不应该等待 Buffer 被填满或者 Output flusher 的下一次 Flush。当然如果出现反压，CheckPoint barrier 也会等待，不能发送到下游。 引入 Network buffers 以获得更高的资源利用率和更高的吞吐量，代价是让一些记录在 Buffer 中等待一段时间。虽然可以通过缓冲区超时给出此等待时间的上限，但你可能知道有关这两个维度（延迟和吞吐量）之间权衡的更多信息：显然，无法同时获得这两者。下图是 Flink 官网的博客展示的不同的 buffer timeout 下对应的吞吐量，从0毫秒开始（每个记录都 flush）到100毫秒（默认值），测试在具有 100 个节点每个节点 8 个 Slot 的群集上运行，每个节点运行没有业务逻辑的 Task，因此只用于测试网络协议栈。为了进行比较，还测试了低延迟改进之前的 Flink 1.4 版本。 如图，使用 Flink 1.5+，即使是非常低的 Buffer timeout（例如1ms，对于低延迟场景）也提供高达超时默认参数（100ms）75％ 的最大吞吐，但会缓存更少的数据。但是笔者仍然不理解为什么 timeout 设置为0时，吞吐量竟然能比 Flink 1.4 的吞吐量提高那么多。Credit 只是解决了反压的问题，并不能优化低延迟的吞吐量。杨华老师的回答是网络协议栈做了其他优化而且性能测试是在特定场景下做的。笔者后续会继续深入学习研究 Flink 网络通信来解决笔者目前的疑问。 本文作者：范磊 原文链接地址：https://www.jianshu.com/p/2779e73abcb8 参考文献Flink官网 flink-china系列课程—-2.7 Flink网络流控及反压剖析 Flink 官网两篇关于 Flink 网络协议栈的博客: A Deep-Dive into Flink’s Network Stack Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 另外你如果感兴趣的话，也可以关注我的公众号。 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"}]},{"title":"滴滴实时计算发展之路及平台架构实践","date":"2019-08-24T16:00:00.000Z","path":"2019/08/25/flink-didi/","text":"滴滴的核心业务是一个实时在线服务，因此具有丰富的实时数据和实时计算场景。本文将介绍滴滴实时计算发展之路以及平台架构实践。 实时计算演进随着滴滴业务的发展，滴滴的实时计算架构也在快速演变。到目前为止大概经历了三个阶段： 业务方自建小集群阶段； 集中式大集群、平台化阶段； SQL化阶段。 下图标识了其中重要的里程碑，稍后会给出详细阐述： 在2017年以前，滴滴并没有统一的实时计算平台，而是各个业务方自建小集群。其中用到的引擎有Storm、JStorm、Spark Streaming、Samza等。业务方自建小集群模式存在如下弊端： 需要预先采购大量机器，由于单个业务独占，资源利用率通常比较低； 缺乏有效的监控报警体系； 维护难度大，需要牵涉业务方大量精力来保障集群的稳定性； 缺乏有效技术支持，且各自沉淀的东西难以共享。 为了有效解决以上问题，滴滴从2017年年初开始构建统一的实时计算集群及平台。 技术选型上，我们基于滴滴现状选择了内部用大规模数据清洗的Spark Streaming引擎，同时引入On-YARN模式，并利用YARN的多租户体系构建了认证、鉴权、资源隔离、计费等机制。 相对于离线计算，实时计算任务对于稳定性有着更高的要求，为此我们构建了两层资源隔离体系： 第一层是基于CGroup做进程（Container）级别的CPU及内存隔离； 第二层是物理机器级别的隔离。 我们通过改造YARN的FairScheduler使其支持Node Label。达到的效果如下图所示： 普通业务的任务混跑在同一个Label机器上，而特殊业务的任务跑在专用Label的机器上。 通过集中式大集群和平台化建设，基本消除了业务方自建小集群带来的弊端，实时计算也进入了第二阶段。 伴随着业务的发展，我们发现Spark Streaming的Micro Batch模式在一些低延时的报警业务及在线业务上显得捉襟见肘。于是我们引入了基于Native Streaming模式的Flink作为新一代实时计算引擎。 Flink不仅延时可以做到毫秒级，而且提供了基于Process Time/Event Time丰富的窗口函数。基于Flink我们联合业务方构架了滴滴流量最大的业务网关监控系统，并快速支持了诸如乘客位置变化通知、轨迹异常检测等多个线上业务。 实时计算平台架构为了最大程度方便业务方开发和管理流计算任务，我们构建了如图所示的实时计算平台： 在流计算引擎基础上提供了StreamSQL IDE、监控报警、诊断体系、血缘关系、任务管控等能力。各自的作用如下： StreamSQL IDE。下文会介绍，是一个Web化的SQL IDE； 监控报警。提供任务级的存活、延时、流量等监控以及基于监控的报警能力； 诊断体系。包括流量曲线、Checkpoint、GC、资源使用等曲线视图，以及实时日志检索能力。 血缘关系。我们在流计算引擎中内置了血缘上报能力，进而在平台上呈现流任务与上下游的血缘关系； 任务管控。实现了多租户体系下任务提交、启停、资产管理等能力。通过Web化任务提交消除了传统客户机模式，使得平台入口完全可控，内置参数及版本优化得以快速上线。 实时规则匹配服务建设在滴滴内部有大量的实时运营场景，比如“某城市乘客冒泡后10秒没有下单”。针对这类检测事件之间依赖关系的场景，用Fink的CEP是非常合适的。 但是社区版本的CEP不支持描述语言，每个规则需要开发一个应用，同时不支持动态更新规则。为了解决这些问题，滴滴做了大量功能扩展及优化工作。功能扩展方面主要改动有： 支持wait算子。对于刚才例子中的运营规则，社区版本是表达不了的。滴滴通过增加wait算子，实现了这类需求； 支持DSL语言。基于Groovy和Aviator解析引擎，我们实现了如下图所示的DSL描述规则能力： 单任务多规则及规则动态更新。由于实时运营规则由一线运营同学来配置，所以规则数量，规则内容及规则生命周期会经常发生变化。这种情况每个规则一个应用是不太现实的。为此我们开发了多规则模式且支持了动态更新。 除了功能拓展之外，为了应对大规模运营规则的挑战，滴滴在CEP性能上也做了大量优化，主要有： SharedBuffer重构。基于Flink MapState重构SharedBuffer，减少每次数据处理过程中的状态交互。同时剥离规则和用户数据极大降低每次匹配的时候从状态中反序列化的数据量； 增加访问缓存（已贡献社区）。缓存SharedBuffer数据中每次处理所需要更新的引用计数，延缓更新； 简化event time语义处理。避免key在很分散情况下每次watermark更新时要遍历所有key的数据； 复用conditionContext（已贡献社区）。减少条件查询时对partialMatch元素的反复查询。 以上优化将CEP性能提升了多个数量级。配合功能扩展，我们在滴滴内部提供了如图所示的服务模式： 业务方只需要清洗数据并提供规则列表API即可具备负责规则的实时匹配能力。 目前滴滴CEP已经在快车个性化运营、实时异常工单检测等业务上落地，取得了良好的效果。 StreamSQL建设正如离线计算中Hive之于MapReduce一样，流式SQL也是必然的发展趋势。通过SQL化可以大幅度降低业务方开发流计算的难度，业务方不再需要学习Java/Scala，也不需要理解引擎执行细节及各类参数调优。 为此我们在2018年启动了StreamSQL建设项目，在社区Flink SQL基础上拓展了以下能力： 扩展DDL语法。如下图所示，打通了滴滴内部主流的消息队列以及实时存储系统(StreamSQL内置打通消息队列及实施存储)： 通过内置常见消息格式（如json、binlog、标准日志）的解析能力，使得用户可以轻松写出DDL语法，并避免重复写格式解析语句。 拓展UDF。针对滴滴内部常见处理逻辑，内置了大量UDF，包括字符串处理、日期处理、Map对象处理、空间位置处理等。 支持分流语法。单个输入源多个输出流在滴滴内部非常常见，为此我们改造了Calcite使其支持分流语义。 支持基于TTL的join语义。传统的Window Join因为存在window边界数据突变情况，不能满足滴滴内部的需求。为此我们引入了TTL State，并基于此开发了基于TTL Join的双流join以及维表join。 StreamSQL IDE。前文提到平台化之后我们没有提供客户机，而是通过Web提交和管控任务。因此我们也相应开发了StreamSQL IDE，实现Web上开发StreamSQL，同时提供了语法检测、DEBUG、诊断等能力。 目前StreamSQL在滴滴已经成功落地，流计算开发成本得到大幅度降低。预期未来将承担80%的流计算业务量。 总结作为一家出行领域的互联网公司，滴滴对实时计算有天然的需求。 过去的一年多时间里，我们从零构建了集中式实时计算平台，改变了业务方自建小集群的局面。为满足低延时业务的需求，成功落地了Flink Streaming，并基于Flink构建了实时规则匹配（CEP）服务以及StreamSQL，使得流计算开发能力大幅度降低。未来将进一步拓展StreamSQL，并在批流统一、IoT、实时机器学习等领域探索和建设。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"}]},{"title":"Flink Connector 深度解析","date":"2019-08-23T16:00:00.000Z","path":"2019/08/24/Flink-Connector/","text":"作者介绍：董亭亭，快手大数据架构实时计算引擎团队负责人。目前负责 Flink 引擎在快手内的研发、应用以及周边子系统建设。2013 年毕业于大连理工大学，曾就职于奇虎 360、58 集团。主要研究领域包括：分布式计算、调度系统、分布式存储等系统。 本文主要分享Flink connector相关内容，分为以下三个部分的内容：第一部分会首先介绍一下Flink Connector有哪些。第二部分会重点介绍在生产环境中经常使用的kafka connector的基本的原理以及使用方法。第三部分答疑环节，看大家有没有一些问题。 Flink Streaming ConnectorFlink是新一代流批统一的计算引擎，它需要从不同的第三方存储引擎中把数据读过来，进行处理，然后再写出到另外的存储引擎中。Connector的作用就相当于一个连接器，连接 Flink 计算引擎跟外界存储系统。Flink里有以下几种方式，当然也不限于这几种方式可以跟外界进行数据交换：第一种 Flink里面预定义了一些source和sink。第二种 FLink内部也提供了一些Boundled connectors。第三种 可以使用第三方apache Bahir项目中提供的连接器。第四种是通过异步IO方式。下面分别简单介绍一下这四种数据读写的方式。 预定义的source和sinkFlink里预定义了一部分source和sink。在这里分了几类。 基于文件的source和sink。 如果要从文本文件中读取数据，可以直接使用 env.readTextFile(path) 就可以以文本的形式读取该文件中的内容。当然也可以使用 env.readFile(fileInputFormat, path) 根据指定的fileInputFormat格式读取文件中的内容。 如果数据在FLink内进行了一系列的计算，想把结果写出到文件里，也可以直接使用内部预定义的一些sink，比如将结果已文本或csv格式写出到文件中，可以使用DataStream的writeAsText(path)和 writeAsCsv(path)。 基于Socket的Source和Sink 提供Socket的host name及port，可以直接用StreamExecutionEnvironment预定的接口socketTextStream创建基于Socket的source，从该socket中以文本的形式读取数据。当然如果想把结果写出到另外一个Socket，也可以直接调用DataStream writeToSocket。 基于内存 Collections、Iterators 的Source可以直接基于内存中的集合或者迭代器，调用StreamExecutionEnvironment fromCollection、fromElements构建相应的source。结果数据也可以直接print、printToError的方式写出到标准输出或标准错误。 详细也可以参考Flink源码中提供的一些相对应的Examples来查看异常预定义source和sink的使用方法，例如WordCount、SocketWindowWordCount。 Bundled ConnectorsFlink里已经提供了一些绑定的Connector，例如kafka source和sink，Es sink等。读写kafka、es、rabbitMQ时可以直接使用相应connector的api即可。第二部分会详细介绍生产环境中最常用的kafka connector。 虽然该部分是Flink 项目源代码里的一部分，但是真正意义上不算作flink引擎相关逻辑，并且该部分没有打包在二进制的发布包里面。所以在提交Job时候需要注意，job代码jar包中一定要将相应的connetor相关类打包进去，否则在提交作业时就会失败，提示找不到相应的类，或初始化某些类异常。 Apache Bahir中的连接器Apache Bahir 最初是从 Apache Spark 中独立出来项目提供，以提供不限于 Spark 相关的扩展/插件、连接器和其他可插入组件的实现。通过提供多样化的流连接器（streaming connectors）和 SQL 数据源扩展分析平台的覆盖面。如有需要写到flume、redis的需求的话，可以使用该项目提供的connector。 Async I/O流计算中经常需要与外部存储系统交互，比如需要关联mysql中的某个表。一般来说，如果用同步I/O的方式，会造成系统中出现大的等待时间，影响吞吐和延迟。为了解决这个问题，异步I/O可以并发处理多个请求，提高吞吐，减少延迟。 Async的原理可参考官方文档：&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/asyncio.html Flink Kafka Connector本章重点介绍生产环境中最常用到的Flink kafka connector。使用flink的同学，一定会很熟悉kafka，它是一个分布式的、分区的、多副本的、 支持高吞吐的、发布订阅消息系统。生产环境环境中也经常会跟kafka进行一些数据的交换，比如利用kafka consumer读取数据，然后进行一系列的处理之后，再将结果写出到kafka中。这里会主要分两个部分进行介绍，一是Flink kafka Consumer，一个是Flink kafka Producer。 首先看一个例子来串联下Flink kafka connector。代码逻辑里主要是从kafka里读数据，然后做简单的处理，再写回到kafka中。 分别用红色框 框出 如何构造一个Source sink Function. Flink提供了现成的构造FLinkKafkaConsumer、Producer的接口，可以直接使用。这里需要注意，因为kafka有多个版本，多个版本之间的接口协议会不同。Flink针对不同版本的kafka有相应的版本的Consumer和Producer。例如：针对08、09、10、11版本，Flink对应的consumer分别是FlinkKafkaConsumer08、09、010、011，producer也是。 Flink kafka Consumer反序列化数据因为kafka中数据都是以二进制byte形式存储的。读到flink系统中之后，需要将二进制数据转化为具体的java、scala对象。具体需要实现一个schema类，定义如何序列化和反序列数据。反序列化时需要实现DeserializationSchema接口，并重写deserialize(byte[] message)函数，如果是反序列化kafka中kv的数据时，需要实现KeyedDeserializationSchema接口，并重写deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset)函数。 另外Flink中也提供了一些常用的序列化反序列化的schema类。例如，SimpleStringSchema，按字符串方式进行序列化、反序列化。TypeInformationSerializationSchema，它可根据Flink的TypeInformation信息来推断出需要选择的schema。JsonDeserializationSchema 使用jackson反序列化json格式消息，并返回ObjectNode，可以使用.get(“property”)方法来访问相应字段。 消费起始位置设置如何设置作业从kafka消费数据最开始的起始位置，这一部分flink也提供了非常好的封装。在构造好的FlinkKafkaConsumer类后面调用如下相应函数，设置合适的其实位置。 setStartFromGroupOffsets，也是默认的策略，从group offset位置读取数据，group offset指的是kafka broker端记录的某个group的最后一次的消费位置。但是kafka broker端没有该group信息，会根据kafka的参数”auto.offset.reset”的设置来决定从哪个位置开始消费。 setStartFromEarliest，从kafka最早的位置开始读取。 setStartFromLatest，从kafka最新的位置开始读取。 setStartFromTimestamp(long)，从时间戳大于或等于指定时间戳的位置开始读取。Kafka时戳，是指kafka为每条消息增加另一个时戳。该时戳可以表示消息在proudcer端生成时的时间、或进入到kafka broker时的时间。 setStartFromSpecificOffsets，从指定分区的offset位置开始读取，如指定的offsets中不存某个分区，该分区从group offset位置开始读取。此时需要用户给定一个具体的分区、offset的集合。 一些具体的使用方法可以参考下图。需要注意的是，因为flink框架有容错机制，如果作业故障，如果作业开启checkpoint，会从上一次checkpoint状态开始恢复。或者在停止作业的时候主动做savepoint，启动作业时从savepoint开始恢复。这两种情况下恢复作业时，作业消费起始位置是从之前保存的状态中恢复，与上面提到跟kafka这些单独的配置无关。 topic和partition动态发现实际的生产环境中可能有这样一些需求，比如场景一，有一个flink作业需要将五份数据聚合到一起，五份数据对应五个kafka topic，随着业务增长，新增一类数据，同时新增了一个kafka topic，如何在不重启作业的情况下作业自动感知新的topic。场景二，作业从一个固定的kafka topic读数据，开始该topic有10个partition，但随着业务的增长数据量变大，需要对kafka partition个数进行扩容，由10个扩容到20。该情况下如何在不重启作业情况下动态感知新扩容的partition？ 针对上面的两种场景，首先需要在构建FlinkKafkaConsumer时的properties中设置flink.partition-discovery.interval-millis参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时FLinkKafkaConsumer内部会启动一个单独的线程定期去kafka获取最新的meta信息。针对场景一，还需在构建FlinkKafkaConsumer时，topic的描述可以传一个正则表达式描述的pattern。每次获取最新kafka meta时获取正则匹配的最新topic列表。针对场景二，设置前面的动态发现参数，在定期获取kafka最新meta信息时会匹配新的partition。为了保证数据的正确性，新发现的partition从最早的位置开始读取。 commit offset方式Flink kafka consumer commit offset方式需要区分是否开启了checkpoint。 如果checkpoint关闭，commit offset要依赖于kafka客户端的auto commit。需设置enable.auto.commit， auto.commit.interval.ms 参数到consumer properties，就会按固定的时间间隔定期auto commit offset到kafka。 如果开启checkpoint，这个时候作业消费的offset是Flink在state中自己管理和容错。此时提交offset到kafka，一般都是作为外部进度的监控，想实时知道作业消费的位置和lag情况。此时需要setCommitOffsetsOnCheckpoints为true来设置当checkpoint成功时提交offset到kafka。此时commit offset的间隔就取决于checkpoint的间隔，所以此时从kafka一侧看到的lag可能并非完全实时，如果checkpoint间隔比较长lag曲线可能会是一个锯齿状。 Timestamp Extraction/Watermark生成我们知道当flink作业内使用EventTime属性时，需要指定从消息中提取时戳和生成水位的函数。FlinkKakfaConsumer构造的source后直接调用assignTimestampsAndWatermarks函数设置水位生成器的好处是此时是每个partition一个watermark assigner，如下图。source生成的睡戳为多个partition时戳对齐后的最小时戳。此时在一个source读取多个partition，并且partition之间数据时戳有一定差距的情况下，因为在source端watermark在partition级别有对齐，不会导致数据读取较慢partition数据丢失。 Flink kafka ProducerProducer 分区使用FlinkKafkaProducer往kafka中写数据时，如果不单独设置partition策略，会默认使用FlinkFixedPartitioner，该partitioner分区的方式是task所在的并发id对topic 总partition数取余：parallelInstanceId % partitions.length。此时如果sink为4，paritition为1，则4个task往同一个partition中写数据。但当sink task&lt; partition 个数时会有部分partition没有数据写入，例如sink task为2，partition总数为4，则后面两个partition将没有数据写入。如果构建FlinkKafkaProducer时，partition设置为null，此时会使用kafka producer默认分区方式，非key写入的情况下，使用round-robin的方式进行分区，每个task都会轮训的写下游的所有partition。该方式下游的partition数据会比较均衡，但是缺点是partition个数过多的情况下维持过多的网络链接，即每个task都会维持跟所有partition所在broker的链接。 容错Flink kafka 09、010版本下，通过setLogFailuresOnly为false，setFlushOnCheckpoint为true，能达到at-least-once语义。setLogFailuresOnly，默认为false，是控制写kafka失败时，是否只打印失败的log不抛异常让作业停止。setFlushOnCheckpoint，默认为true，是控制是否在checkpoint时fluse数据到kafka，保证数据已经写到kafka。否则数据有可能还缓存在kafka 客户端的buffer中，并没有真正写出到kafka，此时作业挂掉数据即丢失，不能做到至少一次的语义。 Flink kafka 011版本下，通过两阶段提交的sink结合kafka事务的功能，可以保证端到端精准一次。详细原理可以参考：https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka。 Q&amp;A(1)在flink consumer的并行度的设置：是对应topic的partitions个数吗？要是有多个主题数据源，并行度是设置成总体的partitions数吗？答：这个并不是绝对的，跟topic的数据量也有关，如果数据量不大，也可以设置小于partitions个数的并发数。但不要设置并发数大于partitions总数，因为这种情况下某些并发因为分配不到partition导致没有数据处理。 (2)如果 partitioner 传 null 的时候是 round-robin 发到每一个partition？如果有 key 的时候行为是 kafka 那种按照 key 分布到具体分区的行为吗？答：如果在构造FlinkKafkaProducer时，如果没有设置单独的partitioner，则默认使用FlinkFixedPartitioner，此时无论是带key的数据，还是不带key。如果主动设置partitioner为null时，不带key的数据会round-robin的方式写出，带key的数据会根据key，相同key数据分区的相同的partition，如果key为null，再轮询写。不带key的数据会轮询写各partition。 (3)如果checkpoint时间过长，offset未提交到kafka，此时节点宕机了，重启之后的重复消费如何保证呢？首先开启checkpoint时offset是flink通过状态state管理和恢复的，并不是从kafka的offset位置恢复。在checkpoint机制下，作业从最近一次checkpoint恢复，本身是会回放部分历史数据，导致部分数据重复消费，Flink引擎仅保证计算状态的精准一次，要想做到端到端精准一次需要依赖一些幂等的存储系统或者事务操作。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"}]},{"title":"如何使用 Flink 每天实时处理百亿条日志？","date":"2019-08-22T16:00:00.000Z","path":"2019/08/23/flink-ebay/","text":"Sherlock.IO 是 eBay 现有的监控平台，每天要处理上百亿条日志、事件和指标。Flink Streaming job 实时处理系统用于处理其中的日志和事件。 本文将结合监控系统 Flink 的现状，具体讲述 Flink 在监控系统上的实践和应用，希望给同业人员一些借鉴和启发。 监控系统 Flink 的现状eBay 的监控平台 Sherlock.IO 每天处理着上百亿条日志（log），事件（event）和指标（metric）。 通过构建 Flink Streaming job 实时处理系统，监控团队能够及时将日志和事件的处理结果反馈给用户。 当前，监控团队维护着 8 个 Flink 集群，最大的集群规模达到上千个 TaskManager，总共运行着上百个作业（job），一些作业已经稳定运行了半年以上。 元数据驱动为了让用户和管理员能够更加快捷地创建 Flink 作业并调整参数，监控团队在 Flink 上搭建了一套元数据微服务（metadata service）。 该服务能够用 Json 来描述一个作业的 DAG，且相同的 DAG 共用同一个作业，能够更加方便地创建作业，无需调用 Flink API。 Sherlock.IO 流处理整体的架构如图 1 所示： 目前，用这套元数据微服务创建的作业仅支持以 Kafka 作为数据源，只要数据接入到 Kafka，用户就可以定义 Capability 来处理逻辑从而通过 Flink Streaming 处理数据。 元数据微服务元数据微服务框架如图 2 所示，最上层是元数据微服务提供的 Restful API, 用户通过调用 API 来描述和提交作业。 描述作业的元数据包含三个部分： Capability Policy Resource Flink 适配器（Adaptor）连接了 Flink Streaming API 和元数据微服务 API，且会根据元数据微服务描述的作业调用 Flink Streaming API 来创建作业，从而屏蔽 Flink Stream API。 因此，用户不用了解 Flink Streaming API 就可以创建 Flink 作业。未来如果需要迁移到其他的流处理框架，只要增加一个适配器，就可以将现有的作业迁移到新的流处理框架上。 ①Capability Capability 定义了作业的 DAG 以及每个算子（Operator）所用的 Class，图 3 是事件处理（eventProcess）Capability，它最终会生成如图 4 的 DAG： 事件处理 Capability 先从 Kafka 读出数据，再写到 Elasticsearch 中。 该 Capability 将该作业命名为“eventProcess”，并定义其并行度为“5”，其算子为“EventEsIndexSinkCapability”， 其数据流为“Source→Sink”。 ②Policy 每个命名空间（Namespace）需要定义一个或多个 Policy，每个 Policy 指定了相应的 Capability，即指定了用哪一套 DAG 来运行这个 Policy。 Policy 还定义了这个作业的相关配置，例如从哪个 Kafka topic 中读取数据，写到 ElasticSearch 的哪个索引（Index）中，中间是否要跳过某些算子等等。 其次，Policy 还能作为一个简易的过滤器（Filter），可以通过配置 Jexl 表达式过滤掉一些不需要的数据，提高作业的吞吐量。 另外，我们还实现了 Zookeeper 定时更新的机制，使得 Policy 修改后不再需要重启作业，只要是在更新时间间隔内，该命名空间的 Policy 修改就会被自动应用到作业上。 图 5 是命名空间为 paas 的 Policy 示例： ③Resource Resource 定义了某个命名空间所需要的资源，比如 Flink 集群， Kafka broker，ES 集群等等。 我们有多个 Flink 集群和 ES 集群，通过 Resource 配置，作业可以知道某个命名空间的日志应该写到哪个 ES 集群，并可以判断该命名空间的数据应该从哪个 Kafka 集群读取。 共享作业为了减少作业数量，我们可以让相同的 DAG 复用同一个作业。我们先给不同的 Policy 指定相同的 Capability，在该 Capability 资源足够的情况下，这些 Policy 就会被调度到同一个作业上。 以 SQL 的 Capability 为例，每个 Policy 的 SQL 语句不尽相同，如果为每个 Policy 都创建一个作业， Job Manager 的开销就会很大，且不好管理。 因此，我们可以为 SQL Capability 配置 20 个 Slot，每个 Policy 占用一个 Slot。那么该 Capability 生成的作业就可以运行 20 个 Policy。 作业运行时，从 Source 读进来的数据会被打上相应 Policy 的标签，并执行该 Policy 定义的 SQL 语句，从而实现不同 Policy 共享同一个作业，大大减少了作业的数量。 用共享作业还有一个好处：如果多个命名空间的数据在一个 Kafka topic 里，那么只要读一遍数据即可，不用每个命名空间都读一次 topic 再过滤，这样就大大提高了处理的效率。 Flink 作业的优化和监控了解元数据驱动后，让我们来看看可以通过哪些方法实现 Flink 作业的优化和监控。 Heartbeat 在 Flink 集群的运维过程中，我们很难监控作业的运行情况。即使开启了检查点（checkpoint），我们也无法确定是否丢失数据或丢失了多少数据。因此，我们为每个作业注入了 Heartbeat 以监控其运行情况。 Heartbeat 就像 Flink 中用来监控延迟的“LatencyMarker”一样，它会流过每个作业的管道。 但与 LatencyMarker 不同的是，当 Heartbeat 遇到 DAG 的分支时，它会分裂并流向每个分支，而不像 LatencyMarker 那样随机流向某一个分支。 另一个不同点在于 Heartbeat 不是由 Flink 自身产生，而是由元数据微服务定时产生，而后由每个作业消费。 如上文中图 4 所示，每个作业在启动的时候会默认加一个 Heartbeat 的数据源。 Heartbeat 流入每个作业后，会随数据流一起经过每个节点，在每个节点上打上当前节点的标签，然后跳过该节点的处理逻辑流向下个节点。 直到 Heartbeat 流到最后一个节点时，它会以指标（Metric）的形式发送到 Sherlock.IO（eBay 监控平台）。 该指标包含了 Heartbeat 产生的时间，流入作业的时间以及到达每个节点的时间。 通过这个指标，我们可以判断该作业在读取 Kafka 时是否延时，以及一条数据被整个管道处理所用的时间和每个节点处理数据所用的时间，进而判断该作业的性能瓶颈。 由于 Heartbeat 是定时发送的，因此每个作业收到的 Heartbeat 个数应该一致。若最后发出的指标个数与期望不一致，则可以进一步判断是否有数据丢失。 图 6 描述了某 Flink 作业中的数据流以及 Heartbeat 的运行状态： 可用性有了 Heartbeat，我们就可以用来定义集群的可用性。首先，我们需要先定义在什么情况下属于不可用的： ①Flink 作业重启 当内存不足（OutofMemory）或代码运行错误时，作业就可能会意外重启。我们认为重启过程中造成的数据丢失是不可用的情况之一。因此我们的目标之一是让 Flink 作业能够长时间稳定运行。 ②Flink 作业中止 有时因为基础设施的问题导致物理机或者容器没启动起来，或是在 Flink 作业发生重启时由于 Slot 不够而无法启动，或者是因为 Flink 作业的重启次数已经超过了最大重启次数（rest.retry.max-attempts），Flink 作业就会中止。 此时需要人工干预才能将作业重新启动起来。我们认为 Flink 作业中止时，也是不可用的情况之一。 ④Flink 作业在运行中不再处理数据 发生这种情况，一般是因为遇到了反压（BackPressure）。造成反压的原因有很多种，比如上游的流量过大，或者是中间某个算子的处理能力不够，或者是下游存储节点遇到性能瓶颈等等。 虽然短时间内的反压不会造成数据丢失，但它会影响数据的实时性，最明显的变化是延迟这个指标会变大。 我们认为反压发生时是不可用的情况之一。针对以上三种情况，我们都可以用 Heartbeat 来监控，并计算可用性。 比如第一种情况，如果作业重启时发生了数据丢失，那么相应的那段管道的 Heartbeat 也会丢失，从而我们可以监测出是否有数据丢失以及粗粒度地估算数据丢了多少。 对于第二种情况，当作业中止时，HeartBeat 也不会被处理，因此可以很快发现作业停止运行并让 on-call 及时干预。 第三种情况当反压发生时，HeartBeat 也会被阻塞在发生反压的上游，因此 on-call 也可以很快地发现反压发生并进行人工干预。 综上，Heartbeat 可以很快监测出 Flink 作业的运行情况。那么，如何评估可用性呢？ 由于 Heartbeat 是定时发生的，默认情况下我们设置每 10 秒发一次。1 分钟内我们期望每个作业的每条管道能够发出 6 个带有作业信息的 Heartbeat，那么每天就可以收到 8640 个 Heartbeat。 因此，一个作业的可用性可以定义为： Flink 作业隔离Slot 是 Flink 运行作业的最小单位[1]，每个 TaskManager 可以分配一个至多个 Slot（一般分配的个数为该 TaskManager 的 CPU 数）。 根据 Flink 作业的并行度，一个作业可以分配到多个 TaskManager 上，而一个 TaskManager 也可能运行着多个作业。 然而，一个 TaskManager 就是一个 JVM，当多个作业分配到一个 TaskManager 上时，就会有抢夺资源的情况发生。 例如，我一个 TaskManager 分配了 3 个 Slot（3 个 CPU）和 8G 堆内存。 当 JobManager 调度作业的时候，有可能将 3 个不同作业的线程调度到该 TaskManager 上，那么这 3 个作业就会同时抢夺 CPU 和内存的资源。当其中一个作业特别耗 CPU 或内存的时候，就会影响其他两个作业。 在这种情况下，我们通过配置 Flink 可以实现作业的隔离，如图 7 所示： 通过以上配置，可以限定每个 TaskManager 独占 CPU 和内存的资源，且不会多个作业抢占，实现作业之间的隔离。 反压我们运维 Flink 集群的时候发现，出现最多的问题就是反压。在 3.2 中提到过，发生反压的原因有很多种，但无论什么原因，数据最终都会被积压在发生反压上游的算子的本地缓冲区（localBuffer）中。 我们知道，每一个 TaskManager 有一个本地缓冲池, 每一个算子数据进来后会把数据填充到本地缓冲池中，数据从这个算子出去后会回收这块内存。 当被反压后，数据发不出去，本地缓冲池内存就无法释放，导致一直请求缓冲区（requestBuffer）。 由于 Heartbeat 只能监控出是否发生了反压，但无法定位到是哪个算子出了问题。 因此我们定时地将每个算子的 StackTrace 打印出来，当发生反压时，通过 StackTrace 就可以知道是哪个算子的瓶颈。 如图8所示，我们可以清晰地看到发生反压的 Flink 作业及其所在的 Taskmanager。再通过 Thread Dump，我们就可以定位到代码的问题。 其他监控手段Flink 本身提供了很多有用的指标[2]来监控 Flink 作业的运行情况，在此基础上我们还加了一些业务上的指标。除此之外，我们还使用了以下工具监控 Flink 作业。 ①History server Flink 的 History server[3]可以查询已完成作业的状态和指标。比如一个作业的重启次数、它运行的时间。 我们常常用它找出运行不正常的作业。比如，我们可以通过 History server 的 Attempt 指标知道每个作业重启的次数，从而快速去现场找到重启的原因，避免下次再发生。 ②监控作业和集群 虽然 Flink 有 HA 的模式，但在极端情况下，例如整个集群出现问题时，需要 on-call 即时发觉并人工干预。 我们在元数据微服务中保存了最后一次提交作业成功的元数据，它记录了在每个 Flink 集群上应该运行哪些作业。 守护线程（Daemon thread）会每分钟去比较这个元数据和 Flink 上运行的作业，若发现 JobManager 连不通或者有作业运行不一致则立刻发出告警（Alert）通知 on-call。 实例下面介绍几个已经运行在监控系统上的 Flink 流处理系统的应用： Event Alerting 当前监控团队是基于 Flink Streaming 做事件告警（Event alerting），我们定义了一个告警算子 EventAlertingCapability，该 Capability 可以处理每个 Policy 自定义的规则。 如图 9 定义的一条性能监控规则： 该规则的含义是当性能检测器的应用为“r1rover”, 主机以“r1rover”开头，且数值大于 90 时，就触发告警。且生成的告警会发送到指定的 Kafka topic 中供下游继续处理。 EventzonEventzon 就像 eBay 的事件中心，它收集了从各个应用，框架，基础架构发过来的事件，最后通过监控团队的 Flink Streaming 实时生成告警。 由于各个事件的数据源不同，它们的元数据也不同，因此无法用一条统一的规则来描述它。 我们专门定义了一套作业来处理 Eventzon 的事件，它包含了多个 Capability，比如 Filter Capability，用来过滤非法的或者不符合条件的事件；又比如 Deduplicate Capability，可以用来去除重复的事件。 Eventzon 的所有事件经过一整套作业后，会生成有效的告警，并根据通知机制通过 E-mail、Slack 或 Pagerduty 发给相关团队。 NetmonNetmon 的全称为 Network Monitoring, 即网络监控，它可以用来监控整个 eBay 网络设备的健康状态。它的数据源来自 eBay 的交换机，路由器等网络设备的日志。 Netmon 的作用是根据这些日志找出一些特定的信息，往往是一些错误的日志，以此来生成告警。 eBay 的每一台设备都要“登记造册”，每台设备将日志发过来后，我们通过 EnrichCapability 从“册子”中查询这台设备的信息，并把相关信息比如 IP 地址，所在的数据中心，所在的机架等填充到日志信息中作为事件保存。 当设备产生一些特定的错误日志时, 它会被相应的规则匹配然后生成告警，该告警会被 EventProcess Capability 保存到 Elasticsearch 中实时显示到 Netmon 的监控平台（dashboard）上。 有时因为网络抖动导致一些短暂的错误发生，但系统过一会儿就会自动恢复。 当上述情况发生时，Netmon 会有相应的规则将发生在网络抖动时生成的告警标记为“已解决”（Resolved）。 对于一些必须人工干预的告警，运维人员可以通过网络监控平台（Netmon dashboard）手动点击“已解决”，完成该告警的生命周期。 总结与展望eBay 的监控团队希望能根据用户提供的指标、事件和日志以及相应的告警规则实时告警用户。 Flink Streaming 能够提供低延时的处理从而能够达到我们低延时的要求，并且它适合比较复杂的处理逻辑。 然而在运维 Flink 的过程中，我们也发现了由于作业重启等原因导致误报少报告警的情况发生，从而误导客户。因此今后我们会在 Flink 的稳定性和高可用性上投入更多。 我们也希望在监控指标、日志上能够集成一些复杂的 AI 算法，从而能够生成更加有效精确的告警，成为运维人员的一把利器。 参考文献：[1]https://ci.apache.org/projects/flink/flink-docs-release-1.7/concepts/runtime.html#task-slots-and-resources [2]https://ci.apache.org/projects/flink/flink-docs-release-1.7/monitoring/metrics.html [3]https://ci.apache.org/projects/flink/flink-docs-release-1.4/monitoring/historyserver.html 作者：顾欣怡编译出处：https://yq.aliyun.com/articles/714592?utm_content=g_1000072645 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"}]},{"title":"修改代码150万行！Apache Flink 1.9.0做了这些重大修改！","date":"2019-08-21T16:00:00.000Z","path":"2019/08/22/flink-1.9/","text":"导读：8月22日，Apache Flink 1.9.0 正式发布。早在今年1月，阿里便宣布将内部过去几年打磨的大数据处理引擎Blink进行开源并向 Apache Flink 贡献代码。此次版本在结构上有重大变更，修改代码达150万行，接下来，我们一起梳理 Flink 1.9.0 中非常值得关注的重要功能与特性。 本文作者：杨克特(鲁尼) Flink 1.9.0是阿里内部版本 Blink 合并入 Flink 后的首次发版，修改代码150万行，此次发版不仅在结构上有重大变更，在功能特性上也更加强大与完善。本文将为大家介绍 Flink 1.9.0 有哪些重大变更与新增功能特性。 在此先简单回顾一下阿里巴巴Blink 开源的部分要点： Blink 开源的内容主要是阿里巴巴基于开源 Flink 引擎，依托集团内部业务，在流计算和批处理上积累的大量新功能、性能优化、稳定性提升等核心代码。 Blink 以分支的形式开源，即开源后会成为 Apache Flink项目下的一个分支。 Blink 开源的目标不是希望成为另一个活跃的项目，而是将Flink 做的更好。通过开源的方式让大家了解所有 Blink 的实现细节，提高 Blink 功能merge进入Flink 的效率，与社区协作更高效。 半年的时间过去了，随着 Flink 1.9.0 版本的发布，在此我们可以骄傲的宣布：Blink 团队已经实现了之前的诺言！尽管不是所有功能都顺利 merge 回了社区，但是在我们和社区的共同努力下，Flink 正在朝着它最初的梦想大踏步的迈进。 先和大家分享几个 Flink 1.9.0 版本与之前个版本的对比数字： 从解决的 issue 数量和代码 commit 数量来看，1.9.0 已经达到甚至超过了之前两个版本的总和。 从修改的代码行数来看，达到了惊人的150 万行。虽然受一些模块重构以及 Blink merge 等因素的影响，但不可否认的是，1.9.0 版本一定是 Flink 有史以来开发者们最活跃的版本。 从Contributor 数量来看，Flink 也已经吸引了越来越多的贡献者。我相信其中就有不少来自中国的用户和开发者，社区也响应号召开通了中文邮件列表。 那么，1.9.0 版本究竟由哪些变更而引发了如此大量的修改，以下将详细说明。 架构升级基本上，系统如果有非常大的变动，那一定是架构升级带来的。这次也不例外，Flink 在流批融合的方向上迈进了一大步。首先我们来看一下 Flink之前版本的架构图： 相信熟悉Flink 的读者们对左边的架构图一定不会感到陌生。简单来说，Flink 在其分布式流式执行引擎之上，有两套相对独立的 DataStream 和 DataSet API，分别来描述流计算和批处理的作业。在这两个 API之上，则提供了一个流批统一的API，即 Table API 和SQL。用户可以使用相同的Table API 程序或者 SQL 来描述流批作业，只是在运行时需要告诉 Flink 引擎希望以流的形式运行还是以批的流式运行，此时 Table 层的优化器就会将程序优化成 DataStream 作业或者 DataSet 作业。 但是如果我们仔细查看 DataStream 和 DataSet 底层的实现细节，会发现这两个 API 共享的东西其实不多。它们有各自独立的翻译和优化的流程，而且在真正运行的时候，两者也使用了完全不同的 Task。这样的不一致对用户和开发者来讲可能存在问题。 从用户的角度来说，他们在编写作业的时候需要在两个 API 之间进行选择，而这两个 API 不仅语义不同，同时支持的 connector 种类也不同，难免会造成一些困扰。Table 尽管在 API 上已经进行了统一，但因为底层实现还是基于 DataStream 和 DataSet，也会受到刚才不一致的问题的影响。 从开发者角度来说，由于这两套流程相对独立，因此基本上很难做到代码的复用。我们在开发一些新功能的时候，往往需要将类似的功能开发两次，并且每种 API 的开发路径都比较长，基本都属于端到端的修改，这大大降低了我们的开发效率。如果两条独立的技术栈长期存在，不仅会造成人力的长期浪费，最终可能还会导致整个 Flink 的功能开发变慢。 在 Blink 一些先行探索的基础之上，我们和社区的开发人员进行了密切的讨论，最终基本敲定了 Flink 未来的技术架构路线。 在 Flink 的未来版本中，我们将舍弃 DataSet API，用户的 API 主要会分为偏描述物理执行计划的 DataStream API 以及偏描述关系型计划的 Table &amp; SQL。DataStream API 提供给用户更多的是一种“所见即所得”的体验，由用户自行描述和编排算子的关系，引擎不会做过多的干涉和优化。而Table API &amp; SQL 则继续保持现在的风格，提供关系表达式API，引擎会根据用户的意图来进行优化，并选择最优的执行计划。值得一提的是，以后这两个 API 都会各自同时提供流计算和批处理的功能。这两个用户 API 之下，在实现层它们都会共享相同的技术栈，比如会用统一的 DAG 数据结构来描述作业，使用统一的 StreamOperator 来编写算子逻辑，包括使用统一的流式分布式执行引擎。 TableAPI &amp; SQL在开源 Blink 时，Blink 的Table 模块已经使用了 Flink 未来设想的新架构。因此 Flink 1.9 版本中，Table 模块顺理成章的成为了架构调整后第一个吃螃蟹的人。但是，为了尽量不影响之前版本用户的体验，我们还是需要找到一个方式让两种架构能够并存。 基于这个目的，社区的开发人员做了一系列的努力，包括将 Table 模块进行拆分（FLIP-32，FLIP 即 Flink Improvement Proposals，专门记录一些对Flink 做较大修改的提议），对 Java 和 Scala 的 API 进行依赖梳理，并且提出了 Planner 接口以支持多种不同的 Planner 实现。Planner 将负责具体的优化和将 Table 作业翻译成执行图的工作，我们可以将原来的实现全部挪至 Flink Planner 中，然后把对接新架构的代码放在 Blink Planner里。 图中的 Query Processor 就是 Planner 的实现 这样的做法一举两得。不仅让 Table 模块在经过拆分后更加清晰，更重要的是不影响老版本用户的体验。 在 1.9 版本中，我们已经merge 了大部分当初从 Blink 开源出来的 SQL功能。这些都是近几年在阿里内部场景经过千锤百炼而沉淀出来的新功能和性能上的优化，相信能够促使Flink 更上一个台阶！ 除了架构升级之外，Table 模块在 1.9 版本还做了几个相对比较大的重构和新功能，包括： FLIP-37：重构 Table API 类型系统 FLIP-29：Table 增加面向多行多列操作的 API FLINK-10232：初步的 SQL DDL 支持 FLIP-30：全新的统一的 Catalog API FLIP-38：Table API 增加 Python 版本 有了这些新功能加持，再经过后续修复和完善，Flink Table API 和 SQL 在未来将会发挥越来越重要的作用。 批处理改进Flink的批处理功能在 1.9 版本有了重大进步，在架构调整后，Flink 1.9 加入了好几项对批处理的功能改进。 首当其冲的是优化批处理的错误恢复代价：FLIP-1（Fine Grained Recovery from Task Failures），从这个 FLIP 的编号就可以看出，该优化其实很早就已经提出，1.9 版本终于有机会将 FLIP-1 中未完成的功能进行了收尾。在新版本中，如果批处理作业有错误发生，那么 Flink 首先会去计算这个错误的影响范围，即 Failover Region。因为在批处理作业中，有些节点之间可以通过网络进行Pipeline 的数据传输，但其他一些节点可以通过 Blocking 的方式先把输出数据存下来，然后下游再去读取存储的数据的方式进行数据传输。如果算子输出的数据已经完整的进行了保存，那么就没有必要把这个算子拉起重跑，这样一来就可以把错误恢复控制在一个相对较小的范围里。 如果作业极端一点，在每一个需要Shuffle 的地方都进行数据落盘，那么就和 MapReduce 以及 Spark 的行为类似了。只是 Flink 支持更高级的用法，你可以自行控制每种 Shuffle 是使用网络来直连，还是通过文件落盘来进行。 有了基于文件的Shuffle 之后，大家很容易就会联想到，是不是可以把这个 Shuffle 的实现变成插件化。没错，社区也正在朝这个方向进行改进：FLIP-31（Pluggable Shuffle Service）。比如，我们可以利用 Yarn 的 Auxliary Service 来作为一种 Shuffle 的实现，我们甚至可以去写一个分布式服务来帮助批处理任务进行Shuffle。最近，Facebook 也分享了一些这方面的工作，而且在阿里内部，我们已经使用这样的架构，支持了单作业处理数百TB 量级的规模。Flink 具备了这样的插件机制后，可以轻松的对接这些更加高效灵活的实现，让Shuffle 这个批处理的老大难问题得到较好的解决。 流处理改进流计算毕竟还是 Flink 发迹的主要领域，在 1.9 版本当然也不能忘了在这方面做一些改进。这个版本增加了一个非常实用的功能，即FLIP-43（State Processor API）。Flink 的 State 数据的访问，以及由 State 数据组成的 Savepoint 的访问一直是社区用户呼声比较高的一个功能。在 1.9 之前的版本，Flink 开发了 Queryable State，不过这个功能的使用场景比较有限，使用效果也不太理想，因此用的人一直不多。这次的 State Processor API 则提供了更加灵活的访问手段，也能够让用户完成一些比较黑科技的功能： 用户可以使用这个 API 事先从其他外部系统读取数据，把它们转存为 Flink Savepoint 的格式，然后让 Flink 作业从这个 Savepoint 启动。这样一来，就能避免很多冷启动的问题。 使用 Flink 的批处理 API 直接分析State 的数据。State 数据一直以来对用户是个黑盒，这里面存储的数据是对是错，是否有异常，用户都无从而知。有了这个 API 之后，用户就可以像分析其他数据一样，来对 State 数据进行分析。 脏数据订正。假如有一条脏数据污染了你的 State，用户还可以使用这个 API 对这样的问题进行修复和订正。 状态迁移。当用户修改了作业逻辑，想复用大部分原来作业的 State，但又希望做一些微调。那么就可以使用这个 API 来完成相应的工作。 上面列举的都是流计算领域非常常见的需求和问题，都有机会通过这个灵活的 API 进行解决，因此我个人非常看好这个 API 的应用前景。 说到 Savepoint，这里也提一下社区完成的另外一个实用功能，即FLIP-34（Stop with Savepoint）。大家都知道 Flink 会周期性的进行 Checkpoint，并且维护了一个全局的状态快照。假如我们碰到这种场景：用户在两个Checkpoint 周期中间主动暂停了作业，然后过一会又进行重启。这样，Flink 会自动读取上一次成功保存的全局状态快照，并开始计算上一次全局快照之后的数据。虽然这么做能保证状态数据的不多不少，但是输出到 Sink 的却已经有重复数据了。有了这个功能之后，Flink 会在暂停作业的同时做一次全局快照，并存储到Savepoint。下次启动时，会从这个 Savepoint 启动作业，这样 Sink 就不会收到预期外的重复数据了。不过，这个做法并不能解决作业在运行过程中自动Failover而引起的输出到 Sink 数据重复问题。 Hive集成Hive一直是 Hadoop 生态中一股不可忽视的重要力量。为了更好的推广 Flink 的批处理功能，和 Hive 的集成必不可少。在 1.9 版本的开发过程中，我们也很开心迎来了两位 Apache Hive PMC 来推进 Flink 和 Hive 的集成工作。 首先要解决的是使用 Flink 读取 Hive 数据的问题。通过 FLIP-30 提出的统一的 Catalog API 的帮助，目前 Flink 已经完整打通了对 Hive Meta Store 的访问。同时，我们也增加了 Hive 的 Connector，目前已支持 CSV, Sequence File, Orc, Parquet 等格式。用户只需要配置 HMS 的访问方式，就可以使用 Flink 直接读取 Hive 的表进行操作。在此基础之上，Flink 还增加了对 Hive 自定义函数的兼容，像 UDF， UDTF和 UDAF，都可以直接运行在Flink SQL里。 在写的支持上，目前Flink 还支持的比较简单，暂时只能 INSERT INTO 一张新表。不过和 Hive 的兼容一直是社区工作中一个高优先级的事情，相信后续的版本会有持续的改善。 总结Flink1.9.0 版本经过大半年的紧张开发，终于顺利发布。在这过程中，Flink 社区不仅迎来了相当多的中国开发者和用户，还迎来了海量的代码贡献，预示着一个良好的开端。未来，无论是功能还是生态，我们会继续在 Flink 社区加大投入，让 Flink 在整个中国乃至全世界大规模的使用起来。我们也衷心希望有更多的开发者可以加入我们，加入Flink 社区，一起把 Apache Flink 做的越来越好！ 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"}]},{"title":"一文搞懂 Flink 的 Exactly Once 和 At Least Once","date":"2019-08-20T16:00:00.000Z","path":"2019/08/21/Flink-Exactly_Once_vs_At_Least_Once/","text":"本文主要为了让你搞懂 Flink 的 Exactly Once 和 At Least Once，如果看完之后，你有什么不懂的，可以留言 本文由知识星球小伙伴 范瑞 投稿，原文地址：https://www.jianshu.com/p/8d6569361999 看完本文，你能 get 到以下知识 介绍 CheckPoint 如何保障 Flink 任务的高可用 CheckPoint 中的状态简介 如何实现全域一致的分布式快照？ 什么是 barrier？什么是 barrier 对齐？ 证明了：为什么 barrier 对齐就是 Exactly Once？为什么 barrier 不对齐就是 At Least Once？ Flink简介有状态函数和运算符在各个元素/事件的处理中存储数据（状态数据可以修改和查询，可以自己维护，根据自己的业务场景，保存历史数据或者中间结果到状态中） 例如： 当应用程序搜索某些事件模式时，状态将存储到目前为止遇到的事件序列。 在每分钟/小时/天聚合事件时，状态保存待处理的聚合。 当在数据点流上训练机器学习模型时，状态保持模型参数的当前版本。 当需要管理历史数据时，状态允许有效访问过去发生的事件。 什么是状态？ 无状态计算的例子 比如：我们只是进行一个字符串拼接，输入 a，输出 a_666,输入b，输出 b_666 输出的结果跟之前的状态没关系，符合幂等性。 幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用 有状态计算的例子 计算pv、uv 输出的结果跟之前的状态有关系，不符合幂等性，访问多次，pv会增加 Flink的CheckPoint功能简介 Flink CheckPoint 的存在就是为了解决flink任务failover掉之后，能够正常恢复任务。那CheckPoint具体做了哪些功能，为什么任务挂掉之后，通过CheckPoint能使得任务恢复呢？ CheckPoint是通过给程序快照的方式使得将历史某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。问题来了，快照是什么鬼？能吃吗？ SnapShot翻译为快照，指将程序中某些信息存一份，后期可以用来恢复。对于一个Flink任务来讲，快照里面到底保存着什么信息呢？ 晦涩难懂的概念怎么办？当然用案例来代替咯，用案例让大家理解快照里面到底存什么信息。选一个大家都比较清楚的指标，app的pv，flink该怎么统计呢？ 我们从Kafka读取到一条条的日志，从日志中解析出app_id，然后将统计的结果放到内存中一个Map集合，app_id做为key，对应的pv做为value，每次只需要将相应app_id 的pv值+1后put到Map中即可 flink的Source task记录了当前消费到kafka test topic的所有partition的offset，为了方便理解CheckPoint的作用，这里先用一个partition进行讲解，假设名为 “test”的 topic只有一个partition0 例：（0，1000） 表示0号partition目前消费到offset为1000的数据 flink的pv task记录了当前计算的各app的pv值，为了方便讲解，我这里有两个app：app1、app2 例：（app1，50000）（app2，10000） 表示app1当前pv值为50000 表示app2当前pv值为10000 每来一条数据，只需要确定相应app_id，将相应的value值+1后put到map中即可 该案例中，CheckPoint到底记录了什么信息呢？ offset：（0，1000） pv：（app1，50000）（app2，10000） 记录的其实就是第n次CheckPoint消费的offset信息和各app的pv值信息，记录一下发生CheckPoint当前的状态信息，并将该状态信息保存到相应的状态后端。（注：状态后端是保存状态的地方，决定状态如何保存，如何保障状态高可用，我们只需要知道，我们能从状态后端拿到offset信息和pv信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过checkpoint来恢复我们的应用程序） chk-100 该状态信息表示第100次CheckPoint的时候， partition 0 offset消费到了1000，pv统计结果为（app1，50000）（app2，10000） 任务挂了，如何恢复？ 假如我们设置了三分钟进行一次CheckPoint，保存了上述所说的 chk-100 的CheckPoint状态后，过了十秒钟，offset已经消费到 （0，1100），pv统计结果变成了（app1，50080）（app2，10020），但是突然任务挂了，怎么办？ 莫慌，其实很简单，flink只需要从最近一次成功的CheckPoint保存的offset（0，1000）处接着消费即可，当然pv值也要按照状态里的pv值（app1，50000）（app2，10000）进行累加，不能从（app1，50080）（app2，10020）处进行累加，因为 partition 0 offset消费到 1000时，pv统计结果为（app1，50000）（app2，10000） 当然如果你想从offset （0，1100）pv（app1，50080）（app2，10020）这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来。我们能做的最高效方式就是从最近一次成功的CheckPoint处恢复，也就是我一直所说的chk-100 以上讲解，基本就是CheckPoint承担的工作，描述的场景比较简单 疑问，计算pv的task在一直运行，它怎么知道什么时候去做这个快照？或者说计算pv的task怎么保障它自己计算的pv值（app1，50000）（app2，10000）就是offset（0，1000）那一刻的统计结果呢？ + barrier从Source Task处生成，一直流到Sink Task，期间所有的Task只要碰到barrier，就会触发自身进行快照 + CheckPoint barrier n-1处做的快照就是指Job从开始处理到 barrier n-1所有的状态数据 + barrier n 处做的快照就是指从Job开始到处理到 barrier n所有的状态数据 + 对应到pv案例中就是，Source Task接收到JobManager的编号为chk-100的CheckPoint触发请求后，发现自己恰好接收到kafka offset（0，1000）处的数据，所以会往offset（0，1000）数据之后offset（0，1001）数据之前安插一个barrier，然后自己开始做快照，也就是将offset（0，1000）保存到状态后端chk-100中。然后barrier接着往下游发送，当统计pv的task接收到barrier后，也会暂停处理数据，将自己内存中保存的pv信息（app1，50000）（app2，10000）保存到状态后端chk-100中。OK，flink大概就是通过这个原理来保存快照的 + 统计pv的task接收到barrier，就意味着barrier之前的数据都处理了，所以说，不会出现丢数据的情况 + barrier的作用就是为了把数据区分开，CheckPoint过程中有一个同步做快照的环节不能处理barrier之后的数据，为什么呢？ + 如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据 + 结合案例来讲就是，统计pv的task想对（app1，50000）（app2，10000）做快照，但是如果数据还在处理，可能快照还没保存下来，状态已经变成了（app1，50001）（app2，10001），快照就不准确了，就不能保障Exactly Once了 + flink是在数据中加了一个叫做barrier的东西（barrier中文翻译：栅栏），下图中红圈处就是两个barrier 总结 流式计算中状态交互 简易场景精确一次的容错方法 + 消费到Y位置的时候，将Y对应的状态保存下来 + 消费到X位置的时候，将X对应的状态保存下来 + 周期性地对消费offset和统计的状态信息或统计结果进行快照 多并行度、多Operator情况下，CheckPoint过程 分布式状态容错面临的问题与挑战 如何确保状态拥有精确一次的容错保证？ 如何在分布式场景下替多个拥有本地状态的算子产生一个全域一致的快照？ 如何在不中断运算的前提下产生快照？ 多并行度、多Operator实例的情况下，如何做全域一致的快照 所有的Operator运行过程中遇到barrier后，都对自身的状态进行一次快照，保存到相应状态后端 + 对应到pv案例：有的Operator计算的app1的pv，有的Operator计算的app2的pv，当他们碰到barrier时，都需要将目前统计的pv信息快照到状态后端 多Operator状态恢复 具体怎么做这个快照呢？ 利用之前所有的barrier策略 JobManager向Source Task发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier Source Task自身做快照，并保存到状态后端 Source Task将barrier跟数据流一块往下游发送 当下游的Operator实例接收到CheckPoint barrier后，对自身做快照 上述图中，有4个带状态的Operator实例，相应的状态后端就可以想象成填4个格子。整个CheckPoint 的过程可以当做Operator实例填自己格子的过程，Operator实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单的认为一次完整的CheckPoint做完了 上面只是快照的过程，整个CheckPoint执行过程如下 1、JobManager端的 CheckPointCoordinator向 所有SourceTask发送CheckPointTrigger，Source Task会在数据流中安插CheckPoint barrier 2、当task收到所有的barrier后，向自己的下游继续传递barrier，然后自身执行快照，并将自己的状态异步写入到持久化存储中 增量CheckPoint只是把最新的一部分更新写入到 外部存储 为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照 3、当task完成备份后，会将备份数据的地址（state handle）通知给JobManager的CheckPointCoordinator 如果CheckPoint的持续时长超过 了CheckPoint设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次CheckPoint失败，会把这次CheckPoint产生的所有 状态数据全部删除 4、 最后 CheckPoint Coordinator 会把整个 StateHandle 封装成 completed CheckPoint Meta，写入到hdfs barrier对齐 什么是barrier对齐？ + 一旦Operator从输入流接收到CheckPoint barrier n，它就不能处理来自该流的任何数据记录，直到它从其他所有输入接收到barrier n为止。否则，它会混合属于快照n的记录和属于快照n + 1的记录 + 接收到barrier n的流暂时被搁置。从这些流接收的记录不会被处理，而是放入输入缓冲区。 + 上图中第2个图，虽然数字流对应的barrier已经到达了，但是barrier之后的1、2、3这些数据只能放到buffer中，等待字母流的barrier到达 + 一旦最后所有输入流都接收到barrier n，Operator就会把缓冲区中pending 的输出数据发出去，然后把CheckPoint barrier n接着往下游发送 + 这里还会对自身进行快照 + 之后，Operator将继续处理来自所有输入流的记录，在处理来自流的记录之前先处理来自输入缓冲区的记录 什么是barrier不对齐？ 上述图2中，当还有其他输入流的barrier还没有到达时，会把已到达的barrier之后的数据1、2、3搁置在缓冲区，等待其他流的barrier到达后才能处理 barrier不对齐就是指当还有其他流的barrier还没到达时，为了不影响性能，也不用理会，直接处理barrier之后的数据。等到所有流的barrier的都到达后，就可以对该Operator做CheckPoint了 为什么要进行barrier对齐？不对齐到底行不行？ 答：Exactly Once时必须barrier对齐，如果barrier不对齐就变成了At Least Once 后面的部分主要证明这句话 CheckPoint的目的就是为了保存快照，如果不对齐，那么在chk-100快照之前，已经处理了一些chk-100 对应的offset之后的数据，当程序从chk-100恢复任务时，chk-100对应的offset之后的数据还会被处理一次，所以就出现了重复消费。如果听不懂没关系，后面有案例让您懂 结合pv案例来看，之前的案例为了简单，描述的kafka的topic只有1个partition，这里为了讲述barrier对齐，所以topic有2个partittion + Flink 同样会起四个Operator实例，我还称他们是 TaskA0、TaskA1、TaskB0、TaskB1。四个Operator会从状态后端读取保存的状态信息。 + 从offset：(0，10000)(1，10005) 开始消费，并且基于 pv：(app0，8000) (app1，12050)值进行累加统计 + 然后你就应该会发现这个app1的pv值12050实际上已经包含了partition1的offset 10005~10200的数据，所以partition1从offset 10005恢复任务时，partition1的offset 10005~10200的数据被消费了两次 + TaskB1设置的barrier不对齐，所以CheckPoint chk-100对应的状态中多消费了barrier之后的一些数据（TaskA1发送），重启后是从chk-100保存的offset恢复，这就是所说的At Least Once + 由于上面说TaskB0设置的barrier对齐，所以app0不会出现重复消费，因为app0没有消费offset：(0，10000)(1，10005) 之后的数据，也就是所谓的Exactly Once + chk-100 + offset：(0，10000)(1，10005) + pv：(app0，8000) (app1，12050) + 虽然状态保存的pv值偏高了，但是不能说明重复处理，因为我的TaskA1并没有再次去消费partition1的offset 10005~10200的数据，所以相当于也没有重复消费，只是展示的结果更实时了 + 这里假如TaskA0消费的partition0的offset为10000，TaskA1消费的partition1的offset为10005。那么状态中会保存 (0，10000)(1，10005)，表示0号partition消费到了offset为10000的位置，1号partition消费到了offset为10005的位置 + 结合业务，先介绍一下上述所有算子在业务中的功能 + Source的kafka的Consumer，从kakfa中读取数据到flink应用中 + TaskA中的map将读取到的一条kafka日志转换为我们需要统计的app_id + keyBy 按照app_id进行keyBy，相同的app_id 会分到下游TaskB的同一个实例中 + TaskB的map在状态中查出该app_id 对应的pv值，然后+1，存储到状态中 + 利用Sink将统计的pv值写入到外部存储介质中 + 我们从kafka的两个partition消费数据，TaskA和TaskB都有两个并行度，所以总共flink有4个Operator实例，这里我们称之为 TaskA0、TaskA1、TaskB0、TaskB1 + 假设已经成功做了99次CheckPoint，这里详细解释第100次CheckPoint过程 + JobManager内部有个定时调度，假如现在10点00分00秒到了第100次CheckPoint的时间了，JobManager的CheckPointCoordinator进程会向所有的Source Task发送CheckPointTrigger，也就是向TaskA0、TaskA1发送CheckPointTrigger + TaskA0、TaskA1接收到CheckPointTrigger，会往数据流中安插barrier，将barrier发送到下游，在自己的状态中记录barrier安插的offset位置，然后自身做快照，将offset信息保存到状态后端 + 然后TaskA的map和keyBy算子中并没有状态，所以不需要进行快照 + 接着数据和barrier都向下游TaskB发送，相同的app_id 会发送到相同的TaskB实例上，这里假设有两个app：app0和app1，经过keyBy后，假设app0分到了TaskB0上，app1分到了TaskB1上。基于上面描述，TaskA0和TaskA1中的所有app0的数据都发送到TaskB0上，所有app1的数据都发送到TaskB1上 + 现在我们假设TaskB0做CheckPoint的时候barrier对齐了，TaskB1做CheckPoint的时候barrier不对齐，当然不能这么配置，我就是举这么个例子，带大家分析一下barrier对不对齐到底对统计结果有什么影响？ + 上面说了chk-100的这次CheckPoint，offset位置为(0，10000)(1，10005)，TaskB0使用barrier对齐，也就是说TaskB0不会处理barrier之后的数据，所以TaskB0在chk-100快照的时候，状态后端保存的app0的pv数据是从程序开始启动到kafka offset位置为(0，10000)(1，10005)的所有数据计算出来的pv值，一条不多（没处理barrier之后，所以不会重复），一条不少(barrier之前的所有数据都处理了，所以不会丢失)，假如保存的状态信息为(app0，8000)表示消费到(0，10000)(1，10005)offset的时候，app0的pv值为8000 + TaskB1使用的barrier不对齐，假如TaskA0由于服务器的CPU或者网络等其他波动，导致TaskA0处理数据较慢，而TaskA1很稳定，所以处理数据比较快。导致的结果就是TaskB1先接收到了TaskA1的barrier，由于配置的barrier不对齐，所以TaskB1会接着处理TaskA1 barrier之后的数据，过了2秒后，TaskB1接收到了TaskA0的barrier，于是对状态中存储的app1的pv值开始做CheckPoint 快照，保存的状态信息为(app1，12050)，但是我们知道这个(app1，12050)实际上多处理了2秒TaskA1发来的barrier之后的数据，也就是kafka topic对应的partition1 offset 10005之后的数据，app1真实的pv数据肯定要小于这个12050，partition1的offset保存的offset虽然是10005，但是我们实际上可能已经处理到了offset 10200的数据，假设就是处理到了10200 + 分析到这里，我们先梳理一下我们的状态保存了什么： + 接着程序在继续运行，过了10秒，由于某个服务器挂了，导致我们的四个Operator实例有一个Operator挂了，所以Flink会从最近一次的状态恢复，也就是我们刚刚详细讲的chk-100处恢复，那具体是怎么恢复的呢？ 看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么barrier对齐就是Exactly Once，为什么barrier不对齐就是 At Least Once 分析了这么多，这里我再补充一个问题，到底什么时候会出现barrier对齐？ 首先设置了Flink的CheckPoint语义是：Exactly Once Operator实例必须有多个输入流才会出现barrier对齐 对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以，必须有多个流才叫对齐。barrier对齐其实也就是上游多个流配合使得数据对齐的过程 言外之意：如果Operator实例只有一个输入流，就根本不存在barrier对齐，自己跟自己默认永远都是对齐的 博客发出去后，感谢上海姜同学提问的几个问题，最后跟姜同学语音了2个多小时，交流了很多Flink相关技术，最后提炼了以下三个问题，当然讨论的很多Flink的其他技术并没有放到该博客中 第一种场景计算PV，kafka只有一个partition，精确一次，至少一次就没有区别？ 答：如果只有一个partition，对应flink任务的Source Task并行度只能是1，确实没有区别，不会有至少一次的存在了，肯定是精确一次。因为只有barrier不对齐才会有可能重复处理，这里并行度都已经为1，默认就是对齐的，只有当上游有多个并行度的时候，多个并行度发到下游的barrier才需要对齐，单并行度不会出现barrier不对齐，所以必然精确一次。其实还是要理解barrier对齐就是Exactly Once不会重复消费，barrier不对齐就是 At Least Once可能重复消费，这里只有单个并行度根本不会存在barrier不对齐，所以不会存在至少一次语义 为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照；这一步，如果向下发送barrier后，自己同步快照慢怎么办？下游已经同步好了，自己还没？ 答: 可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游快照的更及时了，我只要保障下游把barrier之前的数据都处理了，并且不处理barrier之后的数据，然后做快照，那么下游也同样支持精确一次。这个问题你不要从全局思考，你单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意一点，如果有一个Operator 的CheckPoint失败了或者因为CheckPoint超时也会导致失败，那么JobManager会认为整个CheckPoint失败。失败的CheckPoint是不能用来恢复任务的，必须所有的算子的CheckPoint都成功，那么这次CheckPoint才能认为是成功的，才能用来恢复任务 我程序中Flink的CheckPoint语义设置了 Exactly Once，但是我的mysql中看到数据重复了？程序中设置了1分钟1次CheckPoint，但是5秒向mysql写一次数据，并commit 答：Flink要求end to end的精确一次都必须实现TwoPhaseCommitSinkFunction。如果你的chk-100成功了，过了30秒，由于5秒commit一次，所以实际上已经写入了6批数据进入mysql，但是突然程序挂了，从chk100处恢复，这样的话，之前提交的6批数据就会重复写入，所以出现了重复消费。Flink的精确一次有两种情况，一个是Flink内部的精确一次，一个是端对端的精确一次，这个博客所描述的都是关于Flink内部去的精确一次，我后期再发一个博客详细介绍一下Flink端对端的精确一次如何实现 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"}]},{"title":"美团点评基于 Flink 的实时数仓建设实践","date":"2019-08-19T16:00:00.000Z","path":"2019/08/20/Flink-meituan-dw/","text":"引言近些年，企业对数据服务实时化服务需求日益增多。本文整理了常见实时数据组件的性能特点和适用场景，介绍了美团如何通过 Flink 引擎构建实时数据仓库，从而提供高效、稳健的实时数据服务。此前我们美团技术博客发布过一篇文章《流计算框架 Flink 与 Storm 的性能对比》，对 Flink 和 Storm 两个引擎的计算性能进行了比较。本文主要阐述使用 Flink 在实际数据生产上的经验。 实时平台初期架构在实时数据系统建设初期，业务需求也相对较少，还没有形成完整的数据体系。我们采用的是“一路到底”的开发模式：通过在实时计算平台上部署 Storm 作业处理实时数据队列来提取数据指标，直接推送到实时应用服务中。 但是，随着产品和业务人员对实时数据需求的不断增多，新的挑战也随之发生。 1、数据指标越来越多，“烟囱式”的开发导致代码耦合问题严重。 2、需求越来越多，有的需要明细数据，有的需要 OLAP 分析。单一的开发模式难以应付多种需求。 3、缺少完善的监控系统，无法在对业务产生影响之前发现并修复问题。 实时数据仓库的构建为解决以上问题，我们根据生产离线数据的经验，选择使用分层设计方案来建设实时数据仓库，其分层架构如下图所示： 该方案由以下四层构成： ODS 层：Binlog 和流量日志以及各业务实时队列。 数据明细层：业务领域整合提取事实数据，离线全量和实时变化数据构建实时维度数据。 数据汇总层：使用宽表模型对明细数据补充维度数据，对共性指标进行汇总。 App 层：为了具体需求而构建的应用层，通过 RPC 框架对外提供服务。 通过多层设计我们可以将处理数据的流程沉淀在各层完成。比如在数据明细层统一完成数据的过滤、清洗、规范、脱敏流程；在数据汇总层加工共性的多维指标汇总数据。提高了代码的复用率和整体生产效率。同时各层级处理的任务类型相似，可以采用统一的技术方案优化性能，使数仓技术架构更简洁。 技术选型1. 存储引擎的调研实时数仓在设计中不同于离线数仓在各层级使用同种储存方案，比如都存储在 Hive 、DB 中的策略。首先对中间过程的表，采用将结构化的数据通过消息队列存储和高速 KV 存储混合的方案。实时计算引擎可以通过监听消息消费消息队列内的数据，进行实时计算。而在高速 KV 存储上的数据则可以用于快速关联计算，比如维度数据。 其次在应用层上，针对数据使用特点配置存储方案直接写入。避免了离线数仓应用层同步数据流程带来的处理延迟。为了解决不同类型的实时数据需求，合理的设计各层级存储方案，我们调研了美团内部使用比较广泛的几种存储方案。 根据不同业务场景，实时数仓各个模型层次使用的存储方案大致如下： 1、数据明细层 对于维度数据部分场景下关联的频率可达 10万多TPS，我们选择 Cellar（美团内部基于Tair开发的KV存储） 作为存储，封装维度服务为实时数仓提供维度数据。 2、数据汇总层 对于通用的汇总指标，需要进行历史数据关联的数据，采用和维度数据一样的方案通过 Cellar 作为存储，用服务的方式进行关联操作。 3、数据应用层 应用层设计相对复杂，再对比了几种不同存储方案后。我们制定了以数据读写频率 1000 QPS 为分界的判断依据。对于读写平均频率高于 1000 QPS 但查询不太复杂的实时应用，比如商户实时的经营数据。采用 Cellar 为存储，提供实时数据服务。对于一些查询复杂的和需要明细列表的应用，使用 Elasticsearch 作为存储则更为合适。而一些查询频率低，比如一些内部运营的数据。Druid 通过实时处理消息构建索引，并通过预聚合可以快速的提供实时数据 OLAP 分析功能。对于一些历史版本的数据产品进行实时化改造时，也可以使用 MySQL 存储便于产品迭代。 2. 计算引擎的调研在实时平台建设初期我们使用 Storm 引擎来进行实时数据处理。Storm 引擎虽然在灵活性和性能上都表现不错。但是由于 API 过于底层，在数据开发过程中需要对一些常用的数据操作进行功能实现。比如表关联、聚合等，产生了很多额外的开发工作，不仅引入了很多外部依赖比如缓存，而且实际使用时性能也不是很理想。同时 Storm 内的数据对象 Tuple 支持的功能也很简单，通常需要将其转换为 Java 对象来处理。 对于这种基于代码定义的数据模型，通常我们只能通过文档来进行维护。不仅需要额外的维护工作，同时在增改字段时也很麻烦。综合来看使用 Storm 引擎构建实时数仓难度较大。我们需要一个新的实时处理方案，要能够实现： 提供高级 API，支持常见的数据操作比如关联聚合，最好是能支持 SQL。 具有状态管理和自动支持久化方案，减少对存储的依赖。 便于接入元数据服务，避免通过代码管理数据结构。 处理性能至少要和 Storm 一致。 我们对主要的实时计算引擎进行了技术调研。总结了各类引擎特性如下表所示： 从调研结果来看，Flink 和 Spark Streaming 的 API 、容错机制与状态持久化机制都可以解决一部分，我们目前使用 Storm 中遇到的问题。但 Flink 在数据延迟上和 Storm 更接近，对现有应用影响最小。而且在公司内部的测试中 Flink 的吞吐性能对比 Storm 有十倍左右提升。综合考量我们选定 Flink 引擎作为实时数仓的开发引擎。 更加引起我们注意的是，Flink 的 Table 抽象和 SQL 支持。虽然使用 Strom 引擎也可以处理结构化数据。但毕竟依旧是基于消息的处理 API ，在代码层层面上不能完全享受操作结构化数据的便利。而 Flink 不仅支持了大量常用的 SQL 语句，基本覆盖了我们的开发场景。而且 Flink 的 Table 可以通过 TableSchema 进行管理，支持丰富的数据类型和数据结构以及数据源。可以很容易的和现有的元数据管理系统或配置管理系统结合。通过下图我们可以清晰的看出 Storm 和 Flink 在开发统过程中的区别。 在使用 Storm 开发时处理逻辑与实现需要固化在 Bolt 的代码。Flink 则可以通过 SQL 进行开发，代码可读性更高，逻辑的实现由开源框架来保证可靠高效，对特定场景的优化只要修改 Flink SQL 优化器功能实现即可，而不影响逻辑代码。使我们可以把更多的精力放到数据开发中，而不是逻辑的实现。当需要离线数据和实时数据口径统一的场景时，我们只需对离线口径的 SQL 脚本稍加改造即可，极大地提高了开发效率。同时对比图中 Flink 和 Storm 使用的数据模型，Storm 需要通过一个 Java 的 Class 去定义数据结构，Flink Table 则可以通过元数据来定义。可以很好的和数据开发中的元数据，数据治理等系统结合，提高开发效率。 Flink使用心得在利用 Flink-Table 构建实时数据仓库过程中。我们针对一些构建数据仓库的常用操作，比如数据指标的维度扩充，数据按主题关联，以及数据的聚合运算通过 Flink 来实现总结了一些使用心得。 维度扩充 数据指标的维度扩充，我们采用的是通过维度服务获取维度信息。虽然基于 Cellar 的维度服务通常的响应延迟可以在 1ms 以下。但是为了进一步优化 Flink 的吞吐，我们对维度数据的关联全部采用了异步接口访问的方式，避免了使用 RPC 调用影响数据吞吐。 对于一些数据量很大的流，比如流量日志数据量在 10万秒/条这个量级。在关联 UDF 的时候内置了缓存机制，可以根据命中率和时间对缓存进行淘汰，配合用关联的 Key 值进行分区，显著减少了对外部服务的请求次数，有效的减少了处理延迟和对外部系统的压力。 数据关联 数据主题合并，本质上就是多个数据源的关联，简单的来说就是 Join 操作。Flink 的 Table 是建立在无限流这个概念上的。在进行 Join 操作时并不能像离线数据一样对两个完整的表进行关联。采用的是在窗口时间内对数据进行关联的方案，相当于从两个数据流中各自截取一段时间的数据进行 Join 操作。有点类似于离线数据通过限制分区来进行关联。同时需要注意 Flink 关联表时必须有至少一个“等于”关联条件，因为等号两边的值会用来分组。 由于 Flink 会缓存窗口内的全部数据来进行关联，缓存的数据量和关联的窗口大小成正比。因此 Flink 的关联查询，更适合处理一些可以通过业务规则限制关联数据时间范围的场景。比如关联下单用户购买之前 30 分钟内的浏览日志。过大的窗口不仅会消耗更多的内存，同时会产生更大的 Checkpoint ，导致吞吐下降或 Checkpoint 超时。在实际生产中可以使用 RocksDB 和启用增量保存点模式，减少 Checkpoint 过程对吞吐产生影响。 对于一些需要关联窗口期很长的场景，比如关联的数据可能是几天以前的数据。对于这些历史数据，我们可以将其理解为是一种已经固定不变的”维度”。可以将需要被关联的历史数据采用和维度数据一致的处理方法：”缓存 + 离线”数据方式存储，用接口的方式进行关联。另外需要注意 Flink 对多表关联是直接顺序链接的，因此需要注意先进行结果集小的关联。 聚合运算 使用聚合运算时，Flink 对常见的聚合运算如求和、极值、均值等都有支持。美中不足的是对于 Distinct 的支持，Flink-1.6 之前的采用的方案是通过先对去重字段进行分组再聚合实现。对于需要对多个字段去重聚合的场景，只能分别计算再进行关联处理效率很低。为此我们开发了自定义的 UDAF，实现了 MapView 精确去重、BloomFilter 非精确去重、 HyperLogLog 超低内存去重方案应对各种实时去重场景。 但是在使用自定义的 UDAF 时，需要注意 RocksDBStateBackend 模式对于较大的 Key 进行更新操作时序列化和反序列化耗时很多。可以考虑使用 FsStateBackend 模式替代。另外要注意的一点 Flink 框架在计算比如 Rank 这样的分析函数时，需要缓存每个分组窗口下的全部数据才能进行排序，会消耗大量内存。建议在这种场景下优先转换为 TopN 的逻辑，看是否可以解决需求。 下图展示一个完整的使用 Flink 引擎生产一张实时数据表的过程： 实时数仓成果通过使用实时数仓代替原有流程，我们将数据生产中的各个流程抽象到实时数仓的各层当中。实现了全部实时数据应用的数据源统一，保证了应用数据指标、维度的口径的一致。在几次数据口径发生修改的场景中，我们通过对仓库明细和汇总进行改造，在完全不用修改应用代码的情况下就完成全部应用的口径切换。在开发过程中通过严格的把控数据分层、主题域划分、内容组织标准规范和命名规则。使数据开发的链路更为清晰，减少了代码的耦合。再配合上使用 Flink SQL 进行开发，代码加简洁。单个作业的代码量从平均 300+ 行的 Java 代码 ，缩减到几十行的 SQL 脚本。项目的开发时长也大幅减短，一人日开发多个实时数据指标情况也不少见。 除此以外我们通过针对数仓各层级工作内容的不同特点，可以进行针对性的性能优化和参数配置。比如 ODS 层主要进行数据的解析、过滤等操作，不需要 RPC 调用和聚合运算。我们针对数据解析过程进行优化，减少不必要的 JSON 字段解析，并使用更高效的 JSON 包。在资源分配上，单个 CPU 只配置 1GB 的内存即可满需求。 而汇总层主要则主要进行聚合与关联运算，可以通过优化聚合算法、内外存共同运算来提高性能、减少成本。资源配置上也会分配更多的内存，避免内存溢出。通过这些优化手段，虽然相比原有流程实时数仓的生产链路更长，但数据延迟并没有明显增加。同时实时数据应用所使用的计算资源也有明显减少。 展望我们的目标是将实时仓库建设成可以和离线仓库数据准确性，一致性媲美的数据系统。为商家，业务人员以及美团用户提供及时可靠的数据服务。同时作为到餐实时数据的统一出口，为集团其他业务部门助力。 未来我们将更加关注在数据可靠性和实时数据指标管理。建立完善的数据监控，数据血缘检测，交叉检查机制。及时对异常数据或数据延迟进行监控和预警。同时优化开发流程，降低开发实时数据学习成本。让更多有实时数据需求的人，可以自己动手解决问题。 关于作者 本文转载自美团技术公众号，作者是伟伦， 美团到店餐饮技术部实时数据负责人，2017年加入美团，长期从事数据平台、实时数据计算、数据架构方面的开发工作。在使用 Flink 进行实时数据生产和提高生产效率上，有一些心得和产出。同时也在积极推广 Flink 在实时数据处理中的实战经验 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"数据仓库","slug":"数据仓库","permalink":"http://www.54tianzhisheng.cn/tags/数据仓库/"}]},{"title":"一文让你彻底了解大数据实时计算引擎 Flink","date":"2019-08-18T16:00:00.000Z","path":"2019/08/19/flink/","text":"前言在上一篇文章 你公司到底需不需要引入实时计算引擎？ 中我讲解了日常中常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。 第一代：Hadoop 承载的 MapReduce 第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务 第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表 第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表 或许会有人不同意以上的分类，我觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。 本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看 数据集类型 和 数据运算模型 的种类。 数据集类型 无穷数据集：无穷的持续集成的数据集合 有界数据集：有限不会改变的数据集合 那么那些常见的无穷数据集有哪些呢？ 用户与客户端的实时交互数据 应用实时产生的日志 金融市场的实时交易记录 … 数据运算模型 流式：只要数据一直在产生，计算就持续地进行 批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源 那么我们再来看看 Flink 它是什么呢？ Flink 是什么？ Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。 Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。 Flink 整体结构 从下至上： 1、部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。 2、运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。 3、API：DataStream、DataSet、Table、SQL API。 4、扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。 Flink 支持多种方式部署 Flink 支持多种模式下的运行。 Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群 Standalone：在 Flink 目录下执行 bin/start-cluster.sh 脚本则会启动一个 Standalone 模式的集群 YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下： Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 GitHub 看到有下面这种运行架构的。 通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。 Flink 分布式运行Flink 作业提交架构流程可见下图： 1、Program Code：我们编写的 Flink 应用程序代码 2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户 3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件 4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 Flink API Flink 提供了不同的抽象级别的 API 以开发流式或批处理应用。 最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。 DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。 Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。 Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。 Flink 程序与数据流结构 一个完整的 Flink 应用程序结构就是如上两图所示： 1、Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。 Flink 支持多种扩展库Flink 拥有丰富的库来进行机器学习，图形处理，关系数据处理等。由于其架构，很容易执行复杂的事件处理和警报。 Flink 提供多种 Time 语义Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，后面的文章 Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析 中会很详细的讲解 Flink 中 Time 的概念。 Flink 提供灵活的窗口机制Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window。后面的文章 如何使用 Flink Window 及 Window 基本概念与实现原理 中会很详细的讲解 Flink 中 Window 的概念。 Flink 并行的执行任务Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1： Flink 支持状态存储Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构： ValueState MapState ListState ReducingState AggregatingState 另外状态存储也支持多种方式： MemoryStateBackend：存储在内存中 FsStateBackend：存储在文件中 RocksDBStateBackend：存储在 RocksDB 中 Flink 支持容错机制Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。 另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。 Flink 实现了自己的内存管理机制Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。我们在后面的文章 深入探索 Flink 内存管理机制 会深入讲解 Flink 里面的内存管理机制。 总结本篇文章对 Flink 做了一个详细的介绍，将 Flink 的特点一一做了描述，后面文章中我们也会进一步地对这里面的特点进行原理解析。本文的地址是 http://www.54tianzhisheng.cn/2019/08/19/flink/ ，未经允许禁止任何形式的转载，违者必究。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习 —— 如何使用 Side Output 来分流？","date":"2019-08-17T16:00:00.000Z","path":"2019/08/18/flink-side-output/","text":"前言之前在 Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 讲过 Flink 使用连续的 Split 会有问题，当时提供了几种解决方法，有一种方法就是使用 Side Output 来进行，当时留了个余念，那么就在这篇文章详细的讲一波，教大家如何使用 Side Output 来分流。 Side Output通常我们在处理数据的时候，有时候想对不同情况的数据进行不同的处理，那么就需要把数据流进行分流。比如我们在那篇文章里面的例子：需要将从 Kafka 过来的告警和恢复数据进行分类拆分，然后在对每种数据再分为告警数据和恢复数据。 如果是使用 filter 来进行拆分，也能满足我们的需求，但每次筛选过滤都要保留整个流，然后通过遍历整个流来获取相应的数据，显然很浪费性能。假如能够在一个流里面就进行多次输出就好了，恰好 Flink 的 Side Output 则提供了这样的功能。 如何使用？要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。例如：如果我要将告警/恢复的数据分为机器、容器、中间件等的数据，那么我们起码就得定义三个 OutputTag，如下： 123456private static final OutputTag&lt;AlertEvent&gt; middleware = new OutputTag&lt;AlertEvent&gt;(\"MIDDLEWARE\") &#123;&#125;;private static final OutputTag&lt;AlertEvent&gt; machine = new OutputTag&lt;AlertEvent&gt;(\"MACHINE\") &#123;&#125;;private static final OutputTag&lt;AlertEvent&gt; docker = new OutputTag&lt;AlertEvent&gt;(\"DOCKER\") &#123;&#125;; 然后呢，你可以使用下面几种函数来处理数据，在处理数据的过程中，进行判断将不同种类型的数据存到不同的 OutputTag 中去。 ProcessFunction KeyedProcessFunction CoProcessFunction ProcessWindowFunction ProcessAllWindowFunction 比如： 12345678910111213141516//dataStream 是总的数据流SingleOutputStreamOperator&lt;AlertEvent, AlertEvent&gt; outputStream = dataStream.process(new ProcessFunction&lt;AlertEvent, AlertEvent&gt;() &#123; @Override public void processElement(AlertEvent value, Context ctx, Collector&lt;AlertEvent&gt; out) throws Exception &#123; if (\"MACHINE\".equals(value.type)) &#123; ctx.output(machine, value); &#125; else if (\"DOCKER\".equals(value.type)) &#123; ctx.output(docker, value); &#125; else if (\"MIDDLEWARE\".equals(value.type)) &#123; ctx.output(middleware, value); &#125; else &#123; //其他的业务逻辑 out.collect(value); &#125; &#125;&#125;) 好了，既然上面我们已经将不同类型的数据进行放到不同的 OutputTag 里面了，那么我们该如何去获取呢？你可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如： 12345678//机器相关的告警&amp;恢复数据outputStream.getSideOutput(machine).print();//容器相关的告警&amp;恢复数据outputStream.getSideOutput(docker).print();//中间件相关的告警&amp;恢复数据outputStream.getSideOutput(middleware).print(); 这样你就可以获取到 Side Output 数据了。 另外你还可以看下我在 Github 放的一个完整 demo 代码: https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput/Main.java 总结本文讲了如何使用 Side Output 来进行分流，比较简单，大家可以稍微阅读一下 demo 代码就可以很清楚了解。 本文地址是：http://www.54tianzhisheng.cn/2019/08/18/flink-side-output/ 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 知识星球 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"你公司到底需不需要引入实时计算引擎？","date":"2019-08-05T16:00:00.000Z","path":"2019/08/06/flink-streaming-system/","text":"合理的需求选择恰当的技术栈 前言 先广而告之，本文摘自本人《大数据重磅炸弹——实时计算框架 Flink》课程第二篇，内容首发自我的知识星球，后面持续在星球里更新。 自己之前发布过一篇 Chat 《大数据“重磅炸弹”：实时计算框架 Flink》，里面介绍了多种需求： 1234567891011121314151617小田，你看能不能做个监控大屏实时查看促销活动销售额（GMV）？小朱，搞促销活动的时候能不能实时统计下网站的 PV/UV 啊？小鹏，我们现在搞促销活动能不能实时统计销量 Top5 啊？小李，怎么回事啊？现在搞促销活动结果服务器宕机了都没告警，能不能加一个？小刘，服务器这会好卡，是不是出了什么问题啊，你看能不能做个监控大屏实时查看机器的运行情况？小赵，我们线上的应用频繁出现 Error 日志，但是只有靠人肉上机器查看才知道情况，能不能在出现错误的时候及时告警通知？小夏，我们 1 元秒杀促销活动中有件商品被某个用户薅了 100 件，怎么都没有风控啊？小宋，你看我们搞促销活动能不能根据每个顾客的浏览记录实时推荐不同的商品啊？…… 大数据发展至今，数据呈指数倍的增长，对实效性的要求也越来越高，于是像上面这种需求也变得越来越多了。 那这些场景对应着什么业务需求呢？我们来总结下，大概如下： 初看这些需求，是不是感觉很难？ 那么我们接下来来分析一下该怎么去实现？ 从这些需求来看，最根本的业务都是需要实时查看数据信息，那么首先我们得想想如何去采集这些实时数据，然后将采集的实时数据进行实时的计算，最后将计算后的结果下发到第三方。 数据实时采集就上面这些需求，我们需要采集些什么数据呢？ 买家搜索记录信息 买家浏览的商品信息 买家下单订单信息 网站的所有浏览记录 机器 CPU/MEM/IO 信息 应用日志信息 数据实时计算采集后的数据实时上报后，需要做实时的计算，那我们怎么实现计算呢？ 计算所有商品的总销售额 统计单个商品的销量，最后求 Top5 关联用户信息和浏览信息、下单信息 统计网站所有的请求 IP 并统计每个 IP 的请求数量 计算一分钟内机器 CPU/MEM/IO 的平均值、75 分位数值 过滤出 Error 级别的日志信息 数据实时下发实时计算后的数据，需要及时的下发到下游，这里说的下游代表可能是： 告警方式（邮件、短信、钉钉、微信） 在计算层会将计算结果与阈值进行比较，超过阈值触发告警，让运维提前收到通知，及时做好应对措施，减少故障的损失大小。 存储（消息队列、DB、文件系统等） 数据存储后，监控大盘（Dashboard）从存储（ElasticSearch、HBase 等）里面查询对应指标的数据就可以查看实时的监控信息，做到对促销活动的商品销量、销售额，机器 CPU、MEM 等有实时监控，运营、运维、开发、领导都可以实时查看并作出对应的措施。 让运营知道哪些商品是爆款，哪些店铺成交额最多，哪些商品成交额最高，哪些商品浏览量最多； 让运维可以时刻了解机器的运行状况，出现宕机或者其他不稳定情况可以及时处理； 让开发知道自己项目运行的情况，从 Error 日志知道出现了哪些 Bug； 让领导知道这次促销赚了多少 money。 从数据采集到数据计算再到数据下发，整个流程在上面的场景对实时性要求还是很高的，任何一个地方出现问题都将影响最后的效果！ 实时计算场景前面说了这么多场景，这里我们总结一下实时计算常用的场景有哪些呢？ 交通信号灯数据 道路上车流量统计（拥堵状况） 公安视频监控 服务器运行状态监控 金融证券公司实时跟踪股市波动，计算风险价值 数据实时 ETL 银行或者支付公司涉及金融盗窃的预警 …… 另外我自己在我的群里也有做过调研（不完全统计），他们在公司 Flink（一个实时计算框架）使用场景有这些： 总结一下大概有下面这四类： 实时数据存储 实时数据存储的时候做一些微聚合、过滤某些字段、数据脱敏，组建数据仓库，实时 ETL。 实时数据分析 实时数据接入机器学习框架（TensorFlow）或者一些算法进行数据建模、分析，然后动态的给出商品推荐、广告推荐 实时监控告警 金融相关涉及交易、实时风控、车流量预警、服务器监控告警、应用日志告警 实时数据报表 活动营销时销售额/销售量大屏，TopN 商品 说到实时计算，这里不得不讲一下和传统的离线计算的区别！ 实时计算 VS 离线计算再讲这两个区别之前，我们先来看看流处理和批处理的区别： 流处理与批处理 看完流处理与批处理这两者的区别之后，我们来抽象一下前面文章的场景需求（实时计算）： 实时计算需要不断的从 MQ 中读取采集的数据，然后处理计算后往 DB 里存储，在计算这层你无法感知到会有多少数据量过来、要做一些简单的操作（过滤、聚合等）、及时将数据下发。 相比传统的离线计算，它却是这样的： 在计算这层，它从 DB（不限 MySQL，还有其他的存储介质）里面读取数据，该数据一般就是固定的（前一天、前一星期、前一个月），然后再做一些复杂的计算或者统计分析，最后生成可供直观查看的报表（dashboard）。 离线计算的特点 数据量大且时间周期长（一天、一星期、一个月、半年、一年） 在大量数据上进行复杂的批量运算 数据在计算之前已经固定，不再会发生变化 能够方便的查询批量计算的结果 实时计算的特点在大数据中与离线计算对应的则是实时计算，那么实时计算有什么特点呢？由于应用场景的各不相同，所以这两种计算引擎接收数据的方式也不太一样：离线计算的数据是固定的（不再会发生变化），通常离线计算的任务都是定时的，如：每天晚上 0 点的时候定时计算前一天的数据，生成报表；然而实时计算的数据源却是流式的。 这里我不得不讲讲什么是流式数据呢？我的理解是比如你在淘宝上下单了某个商品或者点击浏览了某件商品，你就会发现你的页面立马就会给你推荐这种商品的广告和类似商品的店铺，这种就是属于实时数据处理然后作出相关推荐，这类数据需要不断的从你在网页上的点击动作中获取数据，之后进行实时分析然后给出推荐。 流式数据的特点 数据实时到达 数据到达次序独立，不受应用系统所控制 数据规模大且无法预知容量 原始数据一经处理，除非特意保存，否则不能被再次取出处理，或者再次提取数据代价昂贵 实时计算的优势实时计算一时爽，一直实时计算一直爽，对于持续生成最新数据的场景，采用流数据处理是非常有利的。例如，再监控服务器的一些运行指标的时候，能根据采集上来的实时数据进行判断，当超出一定阈值的时候发出警报，进行提醒作用。再如通过处理流数据生成简单的报告，如五分钟的窗口聚合数据平均值。复杂的事情还有在流数据中进行数据多维度关联、聚合、塞选，从而找到复杂事件中的根因。更为复杂的是做一些复杂的数据分析操作，如应用机器学习算法，然后根据算法处理后的数据结果提取出有效的信息，作出、给出不一样的推荐内容，让不同的人可以看见不同的网页（千人千面）。 使用实时数据流面临的挑战 数据处理唯一性（如何保证数据只处理一次？至少一次？最多一次？） 数据处理的及时性（采集的实时数据量太大的话可能会导致短时间内处理不过来，如何保证数据能够及时的处理，不出现数据堆积？） 数据处理层和存储层的可扩展性（如何根据采集的实时数据量的大小提供动态扩缩容？） 数据处理层和存储层的容错性（如何保证数据处理层和存储层高可用，出现故障时数据处理层和存储层服务依旧可用？） 总结本文从日常需求来分析该如何去实现这类需求，需要实时采集、实时计算、实时下发，并用图片把需求完成后的效果图展示了出来，接着我们分析了对实时性要求高的计算这块，然后将离线计算与实时计算进行了对比、批处理与流处理进行对比、离线计算的特点与实时计算的特点进行了对比，再加上我自己的调研结果，归纳了实时计算的四种使用场景，提出了使用实时计算时要面临的挑战。因为各种需求，也就造就了现在不断出现实时计算框架，而下文我们将重磅介绍我们推荐的实时计算框架 —— Flink。 Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Clients 源码解析","date":"2019-07-03T16:00:00.000Z","path":"2019/07/04/Flink-code-clients/","text":"Flink-Client 模块中的类结构如下： https://t.zsxq.com/IMzNZjY 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Annotations 源码解析","date":"2019-07-02T16:00:00.000Z","path":"2019/07/03/Flink-code-Annotations/","text":"Flink-Annotations 模块中的类结构如下： https://t.zsxq.com/f6eAu3J 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Metrics 源码解析","date":"2019-07-01T16:00:00.000Z","path":"2019/07/02/Flink-code-metrics/","text":"Flink Metrics 有如下模块： Flink Metrics 源码解析 —— Flink-metrics-core Flink Metrics 源码解析 —— Flink-metrics-datadog Flink Metrics 源码解析 —— Flink-metrics-dropwizard Flink Metrics 源码解析 —— Flink-metrics-graphite Flink Metrics 源码解析 —— Flink-metrics-influxdb Flink Metrics 源码解析 —— Flink-metrics-jmx Flink Metrics 源码解析 —— Flink-metrics-slf4j Flink Metrics 源码解析 —— Flink-metrics-statsd Flink Metrics 源码解析 —— Flink-metrics-prometheus 使用 InflubDB 和 Grafana 监控 Flink JobManager TaskManager 和 Job 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Apache Flink 1.9 重大特性提前解读","date":"2019-06-30T16:00:00.000Z","path":"2019/07/01/flink-1.9-preview/","text":"今天在 Apache Flink meetup ·北京站进行 Flink 1.9 重大新特性进行了讲解，两位讲师分别是 戴资力/杨克特，zhisheng 我也从看完了整个 1.9 特性解读的直播，预计 Flink 1.9 版本正式发布时间大概是 7 月底 8 月初左右正式发布，下面一起来看看直播内容： 架构改动 Table/SQL API Runtime 生态 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了","date":"2019-06-25T16:00:00.000Z","path":"2019/06/26/flink-TensorFlow/","text":"1．前言随着互联网的迅速发展，各个公司都建立了自己的监控体系，用于提前发现问题降低损失，携程亦是如此。然而携程的监控体系存在以下三个问题： 本文转自 AI 前线公众号，作者 | 潘国庆 编辑 | Natalie Flink 已经渐渐成为实时计算引擎的首选之一，从简单的实时 ETL 到复杂的 CEP 场景，Flink 都能够很好地驾驭。本文整理自携程实时计算负责人潘国庆在 QCon 全球软件开发大会（北京站）2019 的演讲，他介绍了携程如何基于 Flink 与 TensorFlow 构建实时智能异常检测平台，以解决规则告警系统准确率低、时效性低、规则配置复杂与耗费人力等诸多问题，实现了业务指标毫秒级延迟与智能化检测，同时依托 Flink 实现了强大的容错机制。 监控系统繁多 监控告警配置复杂 没有统一规范 首先携程目前光公司级别的监控系统就有三套，各个 BU 为了满足自己的业务监控需求也陆续开发了许多自己的监控系统。其次这些监控系统都是基于规则来判断是否存在异常，比如当满足同环比连续几个点上升或下降到用户配置的阈值时触发告警。最后是没有统一的规范，这里指的是两个规范，第一，没有统一的规则告警配置规范，不同的监控系统都带有不同的规则告警配置方式；第二，没有统一的异常判断规范，研发人员或 QA 人员都是根据自己对业务的理解，通过主观判断指标达到一定阀值时监控系统需要进行告警。 基于以上的三点问题给用户带来了诸多不便，首先是规则告警维护成本高，用户时常需要基于多个监控系统以不同的方式配置规则告警，而且还需要根据告警的情况持续调整阈值，导致一个规则告警从配置到最终能够产生较好的效果需要一个很长的周期。其次，基于规则告警往往表现不尽如人意，会导致准确率低、覆盖率低和时效性低的三低状况。用户很多情况下为了提高异常的覆盖率降低漏报的情况，不得不将规则告警的阀值设置的非常敏感，虽然这样能够覆盖更多的异常场景，却导致了大量的误报，规则告警的准确性也就大大折扣。 为了应对上述的诸多问题，携程打造了自己的实时智能异常检测平台 Prophet。简单概括，Prophet 是一个基于时序类型数据、以平台为接入对象、去规则化为目标的异常检测系统，基于深度学习算法实现异常的智能检测，基于实时计算引擎实现异常的实时检测，提供了统一的异常检测解决方案。接下来的文章会详细介绍我们是如何依次实现了异常的智能化、实时化检测以及平台的构建。 2． 智能化2.1 深度学习算法选择目前业界采用比较多的方式是引入统计分析的各种方法，框定一个滑动的样本集，对这个样本集进行一些数据处理和转化，经过归一化，去周期，去趋势，再将最新采集到的数据点经过同样的转换，和样本集的残差序列的统计量进行比较，比如距离、方差、移动平均、分位数等，超出一定的范围就判断为异常，或是综合各种离群点计算的方法来做个投票，多数算法认为异常则报异常。起初我们也借鉴了这种做法，却发现虽然可以不用维护告警规则了，但报警的质量并没有提升。 我们需要设计一套新的算法，降低报警总量到可以人工逐个处理的程度，同时不能以增加漏报真正的生产订单故障为代价，并且这套算法的设计还不能太复杂，影响到告警的实时性，最好还能做到算法即服务，有较强的可移植性，提供给其他的监控系统使用。自然而然的，基于神经网络的深度学习算法 成为我们进一步探索的工具。 RNN 算法比较适合处理序列变化的数据，符合我们时序特征的场景，但是存在梯度消失和过拟合的现象。而他的改进版 LSTM 算法，能够通过控制传输状态来选择性地记住较重要的长期数据，能在更长的序列上有良好的表现，业界也有很多成功的应用。LSTM 算法的异常检测方式是基于指标的历史数据训练出模型并基于现有数据预测指标未来的走势，基于预测数据与现实数据各种偏差来判断指标是否有异常。这样好处在于每个指标都会训练一个自己的模型，能够达到很高的精度，但是也带来了一定的弊端，需要消耗较多的训练与检测资源。 DNN 算法的检测方式与 LSTM 的方式不同，我们基于小波变换算法提取监控指标不同频域的特征喂给 DNN 模型，直接输出是否存在异常。这种的好处在于一个 DNN 模型就能够满足所有异常检测场景的需求，但是相对的特征工程也要复杂很多，我们需要大量的人工标记数据来提高模型的精度。 最后无论是基于 LSTM 算法还是 DNN 算法实现的异常检测需要根据各自所需的不同场景来决定使用哪个。在携程，对于最重要的订单、支付类指标，我们都是采取 LSTM 算法，单个指标训练单个模型，对于其他一些非重要的指标可以使用 DNN 算法。 2.2 模型训练选定好深度学习算法之后，我们也就开始尝试模型的训练。我们首先取得监控指标的历史数据对其进行清洗，其中需要对一些空值进行插补，节假日数据对于数据模型的影响很大，导致训练出来的数据有偏差，我们也选择性的剔除节假日期间的数据；如果历史数据中的某个区间数据是异常区间，我们也需要使用预测值替换异常区间的数值。 做完数据清洗之后，也就需要实现特征工程。我们使用了多尺度滑动窗口时序特征的方法，将一个滑动窗口内的数据和前 n 个周期做统计量上的对比，均值、方差、变化率等这些，这样基本上就可以把明显的周期性和平稳型数据给分离出来。剩下的时序中，有些是波动很大的随机序列，有的则是带有趋势的周期性序列，通过时序分析法把周期性去掉，再用频域分析尝试分解成频谱。对于带有明显频谱的，则归类为周期型时序，而频谱杂乱的，则归类为非周期性。 在做完特征提取与指标分类之后，我们也就根据指标的类型使用不同的算法进行模型训练。我们根据线上的人工标注数据持续性的优化我们的模型。我们经历过初期不停的调参和验证之后，我们将模型训练的频率设为了两周，我们每两周重新走下图中的整个流程，这个也是根据我们业务变更的频率所做的考虑。 3． 实时化3.1 Why Flink？在解决了智能化异常检测的问题后，我们开始考虑提高我们的时效性。以往的规则告警，从数据产生到落地到监控系统，再到触发规则判断，期间已经经历了一定延迟。并且很多规则告警往往需要连续 3 个点或则 5 个点触发下跌或上升规则判断才会告警，这样如果一个指标的采集粒度是一分钟，那么异常往往需要过好几分钟才会被发现。为了解决时效性的问题，我们尝试引入实时计算引擎。现在常见的实时计算引擎有 Storm、Spark Streaming 以及 Flink，那么为什么我们最终选择了 Flink？ 首先第一点就是 Flink 提供了强大的容错保障，所有的实时作业无论提供了多么繁多的功能，如果在作业的容错保障上做的不好，对于用户都是不可接受的。我们的数据源是 Kafka，基于 Flink 的 Checkpoint 与 Kafka 的 Offset 回溯功能能够实现数据源到执行引擎层面的 Exactly Once 的语义保证，基于幂等或事物保证最终输出的 Exactly Once 语义。 第二点，Flink 提供了高效的状态管理，我们在做异常检测的时候需要保存异常区间的预测数据用于下一轮的异常检测，这个后续会讲到。 第三点与第四点放在一起讲就是，Flink 提供了基于 Event Time 的丰富窗口函数，Spark Streaming 虽然也提供了对窗口的支持，但是其本质上还都是基于 Processing Time 的数据处理。终上所述，我们最终选择了 Flink 作为我们的实时计算引擎。 3.2 实时检测在选择好实时计算引擎后，我们也就开始尝试在 Flink 中加载 Tensorflow 的模型用来实时做异常检测。首先我们将所有训练好的 Tensorflow 模型以.pb 的格式上传到 HDFS 并将新增或更新的模型配置更新到配置中心 QConfig 上。Flink 作业在启动或运行中时，监听配置中心中需要监控的指标并尝试从 HDFS 上加载模型。由于后期模型较多，为了避免重复加载和负载均衡，所有指标会先根据 id keyBy 分发到不同的 TaskManager 上，每个 TaskManager 只加载属于自己那部分的模型。 模型加载完毕后，我们基于 Flink 滑动窗口与 Event Time 实现数据实时消费与预测。窗口滑动的时间为指标的时间粒度（下图中为 1 分钟），窗口长度为十个指标时间粒度（下图中为 10 分钟）。一个窗口中总计 10 条数据，我们采用前面 5 条数据预测第 6 个位置的数据，然后基于 2 到 4 的实际数值加上第 6 条的预测数据预测第 7 个数据。依此类推，最终我们获取到了窗口中后 5 位的预测值与实际值，基于 5 个预测值与实际值对比检测是否存在异常。 然而实际的消费过程中并不会像上面说的那么简单，首先一个窗口内可能存在缺失数据的情况，我们采用窗口内其余数据的均值与标准差补齐。其次，在上个时间段如果存在异常，我们无法直接使用原始的值去预测数值，因为这个原始值可能是一个异常值，我们需要使用上个时间段的预测值来替换这个异常值，这样能够保证我们的预测线不被带跑偏。上一个窗口的预测值我们采用 flink 中的 state 来存储。 在取得当前窗口后 5 个预测值与实际值之后，我们就开始进异常检测了。我们会根据异常的类型（比如上升或下降）与敏感度来做不同的判断，下图中的三个异常曲线分别对应了高中低三个敏感的场景，在使用高敏度时，可能只要有一个下跌的抖动，我们可能就认为其是一个潜在的异常，中敏感度需要连续两个下跌的情况，低敏感度则需在下降幅度非常大的情况下才会认定为潜在异常。 我们会基于预测值与实际数据的偏差来先做一个潜在判断，当认定它是一个潜在异常时，我们会在基于预测值与历史同期数据的均值与标准差做判断，这样最终得出当前的窗口是否存在异常。我们这边在异常判断的时候还是采用了统计学作为判断方式，如果在样本足够的情况下，完全可以使用机器学习，训练一个异常检测模型来判断是否存在异常。 4. Prophet4.1 Prophet 系统架构在讲述完如何实现智能化与实时化异常检测之后，相信大家对于 Prophet 已经有了一定的认知。下图展示了整个 Prophet 平台的系统架构，首先是最底层的 Hadoop 集群承担了分布式存储与资源调度的功能，HDFS 用来存储 Tensorflow 训练好的模型，所有 Flink 作业运行在 Yarn 集群上。中间层的消息队列承担了实时数据源的作用，所有指标的历史数据存储在时序数据库中，实时化与智能化检测依托于 Flink 与 Tensorflow 两套引擎实现。最上层的 Prophet 以平台的方式对外提供服务，Clog 用于日志存储与排障，Muise 是我们的实时计算平台，Qconfig 用于存储于监控指标相关的配置信息，最后 Hickwall 用于监控作业的各项指标。 4.2 Prophet 操作流程一个用户想要配置智能告警只需要做两件事，首先在我们的平台上配置智能告警，由于我们大部分对接的是监控平台，所以用户大多是在各个监控平台上配置智能告警，然后监控平台调用我们的服务注册监控指标。然后用户需要按照我们定义好的格式将原始数据发送到我们的 Kafka 消息队列，这一步在对接平台时，也由平台做了，所以直接在我们平台上配置监控指标的用户很少。当一个用户注册好监控指标后，我们平台会先检测该指标的历史数据是否足够，如果足够则触发模型训练的流程，训练好的模型会上传到 HDFS。如果历史数据不足，Prophet 会持续实时存储用户指标的数据，当满足数据量的需求时，重新触发模型训练。当模型训练完成后，我们会更新配置中心，告知 Flink 作业有新的或更新的指标模型已经就位。 实时这块的流程是 Flink 启动或运行中一旦监听到有新的或更新的模型，作业会重新加载模型。另外 Flink 会实时从 Kafka 中消费数据，实时的过模型做异常检测，最终将异常告警回吐到 Kafka，各个平台消费自己的异常告警数据并给相关的负责人发送告警通知。 4.3 平台现状目前 Prophet 已经覆盖了携程所有的业务线，接入了十余个监控平台，其中包含公司级的监控系统 Sitemon 与 Hickwall，监控了 7000+ 个业务指标，包含订单、支付、应用、服务等多种业务类型指标。 在平台运行的半年时间内，我们的算法能够达到 90% 的召回率（也就是异常覆盖率）；由于我们业务方需求是尽量覆盖更多的异常，不要漏报，所以我们的准确率保持在 75% 左右；在引入了 Flink 实时消费数据与检测，极大的降低了我们告警的延迟，达到了毫秒级的延迟；对比规则告警，我们帮助用户降低了 10 倍的告警数量，提升了 10 倍的用户效率。 下图展示了从 18 年 10 月 Prophet 上线以来至 19 年 4 月底，智能告警与规则告警对异常的覆盖率对比。总计发生 176 起异常，其中 Prophet 图表中显示的是覆盖了 90% 的异常，但其实真正的覆盖率要高于 90%，其中 18 个未覆盖异常有 15 个是由于初期算法一直处于调整阶段导致了漏报。在 19 年之后，我们的异常覆盖率能够达到接近 100%。相比较规则告警，我们的覆盖率上升了 22%，及时的帮助用户降低损失。 下图展示了智能告警与规则告警在告警数量上的对比，规则告警的数量基本是智能告警的 2 到 5 倍，但是这并非是站在同一层面上的对比，其中智能告警的数量是基于 800 监控指标，而规则告警是基于 200 个监控，如果规则告警的指标数量与智能告警的持平，那智能告警降低的告警数量会更为显著。告警数量对于用户的效率提升是十分明显的，以往用户每天需要花费大量的精力去排查每一个告警邮件，在使用了智能告警后，这部分帮助用户减少的时间是实实在在的效率提升。 5． 挑战与展望Prophet 在携程投入生产使用已有半年之久，在这期间我们也遇到过形形色色的挑战。 首先，基于 LSTM 算法的异常检测方式存在一个明显的弊端，我们需要对每一个指标训练一个模型，这样无论是模型训练所需的资源以及实时作业加载模型所需的资源都消耗比较大。 其次，LSTM 算法对于波动剧烈的非周期型指标表现不是十分良好，有一些业务会不定期的做一些活动导致业务指标的突增或突减，这种趋势是无法从历史数据中学习到。 然后，对于一些系统性能指标类型的数据也无需使用智能告警，规则告警可能更加方便，比如当服务器的 cpu 使用率达到 95% 的时候就告警。 最后，节假日对于智能告警的影响十分之大，业务指标通常会在节假日前呈倍数的增长，假日期间又曾倍数的下降，这样导致了大量漏报或误报。 针对以上的问题，我们也在持续的改进之中。首先，基于 DNN 算法的通用模型已经在线下陪跑了数月之久，虽然在精度上比 LSTM 算法的异常检测方式稍有逊色，但在我们持续优化之后已经基本能够 hold 住线上非重要指标的告警需求，实现单个模型监控数千个指标的功能，大大降低了资源损耗。我们在应对节假日对智能检测影响时引入了增长系数的概念，用来拉升或降低预测值，并且采用一定方式将增长系数持续衰减，防止增长系数导致预测值的跑偏。关于算法的细节以及各种场景下的应对方式由于篇幅关系无法在本篇文章中一一展开，如果对算法相关细节感兴趣的朋友可以在评论区留言，我们这边也会考虑让算法同事另起炉灶，详细的介绍算法、特征工程等相关话题。 Prophet 后续也会陆续的接入携程所有的监控系统，这也是我们一直努力在做的事。实时计算与人工智能不光在异常检测这个场景下有很好的发挥，在很多其他的场景下也能够有亮眼的表现，比如风控、个性化推荐、排序等，本篇文章也算是抛砖引玉，希望给大家能够带来一些其法，这样可以将这套方式更多的使用在其他的场景下。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"360深度实践：Flink与Storm协议级对比","date":"2019-06-20T16:00:00.000Z","path":"2019/06/21/flink-in-360/","text":"本文从数据传输和数据可靠性的角度出发，对比测试了Storm与Flink在流处理上的性能，并对测试结果进行分析，给出在使用Flink时提高性能的建议。 作者 张馨予，360 大数据计算平台负责人。北京邮电大学硕士，2015年加入360系统部，一直致力于公司大数据计算平台的易用性、稳定性和性能优化的研发工作。目前主要负责Flink的研发，完成公司计算引擎的大一统。 Apache Storm、Apache Spark和Apache Flink都是开源社区中非常活跃的分布式计算平台，在很多公司可能同时使用着其中两种甚至三种。对于实时计算来说，Storm与Flink的底层计算引擎是基于流的，本质上是一条一条的数据进行处理，且处理的模式是流水线模式，即所有的处理进程同时存在，数据在这些进程之间流动处理。而Spark是基于批量数据的处理，即一小批一小批的数据进行处理，且处理的逻辑在一批数据准备好之后才会进行计算。在本文中，我们把同样基于流处理的Storm和Flink拿来做对比测试分析。 在我们做测试之前，调研了一些已有的大数据平台性能测试报告，比如，雅虎的Streaming-benchmarks，或者Intel的HiBench等等。除此之外，还有很多的论文也从不同的角度对分布式计算平台进行了测试。虽然这些测试case各有不同的侧重点，但他们都用到了同样的两个指标，即吞吐和延迟。吞吐表示单位时间内所能处理的数据量，是可以通过增大并发来提高的。延迟代表处理一条数据所需要的时间，与吞吐量成反比关系。 在我们设计计算逻辑时，首先考虑一下流处理的计算模型。上图是一个简单的流计算模型，在Source中将数据取出，发往下游Task，并在Task中进行处理，最后输出。对于这样的一个计算模型，延迟时间由三部分组成：数据传输时间、Task计算时间和数据排队时间。我们假设资源足够，数据不用排队。则延迟时间就只由数据传输时间和Task计算时间组成。而在Task中处理所需要的时间与用户的逻辑息息相关，所以对于一个计算平台来说，数据传输的时间才更能反映这个计算平台的能力。因此，我们在设计测试Case时，为了更好的体现出数据传输的能力，Task中没有设计任何计算逻辑。 在确定数据源时，我们主要考虑是在进程中直接生成数据，这种方法在很多之前的测试标准中也同样有使用。这样做是因为数据的产生不会受到外界数据源系统的性能限制。但由于在我们公司内部大部分的实时计算数据都来源于kafka，所以我们增加了从kafka中读取数据的测试。 对于数据传输方式，可以分为两种：进程间的数据传输和进程内的数据传输。 进程间的数据传输是指这条数据会经过序列化、网络传输和反序列化三个步骤。在Flink中，2个处理逻辑分布在不同的TaskManager上，这两个处理逻辑之间的数据传输就可以叫做进程间的数据传输。Flink网络传输是采用的Netty技术。在Storm中，进程间的数据传输是worker之间的数据传输。早版本的storm网络传输使用的ZeroMQ，现在也改成了Netty。 进程内的数据传输是指两个处理逻辑在同一个进程中。在Flink中，这两个处理逻辑被Chain在了一起，在一个线程中通过方法调用传参的形式进程数据传输。在Storm中，两个处理逻辑变成了两个线程，通过一个共享的队列进行数据传输。 Storm和Flink都有各自的可靠性机制。在Storm中，使用ACK机制来保证数据的可靠性。而在Flink中是通过checkpoint机制来保证的，这是来源于chandy-lamport算法。 事实上exactly-once可靠性的保证跟处理的逻辑和结果输出的设计有关。比如结果要输出到kafka中，而输出到kafka的数据无法回滚，这就无法保证exactly-once。我们在测试的时候选用的at-least-once语义的可靠性和不保证可靠性两种策略进行测试。 上图是我们测试的环境和各个平台的版本。 上图展示的是Flink在自产数据的情况下，不同的传输方式和可靠性的吞吐量：在进程内+不可靠、进程内+可靠、进程间+不可靠、进程间+可靠。可以看到进程内的数据传输是进程间的数据传输的3.8倍。是否开启checkpoint机制对Flink的吞吐影响并不大。因此我们在使用Flink时，进来使用进程内的传输，也就是尽可能的让算子可以Chain起来。 那么我们来看一下为什么Chain起来的性能好这么多，要如何在写Flink代码的过程中让Flink的算子Chain起来使用进程间的数据传输。 大家知道我们在Flink代码时一定会创建一个env，调用env的disableOperatorChainning()方法会使得所有的算子都无法chain起来。我们一般是在debug的时候回调用这个方法，方便调试问题。 如果允许Chain的情况下，上图中Source和mapFunction就会Chain起来，放在一个Task中计算。反之，如果不允许Chain，则会放到两个Task中。 对于没有Chain起来的两个算子，他们被放到了不同的两个Task中，那么他们之间的数据传输是这样的：SourceFunction取到数据序列化后放入内存，然后通过网络传输给MapFunction所在的进程，该进程将数据方序列化后使用。 对于Chain起来的两个算子，他们被放到同一个Task中，那么这两个算子之间的数据传输则是：SourceFunction取到数据后，进行一次深拷贝，然后MapFunction把深拷贝出来的这个对象作为输入数据。 虽然Flink在序列化上做了很多优化，跟不用序列化和不用网络传输的进程内数据传输对比，性能还是差很多。所以我们尽可能的把算子Chain起来。 不是任何两个算子都可以Chain起来的，要把算子Chain起来有很多条件：第一，下游算子只能接受一种上游数据流，比如Map接受的流不能是一条union后的流；其次上下游的并发数一定要一样；第三，算子要使用同一个资源Group，默认是一致的，都是default；第四，就是之前说的env中不能调用disableOperatorChainning()方法，最后，上游发送数据的方法是Forward的，比如，开发时没有调用rebalance()方法，没有keyby()，没有boardcast等。 对比一下自产数据时，使用进程内通信，且不保证数据可靠性的情况下，Flink与Storm的吞吐。在这种情况下，Flink的性能是Storm的15倍。Flink吞吐能达到2060万条/s。不仅如此，如果在开发时调用了env.getConfig().enableObjectReuse()方法，Flink的但并发吞吐能达到4090万条/s。 当调用了enableObjectReuse方法后，Flink会把中间深拷贝的步骤都省略掉，SourceFunction产生的数据直接作为MapFunction的输入。但需要特别注意的是，这个方法不能随便调用，必须要确保下游Function只有一种，或者下游的Function均不会改变对象内部的值。否则可能会有线程安全的问题。 当对比在不同可靠性策略的情况下，Flink与Storm的表现时，我们发现，保证可靠性对Flink的影响非常小，但对Storm的影响非常大。总的来说，在保证可靠的情况下，Flink单并发的吞吐是Storm的15倍，而不保证可靠的情况下，Flink的性能是Storm的66倍。会产生这样的结果，主要是因为Flink与Storm保证数据可靠性的机制不同。 而Storm的ACK机制为了保证数据的可靠性，开销更大。 左边的图展示的是Storm的Ack机制。Spout每发送一条数据到Bolt，就会产生一条ack的信息给acker，当Bolt处理完这条数据后也会发送ack信息给acker。当acker收到这条数据的所有ack信息时，会回复Spout一条ack信息。也就是说，对于一个只有两级（spout+bolt）的拓扑来说，每发送一条数据，就会传输3条ack信息。这3条ack信息则是为了保证可靠性所需要的开销。 右边的图展示的是Flink的Checkpoint机制。Flink中Checkpoint信息的发起者是JobManager。它不像Storm中那样，每条信息都会有ack信息的开销，而且按时间来计算花销。用户可以设置做checkpoint的频率，比如10秒钟做一次checkpoint。每做一次checkpoint，花销只有从Source发往map的1条checkpoint信息（JobManager发出来的checkpoint信息走的是控制流，与数据流无关）。与storm相比，Flink的可靠性机制开销要低得多。这也就是为什么保证可靠性对Flink的性能影响较小，而storm的影响确很大的原因。 最后一组自产数据的测试结果对比是Flink与Storm在进程间的数据传输的对比，可以看到进程间数据传输的情况下，Flink但并发吞吐是Storm的4.7倍。保证可靠性的情况下，是Storm的14倍。 上图展示的是消费kafka中数据时，Storm与Flink的但并发吞吐情况。因为消费的是kafka中的数据，所以吞吐量肯定会收到kafka的影响。我们发现性能的瓶颈是在SourceFunction上，于是增加了topic的partition数和SourceFunction取数据线程的并发数，但是MapFunction的并发数仍然是1.在这种情况下，我们发现flink的瓶颈转移到上游往下游发数据的地方。而Storm的瓶颈确是在下游收数据反序列化的地方。 之前的性能分析使我们基于数据传输和数据可靠性的角度出发，单纯的对Flink与Storm计算平台本身进行了性能分析。但实际使用时，task是肯定有计算逻辑的，这就势必更多的涉及到CPU，内存等资源问题。我们将来打算做一个智能分析平台，对用户的作业进行性能分析。通过收集到的指标信息，分析出作业的瓶颈在哪，并给出优化建议。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理","date":"2019-06-19T16:00:00.000Z","path":"2019/06/20/flink-kafka-Exactly-Once/","text":"Apache Flink 自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFunction（相关的 Jira）。它提取了两阶段提交协议的通用逻辑，使得通过 Flink 来构建端到端的 Exactly-Once 程序成为可能。同时支持一些数据源（source）和输出端（sink），包括 Apache Kafka 0.11及更高版本。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的 Exactly-Once 语义。 本文作者是 Piotr Nowojski，翻译自 周凯波原文地址：https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka 有关 TwoPhaseCommitSinkFunction 的使用详见文档: TwoPhaseCommitSinkFunction。或者可以直接阅读 Kafka 0.11 sink 的文档: kafka。 接下来会详细分析这个新功能以及Flink的实现逻辑，分为如下几点。 描述 Flink checkpoint 机制是如何保证Flink程序结果的 Exactly-Once 的 显示 Flink 如何通过两阶段提交协议与数据源和数据输出端交互，以提供端到端的 Exactly-Once 保证 通过一个简单的示例，了解如何使用 TwoPhaseCommitSinkFunction 实现 Exactly-Once 的文件输出 Flink 应用程序中的 Exactly-Once 语义当我们说『Exactly-Once』时，指的是每个输入的事件只影响最终结果一次。即使机器或软件出现故障，既没有重复数据，也不会丢数据。 Flink 很久之前就提供了 Exactly-Once 语义。在过去几年中，我们对 Flink 的 checkpoint 机制有过深入的描述，这是 Flink 有能力提供 Exactly-Once 语义的核心。Flink 文档还提供了该功能的全面概述。 在继续之前，先看下对 checkpoint 机制的简要介绍，这对理解后面的主题至关重要。 一次 checkpoint 是以下内容的一致性快照： 应用程序的当前状态 输入流的位置 Flink 可以配置一个固定的时间点，定期产生 checkpoint，将 checkpoint 的数据写入持久存储系统，例如 S3 或 HDFS 。将 checkpoint 数据写入持久存储是异步发生的，这意味着 Flink 应用程序在 checkpoint 过程中可以继续处理数据。 如果发生机器或软件故障，重新启动后，Flink 应用程序将从最新的 checkpoint 点恢复处理； Flink 会恢复应用程序状态，将输入流回滚到上次 checkpoint 保存的位置，然后重新开始运行。这意味着 Flink 可以像从未发生过故障一样计算结果。 在 Flink 1.4.0 之前，Exactly-Once 语义仅限于 Flink 应用程序内部，并没有扩展到 Flink 数据处理完后发送的大多数外部系统。Flink 应用程序与各种数据输出端进行交互，开发人员需要有能力自己维护组件的上下文来保证 Exactly-Once 语义。 为了提供端到端的 Exactly-Once 语义 - 也就是说，除了 Flink 应用程序内部， Flink 写入的外部系统也需要能满足 Exactly-Once 语义 - 这些外部系统必须提供提交或回滚的方法，然后通过 Flink 的 checkpoint 机制来协调。 分布式系统中，协调提交和回滚的常用方法是两阶段提交协议。在下一节中，我们将讨论 Flink 的 TwoPhaseCommitSinkFunction 是如何利用两阶段提交协议来提供端到端的 Exactly-Once 语义。 Flink 应用程序端到端的 Exactly-Once 语义我们将介绍两阶段提交协议，以及它如何在一个读写 Kafka 的 Flink 程序中实现端到端的 Exactly-Once 语义。Kafka 是一个流行的消息中间件，经常与 Flink 一起使用。Kafka 在最近的 0.11 版本中添加了对事务的支持。这意味着现在通过 Flink 读写 Kafka ，并提供端到端的 Exactly-Once 语义有了必要的支持。 Flink 对端到端的 Exactly-Once 语义的支持不仅局限于 Kafka ，您可以将它与任何一个提供了必要的协调机制的源/输出端一起使用。例如 Pravega，来自 DELL/EMC 的开源流媒体存储系统，通过 Flink 的 TwoPhaseCommitSinkFunction 也能支持端到端的 Exactly-Once 语义。 在今天讨论的这个示例程序中，我们有： 从 Kafka 读取的数据源（ Flink 内置的 KafkaConsumer） 窗口聚合 将数据写回 Kafka 的数据输出端（ Flink 内置的 KafkaProducer ） 要使数据输出端提供 Exactly-Once 保证，它必须将所有数据通过一个事务提交给 Kafka。提交捆绑了两个 checkpoint 之间的所有要写入的数据。这可确保在发生故障时能回滚写入的数据。但是在分布式系统中，通常会有多个并发运行的写入任务的，简单的提交或回滚是不够的，因为所有组件必须在提交或回滚时“一致”才能确保一致的结果。Flink 使用两阶段提交协议及预提交阶段来解决这个问题。 在 checkpoint 开始的时候，即两阶段提交协议的“预提交”阶段。当 checkpoint 开始时，Flink 的 JobManager 会将 checkpoint barrier（将数据流中的记录分为进入当前 checkpoint 与进入下一个 checkpoint ）注入数据流。 brarrier 在 operator 之间传递。对于每一个 operator，它触发 operator 的状态快照写入到 state backend。 数据源保存了消费 Kafka 的偏移量(offset)，之后将 checkpoint barrier 传递给下一个 operator。 这种方式仅适用于 operator 具有『内部』状态。所谓内部状态，是指 Flink statebackend 保存和管理的 -例如，第二个 operator 中 window 聚合算出来的 sum 值。当一个进程有它的内部状态的时候，除了在 checkpoint 之前需要将数据变更写入到 state backend ，不需要在预提交阶段执行任何其他操作。Flink 负责在 checkpoint 成功的情况下正确提交这些写入，或者在出现故障时中止这些写入。 示例 Flink 应用程序启动预提交阶段但是，当进程具有『外部』状态时，需要作些额外的处理。外部状态通常以写入外部系统（如 Kafka）的形式出现。在这种情况下，为了提供 Exactly-Once 保证，外部系统必须支持事务，这样才能和两阶段提交协议集成。 在本文示例中的数据需要写入 Kafka，因此数据输出端（ Data Sink ）有外部状态。在这种情况下，在预提交阶段，除了将其状态写入 state backend 之外，数据输出端还必须预先提交其外部事务。 当 checkpoint barrier 在所有 operator 都传递了一遍，并且触发的 checkpoint 回调成功完成时，预提交阶段就结束了。所有触发的状态快照都被视为该 checkpoint 的一部分。checkpoint 是整个应用程序状态的快照，包括预先提交的外部状态。如果发生故障，我们可以回滚到上次成功完成快照的时间点。 下一步是通知所有 operator，checkpoint 已经成功了。这是两阶段提交协议的提交阶段，JobManager 为应用程序中的每个 operator 发出 checkpoint 已完成的回调。 数据源和 windnow operator 没有外部状态，因此在提交阶段，这些 operator 不必执行任何操作。但是，数据输出端（Data Sink）拥有外部状态，此时应该提交外部事务。 我们对上述知识点总结下： 一旦所有 operator 完成预提交，就提交一个 commit。 如果至少有一个预提交失败，则所有其他提交都将中止，我们将回滚到上一个成功完成的 checkpoint 。 在预提交成功之后，提交的 commit 需要保证最终成功 - operator 和外部系统都需要保障这点。如果 commit 失败（例如，由于间歇性网络问题），整个 Flink 应用程序将失败，应用程序将根据用户的重启策略重新启动，还会尝试再提交。这个过程至关重要，因为如果 commit 最终没有成功，将会导致数据丢失。 因此，我们可以确定所有 operator 都同意 checkpoint 的最终结果：所有 operator 都同意数据已提交，或提交被中止并回滚。 在 Flink 中实现两阶段提交 Operator完整的实现两阶段提交协议可能有点复杂，这就是为什么 Flink 将它的通用逻辑提取到抽象类 TwoPhaseCommitSinkFunction 中的原因。 接下来基于输出到文件的简单示例，说明如何使用 TwoPhaseCommitSinkFunction 。用户只需要实现四个函数，就能为数据输出端实现 Exactly-Once 语义： beginTransaction - 在事务开始前，我们在目标文件系统的临时目录中创建一个临时文件。随后，我们可以在处理数据时将数据写入此文件。 preCommit - 在预提交阶段，我们刷新文件到存储，关闭文件，不再重新写入。我们还将为属于下一个 checkpoint 的任何后续文件写入启动一个新的事务。 commit - 在提交阶段，我们将预提交阶段的文件原子地移动到真正的目标目录。需要注意的是，这会增加输出数据可见性的延迟。 abort - 在中止阶段，我们删除临时文件。 我们知道，如果发生任何故障，Flink 会将应用程序的状态恢复到最新的一次 checkpoint 点。一种极端的情况是，预提交成功了，但在这次 commit 的通知到达 operator 之前发生了故障。在这种情况下，Flink 会将 operator 的状态恢复到已经预提交，但尚未真正提交的状态。 我们需要在预提交阶段保存足够多的信息到 checkpoint 状态中，以便在重启后能正确的中止或提交事务。在这个例子中，这些信息是临时文件和目标目录的路径。 TwoPhaseCommitSinkFunction 已经把这种情况考虑在内了，并且在从 checkpoint 点恢复状态时，会优先发出一个 commit 。我们需要以幂等方式实现提交，一般来说，这并不难。在这个示例中，我们可以识别出这样的情况：临时文件不在临时目录中，但已经移动到目标目录了。 在 TwoPhaseCommitSinkFunction 中，还有一些其他边界情况也会考虑在内，请参考 Flink 文档了解更多信息。 总结总结下本文涉及的一些要点： Flink 的 checkpoint 机制是支持两阶段提交协议并提供端到端的 Exactly-Once 语义的基础。 这个方案的优点是: Flink 不像其他一些系统那样，通过网络传输存储数据 - 不需要像大多数批处理程序那样将计算的每个阶段写入磁盘。 Flink 的 TwoPhaseCommitSinkFunction 提取了两阶段提交协议的通用逻辑，基于此将 Flink 和支持事务的外部系统结合，构建端到端的 Exactly-Once 成为可能。 从 Flink 1.4.0 开始，Pravega 和 Kafka 0.11 producer 都提供了 Exactly-Once 语义；Kafka 在0.11版本首次引入了事务，为在 Flink 程序中使用 Kafka producer 提供 Exactly-Once 语义提供了可能性。 Kafka 0.11 producer的事务是在 TwoPhaseCommitSinkFunction 基础上实现的，和 at-least-once producer 相比只增加了非常低的开销。 这是个令人兴奋的功能，期待 Flink TwoPhaseCommitSinkFunction 在未来支持更多的数据接收端。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink状态管理和容错机制介绍","date":"2019-06-17T16:00:00.000Z","path":"2019/06/18/flink-state/","text":"有状态的流数据处理 本文整理自去年8月11日在北京举行的 Flink Meetup 会议，分享嘉宾施晓罡，目前在阿里大数据团队部从事Blink方面的研发，现在主要负责Blink状态管理和容错相关技术的研发。 1.1. 什么是有状态的计算计算任务的结果不仅仅依赖于输入，还依赖于它的当前状态，其实大多数的计算都是有状态的计算。 比如wordcount,给一些word,其计算它的count,这是一个很常见的业务场景。count做为输出，在计算的过程中要不断的把输入累加到count上去，那么count就是一个state。 1.2.传统流计算缺少对于程序状态的有效支持状态数据的存储和访问； 状态数据的备份和恢复； 状态数据的划分和动态扩容； 在传统的批处理中，数据是划分为块分片去完成的，然后每一个Task去处理一个分片。当分片执行完成后，把输出聚合起来就是最终的结果。在这个过程当中，对于state的需求还是比较小的。 对于流计算而言，对State有非常高的要求，因为在流系统中输入是一个无限制的流，会运行很长一段时间，甚至运行几天或者几个月都不会停机。在这个过程当中，就需要将状态数据很好的管理起来。很不幸的是，在传统的流计算系统中，对状态管理支持并不是很完善。比如storm,没有任何程序状态的支持，一种可选的方案是storm+hbase这样的方式去实现，把这状态数据存放在Hbase中，计算的时候再次从Hbase读取状态数据，做更新在写入进去。这样就会有如下几个问题 1.3.Flink丰富的状态访问和高效的容错机制Flink在最早设计的时候就意识到了这个问题，并提供了丰富的状态访问和容错机制。如下图所示： Flink中的状态管理 2.1.按照数据的划分和扩张方式 Keyed States Operator States 2.1.1. Keyed States Keyed States的使用 Flink也提供了Keyed States多种数据结构类型 Keyed States的动态扩容 2.1.2.Operator State Operator States的使用 Operator States的数据结构不像Keyed States丰富，现在只支持List。 Operator States多种扩展方式 Operator States的动态扩展是非常灵活的，现提供了3种扩展，下面分别介绍： ListState:并发度在改变的时候，会将并发上的每个List都取出，然后把这些List合并到一个新的List,然后根据元素的个数在均匀分配给新的Task; UnionListState:相比于ListState更加灵活，把划分的方式交给用户去做，当改变并发的时候，会将原来的List拼接起来。然后不做划分，直接交给用户； BroadcastState:如大表和小表做Join时，小表可以直接广播给大表的分区，在每个并发上的数据都是完全一致的。做的更新也相同，当改变并发的时候，把这些数据COPY到新的Task即可； 以上是Flink Operator States提供的3种扩展方式，用户可以根据自己的需求做选择。 使用Checkpoint提高程序的可靠性用户可以根据的程序里面的配置将checkpoint打开，给定一个时间间隔后，框架会按照时间间隔给程序的状态进行备份。当发生故障时，Flink会将所有Task的状态一起恢复到Checkpoint的状态。从哪个位置开始重新执行。 Flink也提供了多种正确性的保障，包括： AT LEAST ONCE; Exactly once; 备份为保存在State中的程序状态数据 Flink也提供了一套机制，允许把这些状态放到内存当中。做Checkpoint的时候，由Flink去完成恢复。 从已停止作业的运行状态中恢复 当组件升级的时候，需要停止当前作业。这个时候需要从之前停止的作业当中恢复，Flink提供了2种机制恢复作业: Savepoint:是一种特殊的checkpoint，只不过不像checkpoint定期的从系统中去触发的，它是用户通过命令触发，存储格式和checkpoint也是不相同的，会将数据按照一个标准的格式存储，不管配置什么样，Flink都会从这个checkpoint恢复，是用来做版本升级一个非常好的工具； External Checkpoint：对已有checkpoint的一种扩展，就是说做完一次内部的一次Checkpoint后，还会在用户给定的一个目录中，多存储一份checkpoint的数据； 状态管理和容错机制实现下面介绍一下状态管理和容错机制实现方式，Flink提供了3种不同的StateBackend MemoryStateBackend FsStateBackend RockDBStateBackend 用户可以根据自己的需求选择，如果数据量较小，可以存放到MemoryStateBackend和FsStateBackend中，如果数据量较大，可以放到RockDB中。 HeapKeyedStateBackend RockDBKeyedStateBackend Checkpoint的执行流程 Checkpoint的执行流程是按照Chandy-Lamport算法实现的 Checkpoint Barrier的对齐 全量Checkpoint 全量Checkpoint会在每个节点做备份数据时，只需要将数据都便利一遍，然后写到外部存储中，这种情况会影响备份性能。在此基础上做了优化。 RockDB的增量Checkpoint RockDB的数据会更新到内存，当内存满时，会写入到磁盘中。增量的机制会将新产生的文件COPY持久化中，而之前产生的文件就不需要COPY到持久化中去了。通过这种方式减少COPY的数据量，并提高性能。 阿里相关工作介绍4.1.Flink在阿里的成长路线 阿里是从2015年开始调研Flink,2015年10月启动Blink项目，并完善Flink在大规模生产下的一些优化和改进。2016年双11采用了Blink系统，为搜索，推荐，广告业务提供服务。2017年5月Blink已成为阿里的实时计算引擎。 4.2.阿里在状态管理和容错相关的工作 正在做的工作，基于State重构Window方面的一些优化，阿里也正在将功能做完善。后续将包括asynchronous Checkpoint的功能完善，并和社区进一步沟通和合作。帮助Flink社区完善相关方面的工作。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"流计算框架 Flink 与 Storm 的性能对比","date":"2019-06-16T16:00:00.000Z","path":"2019/06/17/flink-vs-storm/","text":"1. 背景Apache Flink 和 Apache Storm 是当前业界广泛使用的两个分布式实时计算框架。其中 Apache Storm（以下简称“Storm”）在美团点评实时计算业务中已有较为成熟的运用（可参考 Storm 的可靠性保证测试），有管理平台、常用 API 和相应的文档，大量实时作业基于 Storm 构建。而 Apache Flink（以下简称“Flink”）在近期倍受关注，具有高吞吐、低延迟、高可靠和精确计算等特性，对事件窗口有很好的支持，目前在美团点评实时计算业务中也已有一定应用。 本文转载自美团技术团队公众号，作者：梦瑶 为深入熟悉了解 Flink 框架，验证其稳定性和可靠性，评估其实时处理性能，识别该体系中的缺点，找到其性能瓶颈并进行优化，给用户提供最适合的实时计算引擎，我们以实践经验丰富的 Storm 框架作为对照，进行了一系列实验测试 Flink 框架的性能，计算 Flink 作为确保“至少一次”和“恰好一次”语义的实时计算框架时对资源的消耗，为实时计算平台资源规划、框架选择、性能调优等决策及 Flink 平台的建设提出建议并提供数据支持，为后续的 SLA 建设提供一定参考。 Flink 与 Storm 两个框架对比： 2. 测试目标评估不同场景、不同数据压力下 Flink 和 Storm 两个实时计算框架目前的性能表现，获取其详细性能数据并找到处理性能的极限；了解不同配置对 Flink 性能影响的程度，分析各种配置的适用场景，从而得出调优建议。 2.1 测试场景“输入-输出”简单处理场景 通过对“输入-输出”这样简单处理逻辑场景的测试，尽可能减少其它因素的干扰，反映两个框架本身的性能。 同时测算框架处理能力的极限，处理更加复杂的逻辑的性能不会比纯粹“输入-输出”更高。 用户作业耗时较长的场景 如果用户的处理逻辑较为复杂，或是访问了数据库等外部组件，其执行时间会增大，作业的性能会受到影响。因此，我们测试了用户作业耗时较长的场景下两个框架的调度性能。 窗口统计场景 实时计算中常有对时间窗口或计数窗口进行统计的需求，例如一天中每五分钟的访问量，每 100 个订单中有多少个使用了优惠等。Flink 在窗口支持上的功能比 Storm 更加强大，API 更加完善，但是我们同时也想了解在窗口统计这个常用场景下两个框架的性能。 精确计算场景（即消息投递语义为“恰好一次”） Storm 仅能保证“至多一次” (At Most Once) 和“至少一次” (At Least Once) 的消息投递语义，即可能存在重复发送的情况。有很多业务场景对数据的精确性要求较高，希望消息投递不重不漏。Flink 支持“恰好一次” (Exactly Once) 的语义，但是在限定的资源条件下，更加严格的精确度要求可能带来更高的代价，从而影响性能。因此，我们测试了在不同消息投递语义下两个框架的性能，希望为精确计算场景的资源规划提供数据参考。 2.2 性能指标吞吐量（Throughput） 单位时间内由计算框架成功地传送数据的数量，本次测试吞吐量的单位为：条/秒。 反映了系统的负载能力，在相应的资源条件下，单位时间内系统能处理多少数据。 * 吞吐量常用于资源规划，同时也用于协助分析系统性能瓶颈，从而进行相应的资源调整以保证系统能达到用户所要求的处理能力。假设商家每小时能做二十份午餐（吞吐量 20 份/小时），一个外卖小哥每小时只能送两份（吞吐量 2 份/小时），这个系统的瓶颈就在小哥配送这个环节，可以给该商家安排十个外卖小哥配送。 延迟（Latency） 数据从进入系统到流出系统所用的时间，本次测试延迟的单位为：毫秒。 反映了系统处理的实时性。 金融交易分析等大量实时计算业务对延迟有较高要求，延迟越低，数据实时性越强。 假设商家做一份午餐需要 5 分钟，小哥配送需要 25 分钟，这个流程中用户感受到了 30 分钟的延迟。如果更换配送方案后延迟变成了 60 分钟，等送到了饭菜都凉了，这个新的方案就是无法接受的。 3. 测试环境为 Storm 和 Flink 分别搭建由 1 台主节点和 2 台从节点构成的 Standalone 集群进行本次测试。其中为了观察 Flink 在实际生产环境中的性能，对于部分测内容也进行了 on Yarn 环境的测试。 3.1 集群参数 3.2 框架参数 4. 测试方法4.1 测试流程 数据生产 Data Generator 按特定速率生成数据，带上自增的 id 和 eventTime 时间戳写入 Kafka 的一个 Topic（Topic Data）。 数据处理 Storm Task 和 Flink Task （每个测试用例不同）从 Kafka Topic Data 相同的 Offset 开始消费，并将结果及相应 inTime、outTime 时间戳分别写入两个 Topic（Topic Storm 和 Topic Flink）中。 指标统计 Metrics Collector 按 outTime 的时间窗口从这两个 Topic 中统计测试指标，每五分钟将相应的指标写入 MySQL 表中。Metrics Collector 按 outTime 取五分钟的滚动时间窗口，计算五分钟的平均吞吐（输出数据的条数）、五分钟内的延迟（outTime - eventTime 或 outTime - inTime）的中位数及 99 线等指标，写入 MySQL 相应的数据表中。最后对 MySQL 表中的吞吐计算均值，延迟中位数及延迟 99 线选取中位数，绘制图像并分析。 4.2 默认参数 Storm 和 Flink 默认均为 At Least Once 语义。 Storm 开启 ACK，ACKer 数量为 1。 Flink 的 Checkpoint 时间间隔为 30 秒，默认 StateBackend 为 Memory。 保证 Kafka 不是性能瓶颈，尽可能排除 Kafka 对测试结果的影响。 测试延迟时数据生产速率小于数据处理能力，假设数据被写入 Kafka 后立刻被读取，即 eventTime 等于数据进入系统的时间。 测试吞吐量时从 Kafka Topic 的最旧开始读取，假设该 Topic 中的测试数据量充足。 4.3 测试用例Identity Identity 用例主要模拟“输入-输出”简单处理场景，反映两个框架本身的性能。 输入数据为“msgId, eventTime”，其中 eventTime 视为数据生成时间。单条输入数据约 20 B。 进入作业处理流程时记录 inTime，作业处理完成后（准备输出时）记录 outTime。 作业从 Kafka Topic Data 中读取数据后，在字符串末尾追加时间戳，然后直接输出到 Kafka。 输出数据为“msgId, eventTime, inTime, outTime”。单条输出数据约 50 B。 Sleep Sleep 用例主要模拟用户作业耗时较长的场景，反映复杂用户逻辑对框架差异的削弱，比较两个框架的调度性能。 输入数据和输出数据均与 Identity 相同。 读入数据后，等待一定时长（1 ms）后在字符串末尾追加时间戳后输出 Windowed Word Count Windowed Word Count 用例主要模拟窗口统计场景，反映两个框架在进行窗口统计时性能的差异。 此外，还用其进行了精确计算场景的测试，反映 Flink 恰好一次投递的性能。 输入为 JSON 格式，包含 msgId、eventTime 和一个由若干单词组成的句子，单词之间由空格分隔。单条输入数据约 150 B。 读入数据后解析 JSON，然后将句子分割为相应单词，带 eventTime 和 inTime 时间戳发给 CountWindow 进行单词计数，同时记录一个窗口中最大最小的 eventTime 和 inTime，最后带 outTime 时间戳输出到 Kafka 相应的 Topic。 Spout/Source 及 OutputBolt/Output/Sink 并发度恒为 1，增大并发度时仅增大 JSONParser、CountWindow 的并发度。 由于 Storm 对 window 的支持较弱，CountWindow 使用一个 HashMap 手动实现，Flink 用了原生的 CountWindow 和相应的 Reduce 函数。 5. 测试结果5.1 Identity 单线程吞吐量 上图中蓝色柱形为单线程 Storm 作业的吞吐，橙色柱形为单线程 Flink 作业的吞吐。 Identity 逻辑下，Storm 单线程吞吐为 8.7 万条/秒，Flink 单线程吞吐可达 35 万条/秒。 当 Kafka Data 的 Partition 数为 1 时，Flink 的吞吐约为 Storm 的 3.2 倍；当其 Partition 数为 8 时，Flink 的吞吐约为 Storm 的 4.6 倍。 由此可以看出，Flink 吞吐约为 Storm 的 3-5 倍。 5.2 Identity 单线程作业延迟 采用 outTime - eventTime 作为延迟，图中蓝色折线为 Storm，橙色折线为 Flink。虚线为 99 线，实线为中位数。 从图中可以看出随着数据量逐渐增大，Identity 的延迟逐渐增大。其中 99 线的增大速度比中位数快，Storm 的 增大速度比 Flink 快。 其中 QPS 在 80000 以上的测试数据超过了 Storm 单线程的吞吐能力，无法对 Storm 进行测试，只有 Flink 的曲线。 对比折线最右端的数据可以看出，Storm QPS 接近吞吐时延迟中位数约 100 毫秒，99 线约 700 毫秒，Flink 中位数约 50 毫秒，99 线约 300 毫秒。Flink 在满吞吐时的延迟约为 Storm 的一半。 5.3 Sleep 吞吐量 从图中可以看出，Sleep 1 毫秒时，Storm 和 Flink 单线程的吞吐均在 900 条/秒左右，且随着并发增大基本呈线性增大。 对比蓝色和橙色的柱形可以发现，此时两个框架的吞吐能力基本一致。 5.4 Sleep 单线程作业延迟（中位数） 依然采用 outTime - eventTime 作为延迟，从图中可以看出，Sleep 1 毫秒时，Flink 的延迟仍低于 Storm。 5.5 Windowed Word Count 单线程吞吐量 单线程执行大小为 10 的计数窗口，吞吐量统计如图。 从图中可以看出，Storm 吞吐约为 1.2 万条/秒，Flink Standalone 约为 4.3 万条/秒。Flink 吞吐依然为 Storm 的 3 倍以上。 5.6 Windowed Word Count Flink At Least Once 与 Exactly Once 吞吐量对比 由于同一算子的多个并行任务处理速度可能不同，在上游算子中不同快照里的内容，经过中间并行算子的处理，到达下游算子时可能被计入同一个快照中。这样一来，这部分数据会被重复处理。因此，Flink 在 Exactly Once 语义下需要进行对齐，即当前最早的快照中所有数据处理完之前，属于下一个快照的数据不进行处理，而是在缓存区等待。当前测试用例中，在 JSON Parser 和 CountWindow、CountWindow 和 Output 之间均需要进行对齐，有一定消耗。为体现出对齐场景，Source/Output/Sink 并发度的并发度仍为 1，提高了 JSONParser/CountWindow 的并发度。具体流程细节参见前文 Windowed Word Count 流程图。 上图中橙色柱形为 At Least Once 的吞吐量，黄色柱形为 Exactly Once 的吞吐量。对比两者可以看出，在当前并发条件下，Exactly Once 的吞吐较 At Least Once 而言下降了 6.3% 5.7 Windowed Word Count Storm At Least Once 与 At Most Once 吞吐量对比 Storm 将 ACKer 数量设置为零后，每条消息在发送时就自动 ACK，不再等待 Bolt 的 ACK，也不再重发消息，为 At Most Once 语义。 上图中蓝色柱形为 At Least Once 的吞吐量，浅蓝色柱形为 At Most Once 的吞吐量。对比两者可以看出，在当前并发条件下，At Most Once 语义下的吞吐较 At Least Once 而言提高了 16.8% 5.8 Windowed Word Count 单线程作业延迟 Identity 和 Sleep 观测的都是 outTime - eventTime，因为作业处理时间较短或 Thread.sleep() 精度不高，outTime - inTime 为零或没有比较意义；Windowed Word Count 中可以有效测得 outTime - inTime 的数值，将其与 outTime - eventTime 画在同一张图上，其中 outTime - eventTime 为虚线，outTime - InTime 为实线。 观察橙色的两条折线可以发现，Flink 用两种方式统计的延迟都维持在较低水平；观察两条蓝色的曲线可以发现，Storm 的 outTime - inTime 较低，outTime - eventTime 一直较高，即 inTime 和 eventTime 之间的差值一直较大，可能与 Storm 和 Flink 的数据读入方式有关。 蓝色折线表明 Storm 的延迟随数据量的增大而增大，而橙色折线表明 Flink 的延迟随着数据量的增大而减小（此处未测至 Flink 吞吐量，接近吞吐时 Flink 延迟依然会上升）。 即使仅关注 outTime - inTime（即图中实线部分），依然可以发现，当 QPS 逐渐增大的时候，Flink 在延迟上的优势开始体现出来。 5.9 Windowed Word Count Flink At Least Once 与 Exactly Once 延迟对比 图中黄色为 99 线，橙色为中位数，虚线为 At Least Once，实线为 Exactly Once。图中相应颜色的虚实曲线都基本重合，可以看出 Flink Exactly Once 的延迟中位数曲线与 At Least Once 基本贴合，在延迟上性能没有太大差异。 5.10 Windowed Word Count Storm At Least Once 与 At Most Once 延迟对比 图中蓝色为 99 线，浅蓝色为中位数，虚线为 At Least Once，实线为 At Most Once。QPS 在 4000 及以前的时候，虚线实线基本重合；QPS 在 6000 时两者已有差异，虚线略高；QPS 接近 8000 时，已超过 At Least Once 语义下 Storm 的吞吐，因此只有实线上的点。 可以看出，QPS 较低时 Storm At Most Once 与 At Least Once 的延迟观察不到差异，随着 QPS 增大差异开始增大，At Most Once 的延迟较低。 5.11 Windowed Word Count Flink 不同 StateBackends 吞吐量对比 Flink 支持 Standalone 和 on Yarn 的集群部署模式，同时支持 Memory、FileSystem、RocksDB 三种状态存储后端（StateBackends）。由于线上作业需要，测试了这三种 StateBackends 在两种集群部署模式上的性能差异。其中，Standalone 时的存储路径为 JobManager 上的一个文件目录，on Yarn 时存储路径为 HDFS 上一个文件目录。 对比三组柱形可以发现，使用 FileSystem 和 Memory 的吞吐差异不大，使用 RocksDB 的吞吐仅其余两者的十分之一左右。 对比两种颜色可以发现，Standalone 和 on Yarn 的总体差异不大，使用 FileSystem 和 Memory 时 on Yarn 模式下吞吐稍高，使用 RocksDB 时 Standalone 模式下的吞吐稍高。 5.12 Windowed Word Count Flink 不同 StateBackends 延迟对比 使用 FileSystem 和 Memory 作为 Backends 时，延迟基本一致且较低。 使用 RocksDB 作为 Backends 时，延迟稍高，且由于吞吐较低，在达到吞吐瓶颈前的延迟陡增。其中 on Yarn 模式下吞吐更低，接近吞吐时的延迟更高。 6. 结论及建议6.1 框架本身性能 由 5.1、5.5 的测试结果可以看出，Storm 单线程吞吐约为 8.7 万条/秒，Flink 单线程吞吐可达 35 万条/秒。Flink 吞吐约为 Storm 的 3-5 倍。 由 5.2、5.8 的测试结果可以看出，Storm QPS 接近吞吐时延迟（含 Kafka 读写时间）中位数约 100 毫秒，99 线约 700 毫秒，Flink 中位数约 50 毫秒，99 线约 300 毫秒。Flink 在满吞吐时的延迟约为 Storm 的一半，且随着 QPS 逐渐增大，Flink 在延迟上的优势开始体现出来。 综上可得，Flink 框架本身性能优于 Storm。 6.2 复杂用户逻辑对框架差异的削弱对比 5.1 和 5.3、5.2 和 5.4 的测试结果可以发现，单个 Bolt Sleep 时长达到 1 毫秒时，Flink 的延迟仍低于 Storm，但吞吐优势已基本无法体现。 因此，用户逻辑越复杂，本身耗时越长，针对该逻辑的测试体现出来的框架的差异越小。 6.3 不同消息投递语义的差异 由 5.6、5.7、5.9、5.10 的测试结果可以看出，Flink Exactly Once 的吞吐较 At Least Once 而言下降 6.3%，延迟差异不大；Storm At Most Once 语义下的吞吐较 At Least Once 提升 16.8%，延迟稍有下降。 由于 Storm 会对每条消息进行 ACK，Flink 是基于一批消息做的检查点，不同的实现原理导致两者在 At Least Once 语义的花费差异较大，从而影响了性能。而 Flink 实现 Exactly Once 语义仅增加了对齐操作，因此在算子并发量不大、没有出现慢节点的情况下对 Flink 性能的影响不大。Storm At Most Once 语义下的性能仍然低于 Flink。 6.4 Flink 状态存储后端选择Flink 提供了内存、文件系统、RocksDB 三种 StateBackends，结合 5.11、5.12 的测试结果，三者的对比如下： 6.5 推荐使用 Flink 的场景综合上述测试结果，以下实时计算场景建议考虑使用 Flink 框架进行计算： + 要求消息投递语义为 Exactly Once 的场景； + 数据量较大，要求高吞吐低延迟的场景； + 需要进行状态管理或窗口统计的场景。 7. 展望 本次测试中尚有一些内容没有进行更加深入的测试，有待后续测试补充。例如： Exactly Once 在并发量增大的时候是否吞吐会明显下降？ 用户耗时到 1ms 时框架的差异已经不再明显（Thread.sleep() 的精度只能到毫秒），用户耗时在什么范围内 Flink 的优势依然能体现出来？ 本次测试仅观察了吞吐量和延迟两项指标，对于系统的可靠性、可扩展性等重要的性能指标没有在统计数据层面进行关注，有待后续补充。 Flink 使用 RocksDBStateBackend 时的吞吐较低，有待进一步探索和优化。 关于 Flink 的更高级 API，如 Table API &amp; SQL 及 CEP 等，需要进一步了解和完善。 8. 参考内容 分布式流处理框架——功能对比和性能评估. intel-hadoop/HiBench: HiBench is a big data benchmark suite. Yahoo的流计算引擎基准测试. Extending the Yahoo! Streaming Benchmark. 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库","date":"2019-06-15T16:00:00.000Z","path":"2019/06/16/flink-sql-oppo/","text":"一.OPPO 实时数仓的演进思路 本文转载自 AI 前线公众号，作者张俊，编辑 | Vincent 本文整理自 2019 年 4 月 13 日在深圳举行的 Flink Meetup 会议，分享嘉宾张俊，目前担任 OPPO 大数据平台研发负责人，也是 Apache Flink contributor。本文主要内容如下： OPPO 实时数仓的演进思路； 基于 Flink SQL 的扩展工作； 构建实时数仓的应用案例； 未来工作的思考和展望。 1.1.OPPO 业务与数据规模大家都知道 OPPO 是做智能手机的，但并不知道 OPPO 与互联网以及大数据有什么关系，下图概要介绍了 OPPO 的业务与数据情况： OPPO 作为手机厂商，基于 Android 定制了自己的 ColorOS 系统，当前日活跃用户超过 2 亿。围绕 ColorOS，OPPO 构建了很多互联网应用，比如应用商店、浏览器、信息流等。在运营这些互联网应用的过程中，OPPO 积累了大量的数据，上图右边是整体数据规模的演进：从 2012 年开始每年都是 2~3 倍的增长速度，截至目前总数据量已经超过 100PB，日增数据量超过 200TB。 要支撑这么大的一个数据量，OPPO 研发出一整套的数据系统与服务，并逐渐形成了自己的数据中台体系。 1.2.OPPO 数据中台 今年大家都在谈数据中台，OPPO 是如何理解数据中台的呢？我们把它分成了 4 个层次： 最下层是统一工具体系，涵盖了”接入 - 治理 - 开发 - 消费”全数据链路； 基于工具体系之上构建了数据仓库，划分成”原始层 - 明细层 - 汇总层 - 应用层”，这也是经典的数仓架构； 再往上是全域的数据体系，什么是全域呢？就是把公司所有的业务数据都打通，形成统一的数据资产，比如 ID-Mapping、用户标签等； 最终，数据要能被业务用起来，需要场景驱动的数据产品与服务。 以上就是 OPPO 数据中台的整个体系，而数据仓库在其中处于非常基础与核心的位置。 1.3. 构建 OPPO 离线数仓 过往 2、3 年，我们的重点聚焦在离线数仓的构建。上图大致描述了整个构建过程：首先，数据来源基本是手机、日志文件以及 DB 数据库，我们基于 Apache NiFi 打造了高可用、高吞吐的接入系统，将数据统一落入 HDFS，形成原始层；紧接着，基于 Hive 的小时级 ETL 与天级汇总 Hive 任务，分别负责计算生成明细层与汇总层；最后，应用层是基于 OPPO 内部研发的数据产品，主要是报表分析、用户画像以及接口服务。此外，中间的明细层还支持基于 Presto 的即席查询与自助提数。 伴随着离线数仓的逐步完善，业务对实时数仓的诉求也愈发强烈。 1.4. 数仓实时化的诉求 对于数仓实时化的诉求，大家通常都是从业务视角来看，但其实站在平台的角度，实时化也能带来切实的好处。首先，从业务侧来看，报表、标签、接口等都会有实时的应用场景，分别参见上图左边的几个案例；其次，对平台侧来说，我们可以从三个案例来看：第一，OPPO 大量的批量任务都是从 0 点开始启动，都是通过 T+1 的方式去做数据处理，这会导致计算负载集中爆发，对集群的压力很大；第二，标签导入也属于一种 T+1 批量任务，每次全量导入都会耗费很长的时间；第三，数据质量的监控也必须是 T+1 的，导致没办法及时发现数据的一些问题。 既然业务侧和平台侧都有实时化的这个诉求，那 OPPO 是如何来构建自己的实时数仓呢？ 1.5. 离线到实时的平滑迁移 无论是一个平台还是一个系统，都离不开上下两个层次的构成：上层是 API，是面向用户的编程抽象与接口；下层是 Runtime，是面向内核的执行引擎。我们希望从离线到实时的迁移是平滑的，是什么意思呢？从 API 这层来看，数仓的抽象是 Table、编程接口是 SQL+UDF，离线数仓时代用户已经习惯了这样的 API，迁移到实时数仓后最好也能保持一致。而从 Runtime 这层来看，计算引擎从 Hive 演进到了 Flink，存储引擎从 HDFS 演进到了 Kafka。 基于以上的思路，只需要把之前提到的离线数仓 pipeline 改造下，就得到了实时数仓 pipeline。 1.6. 构建 OPPO 实时数仓 HDFS 替换为 Kafka。从总体流程来看，基本模型是不变的，还是由原始层、明细层、汇总层、应用层的级联计算来构成。 因此，这里的核心问题是如何基于 Flink 构建出这个 pipeline，下面就介绍下我们基于 Flink SQL 所做的一些工作。 二. 基于 Flink SQL 的扩展工作2.1.Why Flink SQL首先，为什么要用 Flink SQL? 下图展示了 Flink 框架的基本结构，最下面是 Runtime，这个执行引擎我们认为最核心的优势是四个：第一，低延迟，高吞吐；第二，端到端的 Exactly-once；第三，可容错的状态管理；第四，Window &amp; Event time 的支持。基于 Runtime 抽象出 3 个层次的 API，SQL 处于最上层。 Flink SQL API 有哪些优势呢？我们也从四个方面去看：第一，支持 ANSI SQL 的标准；第二，支持丰富的数据类型与内置函数，包括常见的算术运算与统计聚合；第三，可自定义 Source/Sink，基于此可以灵活地扩展上下游；第四，批流统一，同样的 SQL，既可以跑离线也可以跑实时。 那么，基于 Flink SQL API 如何编程呢？下面是一个简单的演示： 首先是定义与注册输入 / 输出表，这里创建了 2 张 Kakfa 的表，指定 kafka 版本是什么、对应哪个 topic；接下来是注册 UDF，篇幅原因这里没有列出 UDF 的定义；最后是才是执行真正的 SQL。可以看到，为了执行 SQL，需要做这么多的编码工作，这并不是我们希望暴露给用户的接口。 2.2. 基于 WEB 的开发 IDE 前面提到过，数仓的抽象是 Table，编程接口是 SQL+UDF。对于用户来说，平台提供的编程界面应该是类似上图的那种，有用过 HUE 做交互查询的应该很熟悉。左边的菜单是 Table 列表，右边是 SQL 编辑器，可以在上面直接写 SQL，然后提交执行。要实现这样一种交互方式，Flink SQL 默认是无法实现的，中间存在 gap，总结下来就 2 点：第一，元数据的管理，怎么去创建库表，怎么去上传 UDF，使得之后在 SQL 中可直接引用；第二，SQL 作业的管理，怎么去编译 SQL，怎么去提交作业。 在技术调研过程中，我们发现了 Uber 在 2017 年开源的 AthenaX 框架。 2.3.AthenaX：基于 REST 的 SQL 管理器 AthenaX 可以看作是一个基于 REST 的 SQL 管理器，它是怎么实现 SQL 作业与元数据管理的呢？ 对于 SQL 作业提交，AthenaX 中有一个 Job 的抽象，封装了要执行的 SQL 以及作业资源等信息。所有的 Job 由一个 JobStore 来托管，它定期跟 YARN 当中处于 Running 状态的 App 做一个匹配。如果不一致，就会向 YARN 提交对应的 Job。 对于元数据管理，核心的问题是如何将外部创建的库表注入 Flink，使得 SQL 中可以识别到。实际上，Flink 本身就预留了与外部元数据对接的能力，分别提供了 ExternalCatalog 和 ExternalCatalogTable 这两个抽象。AthenaX 在此基础上再封装出一个 TableCatalog，在接口层面做了一定的扩展。在提交 SQL 作业的阶段，AthenaX 会自动将 TableCatalog 注册到 Flink，再调用 Flink SQL 的接口将 SQL 编译为 Flink 的可执行单元 JobGraph，并最终提交到 YARN 生成新的 App。 AthenaX 虽然定义好了 TableCatalog 接口，但并没有提供可直接使用的实现。那么，我们怎么来实现，以便对接到我们已有的元数据系统呢？ 2.4.Flink SQL 注册库表的过程首先，我们得搞清楚 Flink SQL 内部是如何注册库表的。整个过程涉及到三个基本的抽象：TableDescriptor、TableFactory 以及 TableEnvironment。 TableDescriptor 顾名思义，是对表的描述，它由三个子描述符构成：第一是 Connector，描述数据的来源，比如 Kafka、ES 等；第二是 Format，描述数据的格式，比如 csv、json、avro 等；第三是 Schema，描述每个字段的名称与类型。TableDescriptor 有两个基本的实现——ConnectTableDescriptor 用于描述内部表，也就是编程方式创建的表；ExternalCatalogTable 用于描述外部表。 有了 TableDescriptor，接下来需要 TableFactory 根据描述信息来实例化 Table。不同的描述信息需要不同的 TableFactory 来处理，Flink 如何找到匹配的 TableFactory 实现呢？实际上，为了保证框架的可扩展性，Flink 采用了 Java SPI 机制来加载所有声明过的 TableFactory，通过遍历的方式去寻找哪个 TableFactory 是匹配该 TableDescriptor 的。TableDescriptor 在传递给 TableFactory 前，被转换成一个 map，所有的描述信息都用 key-value 形式来表达。TableFactory 定义了两个用于过滤匹配的方法——一个是 requiredContext()，用于检测某些特定 key 的 value 是否匹配，比如 connector.type 是否为 kakfa；另一个是 supportedProperties()，用于检测 key 是否能识别，如果出现不识别的 key，说明无法匹配。 匹配到了正确的 TableFactory，接下来就是创建真正的 Table，然后将其通过 TableEnvironment 注册。最终注册成功的 Table，才能在 SQL 中引用。 2.5.Flink SQL 对接外部数据源搞清楚了 Flink SQL 注册库表的过程，给我们带来这样一个思路：如果外部元数据创建的表也能被转换成 TableFactory 可识别的 map，那么就能被无缝地注册到 TableEnvironment。基于这个思路，我们实现了 Flink SQL 与已有元数据中心的对接，大致过程参见下图： 通过元数据中心创建的表，都会将元数据信息存储到 MySQL，我们用一张表来记录 Table 的基本信息，然后另外三张表分别记录 Connector、Format、Schema 转换成 key-value 后的描述信息。之所以拆开成三张表，是为了能够能独立的更新这三种描述信息。接下来是定制实现的 ExternalCatalog，能够读取 MySQL 这四张表，并转换成 map 结构。 2.6. 实时表 - 维表关联到目前为止，我们的平台已经具备了元数据管理与 SQL 作业管理的能力，但是要真正开放给用户使用，还有一点基本特性存在缺失。通过我们去构建数仓，星型模型是无法避免的。这里有一个比较简单的案例：中间的事实表记录了广告点击流，周边是关于用户、广告、产品、渠道的维度表。 假定我们有一个 SQL 分析，需要将点击流表与用户维表进行关联，这个目前在 Flink SQL 中应该怎么来实现？我们有两种实现方式，一个基于 UDF，一个基于 SQL 转换，下面分别展开来讲一下。 2.7. 基于 UDF 的维表关联首先是基于 UDF 的实现，需要用户将原始 SQL 改写为带 UDF 调用的 SQL，这里是 userDimFunc，上图右边是它的代码实现。UserDimFunc 继承了 Flink SQL 抽象的 TableFunction，它是其中一种 UDF 类型，可以将任意一行数据转换成一行或多行数据。为了实现维表关联，在 UDF 初始化时需要从 MySQL 全量加载维表的数据，缓存在内存 cache 中。后续对每行数据的处理，TableFunction 会调用 eval() 方法，在 eval() 中根据 user_id 去查找 cache，从而实现关联。当然，这里是假定维表数据比较小，如果数据量很大，不适合全量的加载与缓存，这里不做展开了。 基于 UDF 的实现，对用户和平台来说都不太友好：用户需要写奇怪的 SQL 语句，比如图中的 LATERAL TABLE；平台需要为每个关联场景定制特定的 UDF，维护成本太高。有没有更好的方式呢？下面我们来看看基于 SQL 转换的实现。 2.8. 基于 SQL 转换的维表关联我们希望解决基于 UDF 实现所带来的问题，用户不需要改写原始 SQL，平台不需要开发很多 UDF。有一种思路是，是否可以在 SQL 交给 Flink 编译之前，加一层 SQL 的解析与改写，自动实现维表的关联？经过一定的技术调研与 POC，我们发现是行得通的，所以称之为基于 SQL 转换的实现。下面将该思路展开解释下。 首先，增加的 SQL 解析是为了识别 SQL 中是否存在预先定义的维度表，比如上图中的 user_dim。一旦识别到维表，将触发 SQL 改写的流程，将红框标注的 join 语句改写成新的 Table，这个 Table 怎么得到呢？我们知道，流计算领域近年来发展出“流表二象性”的理念，Flink 也是该理念的践行者。这意味着，在 Flink 中 Stream 与 Table 之间是可以相互转换的。我们把 ad_clicks 对应的 Table 转换成 Stream，再调用 flatmap 形成另一个 Stream，最后再转换回 Table，就得到了 ad_clicks_user。最后的问题是，flatmap 是如何实现维表关联的？ Flink 中对于 Stream 的 flatmap 操作，实际上是执行一个 RichFlatmapFunciton，每来一行数据就调用其 flatmap() 方法做转换。那么，我们可以定制一个 RichFlatmapFunction，来实现维表数据的加载、缓存、查找以及关联，功能与基于 UDF 的 TableFunction 实现类似。 既然 RichFlatmapFunciton 的实现逻辑与 TableFunction 相似，那为什么相比基于 UDF 的方式，这种实现能更加通用呢？核心的点在于多了一层 SQL 解析，可以将维表的信息获取出来（比如维表名、关联字段、select 字段等），再封装成 JoinContext 传递给 RichFlatmapFunciton，使得的表达能力就具备通用性了。 三. 构建实时数仓的应用案例下面分享几个典型的应用案例，都是在我们的平台上用 Flink SQL 来实现的。 3.1. 实时 ETL 拆分这里是一个典型的实时 ETL 链路，从大表中拆分出各业务对应的小表： OPPO 的最大数据来源是手机端埋点，从手机 APP 过来的数据有一个特点，所有的数据是通过统一的几个通道上报过来。因为不可能每一次业务有新的埋点，都要去升级客户端，去增加新的通道。比如我们有个 sdk_log 通道，所有 APP 应用的埋点都往这个通道上报数据，导致这个通道对应的原始层表巨大，一天几十个 TB。但实际上，每个业务只关心它自身的那部分数据，这就要求我们在原始层进行 ETL 拆分。 这个 SQL 逻辑比较简单，无非是根据某些业务字段做筛选，插入到不同的业务表中去。它的特点是，多行 SQL 最终合并成一个 SQL 提交给 Flink 执行。大家担心的是，包含了 4 个 SQL，会不会对同一份数据重复读取 4 次？其实，在 Flink 编译 SQL 的阶段是会做一些优化的，因为最终指向的是同一个 kafka topic，所以只会读取 1 次数据。 另外，同样的 Flink SQL，我们同时用于离线与实时数仓的 ETL 拆分，分别落入 HDFS 与 Kafka。Flink 中本身支持写入 HDFS 的 Sink，比如 RollingFileSink。 3.2. 实时指标统计这里是一个典型的计算信息流 CTR 的这个案例，分别计算一定时间段内的曝光与点击次数，相除得到点击率导入 Mysql，然后通过我们内部的报表系统来可视化。这个 SQL 的特点是它用到了窗口 (Tumbling Window) 以及子查询。 3.3. 实时标签导入这里是一个实时标签导入的案例，手机端实时感知到当前用户的经纬度，转换成具体 POI 后导入 ES，最终在标签系统上做用户定向。 这个 SQL 的特点是用了 AggregateFunction，在 5 分钟的窗口内，我们只关心用户最新一次上报的经纬度。AggregateFunction 是一种 UDF 类型，通常是用于聚合指标的统计，比如计算 sum 或者 average。在这个示例中，由于我们只关心最新的经纬度，所以每次都替换老的数据即可。 四. 未来工作的思考和展望最后，给大家分享一下关于未来工作，我们的一些思考与规划，还不是太成熟，抛出来和大家探讨一下。 4.1. 端到端的实时流处理什么是端到端？一端是采集到的原始数据，另一端是报表 / 标签 / 接口这些对数据的呈现与应用，连接两端的是中间实时流。当前我们基于 SQL 的实时流处理，源表是 Kafka，目标表也是 Kafka，统一经过 Kafka 后再导入到 Druid/ES/HBase。这样设计的目的是提高整体流程的稳定性与可用性：首先，kafka 作为下游系统的缓冲，可以避免下游系统的异常影响实时流的计算（一个系统保持稳定，比起多个系统同时稳定，概率上更高点）；其次，kafka 到 kafka 的实时流，exactly-once 语义是比较成熟的，一致性上有保证。 然后，上述的端到端其实是由割裂的三个步骤来完成的，每一步可能需要由不同角色人去负责处理：数据处理需要数据开发人员，数据导入需要引擎开发人员，数据资产化需要产品开发人员。 我们的平台能否把端到端给自动化起来，只需要一次 SQL 提交就能打通处理、导入、资产化这三步？在这个思路下，数据开发中看到的不再是 Kafka Table，而应该是面向场景的展示表 / 标签表 / 接口表。比如对于展示表，创建表的时候只要指定维度、指标等字段，平台会将实时流结果数据从 Kafka 自动导入 Druid，再在报表系统自动导入 Druid 数据源，甚至自动生成报表模板。 4.2. 实时流的血缘分析关于血缘分析，做过离线数仓的朋友都很清楚它的重要性，它在数据治理中都起着不可或缺的关键作用。对于实时数仓来说也莫不如此。我们希望构建端到端的血缘关系，从采集系统的接入通道开始，到中间流经的实时表与实时作业，再到消费数据的产品，都能很清晰地展现出来。基于血缘关系的分析，我们才能评估数据的应用价值，核算数据的计算成本。 4.3. 离线 - 实时数仓一体化最后提一个方向是离线实时数仓的一体化。我们认为短期内，实时数仓无法替代离线数仓，两者并存是新常态。在离线数仓时代，我们积累的工具体系，如何去适配实时数仓，如何实现离线与实时数仓的一体化管理？理论上来讲，它们的数据来源是一致的，上层抽象也都是 Table 与 SQL，但本质上也有不同的点，比如时间粒度以及计算模式。对于数据工具与产品来说，需要做哪些改造来实现完全的一体化，这也是我们在探索和思考的。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"为什么说流处理即未来？","date":"2019-06-14T16:00:00.000Z","path":"2019/06/15/Stream-processing/","text":"本文整理自 Flink 创始公司 dataArtisans（现在为Ververica） 联合创始人兼 CTO Stephan Ewen 在 Flink Forward China 2018 上的演讲《Stream Processing takes on Everything》。这个演讲主题看似比较激进：流处理解决所有问题。很多人对于 Flink 可能还停留在最初的认知，觉得 Flink 是一个流处理引擎，实际上 Flink 可以做很多其他的工作，比如批处理、应用程序。在这个演讲中，Stephan 首先会简单说明他对 Flink 功能的观点，然后深入介绍一个特定领域的应用和事件处理场景。这个场景乍看起来不是一个流处理的使用场景，但是在 Stephan 看来，它实际上就是一个很有趣的流处理使用场景。 本文转自 AI 前线公众号，作者｜Stephan Ewen，策划编辑｜Natalie，编辑｜Debra，整理｜秦江杰 上图对为什么流处理可以处理一切作出诠释，将数据看做流是一个自然而又十分强大的想法。大部分数据的产生过程都是随时间生成的流，比如一个 Petabyte 的数据不会凭空产生。这些数据通常都是一些事件的积累，比如支付、将商品放入购物车，网页浏览，传感器采样输出， 基于数据是流的想法，我们对数据处理可以有相应的理解。比如将过去的历史数据看做是一个截止到某一时刻的有限的流，或是将一个实时处理应用看成是从某一个时刻开始处理未来到达的数据。可能在未来某个时刻它会停止，那么它就变成了处理从开始时刻到停止时刻的有限数据的批处理。当然，它也有可能一直运行下去，不断处理新到达的数据。这个对数据的重要理解方式非常强大，基于这一理解，Flink 可以支持整个数据处理范畴内的所有场景。 最广为人知的 Flink 使用场景是流分析、连续处理（或者说渐进式处理），这些场景中 Flink 实时或者近实时的处理数据，或者采集之前提到的历史数据并且连续的对这些事件进行计算。晓伟在之前的演讲中提到一个非常好的例子来说明怎么样通过对 Flink 进行一些优化，进而可以针对有限数据集做一些特别的处理，这使得 Flink 能够很好的支持批处理的场景，从性能上来说能够与最先进的批处理引擎相媲美。而在这根轴的另一头，是我今天的演讲将要说明的场景 – 事件驱动的应用。这类应用普遍存在于任何服务或者微服务的架构中。这类应用接收各类事件（可能是 RPC 调用、HTTP 请求），并且对这些事件作出一些响应，比如把商品放进购物车，或者加入社交网络中的某个群组。 在我进一步展开今天的演讲之前，我想先对社区在 Flink 的传统领域（实时分析、连续处理）近期所做的工作做一个介绍。Flink 1.7 在 2018 年 11 月 30 日已经发布。在 Flink 1.7 中为典型的流处理场景加入了一些非常有趣的功能。比如我个人非常感兴趣的在流式 SQL 中带时间版本的 Join。一个基本想法是有两个不同的流，其中一个流被定义为随时间变化的参照表，另一个是与参照表进行 Join 的事件流。比如事件流是一个订单流，参照表是不断被更新的汇率，而每个订单需要使用最新的汇率来进行换算，并将换算的结果输出到结果表。这个例子在标准的 SQL 当中实际上并不容易表达，但在我们对 Streaming SQL 做了一点小的扩展以后，这个逻辑表达变得非常简单，我们发现这样的表达有非常多的应用场景。 另一个在流处理领域十分强大的新功能是将复杂事件处理（CEP）和 SQL 相结合。CEP 应用观察事件模式。比如某个 CEP 应用观察股市，当有两个上涨后紧跟一个下跌时，这个应用可能做些交易。再比如一个观察温度计的应用，当它发现有温度计在两个超过 90 摄氏度的读数之后的两分钟里没有任何操作，可能会进行一些操作。与 SQL 的结合使这类逻辑的表达也变得非常简单。 第三个 Flink 1.7 中做了很多工作的功能是 Schema 升级。这个功能和基于流的应用紧密相关。就像你可以对数据库进行数据 Schema 升级一样，你可以修改 Flink 表中列的类型或者重新写一个列， 另外我想简单介绍的是流处理技术不仅仅是简单对数据进行计算，这还包括了很多与外部系统进行事务交互。流处理引擎需要在采用不同协议的系统之间以事务的方式移动数据，并保证计算过程和数据的一致性。这一部分功能也是在 Flink 1.7 中得到了增强。 以上我对 Flink 1.7 的新功能向大家做了简单总结。下面让我们来看看今天我演讲的主要部分，也就是利用 Flink 来搭建应用和服务。我将说明为什么流处理是一个搭建应用和服务或者微服务的有趣技术。 我将从左边这个高度简化的图说起，我们一会儿将聊一些其中的细节。首先我们来看一个理解应用简单的视角。如左图所示，一个应用可以是一个 Container，一个 Spring 应用，或者 Java 应用、Ruby 应用，等等。这个应用从诸如 RPC，HTTP 等渠道接收请求，然后依据请求进行数据库变更。这个应用也可能调用另一个微服务并进行下一步的处理。我们可以非常自然的想到进入到应用的这些请求可以看做是个事件组成的序列，所以我们可以把它们看做是事件流。可能这些事件被缓存在消息队列中，而应用会从消息队列中消费这些事件进行处理，当应用需要响应一个请求时，它将结果输出到另一个消息队列，而请求发送方可以从这个消息队列中消费得到所发送请求的响应。在这张图中我们已经可以看到一些有趣的不同。 第一个不同是在这张图中应用和数据库不再是分开的两个实体，而是被一个有状态的流处理应用所代替。所以在流处理应用的架构中，不再有应用和数据库的连接了，它们被放到了一起。这个做法有利有弊，但其中有些好处是非常重要的。首先是性能上的好处是明显的，因为应用不再需要和数据库进行交互，处理可以基于内存中的变量进行。其次这种做法有很好并且很简单的一致性。 这张图被简化了很多，实际上我们通常会有很多个应用，而不是一个被隔离的应用，很多情况下你的应用会更符合这张图。系统中有个接收请求的接口，然后请求被发送到第一个应用，可能会再被发到另一个应用，然后得到相应。在图中有些应用会消费中间结果的流。这张图已经展示了为什么流处理是更适合比较复杂的微服务场景的技术。因为很多时候系统中不会有一个直接接收用户请求并直接响应的服务，通常来说一个微服务需要跟其他微服务通信。这正如在流处理的架构中不同应用在创建输出流，同时基于衍生出的流再创建并输出新的流。 到目前为止，我们看到的内容多少还比较直观。而对基于流处理技术的微服务架构而言，人们最常问的一个问题是如何保证事务性？如果系统中使用的是数据库，通常来说都会有非常成熟复杂的数据校验和事务模型。这也是数据库在过去许多年中十分成功的原因。开始一个事务，对数据做一些操作，提交或者撤销一个事务。这个机制使得数据完整性得到了保证（一致性，持久性等等）。 那么在流处理中我们怎么做到同样的事情呢？作为一个优秀的流处理引擎，Flink 支持了恰好一次语义，保证了每个事件只会被处理一遍。但是这依然对某些操作有限制，这也成为了使用流处理应用的一个障碍。我们通过一个非常简单流处理应用例子来看我们可以做一些什么扩展来解决这个问题。我们会看到，解决办法其实出奇的简单。 让我们以这个教科书式的事务为例子来看一下事务性应用的过程。这个系统维护了账户和其中存款余额的信息。这样的信息可能是银行或者在线支付系统的场景中用到的。假设我们想要处理类似下面的事务：如果账户 A 中的余额大于 100，那么从账户 A 中转账 50 元到账户 B。这是个非常简单的两个账户之间进行转账的例子。 数据库对于这样的事务已经有了一个核心的范式，也就是原子性，一致性，隔离性和持久性（ACID）。这是能够让用户放心使用事务的几个基本保证。有了他们，用户不用担心钱在转账过程中会丢失或者其他问题。让我们用这个例子来放到流处理应用中，来让流处理应用也能提供和数据相同的 ACID 支持： 原子性要求一个转账要不就完全完成，也就是说转账金额从一个账户减少，并增加到另一个账户，要不就两个账户的余额都没有变化。而不会只有一个账户余额改变。否则的话钱就会凭空减少或者凭空增加。 一致性和隔离性是说如果有很多用户同时想要进行转账，那么这些转账行为之间应该互不干扰，每个转账行为应该被独立的完成，并且完成后每个账户的余额应该是正确的。也就是说如果两个用户同时操作同一个账户，系统不应该出错。 持久性指的是如果一个操作已经完成，那么这个操作的结果会被妥善的保存而不会丢失。 我们假设持久性已经被满足。一个流处理器有状态，这个状态会被 checkpoint，所以流处理器的状态是可恢复的。也就是说只要我们完成了一个修改，并且这个修改被 checkpoint 了，那么这个修改就是持久化的。 让我们来看看另外三个例子。设想一下，如果我们用流处理应用来实现这样一个转账系统会发生什么。我们先把问题简化一些，假设转账不需要有条件，仅仅是将 50 元从账户 A 转到账户，也就是说账户 A 的余额减少 50 元而账户 B 的余额增加 50 元。我们的系统是一个分布式的并行系统，而不是一个单机系统。简单起见我们假设系统中只有两台机器，这两台机器可以是不同的物理机或者是在 YARN 或者 Kubernetes 上不同的容器。总之它们是两个不同的流处理器实例，数据分布在这两个流处理器上。我们假设账户 A 的数据由其中一台机器维护，而账户 B 的数据有另一台机器维护。 现在我们要做个转账，将 50 元从账户 A 转移到账户 B，我们把这个请求放进队列中，然后这个转账请求被分解为对账户 A 和 B 分别进行操作，并且根据键将这两个操作路由到维护账户 A 和维护账户 B 的这两台机器上，这两台机器分别根据要求对账户 A 和账户 B 的余额进行改动。这并不是事务操作，而只是两个独立无意义的改动。一旦我们将转账的请求改的稍微复杂一些就会发现问题。 下面我们假设转账是有条件的，我们只想在账户 A 的余额足够的情况下才进行转账，这样就已经有些不太对了。如果我们还是像之前那样操作，将这个转账请求分别发送给维护账户 A 和 B 的两台机器，如果 A 没有足够的余额，那么 A 的余额不会发生变化，而 B 的余额可能已经被改动了。我们就违反了一致性的要求。 我们看到我们需要首先以某种方式统一做出是否需要更改余额的决定，如果这个统一的决定中余额需要被修改，我们再进行修改余额的操作。所以我们先给维护 A 的余额的机器发送一个请求，让它查看 A 的余额。我们也可以对 B 做同样的事情，但是这个例子里面我们不关心 B 的余额。然后我们把所有这样的条件检查的请求汇总起来去检验条件是否满足。因为 Flink 这样的流处理器支持迭代，如果满足转账条件，我们可以把这个余额改动的操作放进迭代的反馈流当中来告诉对应的节点来进行余额修改。反之如果条件不满足，那么余额改动的操作将不会被放进反馈流。这个例子里面，通过这种方式我们可以正确的进行转账操作。从某种角度上来说我们实现了原子性，基于一个条件我们可以进行全部的余额修改，或者不进行任何余额修改。这部分依然还是比较直观的，更大的困难是在于如何做到并发请求的隔离性。 假设我们的系统没有变，但是系统中有多个并发的请求。我们在之前的演讲中已经知道，这样的并发可能达到每秒钟几十亿条。如图，我们的系统可能从两个流中同时接受请求。如果这两个请求同时到达，我们像之前那样将每个请求拆分成多个请求，首先检查余额条件，然后进行余额操作。然而我们发现这会带来问题。管理账户 A 的机器会首先检查 A 的余额是否大于 50，然后又会检查 A 的余额是否大于 100，因为两个条件都满足，所以两笔转账操作都会进行，但实际上账户 A 上的余额可能无法同时完成两笔转账，而只能完成 50 元或者 100 元的转账中的一笔。这里我们需要进一步思考怎么样来处理并发的请求，我们不能只是简单地并发处理请求，这会违反事务的保证。从某种角度来说，这是整个数据库事务的核心。数据库的专家们花了一些时间提供了不同解决方案，有的方案比较简单，有的则很复杂。但所有的方案都不是那么容易，尤其是在分布式系统当中。 在流处理中怎么解决这个问题呢？直觉上讲，如果我们能够让所有的事务都按照顺序依次发生，那么问题就解决了，这也被成为可序列化的特性。但是我们当然不希望所有的请求都被依次顺序处理，这与我们使用分布式系统的初衷相违背。所以我们需要保证这些请求最后的产生的影响看起来是按照顺序发生的，也就是一个请求产生的影响是基于前一个请求产生影响的基础之上的。换句话说也就是一个事务的修改需要在前一个事务的所有修改都完成后才能进行。这种希望一件事在另一件事之后发生的要求看起来很熟悉，这似乎是我们以前在流处理中曾经遇到过的问题。是的，这听上去像是事件时间。用高度简化的方式来解释，如果所有的请求都在不同的事件时间产生，即使由于种种原因他们到达处理器的时间是乱序的，流处理器依然会根据他们的事件时间来对他们进行处理。流处理器会使得所有的事件的影响看上去都是按顺序发生的。按事件时间处理是 Flink 已经支持的功能。 那么详细说来，我们到底怎么解决这个一致性问题呢？假设我们有并行的请求输入并行的事务请求，这些请求读取某些表中的记录，然后修改某些表中的记录。我们首先需要做的是把这些事务请求根据事件时间顺序摆放。这些请求的事务时间不能够相同，但是他们之间的时间也需要足够接近，这是因为在事件时间的处理过程中会引入一定的延迟，我们需要保证所处理的事件时间在向前推进。因此第一步是定义事务执行的顺序，也就是说需要有一个聪明的算法来为每个事务制定事件时间。 在图上，假设这三个事务的事件时间分别是 T+2, T 和 T+1。那么第二个事务的影响需要在第一和第三个事务之前。不同的事务所做的修改是不同的，每个事务都会产生不同的操作请求来修改状态。我们现在需要将对访问每个行和状态的事件进行排序，保证他们的访问是符合事件时间顺序的。这也意味着那些相互之间没有关系的事务之间自然也没有了任何影响。比如这里的第三个事务请求，它与前两个事务之间没有访问共同的状态，所以它的事件时间排序与前两个事务也相互独立。而当前两个事务之间的操作的到达顺序与事件时间不符时，Flink 则会依据它们的事件时间进行排序后再处理。 必须承认，这样说还是进行了一些简化，我们还需要做一些事情来保证高效执行，但是总体原则上来说，这就是全部的设计。除此之外我们并不需要更多其他东西。 为了实现这个设计，我们引入了一种聪明的分布式事件时间分配机制。这里的事件时间是逻辑时间，它并不需要有什么现实意义，比如它不需要是真实的时钟。使用 Flink 的乱序处理能力，并且使用 Flink 迭代计算的功能来进行某些前提条件的检查。这些就是我们构建一个支持事务的流处理器的要素。 我们实际上已经完成了这个工作，称之为流式账簿（Streaming Ledger），这是个在 Apache Flink 上很小的库。它基于流处理器做到了满足 ACID 的多键事务性操作。我相信这是个非常有趣的进化。流处理器一开始基本上没有任何保障，然后类似 Storm 的系统增加了至少一次的保证。但显然至少一次依然不够好。然后我们看到了恰好一次的语义，这是一个大的进步，但这只是对于单行操作的恰好一次语义，这与键值库很类似。而支持多行恰好一次或者多行事务操作将流处理器提升到了一个可以解决传统意义上关系型数据库所应用场景的阶段。 Streaming Ledger 的实现方式是允许用户定义一些表和对这些表进行修改的函数。 Streaming Ledger 会运行这些函数和表，所有的这些一起编译成一个 Apache Flink 的有向无环图（DAG）。Streaming Ledger 会注入所有事务时间分配的逻辑，以此来保证所有事务的一致性。 搭建这样一个库并不难，难的是让它高性能的运行。让我们来看看它的性能。这些性能测试是几个月之前的，我们并没有做什么特别的优化，我们只是想看看一些最简单的方法能够有什么样的性能表现。而实际性能表现看起来相当不错。如果你看这些性能条形成的阶梯跨度，随着流处理器数量的增长，性能的增长相当线性。 在事务设计中，没有任何协同或者锁参与其中。这只是流处理，将事件流推入系统，缓存一小段时间来做一些乱序处理，然后做一些本地状态更新。在这个方案中，没有什么特别代价高昂的操作。在图中性能增长似乎超过了线性，我想这主要是因为 JAVA 的 JVM 当中 GC 的工作原因导致的。在 32 个节点的情况下我们每秒可以处理大约两百万个事务。为了与数据库性能测试进行对比，通常当你看数据库的性能测试时，你会看到类似读写操作比的说明，比如 10% 的更新操作。而我们的测试使用的是 100% 的更新操作，而每个写操作至少更新在不同分区上的 4 行数据，我们的表的大小大约是两亿行。即便没有任何优化，这个方案的性能也非常不错。 另一个在事务性能中有趣的问题是当更新的操作对象是一个比较小的集合时的性能。如果事务之间没有冲突，并发的事务处理是一个容易的事情。如果所有的事务都独立进行而互不干扰，那这个不是什么难题，任何系统应该都能很好的解决这样的问题。 当所有的事务都开始操作同一些行时，事情开始变得更有趣了，你需要隔离不同的修改来保证一致性。所以我们开始比较一个只读的程序、一个又读又写但是没有写冲突的程序和一个又读又写并有中等程度写冲突的程序这三者之间的性能。你可以看到性能表现相当稳定。这就像是一个乐观的并发冲突控制，表现很不错。那如果我们真的想要针对这类系统的阿喀琉斯之踵进行考验，也就是反复的更新同一个小集合中的键。 在传统数据库中，这种情况下可能会出现反复重试，反复失败再重试，这是一种我们总想避免的糟糕情况。是的，我们的确需要付出性能代价，这很自然，因为如果你的表中有几行数据每个人都想更新，那么你的系统就失去了并发性，这本身就是个问题。但是这种情况下，系统并没崩溃，它仍然在稳定的处理请求，虽然失去了一些并发性，但是请求依然能够被处理。这是因为我们没有冲突重试的机制，你可以认为我们有一个基于乱序处理天然的冲突避免的机制，这是一种非常稳定和强大的技术。 我们还尝试了在跨地域分布的情况下的性能表现。比如我们在美国、巴西，欧洲，日本和澳大利亚各设置了一个 Flink 集群。也就是说我们有个全球分布的系统。如果你在使用一个关系型数据库，那么你会付出相当高昂的性能代价，因为通信的延迟变得相当高。跨大洲的信息交互比在同一个数据中心甚至同一个机架上的信息交互要产生大得多的延迟。 但是有趣的是，流处理的方式对延迟并不是十分敏感，延迟对性能有所影响，但是相比其它很多方案，延迟对流处理的影响要小得多。所以，在这样的全球分布式环境中执行分布式程序，的确会有更差的性能，部分原因也是因为跨大洲的通信带宽不如统一数据中心里的带宽，但是性能表现依然不差。 实际上，你可以拿它当做一个跨地域的数据库，同时仍然能够在一个大概 10 个节点的集群上获得每秒几十万条事务的处理能力。在这个测试中我们只用了 10 个节点，每个大洲两个节点。所以 10 个节点可以带来全球分布的每秒 20 万事务的处理能力。我认为这是很有趣的结果，这是因为这个方案对延迟并不敏感。 我已经说了很多利用流处理来实现事务性的应用。可能听起来这是个很自然的想法，从某种角度上来说的确是这样。但是它的确需要一些很复杂的机制来作为支撑。它需要一个连续处理而非微批处理的能力，需要能够做迭代，需要复杂的基于事件时间处理乱序处理。为了更好地性能，它需要灵活的状态抽象和异步 checkpoint 机制。这些是真正困难的事情。这些不是由 Ledger Streaming 库实现的，而是 Apache Flink 实现的，所以即使对这类事务性的应用而言，Apache Flink 也是真正的中流砥柱。 至此，我们可以说流处理不仅仅支持连续处理、流式分析、批处理或者事件驱动的处理，你也可以用它做事务性的处理。当然，前提是你有一个足够强大的流处理引擎。这就是我演讲的全部内容。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Flink 实战1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍Flink中的Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 写入数据到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 写入数据到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了? 19、Flink 从0到1学习 —— Flink 中如何管理配置？ Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink 源码解析 —— 如何获取 ExecutionGraph ？","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 架构、原理与部署测试","date":"2019-06-13T16:00:00.000Z","path":"2019/06/14/flink-architecture-deploy-test/","text":"Apache Flink 是一个面向分布式数据流处理和批量数据处理的开源计算平台，它能够基于同一个 Flink 运行时，提供支持流处理和批处理两种类型应用的功能。 本文转载自博客园，作者：Florian 原文地址：https://www.cnblogs.com/fanzhidongyzby/p/6297723.html 现有的开源计算方案，会把流处理和批处理作为两种不同的应用类型，因为它们所提供的 SLA（Service-Level-Aggreement）是完全不相同的：流处理一般需要支持低延迟、Exactly-once 保证，而批处理需要支持高吞吐、高效处理。 Flink 从另一个视角看待流处理和批处理，将二者统一起来：Flink 是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。 Flink 流处理特性： 支持高吞吐、低延迟、高性能的流处理 支持带有事件时间的窗口（Window）操作 支持有状态计算的 Exactly-once 语义 支持高度灵活的窗口（Window）操作，支持基于 time、count、session，以及 data-driven 的窗口操作 支持具有 Backpressure 功能的持续流模型 支持基于轻量级分布式快照（Snapshot）实现的容错 一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理 Flink 在 JVM 内部实现了自己的内存管理 支持迭代计算 支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存 一、架构Flink 以层级式系统形式组件其软件栈，不同层的栈建立在其下层基础上，并且各层接受程序不同层的抽象形式。 1、运行时层以 JobGraph 形式接收程序。JobGraph 即为一个一般化的并行数据流图（data flow），它拥有任意数量的 Task 来接收和产生 data stream。 2、DataStream API 和 DataSet API 都会使用单独编译的处理方式生成 JobGraph。DataSet API 使用optimizer 来决定针对程序的优化方法，而 DataStream API 则使用 stream builder 来完成该任务。 3、在执行 JobGraph 时，Flink 提供了多种候选部署方案（如 local，remote，YARN 等）。 4、Flink 附随了一些产生 DataSet 或 DataStream API 程序的的类库和 API：处理逻辑表查询的 Table，机器学习的 FlinkML，图像处理的 Gelly，复杂事件处理的 CEP。 二、原理1. 流、转换、操作符Flink 程序是由 Stream 和 Transformation 这两个基本构建块组成，其中 Stream 是一个中间结果数据，而Transformation 是一个操作，它对一个或多个输入 Stream 进行计算处理，输出一个或多个结果 Stream。 Flink 程序被执行的时候，它会被映射为 Streaming Dataflow。一个 Streaming Dataflow 是由一组 Stream 和Transformation Operator 组成，它类似于一个 DAG 图，在启动的时候从一个或多个 Source Operator 开始，结束于一个或多个 Sink Operator。 2. 并行数据流一个 Stream 可以被分成多个 Stream 分区（Stream Partitions），一个 Operator 可以被分成多个 Operator Subtask，每一个 Operator Subtask 是在不同的线程中独立执行的。一个 Operator 的并行度，等于 Operator Subtask 的个数，一个 Stream 的并行度总是等于生成它的 Operator 的并行度。 One-to-one 模式 比如从 Source[1] 到 map()[1]，它保持了 Source 的分区特性（Partitioning）和分区内元素处理的有序性，也就是说 map()[1] 的 Subtask 看到数据流中记录的顺序，与 Source[1] 中看到的记录顺序是一致的。 Redistribution模式 这种模式改变了输入数据流的分区，比如从 map()[1]、map()[2] 到 keyBy()/window()/apply()[1]、keyBy()/window()/apply()[2]，上游的 Subtask 向下游的多个不同的 Subtask 发送数据，改变了数据流的分区，这与实际应用所选择的 Operator 有关系。 3. 任务、操作符链Flink 分布式执行环境中，会将多个 Operator Subtask 串起来组成一个 Operator Chain，实际上就是一个执行链，每个执行链会在 TaskManager 上一个独立的线程中执行。 4. 时间处理 Stream 中的记录时，记录中通常会包含各种典型的时间字段： Event Time：表示事件创建时间 Ingestion Time：表示事件进入到 Flink Dataflow 的时间 Processing Time：表示某个 Operator 对事件进行处理的本地系统时间 Flink 使用 WaterMark 衡量时间的时间，WaterMark 携带时间戳 t，并被插入到 stream 中。 WaterMark 的含义是所有时间 t’&lt; t 的事件都已经发生。 针对乱序的的流，WaterMark 至关重要，这样可以允许一些事件到达延迟，而不至于过于影响 window 窗口的计算。 并行数据流中，当 Operator 有多个输入流时，Operator 的 event time 以最小流 event time 为准。 5. 窗口Flink 支持基于时间窗口操作，也支持基于数据的窗口操作： 窗口分类： 按分割标准划分：timeWindow、countWindow 按窗口行为划分：Tumbling Window、Sliding Window、自定义窗口 Tumbling/Sliding Time Window 12345678910111213141516// Stream of (sensorId, carCnt)val vehicleCnts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts // key stream by sensorId .keyBy(0) // tumbling time window of 1 minute length .timeWindow(Time.minutes(1)) // compute sum over carCnt .sum(1) val slidingCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // sliding time window of 1 minute length and 30 secs trigger interval .timeWindow(Time.minutes(1), Time.seconds(30)) .sum(1) Tumbling/Sliding Count Window 12345678910111213141516// Stream of (sensorId, carCnt) val vehicleCnts: DataStream[(Int, Int)] = ... val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts // key stream by sensorId .keyBy(0) // tumbling count window of 100 elements size .countWindow(100) // compute the carCnt sum .sum(1)val slidingCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // sliding count window of 100 elements size and 10 elements trigger interval .countWindow(100, 10) .sum(1) 自定义窗口 基本操作： window：创建自定义窗口 trigger：自定义触发器 evictor：自定义evictor apply：自定义window function 6. 容错Barrier 机制： 出现一个 Barrier，在该 Barrier 之前出现的记录都属于该 Barrier 对应的 Snapshot，在该 Barrier 之后出现的记录属于下一个 Snapshot。 来自不同Snapshot多个Barrier可能同时出现在数据流中，也就是说同一个时刻可能并发生成多个Snapshot。 当一个中间（Intermediate）Operator接收到一个Barrier后，它会发送Barrier到属于该Barrier的Snapshot的数据流中，等到Sink Operator接收到该Barrier后会向Checkpoint Coordinator确认该Snapshot，直到所有的Sink Operator都确认了该Snapshot，才被认为完成了该Snapshot。 对齐： 当Operator接收到多个输入的数据流时，需要在Snapshot Barrier中对数据流进行排列对齐： Operator从一个incoming Stream接收到Snapshot Barrier n，然后暂停处理，直到其它的incoming Stream的Barrier n（否则属于2个Snapshot的记录就混在一起了）到达该Operator 接收到Barrier n的Stream被临时搁置，来自这些Stream的记录不会被处理，而是被放在一个Buffer中。 一旦最后一个Stream接收到Barrier n，Operator会emit所有暂存在Buffer中的记录，然后向Checkpoint Coordinator发送Snapshot n。 继续处理来自多个Stream的记录 基于Stream Aligning操作能够实现Exactly Once语义，但是也会给流处理应用带来延迟，因为为了排列对齐Barrier，会暂时缓存一部分Stream的记录到Buffer中，尤其是在数据流并行度很高的场景下可能更加明显，通常以最迟对齐Barrier的一个Stream为处理Buffer中缓存记录的时刻点。在Flink中，提供了一个开关，选择是否使用Stream Aligning，如果关掉则Exactly Once会变成At least once。 CheckPoint： Snapshot并不仅仅是对数据流做了一个状态的Checkpoint，它也包含了一个Operator内部所持有的状态，这样才能够在保证在流处理系统失败时能够正确地恢复数据流处理。状态包含两种： 系统状态：一个Operator进行计算处理的时候需要对数据进行缓冲，所以数据缓冲区的状态是与Operator相关联的。以窗口操作的缓冲区为例，Flink系统会收集或聚合记录数据并放到缓冲区中，直到该缓冲区中的数据被处理完成。 一种是用户自定义状态（状态可以通过转换函数进行创建和修改），它可以是函数中的Java对象这样的简单变量，也可以是与函数相关的Key/Value状态。 7. 调度在JobManager端，会接收到Client提交的JobGraph形式的Flink Job，JobManager会将一个JobGraph转换映射为一个ExecutionGraph，ExecutionGraph是JobGraph的并行表示，也就是实际JobManager调度一个Job在TaskManager上运行的逻辑视图。 物理上进行调度，基于资源的分配与使用的一个例子： 左上子图：有2个TaskManager，每个TaskManager有3个Task Slot 左下子图：一个Flink Job，逻辑上包含了1个data source、1个MapFunction、1个ReduceFunction，对应一个JobGraph 左下子图：用户提交的Flink Job对各个Operator进行的配置——data source的并行度设置为4，MapFunction的并行度也为4，ReduceFunction的并行度为3，在JobManager端对应于ExecutionGraph 右上子图：TaskManager 1上，有2个并行的ExecutionVertex组成的DAG图，它们各占用一个Task Slot 右下子图：TaskManager 2上，也有2个并行的ExecutionVertex组成的DAG图，它们也各占用一个Task Slot 在2个TaskManager上运行的4个Execution是并行执行的 8. 迭代机器学习和图计算应用，都会使用到迭代计算，Flink通过在迭代Operator中定义Step函数来实现迭代算法，这种迭代算法包括Iterate和Delta Iterate两种类型。 Iterate Iterate Operator是一种简单的迭代形式：每一轮迭代，Step函数的输入或者是输入的整个数据集，或者是上一轮迭代的结果，通过该轮迭代计算出下一轮计算所需要的输入（也称为Next Partial Solution），满足迭代的终止条件后，会输出最终迭代结果。 流程伪代码： 1234567IterationState state = getInitialState();while (!terminationCriterion()) &#123; state = step(state);&#125;setFinalState(state); Delta Iterate Delta Iterate Operator实现了增量迭代。 流程伪代码： 12345678910IterationState workset = getInitialState();IterationState solution = getInitialSolution();while (!terminationCriterion()) &#123; (delta, workset) = step(workset, solution); solution.update(delta)&#125;setFinalState(solution); 最小值传播： 9. Back Pressure监控流处理系统中，当下游Operator处理速度跟不上的情况，如果下游Operator能够将自己处理状态传播给上游Operator，使得上游Operator处理速度慢下来就会缓解上述问题，比如通过告警的方式通知现有流处理系统存在的问题。 Flink Web界面上提供了对运行Job的Backpressure行为的监控，它通过使用Sampling线程对正在运行的Task进行堆栈跟踪采样来实现。 默认情况下，JobManager会每间隔50ms触发对一个Job的每个Task依次进行100次堆栈跟踪调用，过计算得到一个比值，例如，radio=0.01，表示100次中仅有1次方法调用阻塞。Flink目前定义了如下Backpressure状态：OK: 0 &lt;= Ratio &lt;= 0.10LOW: 0.10 &lt; Ratio &lt;= 0.5HIGH: 0.5 &lt; Ratio &lt;= 1 三、库1. TableFlink的Table API实现了使用类SQL进行流和批处理。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/table_api.html 2. CEPFlink的CEP（Complex Event Processing）支持在流中发现复杂的事件模式，快速筛选用户感兴趣的数据。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/concepts/programming-model.html#next-steps 3. GellyGelly是Flink提供的图计算API，提供了简化开发和构建图计算分析应用的接口。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/gelly/index.html 4. FlinkMLFlinkML是Flink提供的机器学习库，提供了可扩展的机器学习算法、简洁的API和工具简化机器学习系统的开发。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/ml/index.html 四、部署当Flink系统启动时，首先启动JobManager和一至多个TaskManager。JobManager负责协调Flink系统，TaskManager则是执行并行程序的worker。当系统以本地形式启动时，一个JobManager和一个TaskManager会启动在同一个JVM中。当一个程序被提交后，系统会创建一个Client来进行预处理，将程序转变成一个并行数据流的形式，交给JobManager和TaskManager执行。 1. 启动测试编译flink，本地启动。 12345678$ java -versionjava version \"1.8.0_111\"$ git clone https://github.com/apache/flink.git$ git checkout release-1.1.4 -b release-1.1.4$ cd flink$ mvn clean package -DskipTests$ cd flink-dist/target/flink-1.1.4-bin/flink-1.1.4$ ./bin/start-local.sh 编写本地流处理demo。 SocketWindowWordCount.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(\"port\"); &#125; catch (Exception e) &#123; System.err.println(\"No port specified. Please run 'SocketWindowWordCount --port &lt;port&gt;'\"); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(\"localhost\", port, \"\\n\"); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(\"\\s\")) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(\"word\") .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(\"Socket Window WordCount\"); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + \" : \" + count; &#125; &#125;&#125; pom.xml 1234567891011121314151617&lt;!-- Use this dependency if you are using the DataStream API --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Use this dependency if you are using the DataSet API --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt; 执行mvn构建。 12mvn clean install$ ls target/flink-demo-1.0-SNAPSHOT.jar 开启9000端口，用于输入数据: 1$ nc -l 9000 提交flink任务： 1$ ./bin/flink run -c com.demo.florian.WordCount $DEMO_DIR/target/flink-demo-1.0-SNAPSHOT.jar --port 9000 在nc里输入数据后，查看执行结果： 1$ tail -f log/flink-*-jobmanager-*.out 查看flink web页面：localhost:8081 2. 代码结构Flink系统核心可分为多个子项目。分割项目旨在减少开发Flink程序需要的依赖数量，并对测试和开发小组件提供便捷。 Flink当前还包括以下子项目： Flink-dist：distribution项目。它定义了如何将编译后的代码、脚本和其他资源整合到最终可用的目录结构中。 Flink-quick-start：有关quickstart和教程的脚本、maven原型和示例程序 flink-contrib：一系列有用户开发的早起版本和有用的工具的项目。后期的代码主要由外部贡献者继续维护，被flink-contirb接受的代码的要求低于其他项目的要求。 3. Flink On YARNFlink在YARN集群上运行时：Flink YARN Client负责与YARN RM通信协商资源请求，Flink JobManager和Flink TaskManager分别申请到Container去运行各自的进程。 YARN AM与Flink JobManager在同一个Container中，这样AM可以知道Flink JobManager的地址，从而AM可以申请Container去启动Flink TaskManager。待Flink成功运行在YARN集群上，Flink YARN Client就可以提交Flink Job到Flink JobManager，并进行后续的映射、调度和计算处理。 1、设置Hadoop环境变量 1$ export HADOOP_CONF_DIR=/etc/hadoop/conf 2、以集群模式提交任务，每次都会新建flink集群 1$ ./bin/flink run -m yarn-cluster -c com.demo.florian.WordCount $DEMO_DIR/target/flink-demo-1.0-SNAPSHOT.jar 3、启动共享flink集群，提交任务 12$ ./bin/yarn-session.sh -n 4 -jm 1024 -tm 4096 -d$ ./bin/flink run -c com.demo.florian.WordCount $DEMO_DIR/target/flink-demo-1.0.SNAPSHOT.jar ###参考资料 http://shiyanjun.cn/archives/1508.htmlhttps://ci.apache.org/projects/flink/flink-docs-release-1.2/index.html 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文","date":"2019-06-12T16:00:00.000Z","path":"2019/06/13/flink-book-paper/","text":"前言之前也分享了不少自己的文章，但是对于 Flink 来说，还是有不少新入门的朋友，这里给大家分享点 Flink 相关的资料（国外数据 pdf 和流处理相关的 Paper），期望可以帮你更好的理解 Flink。 专栏推荐首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 书籍1、《Introduction to Apache Flink book》 这本书比较薄，简单介绍了 Flink，也有中文版，读完可以对 Flink 有个大概的了解。 2、《Learning Apache Flink》 这本书还是讲的比较多的 API 使用，不仅有 Java 版本还有 Scala 版本，入门看这本我觉得还是 OK 的。 3、《Stream Processing with Apache Flink》 这本书是 Flink PMC 写的，质量还是很好的，对 Flink 中的概念讲的很清楚，还有不少图片帮忙理解，美中不足的是没有 Table 和 SQL API 相关的介绍。 4、《Streaming System》 这本书是讲流处理引擎的，对流处理引擎的发展带来不少的推动，书本的质量非常高，配了大量的图，目的就是让你很容易的懂流处理引擎中的概念（比如时间、窗口、水印等），我强烈的推荐大家都看一下，这本书的内容被很多博客和书籍都引用了。 Paper这是一份 streaming systems 领域相关的论文列表 20+ 篇，涉及 streaming systems 的设计，实现，故障恢复，弹性扩展等各方面。也包含自 2014 年以来 streaming system 和 batch system 的统一模型的论文。 2016 年 Drizzle: Fast and Adaptable Stream Processing at Scale (Draft): Record-at-a-time 的系统，如 Naiad, Flink，处理延迟较低、但恢复延迟较高；micro-batch 系统，如 Spark Streaming，恢复延迟低但处理延迟略高。Drizzle 则采用 group scheduling + pre-scheduling shuffles 的方式对 Spark Streaming 做了改进，保留低恢复延迟的同时，降低了处理延迟至 100ms 量级。 Realtime Data Processing at Facebook (SIGMOD): Facebook 明确自己实时的使用场景是 seconds of latency, not milliseconds，并基于自己的需求构建了 3 个实时处理组件：Puma, Swift, 以及 Stylus。Puma, Swift 和 Stylus 都从 Scribe 读数据，并可向 Scribe 写回数据（Scribe 是 Facebook 内部的分布式消息系统，类似 Kafka）。 2015 年 The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing (VLDB): 来自 Google 的将 stream processing 模型和 batch processing 模型统一的尝试。在 Dataflow model 下，底层依赖 FlumeJava 支持 batch processing，依赖 MillWheel 支持 stream processing。Dataflow model 的开源实现是 Apache Beam 项目。 Apache Flink: Stream and Batch Processing in a Single Engine Apache Flink 是一个处理 streaming data 和 batch data 的开源系统。Flink 的设计哲学是，包括实时分析 (real-time analytics)、持续数据处理 (continuous data pipelines)、历史数据处理 (historic data processing / batch)、迭代式算法 (iterative algorithms - machine learning, graph analysis) 等的很多类数据处理应用，都能用 pipelined fault-tolerant 的 dataflows 执行模型来表达。 Lightweight asynchronous snapshots for distributed dataflows: Apache Flink 所实现的一个轻量级的、异步做状态快照的方法。基于此，Flink 得以保证分布式状态的一致性，从而保证整个系统的 exactly-once 语义。具体的，Flink 会持续性的在 stream 里插入 barrier markers，制造一个分布式的顺序关系，使得不同的节点能够在同一批 barrier marker 上达成整个系统的一致性状态。 Twitter Heron: Stream Processing at Scale (SIGMOD): Heron 是 Twitter 开发的用于代替 Storm 的实时处理系统，解决了 Storm 在扩展性、调试能力、性能、管理方式上的一些问题。Heron 实现了 Storm 的接口，因此对 Storm 有很好的兼容性，也成为了 Twitter 内部实时处理系统的事实上的标准。 2014 年 Trill: A High-Performance Incremental Query Processor for Diverse Analytics (VLDB): 此篇介绍了 Microsoft 的 Trill - 一个新的分析查询处理器。Trill 很好的结合以下 3 方面需求：(1) Query Model: Trill 是基于时间-关系 (tempo-relational) 模型，所以很好的支持从实时到离线计算的延迟需求；(2) Fabric and Language Integration: Trill 作为一个类库，可以很好的与高级语言、已有类库结合；以及 (3) Performance: 无论实时还是离线，Trill 的 throughput 都很高 —— 实时计算比流处理引擎高 2-4 个数量级，离线计算与商业的列式 DBMS 同等。从实现角度讲，包括 punctuation 的使用来分 batch 满足 latency 需求，batch 内使用列式存储、code-gen 等技术来提高 performance，都具有很好的借鉴意义 —— 尤其注意这是 2014 年发表的论文。 Summingbird: A Framework for Integrating Batch and Online MapReduce Computations (VLDB): Twitter 开发的目标是将 online Storm 计算和 batch MapReduce 计算逻辑统一描述的一套 domain-specific language。Summingbird 抽象了 sources, sinks, 以及 stores 等，基于此抽象，上层应用就不必为 streaming 和 batch 维护两套计算逻辑，而可以使用同一套计算逻辑，只在运行时分别编译后跑在 streaming 的 Storm 上和 batch 的 MapReduce 上。 Storm@Twitter (SIGMOD): 这是一篇来迟的论文。Apache Storm 最初在 Backtype 及 Twitter，而后在业界范围都有广泛的应用，甚至曾经一度也是事实上的流处理系统标准。此篇介绍了 Storm 的设计，及在 Twitter 内部的应用情况。当然后面我们知道 Apache Storm 也暴露出一些问题，业界也出现了一些更优秀的流处理系统。Twitter 虽没有在 2012 年 Storm 时代开启时发声，但在 2014 年 Storm 落幕时以此文发声向其致敬，也算是弥补了些许遗憾吧。 2013 年 Discretized Streams: Fault-Tolerant Streaming Computation at Scale (SOSP): Spark Streaming 是基于 Spark 执行引擎、micro-batch 模式的准实时处理系统。对比 RDD 是 Spark 引擎的数据抽象，DStream (Discretized Stream) 则是 Spark Streaming 引擎的数据抽象。DStream 像 RDD 一样，具有分布式、可故障恢复的特点，并且能够充分利用 Spark 引擎的推测执行，应对 straggler 的出现。 MillWheel: Fault-Tolerant Stream Processing at Internet Scale (VLDB): MillWheel 是 Google 内部研发的实时流数据处理系统，具有分布式、低延迟、高可用、支持 exactly-once 语义的特点。不出意外，MillWheel 是 Google 强大 infra structure 和强大 engeering 能力的综合体现 —— 利用 Bigtable/Spanner 作为后备状态存储、保证 exactly-once 特性等等。另外，MillWheel 将 watermark 机制发扬光大，对 event time 有着非常好的支持。推荐对 streaming system 感兴趣的朋友一定多读几遍此篇论文 —— 虽然此篇已经发表了几年，但工业界开源的系统尚未完全达到 MillWheel 的水平。 Integrating Scale Out and Fault Tolerance in Stream Processing using Operator State Management (SIGMOD): 针对有状态的算子的状态，此篇的基本洞察是，scale out 和 fault tolerance 其实很相通，应该结合到一起考虑和实现，而不是将其割裂开来。文章提出了算子的 3 类状态：(a) processing state, (b) buffer state, 和 (c) routing state，并提出了算子状态的 4 个操作原语：(1) checkpoint state, (2) backup state, (3) restore state, (4) partition state。 2010 年 S4: Distributed Stream Computing Platform (ICDMW): 2010 年算是 general stream processing engine 元年 —— Yahoo! 研发并发布了 S4, Backtype 开始研发了 Storm 并将在 1 年后（由 Twitter）将其开源。S4 和 Storm 都是 general-purpose 的 stream processing engine，允许用户通过代码自定义计算逻辑，而不是仅仅是使用声明式的语言或算子。 2008 年 Out-of-Order Processing: A New Architecture for HighPerformance Stream System (VLDB): 这篇文章提出了一种新的处理模型，即 out-of-order processing (OOP)，取消了以往 streaming system 里对事件有序的假设。重要的是，这篇文章提出了并实现了 low watermark: lwm(n, S, A) is the smallest value for A that occurs after prefix Sn of stream S。我们看到，在 2 年后 Google 开始研发的 MillWheel 里，watermark 将被发扬光大。 Fast and Highly-Available Stream Processing over Wide Area Networks (ICDE): 针对广域网 (wide area networks) 的 stream processing 设计的快速、高可用方案。主要思想是依靠 replication。 2007 年 A Cooperative, Self-Configuring High-Availability Solution for Stream Processing (ICDE): 与 2005 年 ICDE 的文章一样，此篇也讨论 stream processing 的高可用问题。与 2005 年文章做法不同的是，此篇的 checkpointing 方法更细粒度一些，所以一个节点上的不同状态能够备份到不同的节点上去，因而在恢复的时候能够并行恢复以提高速度。 2005 年 The 8 Requirements of Real-Time Stream Processing (SIGMOD): 图领奖得主 Michael Stonebraker 老爷子与他在 StreamBase 的小伙伴们勾画的 stream processing applications 应当满足的 8 条规则，如 Rule 1: Keep the Data Moving, Rule 2: Query using SQL on Streams (StreamSQL), Rule 3: Handle Stream Imperfections (Delayed, Missing and Out-of-Order Data) … 等等。虽然此篇有引导舆论的嫌疑 —— 不知是先有了这流 8 条、再有了 StreamBase，还是先有了 StreamBase、再有了这流 8 条 —— 但其内容还是有相当的借鉴意义。 The Design of the Borealis Stream Processing Engine (CIDR): Borealis 是 Aurora 的分布式、更优化版本的续作。Borealis 提出并解决了 3 个新一代系统的基础问题：(1) dynamic revision of query results, (2) dynamic query modification, 以及 (3) flexible and highly-scalable optimization. 此篇讲解了 Borealis 的设计与实现 —— p.s. 下，Aurora 及续作 Borealis 的命名还真是非常讲究，是学院派的风格 :-D High-availability algorithms for distributed stream processing (ICDE): 此篇主要聚焦在 streaming system 的高可用性，即故障恢复。文章提出了 3 种 recovery types: (a) precise, (b) gap, 和 (c) rollback，并通过 (1) passive standby, (2) upstream backup, (3) active standby 的方式进行 recover。可与 2007 年 ICDE 的文章对比阅读。 2004 年 STREAM: The Stanford Data Stream Management System (Technique Report): 这篇 technique report 定义了一种 Continuous Query Language (CQL)，讲解了 Query Plans 和 Execution，讨论了一些 Performance Issues。系统也注意到并讨论了 Adaptivity 和 Approximation 的问题。从这篇 technique report 可以看出，这时的流式计算，更多是传统 RDBMS 的思路，扩展到了处理实时流式数据；这大约也是 2010 以前的 stream processing 相关研究的缩影。 2002 年 Monitoring Streams – A New Class of Data Management Applications (VLDB): 大约在 2002 年前后，从实时数据监控（如监控 sensors 数据等）应用出发，大家已经开始区分传统的查询主动、数据被动 (Human-Active, DBMS-Passive) 模式和新兴的数据主动、查询被动 (DBMS-Active, Human-Passive) 模式的区别 —— 此篇即是其中的典型代表。此篇提出了新式的 DBMS 的 Aurora，描述了其基本系统模型、面向流式数据的操作算子集、 优化策略、及实时应用。 Exploiting Punctuation Semantics in Continuous Data Streams (TKDE): 此篇很早的注意到了一些传统的操作算子不能用于无尽的数据流入的场景，因为将导致无尽的状态（考虑 outer join），或者无尽的阻塞（考虑 count 或 max）等。此篇提出，如果在 stream 里加入一些特殊的 punctuation，来标识一段一段的数据，那么我们就可以把无限的 stream 划分为多个有限的数据集的集合，从而使得之前提到的算子变得可用。此篇的价值更多体现在给了 2008 年 watermark 相关的文章以基础，乃至集大成在了 2010 年 Google MillWheel 中。 总结本文分享了四本 Flink 相关的书籍和一份 streaming systems 领域相关的论文列表 20+ 篇，涉及 streaming systems 的设计，实现，故障恢复，弹性扩展等各方面。 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 另外你如果感兴趣的话，也可以关注我的公众号。 本篇文章连接是：http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习—— Flink 不可以连续 Split(分流)？","date":"2019-06-11T16:00:00.000Z","path":"2019/06/12/flink-split/","text":"前言今天上午被 Flink 的一个算子困惑了下，具体问题是什么呢？ 我有这么个需求：有不同种类型的告警数据流(包含恢复数据)，然后我要将这些数据流做一个拆分，拆分后的话，每种告警里面的数据又想将告警数据和恢复数据拆分出来。 结果，这个需求用 Flink 的 Split 运算符出现了问题。 分析需求如下图所示： 我是期望如上这样将数据流进行拆分的，最后将每种告警和恢复用不同的消息模版做一个渲染，渲染后再通过各种其他的方式（钉钉群邮件、短信）进行告警通知。 于是我的代码大概的结构如下代码所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879//dataStream 是总的数据流//split 是拆分后的数据流SplitStream&lt;AlertEvent&gt; split = dataStream.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); switch (value.getType()) &#123; case MIDDLEWARE: tags.add(MIDDLEWARE); break; case HEALTH_CHECK: tags.add(HEALTH_CHECK); break; case DOCKER: tags.add(DOCKER); break; //... //当然这里还可以很多种类型 &#125; return tags; &#125;&#125;);//然后你想获取每种不同的数据类型，你可以使用 selectDataStream&lt;AlertEvent&gt; middleware = split.select(MIDDLEWARE); //选出中间件的数据流//然后你又要将中间件的数据流分流成告警和恢复SplitStream&lt;AlertEvent&gt; middlewareSplit = middleware.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); if(value.isRecover()) &#123; tags.add(RECOVER) &#125; else &#123; tags.add(ALERT) &#125; return tags; &#125;&#125;);middlewareSplit.select(ALERT).print(); DataStream&lt;AlertEvent&gt; healthCheck = split.select(HEALTH_CHECK); //选出健康检查的数据流//然后你又要将健康检查的数据流分流成告警和恢复SplitStream&lt;AlertEvent&gt; healthCheckSplit = healthCheck.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); if(value.isRecover()) &#123; tags.add(RECOVER) &#125; else &#123; tags.add(ALERT) &#125; return tags; &#125;&#125;);healthCheckSplit.select(ALERT).print();DataStream&lt;AlertEvent&gt; docekr = split.select(DOCKER); //选出容器的数据流//然后你又要将容器的数据流分流成告警和恢复SplitStream&lt;AlertEvent&gt; dockerSplit = docekr.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); if(value.isRecover()) &#123; tags.add(RECOVER) &#125; else &#123; tags.add(ALERT) &#125; return tags; &#125;&#125;);dockerSplit.select(ALERT).print(); 结构我抽象后大概就长上面这样，然后我先本地测试的时候只把容器的数据那块代码打开了，其他种告警的分流代码注释掉了，一运行，发现竟然容器告警的数据怎么还掺杂着健康检查的数据也一起打印出来了，一开始我以为自己出了啥问题，就再起码运行了三遍 IDEA 才发现结果一直都是这样的。 于是，我只好在第二步分流前将 docekr 数据流打印出来，发现是没什么问题，打印出来的数据都是容器相关的，没有掺杂着其他种的数据啊。这会儿遍陷入了沉思，懵逼发呆了一会。 解决问题于是还是开始面向 Google 编程： 发现第一条就找到答案了，简直不要太快，点进去可以看到他也有这样的需求： 然后这个小伙伴还挣扎了下用不同的方法（虽然结果更惨）： 最后换了个姿势就好了（果然小伙子会的姿势挺多的）： 但从这篇文章中，我找到了关联到的两个 Flink Issue，分别是： 1、https://issues.apache.org/jira/browse/FLINK-5031 2、https://issues.apache.org/jira/browse/FLINK-11084 然后呢，从第二个 Issue 的讨论中我发现了一些很有趣的讨论： 对话很有趣，但是我突然想到之前我的知识星球里面一位很细心的小伙伴问的一个问题了： 可以发现代码上确实是标明了过期了，但是注释里面没写清楚推荐用啥，幸好我看到了这个 Issue，不然脑子里面估计这个问题一直会存着呢。 那么这个问题解决方法是不是意味着就可以利用 Side Outputs 来解决呢？当然可以啦，官方都推荐了，还不能都话，那么不是打脸啪啪啪的响吗？不过这里还是卖个关子将 Side Outputs 后面专门用一篇文章来讲，感兴趣的可以先看看官网介绍：https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/side_output.html 另外其实也可以通过 split + filter 组合来解决这个问题，反正关键就是不要连续的用 split 来分流。 用 split + filter 的方案代码大概如下： 12345678910111213141516171819DataStream&lt;AlertEvent&gt; docekr = split.select(DOCKER); //选出容器的数据流//容器告警的数据流docekr.filter(new FilterFunction&lt;AlertEvent&gt;() &#123; @Override public boolean filter(AlertEvent value) throws Exception &#123; return !value.isRecover(); &#125;&#125;).print(); //容器恢复的数据流 docekr.filter(new FilterFunction&lt;AlertEvent&gt;() &#123; @Override public boolean filter(AlertEvent value) throws Exception &#123; return value.isRecover(); &#125;&#125;).print(); 上面这种就是多次 filter 也可以满足需求，但是就是代码有点啰嗦。 总结Flink 中不支持连续的 Split/Select 分流操作，要实现连续分流也可以通过其他的方式（split + filter 或者 side output）来实现 本篇文章连接是：http://www.54tianzhisheng.cn/2019/06/12/flink-split/ 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程","date":"2019-03-27T16:00:00.000Z","path":"2019/03/28/Flink-code-TaskManager-submitJob/","text":"TaskManager 处理 SubmitJob 的过程 https://t.zsxq.com/eu7mQZj 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习 —— Flink 中如何管理配置？","date":"2019-03-27T16:00:00.000Z","path":"2019/03/28/flink-additional-data/","text":"前言如果你了解 Apache Flink 的话，那么你应该熟悉该如何像 Flink 发送数据或者如何从 Flink 获取数据。但是在某些情况下，我们需要将配置数据发送到 Flink 集群并从中接收一些额外的数据。 在本文的第一部分中，我将描述如何将配置数据发送到 Flink 集群。我们需要配置很多东西:方法参数、配置文件、机器学习模型。Flink 提供了几种不同的方法，我们将介绍如何使用它们以及何时使用它们。在本文的第二部分中，我将描述如何从 Flink 集群中获取数据。 如何发送数据给 TaskManager？在我们深入研究如何在 Apache Flink 中的不同组件之间发送数据之前，让我们先谈谈 Flink 集群中的组件，下图展示了 Flink 中的主要组件以及它们是如何相互作用的： 当我们运行 Flink 应用程序时，它会与 Flink JobManager 进行交互，这个 Flink JobManager 存储了那些正在运行的 Job 的详细信息，例如执行图。JobManager 它控制着 TaskManager，每个 TaskManager 中包含了一部分数据来执行我们定义的数据处理方法。 在许多的情况下，我们希望能够去配置 Flink Job 中某些运行的函数参数。根据用例，我们可能需要设置单个变量或者提交具有静态配置的文件，我们下面将讨论在 Flink 中该如何实现？ 除了向 TaskManager 发送配置数据外，有时我们可能还希望从 Flink Job 的函数方法中返回数据。 如何配置用户自定义函数？假设我们有一个从 CSV 文件中读取电影列表的应用程序（它要过滤特定类型的所有电影）: 1234567891011121314//读取电影列表数据集合DataSet&lt;Tuple3&lt;Long, String, String&gt;&gt; lines = env.readCsvFile(\"movies.csv\") .ignoreFirstLine() .parseQuotedStrings('\"') .ignoreInvalidLines() .types(Long.class, String.class, String.class);lines.filter((FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt;) movie -&gt; &#123; // 以“|”符号分隔电影类型 String[] genres = movie.f2.split(\"\\\\|\"); // 查找所有 “动作” 类型的电影 return Stream.of(genres).anyMatch(g -&gt; g.equals(\"Action\"));&#125;).print(); 我们很可能想要提取不同类型的电影，为此我们需要能够配置我们的过滤功能。 当你要实现这样的函数时，最直接的配置方法是实现构造函数： 123456789101112131415161718192021// 传递类型名称lines.filter(new FilterGenre(\"Action\")) .print();...class FilterGenre implements FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt; &#123; //类型 String genre; //初始化构造方法 public FilterGenre(String genre) &#123; this.genre = genre; &#125; @Override public boolean filter(Tuple3&lt;Long, String, String&gt; movie) throws Exception &#123; String[] genres = movie.f2.split(\"\\\\|\"); return Stream.of(genres).anyMatch(g -&gt; g.equals(genre)); &#125;&#125; 或者，如果你使用 lambda 函数，你可以简单地使用它的闭包中的一个变量: 12345678final String genre = \"Action\";lines.filter((FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt;) movie -&gt; &#123; String[] genres = movie.f2.split(\"\\\\|\"); //使用变量 return Stream.of(genres).anyMatch(g -&gt; g.equals(genre));&#125;).print(); Flink 将序列化此变量并将其与函数一起发送到集群。 如果你需要将大量变量传递给函数，那么这些方法就会变得非常烦人了。 为了解决这个问题，Flink 提供了 withParameters 方法。 要使用它，你需要实现那些 Rich 函数，比如你不必实现 MapFunction 接口，而是实现 RichMapFunction。 Rich 函数允许你使用 withParameters 方法传递许多参数： 12345678// Configuration 类来存储参数Configuration configuration = new Configuration();configuration.setString(\"genre\", \"Action\");lines.filter(new FilterGenreWithParameters()) // 将参数传递给函数 .withParameters(configuration) .print(); 要读取这些参数，我们需要实现 “open” 方法并读取其中的参数: 1234567891011121314151617class FilterGenreWithParameters extends RichFilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt; &#123; String genre; @Override public void open(Configuration parameters) throws Exception &#123; //读取配置 genre = parameters.getString(\"genre\", \"\"); &#125; @Override public boolean filter(Tuple3&lt;Long, String, String&gt; movie) throws Exception &#123; String[] genres = movie.f2.split(\"\\\\|\"); return Stream.of(genres).anyMatch(g -&gt; g.equals(genre)); &#125;&#125; 所有这些选项都可以使用，但如果需要为多个函数设置相同的参数，则可能会很繁琐。在 Flink 中要处理此种情况， 你可以设置所有 TaskManager 都可以访问的全局环境变量。 为此，首先需要使用 ParameterTool.fromArgs 从命令行读取参数： 12345public static void main(String... args) &#123; //读取命令行参数 ParameterTool parameterTool = ParameterTool.fromArgs(args); ...&#125; 然后使用 setGlobalJobParameters 设置全局作业参数: 1234567final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.getConfig().setGlobalJobParameters(parameterTool);...//该函数将能够读取这些全局参数lines.filter(new FilterGenreWithGlobalEnv()) //这个函数是自己定义的 .print(); 现在我们来看看这个读取这些参数的函数，和上面说的一样，它是一个 Rich 函数： 12345678910111213class FilterGenreWithGlobalEnv extends RichFilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt; &#123; @Override public boolean filter(Tuple3&lt;Long, String, String&gt; movie) throws Exception &#123; String[] genres = movie.f2.split(\"\\\\|\"); //获取全局的配置 ParameterTool parameterTool = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); //读取配置 String genre = parameterTool.get(\"genre\"); return Stream.of(genres).anyMatch(g -&gt; g.equals(genre)); &#125;&#125; 要读取配置，我们需要调用 getGlobalJobParameter 来获取所有全局参数，然后使用 get 方法获取我们要的参数。 广播变量如果你想将数据从客户端发送到 TaskManager，上面文章中讨论的方法都适合你，但如果数据以数据集的形式存在于 TaskManager 中，该怎么办？ 在这种情况下，最好使用 Flink 中的另一个功能 —— 广播变量。 它只允许将数据集发送给那些执行你 Job 里面函数的任务管理器。 假设我们有一个数据集，其中包含我们在进行文本处理时应忽略的单词，并且我们希望将其设置为我们的函数。 要为单个函数设置广播变量，我们需要使用 withBroadcastSet 方法和数据集。 12345678910111213141516171819202122232425DataSet&lt;Integer&gt; toBroadcast = env.fromElements(1, 2, 3);// 获取要忽略的单词集合DataSet&lt;String&gt; wordsToIgnore = ...data.map(new RichFlatMapFunction&lt;String, String&gt;() &#123; // 存储要忽略的单词集合. 这将存储在 TaskManager 的内存中 Collection&lt;String&gt; wordsToIgnore; @Override public void open(Configuration parameters) throws Exception &#123; //读取要忽略的单词的集合 wordsToIgnore = getRuntimeContext().getBroadcastVariable(\"wordsToIgnore\"); &#125; @Override public String map(String line, Collector&lt;String&gt; out) throws Exception &#123; String[] words = line.split(\"\\\\W+\"); for (String word : words) //使用要忽略的单词集合 if (wordsToIgnore.contains(word)) out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; //通过广播变量传递数据集&#125;).withBroadcastSet(wordsToIgnore, \"wordsToIgnore\"); 你应该记住，如果要使用广播变量，那么数据集将会存储在 TaskManager 的内存中，如果数据集和越大，那么占用的内存就会越大，因此使用广播变量适用于较小的数据集。 如果要向每个 TaskManager 发送更多数据并且不希望将这些数据存储在内存中，可以使用 Flink 的分布式缓存向 TaskManager 发送静态文件。 要使用 Flink 的分布式缓存，你首先需要将文件存储在一个分布式文件系统（如 HDFS）中，然后在缓存中注册该文件： 12345678ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();//从 HDFS 注册文件env.registerCachedFile(\"hdfs:///path/to/file\", \"machineLearningModel\")...env.execute() 为了访问分布式缓存，我们需要实现一个 Rich 函数： 12345678910111213class MyClassifier extends RichMapFunction&lt;String, Integer&gt; &#123; @Override public void open(Configuration config) &#123; File machineLearningModel = getRuntimeContext().getDistributedCache().getFile(\"machineLearningModel\"); ... &#125; @Override public Integer map(String value) throws Exception &#123; ... &#125;&#125; 请注意，要访问分布式缓存中的文件，我们需要使用我们用于注册文件的 key，比如上面代码中的 machineLearningModel。 Accumulator(累加器)我们前面已经介绍了如何将数据发送给 TaskManager，但现在我们将讨论如何从 TaskManager 中返回数据。 你可能想知道为什么我们需要做这种事情。 毕竟，Apache Flink 就是建立数据处理流水线，读取输入数据，处理数据并返回结果。 为了表达清楚，让我们来看一个例子。假设我们需要计算每个单词在文本中出现的次数，同时我们要计算文本中有多少行： 1234567891011121314151617181920//要处理的数据集合DataSet&lt;String&gt; lines = ...// Word count 算法lines.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] words = line.split(\"\\\\W+\"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; &#125;&#125;).groupBy(0).sum(1).print();// 计算要处理的文本中的行数int linesCount = lines.count()System.out.println(linesCount); 问题是如果我们运行这个应用程序，它将运行两个 Flink 作业！首先得到单词统计数，然后计算行数。 这绝对是低效的，但我们怎样才能避免这种情况呢？一种方法是使用累加器。它们允许你从 TaskManager 发送数据，并使用预定义的功能聚合此数据。 Flink 有以下内置累加器： IntCounter，LongCounter，DoubleCounter：允许将 TaskManager 发送的 int，long，double 值汇总在一起 AverageAccumulator：计算双精度值的平均值 LongMaximum，LongMinimum，IntMaximum，IntMinimum，DoubleMaximum，DoubleMinimum：累加器，用于确定不同类型的最大值和最小值 直方图 - 用于计算 TaskManager 的值分布 要使用累加器，我们需要创建并注册一个用户定义的函数，然后在客户端上读取结果。下面我们来看看该如何使用呢： 1234567891011121314151617181920212223242526272829lines.flatMap(new RichFlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; //创建一个累加器 private IntCounter linesNum = new IntCounter(); @Override public void open(Configuration parameters) throws Exception &#123; //注册一个累加器 getRuntimeContext().addAccumulator(\"linesNum\", linesNum); &#125; @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] words = line.split(\"\\\\W+\"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; // 处理每一行数据后 linesNum 递增 linesNum.add(1); &#125;&#125;).groupBy(0).sum(1).print();//获取累加器结果int linesNum = env.getLastJobExecutionResult().getAccumulatorResult(\"linesNum\");System.out.println(linesNum); 这样计算就可以统计输入文本中每个单词出现的次数以及它有多少行。 如果需要自定义累加器，还可以使用 Accumulator 或 SimpleAccumulator 接口实现自己的累加器。 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 最后本篇文章由 zhisheng 翻译，禁止任何无授权的转载。 翻译后地址：http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/ 原文地址：https://brewing.codes/2017/10/24/flink-additional-data/ 本文部分代码地址：https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/batch/accumulator 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— JobManager 处理 SubmitJob 的过程","date":"2019-03-26T16:00:00.000Z","path":"2019/03/27/Flink-code-JobManager-submitJob/","text":"JobManager 处理 SubmitJobhttps://t.zsxq.com/3JQJMzZ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 如何获取 ExecutionGraph ？","date":"2019-03-25T16:00:00.000Z","path":"2019/03/26/Flink-code-ExecutionGraph/","text":"ExecutionGraph https://t.zsxq.com/UnA2jIi 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Flink JobManager 有什么作用？","date":"2019-03-24T16:00:00.000Z","path":"2019/03/25/Flink-code-jobmanager/","text":"JobManager 的作用https://t.zsxq.com/2VRrbuf 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Flink TaskManager 有什么作用？","date":"2019-03-24T16:00:00.000Z","path":"2019/03/25/Flink-code-taskmanager/","text":"TaskManager 有什么作用 https://t.zsxq.com/RZbu7yN 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？","date":"2019-03-23T16:00:00.000Z","path":"2019/03/24/Flink-code-memory-management/","text":"前言如今，许多用于分析大型数据集的开源系统都是用 Java 或者是基于 JVM 的编程语言实现的。最着名的例子是 Apache Hadoop，还有较新的框架，如 Apache Spark、Apache Drill、Apache Flink。基于 JVM 的数据分析引擎面临的一个常见挑战就是如何在内存中存储大量的数据（包括缓存和高效处理）。合理的管理好 JVM 内存可以将 难以配置且不可预测的系统 与 少量配置且稳定运行的系统区分开来。 在这篇文章中，我们将讨论 Apache Flink 如何管理内存，讨论其自定义序列化与反序列化机制，以及它是如何操作二进制数据的。 数据对象直接放在堆内存中在 JVM 中处理大量数据最直接的方式就是将这些数据做为对象存储在堆内存中，然后直接在内存中操作这些数据，如果想进行排序则就是对对象列表进行排序。然而这种方法有一些明显的缺点，首先，在频繁的创建和销毁大量对象的时候，监视和控制堆内存的使用并不是一件很简单的事情。如果对象分配过多的话，那么会导致内存过度使用，从而触发 OutOfMemoryError，导致 JVM 进程直接被杀死。另一个方面就是因为这些对象大都是生存在新生代，当 JVM 进行垃圾回收时，垃圾收集的开销很容易达到 50% 甚至更多。最后就是 Java 对象具有一定的空间开销（具体取决于 JVM 和平台）。对于具有许多小对象的数据集，这可以显著减少有效可用的内存量。如果你精通系统设计和系统调优，你可以根据系统进行特定的参数调整，可以或多或少的控制出现 OutOfMemoryError 的次数和避免堆内存的过多使用，但是这种设置和调优的作用有限，尤其是在数据量较大和执行环境发生变化的情况下。 Flink 是怎么做的?Apache Flink 起源于一个研究项目，该项目旨在结合基于 MapReduce 的系统和并行数据库系统的最佳技术。在此背景下，Flink 一直有自己的内存数据处理方法。Flink 将对象序列化为固定数量的预先分配的内存段，而不是直接把对象放在堆内存上。它的 DBMS 风格的排序和连接算法尽可能多地对这个二进制数据进行操作，以此将序列化和反序列化开销降到最低。如果需要处理的数据多于可以保存在内存中的数据，Flink 的运算符会将部分数据溢出到磁盘。事实上，很多Flink 的内部实现看起来更像是 C / C ++，而不是普通的 Java。下图概述了 Flink 如何在内存段中存储序列化数据并在必要时溢出到磁盘： Flink 的主动内存管理和操作二进制数据有几个好处： 1、内存安全执行和高效的核外算法 由于分配的内存段的数量是固定的，因此监控剩余的内存资源是非常简单的。在内存不足的情况下，处理操作符可以有效地将更大批的内存段写入磁盘，后面再将它们读回到内存。因此，OutOfMemoryError 就有效的防止了。 2、减少垃圾收集压力 因为所有长生命周期的数据都是在 Flink 的管理内存中以二进制表示的，所以所有数据对象都是短暂的，甚至是可变的，并且可以重用。短生命周期的对象可以更有效地进行垃圾收集，这大大降低了垃圾收集的压力。现在，预先分配的内存段是 JVM 堆上的长期存在的对象，为了降低垃圾收集的压力，Flink 社区正在积极地将其分配到堆外内存。这种努力将使得 JVM 堆变得更小，垃圾收集所消耗的时间将更少。 3、节省空间的数据存储 Java 对象具有存储开销，如果数据以二进制的形式存储，则可以避免这种开销。 4、高效的二进制操作和缓存敏感性 在给定合适的二进制表示的情况下，可以有效地比较和操作二进制数据。此外，二进制表示可以将相关值、哈希码、键和指针等相邻地存储在内存中。这使得数据结构通常具有更高效的缓存访问模式。 主动内存管理的这些特性在用于大规模数据分析的数据处理系统中是非常可取的，但是要实现这些功能的代价也是高昂的。要实现对二进制数据的自动内存管理和操作并非易事，使用 java.util.HashMap 比实现一个可溢出的 hash-table （由字节数组和自定义序列化支持）。当然，Apache Flink 并不是唯一一个基于 JVM 且对二进制数据进行操作的数据处理系统。例如 Apache Drill、Apache Ignite、Apache Geode 也有应用类似技术，最近 Apache Spark 也宣布将向这个方向演进。 下面我们将详细讨论 Flink 如何分配内存、如果对对象进行序列化和反序列化以及如果对二进制数据进行操作。我们还将通过一些性能表现数据来比较处理堆内存上的对象和对二进制数据的操作。 Flink 如何分配内存?Flink TaskManager 是由几个内部组件组成的：actor 系统（负责与 Flink master 协调）、IOManager（负责将数据溢出到磁盘并将其读取回来）、MemoryManager（负责协调内存使用）。在本篇文章中，我们主要讲解 MemoryManager。 MemoryManager 负责将 MemorySegments 分配、计算和分发给数据处理操作符，例如 sort 和 join 等操作符。MemorySegment 是 Flink 的内存分配单元，由常规 Java 字节数组支持(默认大小为 32 KB)。MemorySegment 通过使用 Java 的 unsafe 方法对其支持的字节数组提供非常有效的读写访问。你可以将 MemorySegment 看作是 Java 的 NIO ByteBuffer 的定制版本。为了在更大的连续内存块上操作多个 MemorySegment，Flink 使用了实现 Java 的 java.io.DataOutput 和 java.io.DataInput 接口的逻辑视图。 MemorySegments 在 TaskManager 启动时分配一次，并在 TaskManager 关闭时销毁。因此，在 TaskManager 的整个生命周期中，MemorySegment 是重用的，而不会被垃圾收集的。在初始化 TaskManager 的所有内部数据结构并且已启动所有核心服务之后，MemoryManager 开始创建 MemorySegments。默认情况下，服务初始化后，70％ 可用的 JVM 堆内存由 MemoryManager 分配（也可以配置全部）。剩余的 JVM 堆内存用于在任务处理期间实例化的对象，包括由用户定义的函数创建的对象。下图显示了启动后 TaskManager JVM 中的内存分布： Flink 如何序列化对象？Java 生态系统提供了几个库，可以将对象转换为二进制表示形式并返回。常见的替代方案是标准 Java 序列化，Kryo，Apache Avro，Apache Thrift 或 Google 的 Protobuf。Flink 包含自己的自定义序列化框架，以便控制数据的二进制表示。这一点很重要，因为对二进制数据进行操作需要对序列化布局有准确的了解。此外，根据在二进制数据上执行的操作配置序列化布局可以显著提升性能。Flink 的序列化机制利用了这一特性，即在执行程序之前，要序列化和反序列化的对象的类型是完全已知的。 Flink 程序可以处理表示为任意 Java 或 Scala 对象的数据。在优化程序之前，需要识别程序数据流的每个处理步骤中的数据类型。对于 Java 程序，Flink 提供了一个基于反射的类型提取组件，用于分析用户定义函数的返回类型。Scala 程序可以在 Scala 编译器的帮助下进行分析。Flink 使用 TypeInformation 表示每种数据类型。 注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删 Flink 有如下几种数据类型的 TypeInformations： BasicTypeInfo：所有 Java 的基础类型或 java.lang.String BasicArrayTypeInfo：Java 基本类型构成的数组或 java.lang.String WritableTypeInfo：Hadoop 的 Writable 接口的任何实现 TupleTypeInfo：任何 Flink tuple（Tuple1 到 Tuple25）。Flink tuples 是具有类型化字段的固定长度元组的 Java 表示 CaseClassTypeInfo：任何 Scala CaseClass（包括 Scala tuples） PojoTypeInfo：任何 POJO（Java 或 Scala），即所有字段都是 public 的或通过 getter 和 setter 访问的对象，遵循通用命名约定 GenericTypeInfo：不能标识为其他类型的任何数据类型 注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删 每个 TypeInformation 都为它所代表的数据类型提供了一个序列化器。例如，BasicTypeInfo 返回一个序列化器，该序列化器写入相应的基本类型；WritableTypeInfo 的序列化器将序列化和反序列化委托给实现 Hadoop 的 Writable 接口的对象的 write() 和 readFields() 方法；GenericTypeInfo 返回一个序列化器，该序列化器将序列化委托给 Kryo。对象将自动通过 Java 中高效的 Unsafe 方法来序列化到 Flink MemorySegments 支持的 DataOutput。对于可用作键的数据类型，例如哈希值，TypeInformation 提供了 TypeComparators，TypeComparators 比较和哈希对象，并且可以根据具体的数据类型有效的比较二进制并提取固定长度的二进制 key 前缀。 Tuple，Pojo 和 CaseClass 类型是复合类型，它们可能嵌套一个或者多个数据类型。因此，它们的序列化和比较也都比较复杂，一般将其成员数据类型的序列化和比较都交给各自的 Serializers（序列化器） 和 Comparators（比较器）。下图说明了 Tuple3&lt;Integer, Double, Person&gt;对象的序列化，其中Person 是 POJO 并定义如下： 1234public class Person &#123; public int id; public String name;&#125; 通过提供定制的 TypeInformations、Serializers（序列化器） 和 Comparators（比较器），可以方便地扩展 Flink 的类型系统，从而提高序列化和比较自定义数据类型的性能。 Flink 如何对二进制数据进行操作？与其他的数据处理框架的 API（包括 SQL）类似，Flink 的 API 也提供了对数据集进行分组、排序和连接等转换操作。这些转换操作的数据集可能非常大。关系数据库系统具有非常高效的算法，比如 merge-sort、merge-join 和 hash-join。Flink 建立在这种技术的基础上，但是主要分为使用自定义序列化和自定义比较器来处理任意对象。在下面文章中我们将通过 Flink 的内存排序算法示例演示 Flink 如何使用二进制数据进行操作。 Flink 为其数据处理操作符预先分配内存，初始化时，排序算法从 MemoryManager 请求内存预算，并接收一组相应的 MemorySegments。这些 MemorySegments 变成了缓冲区的内存池，缓冲区中收集要排序的数据。下图说明了如何将数据对象序列化到排序缓冲区中： 排序缓冲区在内部分为两个内存区域：第一个区域保存所有对象的完整二进制数据，第二个区域包含指向完整二进制对象数据的指针（取决于 key 的数据类型）。将对象添加到排序缓冲区时，它的二进制数据会追加到第一个区域，指针(可能还有一个 key)被追加到第二个区域。分离实际数据和指针以及固定长度的 key 有两个目的：它可以有效的交换固定长度的 entries（key 和指针），还可以减少排序时需要移动的数据。如果排序的 key 是可变长度的数据类型（比如 String），则固定长度的排序 key 必须是前缀 key，比如字符串的前 n 个字符。请注意：并非所有数据类型都提供固定长度的前缀排序 key。将对象序列化到排序缓冲区时，两个内存区域都使用内存池中的 MemorySegments 进行扩展。一旦内存池为空且不能再添加对象时，则排序缓冲区将会被完全填充并可以进行排序。Flink 的排序缓冲区提供了比较和交换元素的方法，这使得实际的排序算法是可插拔的。默认情况下， Flink 使用了 Quicksort（快速排序）实现，可以使用 HeapSort（堆排序）。下图显示了如何比较两个对象： 排序缓冲区通过比较它们的二进制固定长度排序 key 来比较两个元素。如果元素的完整 key（不是前缀 key） 或者二进制前缀 key 不相等，则代表比较成功。如果前缀 key 相等(或者排序 key 的数据类型不提供二进制前缀 key)，则排序缓冲区遵循指向实际对象数据的指针，对两个对象进行反序列化并比较对象。根据比较结果，排序算法决定是否交换比较的元素。排序缓冲区通过移动其固定长度 key 和指针来交换两个元素，实际数据不会移动，排序算法完成后，排序缓冲区中的指针被正确排序。下图演示了如何从排序缓冲区返回已排序的数据： 通过顺序读取排序缓冲区的指针区域，跳过排序 key 并按照实际数据的排序指针返回排序数据。此数据要么反序列化并作为对象返回，要么在外部合并排序的情况下复制二进制数据并将其写入磁盘。 基准测试数据那么，对二进制数据进行操作对性能意味着什么？我们将运行一个基准测试，对 1000 万个Tuple2&lt;Integer, String&gt;对象进行排序以找出答案。整数字段的值从均匀分布中采样。String 字段值的长度为 12 个字符，并从长尾分布中进行采样。输入数据由返回可变对象的迭代器提供，即返回具有不同字段值的相同 Tuple 对象实例。Flink 在从内存，网络或磁盘读取数据时使用此技术，以避免不必要的对象实例化。基准测试在具有 900 MB 堆大小的 JVM 中运行，在堆上存储和排序 1000 万个 Tuple 对象并且不会导致触发 OutOfMemoryError 大约需要这么大的内存。我们使用三种排序方法在Integer 字段和 String 字段上对 Tuple 对象进行排序： 1、对象存在堆中：Tuple 对象存储在常用的 java.util.ArrayList 中，初始容量设置为 1000 万，并使用 Java 中常用的集合排序进行排序。 Flink 序列化：使用 Flink 的自定义序列化程序将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，如上所述排序，最后再次反序列化。在 Integer 字段上进行排序时，完整的 Integer 用作排序 key，以便排序完全发生在二进制数据上（不需要对象的反序列化）。对于 String 字段的排序，使用 8 字节前缀 key，如果前缀 key 相等，则对 Tuple 对象进行反序列化。 3、Kryo 序列化：使用 Kryo 序列化将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，并在没有二进制排序 key 的情况下进行排序。这意味着每次比较需要对两个对象进行反序列化。 所有排序方法都使用单线程实现。结果的时间是十次运行结果的平均值。在每次运行之后，我们调用System.gc()请求垃圾收集运行，该运行不会进入测量的执行时间。下图显示了将输入数据存储在内存中，对其进行排序并将其作为对象读回的时间。 我们看到 Flink 使用自己的序列化器对二进制数据进行排序明显优于其他两种方法。与存储在堆内存上相比，我们看到将数据加载到内存中要快得多。因为我们实际上是在收集对象，没有机会重用对象实例，但必须重新创建每个 Tuple。这比 Flink 的序列化器（或Kryo序列化）效率低。另一方面，与反序列化相比，从堆中读取对象是无性能消耗的。在我们的基准测试中，对象克隆比序列化和反序列化组合更耗性能。查看排序时间，我们看到对二进制数据的排序也比 Java 的集合排序更快。使用没有二进制排序 key 的 Kryo 序列化的数据排序比其他方法慢得多。这是因为反序列化带来很大的开销。在String 字段上对 Tuple 进行排序比在 Integer 字段上排序更快，因为长尾值分布显着减少了成对比较的数量。为了更好地了解排序过程中发生的状况，我们使用 VisualVM 监控执行的 JVM。以下截图显示了执行 10次 运行时的堆内存使用情况、垃圾收集情况和 CPU 使用情况。 测试是在 8 核机器上运行单线程，因此一个核心的完全利用仅对应 12.5％ 的总体利用率。截图显示，对二进制数据进行操作可显著减少垃圾回收活动。对于对象存在堆中，垃圾收集器在排序缓冲区被填满时以非常短的时间间隔运行，并且即使对于单个处理线程也会导致大量 CPU 使用（排序本身不会触发垃圾收集器）。JVM 垃圾收集多个并行线程，解释了高CPU 总体利用率。另一方面，对序列化数据进行操作的方法很少触发垃圾收集器并且 CPU 利用率低得多。实际上，如果使用 Flink 序列化的方式在 Integer 字段上对 Tuple 进行排序，则垃圾收集器根本不运行，因为对于成对比较，不需要反序列化任何对象。Kryo 序列化需要比较多的垃圾收集，因为它不使用二进制排序 key 并且每次排序都要反序列化两个对象。 内存使用情况上图显示 Flink 序列化和 Kryo 序列化不断的占用大量内存 存使用情况图表显示flink-serialized和kryo-serialized不断占用大量内存。这是由于 MemorySegments 的预分配。实际内存使用率要低得多，因为排序缓冲区并未完全填充。下表显示了每种方法的内存消耗。1000 万条数据产生大约 280 MB 的二进制数据（对象数据、指针和排序 key），具体取决于使用的序列化程序以及二进制排序 key 的存在和大小。将其与数据存储在堆上的方法进行比较，我们发现对二进制数据进行操作可以显著提高内存效率。在我们的基准测试中，如果序列化为排序缓冲区而不是将其作为堆上的对象保存，则可以在内存中对两倍以上的数据进行排序。 占用内存 对象存在堆中 Flink 序列化 Kryo 序列化 对 Integer 排序 约 700 MB（堆内存） 277 MB（排序缓冲区） 266 MB（排序缓冲区） 对 String 排序 约 700 MB（堆内存） 315 MB（排序缓冲区） 266 MB（排序缓冲区） 总而言之，测试验证了文章前面说的对二进制数据进行操作的好处。 展望未来Apache Flink 具有相当多的高级技术，可以通过有限的内存资源安全有效地处理大量数据。但是有几点可以使 Flink 更有效率。Flink 社区正在努力将管理内存移动到堆外内存。这将允许更小的 JVM，更低的垃圾收集开销，以及更容易的系统配置。使用 Flink 的 Table API，所有操作（如 aggregation 和 projection）的语义都是已知的（与黑盒用户定义的函数相反）。因此，我们可以为直接对二进制数据进行操作的 Table API 操作生成代码。进一步的改进包括序列化设计，这些设计针对应用于二进制数据的操作和针对序列化器和比较器的代码生成而定制。 总结 Flink 的主动内存管理减少了因触发 OutOfMemoryErrors 而杀死 JVM 进程和垃圾收集开销的问题。 Flink 具有高效的数据序列化和反序列化机制，有助于对二进制数据进行操作，并使更多数据适合内存。 Flink 的 DBMS 风格的运算符本身在二进制数据上运行，在必要时可以在内存中高性能地传输到磁盘。 本文地址: http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/ 本文翻译自：https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html翻译：zhisheng，二次转载请注明地址，否则保留追究法律责任。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 深度解析 Flink Checkpoint 机制","date":"2019-03-22T16:00:00.000Z","path":"2019/03/23/Flink-code-checkpoint/","text":"Flink Checkpoint 机制 https://t.zsxq.com/ynQNbeM 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 深度解析 Flink 序列化机制","date":"2019-03-21T16:00:00.000Z","path":"2019/03/22/Flink-code-serialize/","text":"Flink 序列化机制 https://t.zsxq.com/JaQfeMf 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 如何获取 JobGraph？","date":"2019-03-20T16:00:00.000Z","path":"2019/03/21/Flink-code-JobGraph/","text":"JobGraph https://t.zsxq.com/naaMf6y 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 如何获取 StreamGraph？","date":"2019-03-19T16:00:00.000Z","path":"2019/03/20/Flink-code-StreamGraph/","text":"StreamGraph https://t.zsxq.com/qRFIm6I 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程","date":"2019-03-18T16:00:00.000Z","path":"2019/03/19/Flink-code-streaming-wordcount-start/","text":"流处理 WordCount 程序 https://t.zsxq.com/qnMFEUJ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程","date":"2019-03-17T16:00:00.000Z","path":"2019/03/18/Flink-code-batch-wordcount-start/","text":"批处理的 WordCount 程序分析： https://t.zsxq.com/YJ2Zrfi 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动","date":"2019-03-16T16:00:00.000Z","path":"2019/03/17/Flink-code-Standalone-TaskManager-start/","text":"Task Manager 启动 https://t.zsxq.com/qjEUFau 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动","date":"2019-03-15T16:00:00.000Z","path":"2019/03/16/Flink-code-Standalone-JobManager-start/","text":"Job Manager 启动 https://t.zsxq.com/AurR3rN 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Standalone session 模式启动流程","date":"2019-03-14T16:00:00.000Z","path":"2019/03/15/Flink-code-Standalone-start/","text":"Standalone session 模式启动流程 https://t.zsxq.com/EemAEIi 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 项目结构一览","date":"2019-03-13T16:00:00.000Z","path":"2019/03/14/Flink-code-structure/","text":"Flink 源码项目结构一览 https://t.zsxq.com/MNfAYne 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 你上传的 jar 包藏到哪里去了?","date":"2019-03-12T16:00:00.000Z","path":"2019/03/13/flink-job-jars/","text":"前言写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题： 通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？ 这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。 跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？ 查看配置文件首先想到的是这个肯定可以在配置文件中有设置的地方的： 谷歌大法好虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。 找到了一条，点进去看 Issue 如下： 发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码： 源码确认这个 jobmanager.web.upload.dir 是不是？我去看下 1.8 的源码确认一下： 发现这个 jobmanager.web.upload.dir 还过期了，用 WebOptions 类中的 UPLOAD_DIR 替代了！ 继续跟进去看看这个 UPLOAD_DIR 是啥玩意？ 看这注释的意思是说，如果这个配置 web.upload.dir 没有配置具体的路径的话就会使用 JOB_MANAGER_WEB_TMPDIR_KEY 目录，那么我们来看看是否配置了这个目录呢？ 确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 JOB_MANAGER_WEB_TMPDIR_KEY 是在哪里的？ 又是一个过期的目录，mmp，继续跟下去看下这个目录 TMP_DIR。 我们查看下配置文件是否有配置这个 web.tmpdir 的值，又是没有： so，它肯定使用的是 System.getProperty(&quot;java.io.tmpdir&quot;) 这个目录了， 我查看了下我本地电脑起的 job 它的配置中有这个配置如下： 1java.io.tmpdir /var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/ 再观察了下 job，发现 jobManager 这里有个 web.tmpdir 的配置： 1web.tmpdir /var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7 发现这个 web.tmpdir 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！ 水落石出进入这个目录发现我们上传的 jar 终于被找到了： 配置上传 jar 目录确认上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置： 1web.tmpdir: /usr/local/blink-1.5.1/jars 更改之后再看 web.tmpdir 是这样的: 从 Flink UI 上上传了三个 jar，查看 /usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179 目录下就出现了我们的 jar 了。 我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下： 1web.upload.dir: /usr/local/blink-1.5.1/jars 这样的话，就可以保证你的 jar 不再会被删除了！ 再来看看源码是咋写的哈： 12345678//从配置文件中找 UPLOAD_DIRfinal Path uploadDir = Paths.get( config.getString(WebOptions.UPLOAD_DIR, config.getString(WebOptions.TMP_DIR)), \"flink-web-upload\");return new RestServerEndpointConfiguration( restAddress,restBindAddress,port,sslEngineFactory, uploadDir,maxContentLength,responseHeaders); 他就是从配置文件中找 UPLOAD_DIR，如果为 null 就找 TMP_DIR 目录来当作 jar 上传的路径！ 总结本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。 本篇文章连接是：http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/ 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"阿里巴巴开源的 Blink 实时计算框架真香","date":"2019-02-27T16:00:00.000Z","path":"2019/02/28/blink/","text":"Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？ 我们先看看 Blink 黑色版本： 对比下 Flink 版本你就知道黑色版本多好看了。 你上传 jar 包的时候是这样的： 我们来看看 Blink 运行的 job 长啥样？ 再来对比一下 Flink 的样子： 查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS 查看 subTask，这里可以直接点击这个日志就可以查看 task 日志： 查看背压： 查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优： 查看 job 运行时间段的情况： 查看 running 的 job： 查看已经完成的 job： 查看 Task Manager： Task Manager 分配的资源详情： Task Manager metric 监控信息详情： Task Manager log 文件详情，包含运行产生的日志和 GC 日志： Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。 总结介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！ 本文原创地址是: http://www.54tianzhisheng.cn/2019/02/28/blink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 相关文章1、《从0到1学习Flink》—— Apache Flink 介绍 2、《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、《从0到1学习Flink》—— Flink 配置文件详解 4、《从0到1学习Flink》—— Data Source 介绍 5、《从0到1学习Flink》—— 如何自定义 Data Source ？ 6、《从0到1学习Flink》—— Data Sink 介绍 7、《从0到1学习Flink》—— 如何自定义 Data Sink ？ 8、《从0到1学习Flink》—— Flink Data transformation(转换) 9、《从0到1学习Flink》—— 介绍Flink中的Stream Windows 10、《从0到1学习Flink》—— Flink 中的几种 Time 详解 11、《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 12、《从0到1学习Flink》—— Flink 项目如何运行？ 13、《从0到1学习Flink》—— Flink 写入数据到 Kafka 14、《从0到1学习Flink》—— Flink JobManager 高可用性配置 15、《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍 16、《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Blink","slug":"Blink","permalink":"http://www.54tianzhisheng.cn/tags/Blink/"}]},{"title":"Flink 源码解析 —— 源码编译运行","date":"2019-01-29T16:00:00.000Z","path":"2019/01/30/Flink-code-compile/","text":"更新一篇知识星球里面的源码分析文章，去年写的，周末自己录了个视频，大家看下效果好吗？如果好的话，后面补录发在知识星球里面的其他源码解析文章。 前言之前自己本地 clone 了 Flink 的源码，编译过，然后 share 到了 GitHub 上去了，自己也写了一些源码的中文注释，并且 push 到了 GitHub 上去了。这几天阿里开源了宣传已久的 Blink，结果我那个分支不能够继续 pull 下新的代码，再加上自己对 Flink 研究了也有点时间了，所以打算将这两个东西对比着来看，这样可能会学到不少更多东西，因为 Blink 是另外一个分支，所以自己干脆再重新 fork 了一份，拉到本地来看源码。 fork执行下面命令： 1git clone git@github.com:apache/flink.git 拉取的时候找个网络好点的地方，这样速度可能会更快点。 编译因为自己想看下 Blink 分支的代码，所以需要切换到 blink 分支来， 1git checkout blink 这样你就到了 blink 分支了，接下来我们将 blink 源码编译一下，执行如下命令： 1mvn clean install -Dmaven.test.skip=true -Dhadoop.version=2.7.6 -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true maven 编译的时候跳过测试代码、javadoc 和代码风格检查，这样可以减少不少时间。 注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个：(这样下载依赖才能飞起来) 12345678910111213&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;&lt;mirror&gt; &lt;id&gt;mapr-public&lt;/id&gt; &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt; &lt;name&gt;mapr-releases&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;&lt;/mirror&gt; 执行完这个命令后，然后呢，你可以掏出手机，打开微信，搜索下微信号：zhisheng_tian , 然后点击一波添加好友，欢迎来探讨技术。 等了一波时间之后，你可能会遇到这个问题(看到不少童鞋都遇到这个问题，之前编译 Flink 的时候也遇到过)： 1[ERROR] Failed to execute goal on project flink-mapr-fs: Could not resolve dependencies for project com.alibaba.blink:flink-mapr-fs:jar:1.5.1: Failure to find com.mapr.hadoop:maprfs:jar:5.2.1-mapr in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -&gt; [Help 1] 如果你试了两遍都没编译通过，那么我这里就教大家一种方法（执行完编译命令后啥也没动就 OK 的请跳过，谁叫你运气这么好呢）： 在 flink-filesystems 中把 flink-mapr-fs module 给注释掉。 上图这是我给大家的忠告，特别管用。 再次执行命令编译起来就没有错误了，如果你还有其他的错误，我猜估计还是网络的问题，导致一些国外的 maven 依赖下载不下来或者不完整，导致的错误，暴力的方法就是和我一样，把这些下载不下来的依赖 module 注释掉，或者你可以像已经编译好的童鞋要下 maven 的 .m2 文件里面已经下载好了的依赖，然后复制粘贴到你的本地路径去，注意路径包名不要弄错了，这样一般可以解决所有的问题了，如果还有问题，我也无能为力了。 编译成功就长下图这样： 运行然后我们的目录是长这样的： 标记的那个就是我们的可执行文件，就跟我们在 Flink 官网下载的一样，我们可以将它运行起来看下效果。 我把它移到了 /usr/local/blink-1.5.1 下了，个人习惯，喜欢把一些安装的软件安装在 /usr/local/ 目录下面。 目录结构和我以前的安装介绍文章类似，就是多了 batch_conf 目录，和 conf 目录是一样的东西，不知道为啥要弄两个配置文件目录，问过负责的人，没理我，哈哈哈。 那么我们接下来就是运行下 Blink，进入到 bin 目录，执行可执行文件： 1./start-cluster.sh windows 可以点击 start-cluster.bat 启动，这点对 windows 用户比较友好。 执行完后命令后，在浏览器里访问地址，, 出现下图这样就代表 Blink 成功启动了：123456789101112131415161718192021222324![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQLzMv.jpg)上图是开源版本的白色主题，骚气的黑色主题通过在 Flink 群里得知如何改之后，编译运行后的效果如下：![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JfcR5E.jpg)一次好奇的执行了多次上面启动命令，发现也能够正常的运行。然后启动的日志是这样的：![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fcAhYL.jpg)说明已经启动了 9 个 Task Manager，然后看到我们页面的监控信息如下：![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9Hxhsg.jpg)可以看到监控信息里面已经有 40 个可用的 slot，这是因为 Blink 默认的是一个 Task Manager 4 个 slot，我们总共启动了 10 个 Task Manager，所以才会有 40 个可用的 slot，注意：Flink 默认的配置是 1 个 Task Manager 只含有 1 个 slot，不过这个是可以自己分配的。注意：开启了多个 Task Manager 后，要关闭的话，得执行同样次数的关闭命令：```shell./stop-cluster.sh 中文源码分析https://github.com/zhisheng17/flink 配套视频解析视频录制过程难免说错，还请大家可以指教 相关更多源码解析的文章和 Flink 资料请加知识星球！ 本文地址是：http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/，未经允许，禁止转载！ 总结本篇文章是《从1到100深入学习Flink》的第一篇，zhisheng 我带带大家一起如何 clone 项目源码，进行源码编译，然后运行编译后的可执行文件 blink。下篇文章会分析项目源码的结构组成。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ","date":"2019-01-19T16:00:00.000Z","path":"2019/01/20/Flink-RabbitMQ-sink/","text":"前言之前有文章 《从0到1学习Flink》—— Flink 写入数据到 Kafka 写过 Flink 将处理后的数据后发到 Kafka 消息队列中去，当然我们常用的消息队列可不止这一种，还有 RocketMQ、RabbitMQ 等，刚好 Flink 也支持将数据写入到 RabbitMQ，所以今天我们就来写篇文章讲讲如何将 Flink 处理后的数据写入到 RabbitMQ。 前提准备安装 RabbitMQ这里我直接用 docker 命令安装吧，先把 docker 在 mac 上启动起来。 在命令行中执行下面的命令： 1docker run -d -p 15672:15672 -p 5672:5672 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management 对这个命令不懂的童鞋可以看看我以前的文章：http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/ 登录用户名和密码分别是：admin / admin ，登录进去是这个样子就代表安装成功了： 依赖pom.xml 中添加 Flink connector rabbitmq 的依赖如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-rabbitmq_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 生产者这里我们依旧自己写一个工具类一直的往 RabbitMQ 中的某个 queue 中发数据，然后由 Flink 去消费这些数据。 注意按照我的步骤来一步步操作，否则可能会出现一些错误！ RabbitMQProducerUtil.java 12345678910111213141516171819202122232425262728293031323334353637383940import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class RabbitMQProducerUtil &#123; public final static String QUEUE_NAME = \"zhisheng\"; public static void main(String[] args) throws Exception &#123; //创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); //设置RabbitMQ相关信息 factory.setHost(\"localhost\"); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setPort(5672); //创建一个新的连接 Connection connection = factory.newConnection(); //创建一个通道 Channel channel = connection.createChannel(); // 声明一个队列// channel.queueDeclare(QUEUE_NAME, false, false, false, null); //发送消息到队列中 String message = \"Hello zhisheng\"; //我们这里演示发送一千条数据 for (int i = 0; i &lt; 1000; i++) &#123; channel.basicPublish(\"\", QUEUE_NAME, null, (message + i).getBytes(\"UTF-8\")); System.out.println(\"Producer Send +'\" + message + i); &#125; //关闭通道和连接 channel.close(); connection.close(); &#125;&#125; Flink 主程序12345678910111213141516171819202122232425262728293031323334import com.zhisheng.common.utils.ExecutionEnvUtil;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.rabbitmq.RMQSource;import org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig;/** * 从 rabbitmq 读取数据 */public class Main &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL; //这些配置建议可以放在配置文件中，然后通过 parameterTool 来获取对应的参数值 final RMQConnectionConfig connectionConfig = new RMQConnectionConfig .Builder().setHost(\"localhost\").setVirtualHost(\"/\") .setPort(5672).setUserName(\"admin\").setPassword(\"admin\") .build(); DataStreamSource&lt;String&gt; zhisheng = env.addSource(new RMQSource&lt;&gt;(connectionConfig, \"zhisheng\", true, new SimpleStringSchema())) .setParallelism(1); zhisheng.print(); //如果想保证 exactly-once 或 at-least-once 需要把 checkpoint 开启// env.enableCheckpointing(10000); env.execute(\"flink learning connectors rabbitmq\"); &#125;&#125; 运行 RabbitMQProducerUtil 类，再运行 Main 类！ 注意⚠️： 1、RMQConnectionConfig 中设置的用户名和密码要设置成 admin/admin，如果你换成是 guest/guest，其实是在 RabbitMQ 里面是没有这个用户名和密码的，所以就会报这个错误： 1nested exception is com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile. 不出意外的话应该你运行 RabbitMQProducerUtil 类后，立马两个运行的结果都会出来，速度还是很快的。 2、如果你在 RabbitMQProducerUtil 工具类中把注释的那行代码打开的话： 12 // 声明一个队列// channel.queueDeclare(QUEUE_NAME, false, false, false, null); 就会出现这种错误： 1Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;zhisheng&apos; in vhost &apos;/&apos;: received &apos;true&apos; but current is &apos;false&apos;, class-id=50, method-id=10) 这是因为你打开那个注释的话，一旦你运行了该类就会创建一个叫做 zhisheng 的 Queue，当你再运行 Main 类中的时候，它又会创建这样一个叫 zhisheng 的 Queue，然后因为已经有同名的 Queue 了，所以就有了冲突，解决方法就是把那行代码注释就好了。 3、该 connector（连接器）中提供了 RMQSource 类去消费 RabbitMQ queue 中的消息和确认 checkpoints 上的消息，它提供了三种不一样的保证： Exactly-once(只消费一次): 前提条件有，1 是要开启 checkpoint，因为只有在 checkpoint 完成后，才会返回确认消息给 RabbitMQ（这时，消息才会在 RabbitMQ 队列中删除)；2 是要使用 Correlation ID，在将消息发往 RabbitMQ 时，必须在消息属性中设置 Correlation ID。数据源根据 Correlation ID 把从 checkpoint 恢复的数据进行去重；3 是数据源不能并行，这种限制主要是由于 RabbitMQ 将消息从单个队列分派给多个消费者。 At-least-once(至少消费一次): 开启了 checkpoint，但未使用相 Correlation ID 或 数据源是并行的时候，那么就只能保证数据至少消费一次了 No guarantees(无法保证): Flink 接收到数据就返回确认消息给 RabbitMQ Sink 数据到 RabbitMQRabbitMQ 除了可以作为数据源，也可以当作下游，Flink 消费数据做了一些处理之后也能把数据发往 RabbitMQ，下面演示下 Flink 消费 Kafka 数据后写入到 RabbitMQ。 12345678910111213141516public class Main1 &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); final RMQConnectionConfig connectionConfig = new RMQConnectionConfig .Builder().setHost(\"localhost\").setVirtualHost(\"/\") .setPort(5672).setUserName(\"admin\").setPassword(\"admin\") .build(); //注意，换一个新的 queue，否则也会报错 data.addSink(new RMQSink&lt;&gt;(connectionConfig, \"zhisheng001\", new MetricSchema())); env.execute(\"flink learning connectors rabbitmq\"); &#125;&#125; 是不是很简单？但是需要注意的是，要换一个之前不存在的 queue，否则是会报错的。 不出意外的话，你可以看到 RabbitMQ 的监控页面会出现新的一个 queue 出来，如下图： 总结本文先把 RabbitMQ 作为数据源，写了个 Flink 消费 RabbitMQ 队列里面的数据进行打印出来，然后又写了个 Flink 消费 Kafka 数据后写入到 RabbitMQ 的例子！ 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 本文的项目代码在 https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://www.54tianzhisheng.cn/tags/RabbitMQ/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据批量写入到 MySQL","date":"2019-01-14T16:00:00.000Z","path":"2019/01/15/Flink-MySQL-sink/","text":"前言之前其实在 《从0到1学习Flink》—— 如何自定义 Data Sink ？ 文章中其实已经写了点将数据写入到 MySQL，但是一些配置化的东西当时是写死的，不能够通用，最近知识星球里有朋友叫我: 写个从 kafka 中读取数据，经过 Flink 做个预聚合，然后创建数据库连接池将数据批量写入到 mysql 的例子。 于是才有了这篇文章，更多提问和想要我写的文章可以在知识星球里像我提问，我会根据提问及时回答和尽可能作出文章的修改。 准备你需要将这两个依赖添加到 pom.xml 中 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt;&lt;/dependency&gt; 读取 kafka 数据这里我依旧用的以前的 student 类，自己本地起了 kafka 然后造一些测试数据，这里我们测试发送一条数据则 sleep 10s，意味着往 kafka 中一分钟发 6 条数据。 123456789101112131415161718192021222324252627282930313233343536373839package com.zhisheng.connectors.mysql.utils;import com.zhisheng.common.utils.GsonUtil;import com.zhisheng.connectors.mysql.model.Student;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;/** * Desc: 往kafka中写数据,可以使用这个main函数进行测试 * Created by zhisheng on 2019-02-17 * Blog: http://www.54tianzhisheng.cn/tags/Flink/ */public class KafkaUtil &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"student\"; //kafka topic 需要和 flink 程序用同一个 topic public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 1; i &lt;= 100; i++) &#123; Student student = new Student(i, \"zhisheng\" + i, \"password\" + i, 18 + i); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, GsonUtil.toJson(student)); producer.send(record); System.out.println(\"发送数据: \" + GsonUtil.toJson(student)); Thread.sleep(10 * 1000); //发送一条数据 sleep 10s，相当于 1 分钟 6 条 &#125; producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; writeToKafka(); &#125;&#125; 从 kafka 中读取数据，然后序列化成 student 对象。 1234567891011121314final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"zookeeper.connect\", \"localhost:2181\");props.put(\"group.id\", \"metric-group\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"auto.offset.reset\", \"latest\");SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; GsonUtil.fromJson(string, Student.class)); //，解析字符串成 student 对象 因为 RichSinkFunction 中如果 sink 一条数据到 mysql 中就会调用 invoke 方法一次，所以如果要实现批量写的话，我们最好在 sink 之前就把数据聚合一下。那这里我们开个一分钟的窗口去聚合 Student 数据。 12345678910student.timeWindowAll(Time.minutes(1)).apply(new AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123; @Override public void apply(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out) throws Exception &#123; ArrayList&lt;Student&gt; students = Lists.newArrayList(values); if (students.size() &gt; 0) &#123; System.out.println(\"1 分钟内收集到 student 的数据条数是：\" + students.size()); out.collect(students); &#125; &#125;&#125;); 写入数据库这里使用 DBCP 连接池连接数据库 mysql，pom.xml 中添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp2&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt; 如果你想使用其他的数据库连接池请加入对应的依赖。 这里将数据写入到 MySQL 中，依旧是和之前文章一样继承 RichSinkFunction 类，重写里面的方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.zhisheng.connectors.mysql.sinks;import com.zhisheng.connectors.mysql.model.Student;import org.apache.commons.dbcp2.BasicDataSource;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import javax.sql.DataSource;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.util.List;/** * Desc: 数据批量 sink 数据到 mysql * Created by zhisheng_tian on 2019-02-17 * Blog: http://www.54tianzhisheng.cn/tags/Flink/ */public class SinkToMySQL extends RichSinkFunction&lt;List&lt;Student&gt;&gt; &#123; PreparedStatement ps; BasicDataSource dataSource; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); dataSource = new BasicDataSource(); connection = getConnection(dataSource); String sql = \"insert into Student(id, name, password, age) values(?, ?, ?, ?);\"; ps = this.connection.prepareStatement(sql); &#125; @Override public void close() throws Exception &#123; super.close(); //关闭连接和释放资源 if (connection != null) &#123; connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * 每条数据的插入都要调用一次 invoke() 方法 * * @param value * @param context * @throws Exception */ @Override public void invoke(List&lt;Student&gt; value, Context context) throws Exception &#123; //遍历数据集合 for (Student student : value) &#123; ps.setInt(1, student.getId()); ps.setString(2, student.getName()); ps.setString(3, student.getPassword()); ps.setInt(4, student.getAge()); ps.addBatch(); &#125; int[] count = ps.executeBatch();//批量后执行 System.out.println(\"成功了插入了\" + count.length + \"行数据\"); &#125; private static Connection getConnection(BasicDataSource dataSource) &#123; dataSource.setDriverClassName(\"com.mysql.jdbc.Driver\"); //注意，替换成自己本地的 mysql 数据库地址和用户名、密码 dataSource.setUrl(\"jdbc:mysql://localhost:3306/test\"); dataSource.setUsername(\"root\"); dataSource.setPassword(\"root123456\"); //设置连接池的一些参数 dataSource.setInitialSize(10); dataSource.setMaxTotal(50); dataSource.setMinIdle(2); Connection con = null; try &#123; con = dataSource.getConnection(); System.out.println(\"创建连接池：\" + con); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \" + e.getMessage()); &#125; return con; &#125;&#125; 核心类 Main核心程序如下： 123456789101112131415161718192021222324252627282930public class Main &#123; public static void main(String[] args) throws Exception&#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; GsonUtil.fromJson(string, Student.class)); // student.timeWindowAll(Time.minutes(1)).apply(new AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123; @Override public void apply(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out) throws Exception &#123; ArrayList&lt;Student&gt; students = Lists.newArrayList(values); if (students.size() &gt; 0) &#123; System.out.println(\"1 分钟内收集到 student 的数据条数是：\" + students.size()); out.collect(students); &#125; &#125; &#125;).addSink(new SinkToMySQL()); env.execute(\"flink learning connectors kafka\"); &#125;&#125; 运行项目运行 Main 类后再运行 KafkaUtils.java 类！ 下图是往 Kafka 中发送的数据： 下图是运行 Main 类的日志，会创建 4 个连接池是因为默认的 4 个并行度，你如果在 addSink 这个算子设置并行度为 1 的话就会创建一个连接池： 下图是批量插入数据库的结果： 总结本文从知识星球一位朋友的疑问来写的，应该都满足了他的条件（批量/数据库连接池/写入mysql），的确网上很多的例子都是简单的 demo 形式，都是单条数据就创建数据库连接插入 MySQL，如果要写的数据量很大的话，会对 MySQL 的写有很大的压力。这也是我之前在 《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 中，数据写 ES 强调过的，如果要提高性能必定要批量的写。就拿我们现在这篇文章来说，如果数据量大的话，聚合一分钟数据达万条，那么这样批量写会比来一条写一条性能提高不知道有多少。 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 本文的项目代码在 https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink parallelism 和 Slot 介绍","date":"2019-01-13T16:00:00.000Z","path":"2019/01/14/Flink-parallelism-slot/","text":"前言之所以写这个是因为前段时间自己的项目出现过这样的一个问题： 123Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;. 跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：https://issues.apache.org/jira/browse/FLINK-9056，看下面的评论差不多就是 TaskManager 的 slot 数量不足的原因，导致 job 提交失败。在 Flink 1.63 中已经修复了变成抛出异常了。 竟然知道了是因为 slot 不足的原因了，那么我们就要先了解下 slot 是什么东东呢？不过文章这里先介绍下 parallelism。 什么是 parallelism？ 如翻译这样，parallelism 是并行的意思，在 Flink 里面代表每个任务的并行度，适当的提高并行度可以大大提高 job 的执行效率，比如你的 job 消费 kafka 数据过慢，适当调大可能就消费正常了。 那么在 Flink 中怎么设置并行度呢？ 如何设置 parallelism？ 如上图，在 flink 配置文件中可以查看到默认并行度是 1， 1234cat flink-conf.yaml | grep parallelism# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1 所以你如果在你的 flink job 里面不设置任何的 parallelism 的话，那么他也会有一个默认的 parallelism = 1。那也意味着你可以修改这个配置文件的默认并行度。 如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)： 1./bin/flink run -p 10 ../word-count.jar 你也可以通过这样来设置你整个程序的并行度： 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(10); 注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。 如何给每个算子单独设置并行度呢？ 1234data.keyBy(new xxxKey()) .flatMap(new XxxFlatMapFunction()).setParallelism(5) .map(new XxxMapFunction).setParallelism(5) .addSink(new XxxSink()).setParallelism(1) 如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。 这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度 并行度讲到这里应该都懂了，下面 zhisheng 就继续跟你讲讲 什么是 slot？ 什么是 slot？其实什么是 slot 这个问题之前在第一篇文章 《从0到1学习Flink》—— Apache Flink 介绍 中就介绍过了，这里再讲细一点。 图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。 例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 文字说的比较干，zhisheng 这里我就拿下面的图片来讲解： 上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。 那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。 每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数。 但是 flink 配置文件中设置的 task manager 默认的 slot 是 1。 slot 和 parallelism下面给出官方的图片来更加深刻的理解下 slot： 1、slot 是指 taskmanager 的并发执行能力 taskmanager.numberOfTaskSlots:3 每一个 taskmanager 中的分配 3 个 TaskSlot, 3 个 taskmanager 一共有 9 个 TaskSlot。 2、parallelism 是指 taskmanager 实际使用的并发能力 parallelism.default:1 运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率。 3、parallelism 是可配置、可指定的 上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。 example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。 好了，既然并行度和 slot zhisheng 都带大家过了一遍了，那么再来看文章开头的问题：slot 资源不够。 问题原因现在这个问题的答案其实就已经很明显了，就是我们设置的并行度 parallelism 超过了 Task Manager 能提供的最大 slot 数量，所以才会报这个错误。 再来拿我的代码来看吧，当时我就是只设置了整个项目的并行度： 1env.setParallelism(15); 为什么要设置 15 呢，因为我项目消费的 Kafka topic 有 15 个 parttion，就想着让一个并行去消费一个 parttion，没曾想到 Flink 资源的不够，稍微降低下 并行度为 10 后就没出现这个错误了。 总结本文由自己项目生产环境的一个问题来讲解了自己对 Flink parallelism 和 slot 的理解，并告诉大家如何去设置这两个参数，最后也指出了问题的原因所在。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/ , 未经允许禁止转载。 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink JobManager 高可用性配置","date":"2019-01-12T16:00:00.000Z","path":"2019/01/13/Flink-JobManager-High-availability/","text":"前言之前在 《从0到1学习Flink》—— Flink 配置文件详解 讲过 Flink 的配置，但是后面陆续有人来问我一些配置相关的东西，在加上我现在对 Flink 也更熟悉了些，这里我就再写下 Flink JobManager 的配置相关信息。 在 《从0到1学习Flink》—— Apache Flink 介绍 一文中介绍过了 Flink Job 的运行架构图： JobManager 协调每个 Flink 作业的部署。它负责调度和资源管理。 默认情况下，每个 Flink 集群都有一个 JobManager 实例。这会产生单点故障（SPOF）：如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。 如果我们使用 JobManager 高可用模式，可以避免这个问题。您可以为 standalone 集群和 YARN 集群配置高可用模式。 standalone 集群高可用性standalone 集群的 JobManager 高可用性的概念是，任何时候都有一个主 JobManager 和 多个备 JobManagers，以便在主节点失败时有新的 JobNamager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别。每个 JobManager 都可以充当主备节点。 例如，请考虑以下三个 JobManager 实例的设置： 如何配置要启用 JobManager 高可用性功能，您必须将高可用性模式设置为 zookeeper，配置 ZooKeeper quorum，将所有 JobManagers 主机及其 Web UI 端口写入配置文件。 Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。他在我们的 Flink 安装路径下面 /conf/zoo.cfg 。 Masters 文件要启动 HA 集群，请在以下位置配置 Masters 文件 conf/masters： 12localhost:8081xxx.xxx.xxx.xxx:8081 masters 文件包含启动 JobManagers 的所有主机以及 Web 用户界面绑定的端口，上面一行写一个。 默认情况下，job manager 选一个随机端口作为进程通信端口。您可以通过 high-availability.jobmanager.port 更改此设置。此配置接受单个端口（例如 50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。 配置文件 (flink-conf.yaml)要启动 HA 集群，请将以下配置键添加到 conf/flink-conf.yaml： 高可用性模式（必需）：在 conf/flink-conf.yaml中，必须将高可用性模式设置为 zookeeper，以打开高可用模式。 1high-availability: zookeeper ZooKeeper quorum（必需）：ZooKeeper quorum 是一组 ZooKeeper 服务器，它提供分布式协调服务。 1high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181 每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 zookeeper。 另外就是高可用存储目录，JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS, S3, Ceph, nfs 等，该 storageDir 中保存了 JobManager 恢复状态需要的所有元数据。 1high-availability.storageDir: hdfs:///flink/ha/ 配置 master 文件和 ZooKeeper 配置后，您可以使用提供的集群启动脚本。他们将启动 HA 集群。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 集群配置单独的 ZooKeeper 根路径。 示例具有 2 个 JobManagers 的 Standalone 集群： 1、在 conf/flink-conf.yaml 中配置高可用模式和 Zookeeper : 123high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181high-availability.storageDir: hdfs:///flink/recovery 2、在 conf/masters 中 配置 masters: 12localhost:8081localhost:8082 3、在 conf/zoo.cfg 中配置 Zookeeper 服务: 1server.0=localhost:2888:3888 4、启动 ZooKeeper 集群: 12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 5、启动一个 Flink HA 集群: 12345$ bin/start-cluster.shStarting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.Starting jobmanager daemon on host localhost.Starting jobmanager daemon on host localhost.Starting taskmanager daemon on host localhost. 6、停止 ZooKeeper 和集群: 1234567$ bin/stop-cluster.shStopping taskmanager daemon (pid: 7647) on localhost.Stopping jobmanager daemon (pid: 7495) on host localhost.Stopping jobmanager daemon (pid: 7349) on host localhost.$ bin/stop-zookeeper-quorum.shStopping zookeeper daemon (pid: 7101) on host localhost. 上面的执行脚本如下图可见： YARN 集群高可用性当运行高可用的 YARN 集群时，我们不会运行多个 JobManager 实例，而只会运行一个，该 JobManager 实例失败时，YARN 会将其重新启动。Yarn 的具体行为取决于您使用的 YARN 版本。 如何配置？Application Master 最大重试次数 (yarn-site.xml)在 YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数： 1234567&lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt; The maximum number of application master execution attempts. &lt;/description&gt;&lt;/property&gt; 当前 YARN 版本的默认值为 2（表示允许单个 JobManager 失败两次）。 Application Attempts (flink-conf.yaml)除了上面可以配置最大重试次数外，你还可以在 flink-conf.yaml 配置如下： 1yarn.application-attempts: 10 这意味着在如果程序启动失败，YARN 会再重试 9 次（9 次重试 + 1 次启动），如果启动 10 次作业还失败，yarn 才会将该任务的状态置为失败。如果因为节点硬件故障或重启，NodeManager 重新同步等操作，需要 YARN 继续尝试启动应用。这些重启尝试不计入 yarn.application-attempts 个数中。 容器关闭行为 YARN 2.3.0 &lt; 版本 &lt; 2.4.0. 如果 application master 进程失败，则所有的 container 都会重启。 YARN 2.4.0 &lt; 版本 &lt; 2.6.0. TaskManager container 在 application master 故障期间，会继续工作。这具有以下优点：作业恢复时间更快，且缩短所有 task manager 启动时申请资源的时间。 YARN 2.6.0 &lt;= version: 将尝试失败有效性间隔设置为 Flink 的 Akka 超时值。尝试失败有效性间隔表示只有在系统在一个间隔期间看到最大应用程序尝试次数后才会终止应用程序。这避免了持久的工作会耗尽它的应用程序尝试。 示例：高可用的 YARN Session1、配置 HA 模式和 Zookeeper 集群 在 conf/flink-conf.yaml:123high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181yarn.application-attempts: 10 2、配置 ZooKeeper 服务 在 conf/zoo.cfg：1server.0=localhost:2888:3888 3、启动 Zookeeper 集群:12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 4、启动 HA 集群:1$ bin/yarn-session.sh -n 2 总结本篇文章再次写了下 Flink JobManager 的高可用配置，如何在 standalone 集群和 YARN 集群中配置高可用。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/ , 未经允许禁止转载。 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 写入数据到 Kafka","date":"2019-01-05T16:00:00.000Z","path":"2019/01/06/Flink-Kafka-sink/","text":"前言之前文章 《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 写了如何将 Kafka 中的数据存储到 ElasticSearch 中，里面其实就已经用到了 Flink 自带的 Kafka source connector（FlinkKafkaConsumer）。存入到 ES 只是其中一种情况，那么如果我们有多个地方需要这份通过 Flink 转换后的数据，是不是又要我们继续写个 sink 的插件呢？确实，所以 Flink 里面就默认支持了不少 sink，比如也支持 Kafka sink connector（FlinkKafkaProducer），那么这篇文章我们就讲讲如何将数据写入到 Kafka。 准备添加依赖Flink 里面支持 Kafka 0.8、0.9、0.10、0.11 ，以后有时间可以分析下源码的实现。 这里我们需要安装下 Kafka，请对应添加对应的 Flink Kafka connector 依赖的版本，这里我们使用的是 0.11 版本： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; Kafka 安装这里就不写这块内容了，可以参考我以前的文章 Kafka 安装及快速入门。 这里我们演示把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。 配置文件12345678910kafka.brokers=xxx:9092,xxx:9092,xxx:9092kafka.group.id=metrics-group-testkafka.zookeeper.connect=xxx:2181metrics.topic=xxxstream.parallelism=5kafka.sink.brokers=localhost:9092kafka.sink.topic=metric-teststream.checkpoint.interval=1000stream.checkpoint.enable=falsestream.sink.parallelism=5 目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令： 1bin/kafka-topics.sh --list --zookeeper localhost:2181 可以看到本地的 Kafka 是没有任何 topic 的，如果等下我们的程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明我的程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。 程序代码Main.java 12345678910111213141516public class Main &#123; public static void main(String[] args) throws Exception&#123; final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); data.addSink(new FlinkKafkaProducer011&lt;Metrics&gt;( parameterTool.get(\"kafka.sink.brokers\"), parameterTool.get(\"kafka.sink.topic\"), new MetricSchema() )).name(\"flink-connectors-kafka\") .setParallelism(parameterTool.getInt(\"stream.sink.parallelism\")); env.execute(\"flink learning connectors kafka\"); &#125;&#125; 运行结果启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来： 执行命令可以查看该 topic 的信息： 1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test 分析上面代码我们使用 Flink Kafka Producer 只传了三个参数：brokerList、topicId、serializationSchema（序列化） 其实也可以传入多个参数进去，现在有的参数用的是默认参数，因为这个内容比较多，后面可以抽出一篇文章单独来讲。 总结本篇文章写了 Flink 读取其他 Kafka 集群的数据，然后写入到本地的 Kafka 上。我在 Flink 这层没做什么数据转换，只是原样的将数据转发了下，如果你们有什么其他的需求，是可以在 Flink 这层将数据进行各种转换操作，比如这篇文章中的一些转换：《从0到1学习Flink》—— Flink Data transformation(转换)，然后将转换后的数据发到 Kafka 上去。 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 项目如何运行？","date":"2019-01-04T16:00:00.000Z","path":"2019/01/05/Flink-run/","text":"前言之前写了不少 Flink 文章了，也有不少 demo，但是文章写的时候都是在本地直接运行 Main 类的 main 方法，其实 Flink 是支持在 UI 上上传 Flink Job 的 jar 包，然后运行得。最开始在第一篇 《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 中其实提到过了 Flink 自带的 UI 界面，今天我们就来看看如何将我们的项目打包在这里发布运行。 准备编译打包项目代码就拿我之前的文章 《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 吧，代码地址是在 GitHub 仓库地址：https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6 ，如果感兴趣的可以直接拿来打包试试水。 我们在整个项目 （flink-learning）pom.xml 所在文件夹执行以下命令打包： 1mvn clean install 然后你会发现在 flink-learning-connectors-es6 的 target 目录下有 flink-learning-connectors-es6-1.0-SNAPSHOT.jar 。 启动 ES注意你的 Kafka 数据源和 ES 都已经启动好了, 清空了下 ES 目录下的 data 数据，为了就是查看是不是真的有数据存入进来了。 提交 jar 包将此文件提交到 Flinkserver 上，如下图： 点击下图红框中的”Upload”按钮： 如下图，选中刚刚上传的文件，填写类名，再点击”Submit”按钮即可启动 Job： 查看运行结果如下图，在 Overview 页面可见正在运行的任务： 你可以看到 Task Manager 中关于任务的 metric 数据、日志信息以及 Stdout 信息。 查看 Kibana ，此时 ES 中已经有数据了： 我们可以在 flink ui 界面上的 overview cancel 这个 job，那么可以看到 job 的日志： 总结本篇文章写了下如何将我们的 job 编译打包并提交到 Flink 自带到 Server UI 上面去运行，也算是对前面文章的一个补充，当然了，Flink job 不仅支持这种模式的运行，它还可以运行在 K8s，Mesos，等上面，等以后我接触到再写写。 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/05/Flink-run/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 写入数据到 ElasticSearch","date":"2018-12-29T16:00:00.000Z","path":"2018/12/30/Flink-ElasticSearch-Sink/","text":"前言前面 FLink 的文章中我们已经介绍了说 Flink 已经有很多自带的 Connector。 1、《从0到1学习Flink》—— Data Source 介绍 2、《从0到1学习Flink》—— Data Sink 介绍 其中包括了 Source 和 Sink 的，后面我也讲了下如何自定义自己的 Source 和 Sink。 那么今天要做的事情是啥呢？就是介绍一下 Flink 自带的 ElasticSearch Connector，我们今天就用他来做 Sink，将 Kafka 中的数据经过 Flink 处理后然后存储到 ElasticSearch。 准备安装 ElasticSearch，这里就忽略，自己找我以前的文章，建议安装 ElasticSearch 6.0 版本以上的，毕竟要跟上时代的节奏。 下面就讲解一下生产环境中如何使用 Elasticsearch Sink 以及一些注意点，及其内部实现机制。 Elasticsearch Sink添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 上面这依赖版本号请自己根据使用的版本对应改变下。 下面所有的代码都没有把 import 引入到这里来，如果需要查看更详细的代码，请查看我的 GitHub 仓库地址： https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6 这个 module 含有本文的所有代码实现，当然越写到后面自己可能会做一些抽象，所以如果有代码改变很正常，请直接查看全部项目代码。 ElasticSearchSinkUtil 工具类这个工具类是自己封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面文章还会再讲些其他的配置。 ElasticSearchSinkUtil.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ElasticSearchSinkUtil &#123; /** * es sink * * @param hosts es hosts * @param bulkFlushMaxActions bulk flush size * @param parallelism 并行数 * @param data 数据 * @param func * @param &lt;T&gt; */ public static &lt;T&gt; void addSink(List&lt;HttpHost&gt; hosts, int bulkFlushMaxActions, int parallelism, SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func) &#123; ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = new ElasticsearchSink.Builder&lt;&gt;(hosts, func); esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions); data.addSink(esSinkBuilder.build()).setParallelism(parallelism); &#125; /** * 解析配置文件的 es hosts * * @param hosts * @return * @throws MalformedURLException */ public static List&lt;HttpHost&gt; getEsAddresses(String hosts) throws MalformedURLException &#123; String[] hostList = hosts.split(\",\"); List&lt;HttpHost&gt; addresses = new ArrayList&lt;&gt;(); for (String host : hostList) &#123; if (host.startsWith(\"http\")) &#123; URL url = new URL(host); addresses.add(new HttpHost(url.getHost(), url.getPort())); &#125; else &#123; String[] parts = host.split(\":\", 2); if (parts.length &gt; 1) &#123; addresses.add(new HttpHost(parts[0], Integer.parseInt(parts[1]))); &#125; else &#123; throw new MalformedURLException(\"invalid elasticsearch hosts format\"); &#125; &#125; &#125; return addresses; &#125;&#125; Main 启动类Main.java 123456789101112131415161718192021222324252627public class Main &#123; public static void main(String[] args) throws Exception &#123; //获取所有参数 final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); //准备好环境 StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); //从kafka读取数据 DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); //从配置文件中读取 es 的地址 List&lt;HttpHost&gt; esAddresses = ElasticSearchSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS)); //从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒 int bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, 40); //从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积 int sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, 5); //自己再自带的 es sink 上一层封装了下 ElasticSearchSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data, (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123; requestIndexer.add(Requests.indexRequest() .index(ZHISHENG + \"_\" + metric.getName()) //es 索引名 .type(ZHISHENG) //es type .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); &#125;); env.execute(\"flink learning connectors es6\"); &#125;&#125; 配置文件配置都支持集群模式填写，注意用 , 分隔！ 12345678910kafka.brokers=localhost:9092kafka.group.id=zhisheng-metrics-group-testkafka.zookeeper.connect=localhost:2181metrics.topic=zhisheng-metricsstream.parallelism=5stream.checkpoint.interval=1000stream.checkpoint.enable=falseelasticsearch.hosts=localhost:9200elasticsearch.bulk.flush.max.actions=40stream.sink.parallelism=5 运行结果执行 Main 类的 main 方法，我们的程序是只打印 flink 的日志，没有打印存入的日志（因为我们这里没有打日志）： 所以看起来不知道我们的 sink 是否有用，数据是否从 kafka 读取出来后存入到 es 了。 你可以查看下本地起的 es 终端或者服务器的 es 日志就可以看到效果了。 es 日志如下： 上图是我本地 Mac 电脑终端的 es 日志，可以看到我们的索引了。 如果还不放心，你也可以在你的电脑装个 kibana，然后更加的直观查看下 es 的索引情况（或者直接敲 es 的命令） 我们用 kibana 查看存入 es 的索引如下： 程序执行了一会，存入 es 的数据量就很大了。 扩展配置上面代码已经可以实现你的大部分场景了，但是如果你的业务场景需要保证数据的完整性（不能出现丢数据的情况），那么就需要添加一些重试策略，因为在我们的生产环境中，很有可能会因为某些组件不稳定性导致各种问题，所以这里我们就要在数据存入失败的时候做重试操作，这里 flink 自带的 es sink 就支持了，常用的失败重试配置有: 123456789101112131、bulk.flush.backoff.enable 用来表示是否开启重试机制2、bulk.flush.backoff.type 重试策略，有两种：EXPONENTIAL 指数型（表示多次重试之间的时间间隔按照指数方式进行增长）、CONSTANT 常数型（表示多次重试之间的时间间隔为固定常数）3、bulk.flush.backoff.delay 进行重试的时间间隔4、bulk.flush.backoff.retries 失败重试的次数5、bulk.flush.max.actions: 批量写入时的最大写入条数6、bulk.flush.max.size.mb: 批量写入时的最大数据量7、bulk.flush.interval.ms: 批量写入的时间间隔，配置后则会按照该时间间隔严格执行，无视上面的两个批量写入配置 看下啦，就是如下这些配置了，如果你需要的话，可以在这个地方配置扩充了。 FailureHandler 失败处理器写入 ES 的时候会有这些情况会导致写入 ES 失败： 1、ES 集群队列满了，报如下错误 112:08:07.326 [I/O dispatcher 13] ERROR o.a.f.s.c.e.ElasticsearchSinkBase - Failed Elasticsearch item request: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@566c9379 on EsThreadPoolExecutor[name = node-1/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@f00b373[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 6277]]]] 是这样的，我电脑安装的 es 队列容量默认应该是 200，我没有修改过。我这里如果配置的 bulk flush size * 并发 sink 数量 这个值如果大于这个 queue capacity ，那么就很容易导致出现这种因为 es 队列满了而写入失败。 当然这里你也可以通过调大点 es 的队列。参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html 2、ES 集群某个节点挂了 这个就不用说了，肯定写入失败的。跟过源码可以发现 RestClient 类里的 performRequestAsync 方法一开始会随机的从集群中的某个节点进行写入数据，如果这台机器掉线，会进行重试在其他的机器上写入，那么当时写入的这台机器的请求就需要进行失败重试，否则就会把数据丢失！ 3、ES 集群某个节点的磁盘满了 这里说的磁盘满了，并不是磁盘真的就没有一点剩余空间的，是 es 会在写入的时候检查磁盘的使用情况，在 85% 的时候会打印日志警告。 这里我看了下源码如下图： 如果你想继续让 es 写入的话就需要去重新配一下 es 让它继续写入，或者你也可以清空些不必要的数据腾出磁盘空间来。 解决方法123456789101112131415161718192021222324DataStream&lt;String&gt; input = ...;input.addSink(new ElasticsearchSink&lt;&gt;( config, transportAddresses, new ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;, new ActionRequestFailureHandler() &#123; @Override void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throw Throwable &#123; if (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) &#123; // full queue; re-add document for indexing indexer.add(action); &#125; else if (ExceptionUtils.containsThrowable(failure, ElasticsearchParseException.class)) &#123; // malformed document; simply drop request without failing sink &#125; else &#123; // for all other failures, fail the sink // here the failure is simply rethrown, but users can also choose to throw custom exceptions throw failure; &#125; &#125;&#125;)); 如果仅仅只是想做失败重试，也可以直接使用官方提供的默认的 RetryRejectedExecutionFailureHandler ，该处理器会对 EsRejectedExecutionException 导致到失败写入做重试处理。如果你没有设置失败处理器(failure handler)，那么就会使用默认的 NoOpFailureHandler 来简单处理所有的异常。 总结本文写了 Flink connector es，将 Kafka 中的数据读取并存储到 ElasticSearch 中，文中讲了如何封装自带的 sink，然后一些扩展配置以及 FailureHandler 情况下要怎么处理。（这个问题可是线上很容易遇到的） 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、原理、实战、性能调优、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 中几种 Time 详解","date":"2018-12-10T16:00:00.000Z","path":"2018/12/11/Flink-time/","text":"前言Flink 在流程序中支持不同的 Time 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。 下面我们一起来看看这几个 Time： Processing TimeProcessing Time 是指事件被处理时机器的系统时间。 当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。 例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。 Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。 Event TimeEvent Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。 完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。 假设所有数据都已到达， Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。 请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。 Ingestion TimeIngestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。 Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。 与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。 在 Flink 中，，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。 说了这么多概念比较干涩，下面直接看图： 设定时间特性Flink DataStream 程序的第一部分通常是设置基本时间特性。 该设置定义了数据流源的行为方式（例如：它们是否将分配时间戳），以及像 KeyedStream.timeWindow(Time.seconds(30)) 这样的窗口操作应该使用上面哪种时间概念。 以下示例显示了一个 Flink 程序，该程序在每小时时间窗口中聚合事件。 123456789101112131415final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);// 其他// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);DataStream&lt;MyEvent&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));stream .keyBy( (event) -&gt; event.getUser() ) .timeWindow(Time.hours(1)) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); Event Time 和 Watermarks注意：Flink 实现了数据流模型中的许多技术。有关 Event Time 和 Watermarks 的详细介绍，请查看以下文章： https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101 https://research.google.com/pubs/archive/43864.pdf 支持 Event Time 的流处理器需要一种方法来衡量 Event Time 的进度。 例如，当 Event Time 超过一小时结束时，需要通知构建每小时窗口的窗口操作符，以便操作员可以关闭正在进行的窗口。 Event Time 可以独立于 Processing Time 进行。 例如，在一个程序中，操作员的当前 Event Time 可能略微落后于 Processing Time （考虑到接收事件的延迟），而两者都以相同的速度进行。另一方面，另一个流程序可能只需要几秒钟的时间就可以处理完 Kafka Topic 中数周的 Event Time 数据。 Flink 中用于衡量 Event Time 进度的机制是 Watermarks。 Watermarks 作为数据流的一部分流动并带有时间戳 t。 Watermark（t）声明 Event Time 已到达该流中的时间 t，这意味着流中不应再有具有时间戳 t’&lt;= t 的元素（即时间戳大于或等于水印的事件） 下图显示了带有(逻辑)时间戳和内联水印的事件流。在本例中，事件是按顺序排列的(相对于它们的时间戳)，这意味着水印只是流中的周期性标记。 Watermark 对于无序流是至关重要的，如下所示，其中事件不按时间戳排序。通常，Watermark 是一种声明，通过流中的该点，到达某个时间戳的所有事件都应该到达。一旦水印到达操作员，操作员就可以将其内部事件时间提前到水印的值。 平行流中的水印水印是在源函数处生成的，或直接在源函数之后生成的。源函数的每个并行子任务通常独立生成其水印。这些水印定义了特定并行源处的事件时间。 当水印通过流程序时，它们会提前到达操作人员处的事件时间。当一个操作符提前它的事件时间时，它为它的后续操作符在下游生成一个新的水印。 一些操作员消耗多个输入流; 例如，一个 union，或者跟随 keyBy（…）或 partition（…）函数的运算符。 这样的操作员当前事件时间是其输入流的事件时间的最小值。 由于其输入流更新其事件时间，因此操作员也是如此。 下图显示了流经并行流的事件和水印的示例，以及跟踪事件时间的运算符。 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f 参考https://github.com/zhisheng17/flink/blob/feature%2Fzhisheng_release_1.6/docs/dev/event_time.md 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/12/11/Flink-time/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 介绍Flink中的Stream Windows","date":"2018-12-07T16:00:00.000Z","path":"2018/12/08/Flink-Stream-Windows/","text":"前言目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语（例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” ）。 对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。 Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。 在这篇文章中，我们将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。 什么是 Windows？下面我们结合一个现实的例子来说明。 就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？ 假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图： 可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下： 这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。 因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？这个问题，就相当于一个定义了一个 Window（窗口），window 的界限是1分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图： 第一分钟的数量为8，第二分钟是22，第三分钟是27。。。这样，1个小时内会有60个window。 再考虑一种情况，每30秒统计一次过去1分钟的汽车数量之和： 此时，window 出现了重合。这样，1个小时内会有120个 window。 扩展一下，我们可以在某个地区，收集每一个红绿灯处汽车经过的数量，然后每个红绿灯处都做一次基于1分钟的window统计，即并行处理： 它有什么作用？通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。window 又可以分为基于时间（Time-based）的 window 以及基于数量（Count-based）的 window。 Flink 自带的 windowFlink DataStream API 提供了 Time 和 Count 的 window，同时增加了基于 Session 的 window。同时，由于某些特殊的需要，DataStream API 也提供了定制化的 window 操作，供用户自定义 window。 下面，主要介绍 Time-Based window 以及 Count-Based window，以及自定义的 window 操作，Session-Based Window 操作将会在后续的文章中讲到。 Time Windows正如命名那样，Time Windows 根据时间来聚合流数据。例如：一分钟的 tumbling time window 收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于一个函数。 在 Flink 中定义 tumbling time windows(翻滚时间窗口) 和 sliding time windows(滑动时间窗口) 非常简单： tumbling time windows(翻滚时间窗口) 输入一个时间参数 123data.keyBy(1) .timeWindow(Time.minutes(1)) //tumbling time window 每分钟统计一次数量和 .sum(1); sliding time windows(滑动时间窗口) 输入两个时间参数 123data.keyBy(1) .timeWindow(Time.minutes(1), Time.seconds(30)) //sliding time window 每隔 30s 统计过去一分钟的数量和 .sum(1); 有一点我们还没有讨论，即“收集一分钟的元素”的确切含义，它可以归结为一个问题，“流处理器如何解释时间?” Apache Flink 具有三个不同的时间概念，即 processing time, event time 和 ingestion time。 这里可以参考我下一篇文章： 《从0到1学习Flink》—— 介绍Flink中的Event Time、Processing Time和Ingestion Time Count WindowsApache Flink 还提供计数窗口功能。如果计数窗口设置的为 100 ，那么将会在窗口中收集 100 个事件，并在添加第 100 个元素时计算窗口的值。 在 Flink 的 DataStream API 中，tumbling count window 和 sliding count window 的定义如下: tumbling count window 输入一个时间参数 123data.keyBy(1) .countWindow(100) //统计每 100 个元素的数量之和 .sum(1); sliding count window 输入两个时间参数 123data.keyBy(1) .countWindow(100, 10) //每 10 个元素统计过去 100 个元素的数量之和 .sum(1); 解剖 Flink 的窗口机制Flink 的内置 time window 和 count window 已经覆盖了大多数应用场景，但是有时候也需要定制窗口逻辑，此时 Flink 的内置的 window 无法解决这些问题。为了还支持自定义 window 实现不同的逻辑，DataStream API 为其窗口机制提供了接口。 下图描述了 Flink 的窗口机制，并介绍了所涉及的组件： 到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。 每个窗口都拥有一个 Trigger(触发器)，该 Trigger(触发器) 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发(即、清除(删除窗口并丢弃其内容)，或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。 当 Trigger(触发器) 触发时，可以将窗口元素列表提供给可选的 Evictor，Evictor 可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素。然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。 计算函数接收 Evictor 过滤后的窗口元素，并计算窗口的一个或多个元素的结果。 DataStream API 接受不同类型的计算函数，包括预定义的聚合函数，如 sum（），min（），max（），以及 ReduceFunction，FoldFunction 或 WindowFunction。 这些是构成 Flink 窗口机制的组件。 接下来我们逐步演示如何使用 DataStream API 实现自定义窗口逻辑。 我们从 DataStream [IN] 类型的流开始，并使用 key 选择器函数对其分组，该函数将 key 相同类型的数据分组在一块。 12SingleOutputStreamOperator&lt;xxx&gt; data = env.addSource(...);data.keyBy() 如何自定义 Window？1、Window Assigner 负责将元素分配到不同的 window。 Window API 提供了自定义的 WindowAssigner 接口，我们可以实现 WindowAssigner 的 1public abstract Collection&lt;W&gt; assignWindows(T element, long timestamp) 方法。同时，对于基于 Count 的 window 而言，默认采用了 GlobalWindow 的 window assigner，例如： 1keyBy.window(GlobalWindows.create()) 2、Trigger Trigger 即触发器，定义何时或什么情况下移除 window 我们可以指定触发器来覆盖 WindowAssigner 提供的默认触发器。 请注意，指定的触发器不会添加其他触发条件，但会替换当前触发器。 3、Evictor（可选） 驱逐者，即保留上一 window 留下的某些元素 4、通过 apply WindowFunction 来返回 DataStream 类型数据。 利用 Flink 的内部窗口机制和 DataStream API 可以实现自定义的窗口逻辑，例如 session window。 结论对于现代流处理器来说，支持连续数据流上的各种类型的窗口是必不可少的。 Apache Flink 是一个具有强大功能集的流处理器，包括一个非常灵活的机制，可以在连续数据流上构建窗口。 Flink 为常见场景提供内置的窗口运算符，以及允许用户自定义窗口逻辑。 参考1、https://flink.apache.org/news/2015/12/04/Introducing-windows.html 2、https://blog.csdn.net/lmalds/article/details/51604501 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink Data transformation(转换)","date":"2018-11-03T16:00:00.000Z","path":"2018/11/04/Flink-Data-transformation/","text":"前言在第一篇介绍 Flink 的文章 《《从0到1学习Flink》—— Apache Flink 介绍》 中就说过 Flink 程序的结构 Flink 应用程序结构就是如上图所示： 1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。 在上四篇文章介绍了 Source 和 Sink： 1、《从0到1学习Flink》—— Data Source 介绍 2、《从0到1学习Flink》—— 如何自定义 Data Source ？ 3、《从0到1学习Flink》—— Data Sink 介绍 4、《从0到1学习Flink》—— 如何自定义 Data Sink ？ 那么这篇文章我们就来看下 Flink Data Transformation 吧，数据转换操作还是蛮多的，需要好好讲讲！ TransformationMap这是最简单的转换之一，其中输入是一个数据流，输出的也是一个数据流： 还是拿上一篇文章的案例来将数据进行 map 转换操作： 123456789101112SingleOutputStreamOperator&lt;Student&gt; map = student.map(new MapFunction&lt;Student, Student&gt;() &#123; @Override public Student map(Student value) throws Exception &#123; Student s1 = new Student(); s1.id = value.id; s1.name = value.name; s1.password = value.password; s1.age = value.age + 5; return s1; &#125;&#125;);map.print(); 将每个人的年龄都增加 5 岁，其他不变。 FlatMapFlatMap 采用一条记录并输出零个，一个或多个记录。 123456789SingleOutputStreamOperator&lt;Student&gt; flatMap = student.flatMap(new FlatMapFunction&lt;Student, Student&gt;() &#123; @Override public void flatMap(Student value, Collector&lt;Student&gt; out) throws Exception &#123; if (value.id % 2 == 0) &#123; out.collect(value); &#125; &#125;&#125;);flatMap.print(); 这里将 id 为偶数的聚集出来。 FilterFilter 函数根据条件判断出结果。 12345678910SingleOutputStreamOperator&lt;Student&gt; filter = student.filter(new FilterFunction&lt;Student&gt;() &#123; @Override public boolean filter(Student value) throws Exception &#123; if (value.id &gt; 95) &#123; return true; &#125; return false; &#125;&#125;);filter.print(); 这里将 id 大于 95 的过滤出来，然后打印出来。 KeyByKeyBy 在逻辑上是基于 key 对流进行分区。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流。 1234567KeyedStream&lt;Student, Integer&gt; keyBy = student.keyBy(new KeySelector&lt;Student, Integer&gt;() &#123; @Override public Integer getKey(Student value) throws Exception &#123; return value.age; &#125;&#125;);keyBy.print(); 上面对 student 的 age 做 KeyBy 操作分区 ReduceReduce 返回单个的结果值，并且 reduce 操作每处理一个元素总是创建一个新值。常用的方法有 average, sum, min, max, count，使用 reduce 方法都可实现。 1234567891011121314151617SingleOutputStreamOperator&lt;Student&gt; reduce = student.keyBy(new KeySelector&lt;Student, Integer&gt;() &#123; @Override public Integer getKey(Student value) throws Exception &#123; return value.age; &#125;&#125;).reduce(new ReduceFunction&lt;Student&gt;() &#123; @Override public Student reduce(Student value1, Student value2) throws Exception &#123; Student student1 = new Student(); student1.name = value1.name + value2.name; student1.id = (value1.id + value2.id) / 2; student1.password = value1.password + value2.password; student1.age = (value1.age + value2.age) / 2; return student1; &#125;&#125;);reduce.print(); 上面先将数据流进行 keyby 操作，因为执行 reduce 操作只能是 KeyedStream，然后将 student 对象的 age 做了一个求平均值的操作。 FoldFold 通过将最后一个文件夹流与当前记录组合来推出 KeyedStream。 它会发回数据流。 123456KeyedStream.fold(\"1\", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String accumulator, Integer value) throws Exception &#123; return accumulator + \"=\" + value; &#125;&#125;) AggregationsDataStream API 支持各种聚合，例如 min，max，sum 等。 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合。 12345678910KeyedStream.sum(0) KeyedStream.sum(\"key\") KeyedStream.min(0) KeyedStream.min(\"key\") KeyedStream.max(0) KeyedStream.max(\"key\") KeyedStream.minBy(0) KeyedStream.minBy(\"key\") KeyedStream.maxBy(0) KeyedStream.maxBy(\"key\") max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理。 WindowWindow 函数允许按时间或其他条件对现有 KeyedStream 进行分组。 以下是以 10 秒的时间窗口聚合： 1inputStream.keyBy(0).window(Time.seconds(10)); Flink 定义数据片段以便（可能）处理无限数据流。 这些切片称为窗口。 此切片有助于通过应用转换处理数据块。 要对流进行窗口化，我们需要分配一个可以进行分发的键和一个描述要对窗口化流执行哪些转换的函数 要将流切片到窗口，我们可以使用 Flink 自带的窗口分配器。 我们有选项，如 tumbling windows, sliding windows, global 和 session windows。 Flink 还允许您通过扩展 WindowAssginer 类来编写自定义窗口分配器。 这里先预留下篇文章来讲解这些不同的 windows 是如何工作的。 WindowAllwindowAll 函数允许对常规数据流进行分组。 通常，这是非并行数据转换，因为它在非分区数据流上运行。 与常规数据流功能类似，我们也有窗口数据流功能。 唯一的区别是它们处理窗口数据流。 所以窗口缩小就像 Reduce 函数一样，Window fold 就像 Fold 函数一样，并且还有聚合。 1inputStream.keyBy(0).windowAll(Time.seconds(10)); UnionUnion 函数将两个或多个数据流结合在一起。 这样就可以并行地组合数据流。 如果我们将一个流与自身组合，那么它会输出每个记录两次。 1inputStream.union(inputStream1, inputStream2, ...); Window join我们可以通过一些 key 将同一个 window 的两个数据流 join 起来。 1234inputStream.join(inputStream1) .where(0).equalTo(1) .window(Time.seconds(5)) .apply (new JoinFunction () &#123;...&#125;); 以上示例是在 5 秒的窗口中连接两个流，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性。 Split此功能根据条件将流拆分为两个或多个流。 当您获得混合流并且您可能希望单独处理每个数据流时，可以使用此方法。 12345678910111213SplitStream&lt;Integer&gt; split = inputStream.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) &#123; output.add(\"even\"); &#125; else &#123; output.add(\"odd\"); &#125; return output; &#125;&#125;); Select此功能允许您从拆分流中选择特定流。 1234SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select(\"even\"); DataStream&lt;Integer&gt; odd = split.select(\"odd\"); DataStream&lt;Integer&gt; all = split.select(\"even\",\"odd\"); ProjectProject 函数允许您从事件流中选择属性子集，并仅将所选元素发送到下一个处理流。 12DataStream&lt;Tuple4&lt;Integer, Double, String, String&gt;&gt; in = // [...] DataStream&lt;Tuple2&lt;String, String&gt;&gt; out = in.project(3,2); 上述函数从给定记录中选择属性号 2 和 3。 以下是示例输入和输出记录： 12(1,10.0,A,B)=&gt; (B,A)(2,20.0,C,D)=&gt; (D,C) 最后本文主要介绍了 Flink Data 的常用转换方式：Map、FlatMap、Filter、KeyBy、Reduce、Fold、Aggregations、Window、WindowAll、Union、Window Join、Split、Select、Project 等。并用了点简单的 demo 介绍了如何使用，具体在项目中该如何将数据流转换成我们想要的格式，还需要根据实际情况对待。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 如何自定义 Data Sink ？","date":"2018-10-30T16:00:00.000Z","path":"2018/10/31/flink-create-sink/","text":"前言前篇文章 《从0到1学习Flink》—— Data Sink 介绍 介绍了 Flink Data Sink，也介绍了 Flink 自带的 Sink，那么如何自定义自己的 Sink 呢？这篇文章将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去。 准备工作我们先来看下 Flink 从 Kafka topic 中获取数据的 demo，首先你需要安装好了 FLink 和 Kafka 。 运行启动 Flink、Zookepeer、Kafka， 好了，都启动了！ 数据库建表12345678DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(25) COLLATE utf8_bin DEFAULT NULL, `password` varchar(25) COLLATE utf8_bin DEFAULT NULL, `age` int(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 实体类Student.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.zhisheng.flink.model;/** * Desc: * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Student &#123; public int id; public String name; public String password; public int age; public Student() &#123; &#125; public Student(int id, String name, String password, int age) &#123; this.id = id; this.name = name; this.password = password; this.age = age; &#125; @Override public String toString() &#123; return \"Student&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", password='\" + password + '\\'' + \", age=\" + age + '&#125;'; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 工具类工具类往 kafka topic student 发送数据 12345678910111213141516171819202122232425262728293031323334353637383940import com.alibaba.fastjson.JSON;import com.zhisheng.flink.model.Metric;import com.zhisheng.flink.model.Student;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.HashMap;import java.util.Map;import java.util.Properties;/** * 往kafka中写数据 * 可以使用这个main函数进行测试一下 * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class KafkaUtils2 &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"student\"; //kafka topic 需要和 flink 程序用同一个 topic public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 1; i &lt;= 100; i++) &#123; Student student = new Student(i, \"zhisheng\" + i, \"password\" + i, 18 + i); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, JSON.toJSONString(student)); producer.send(record); System.out.println(\"发送数据: \" + JSON.toJSONString(student)); &#125; producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; writeToKafka(); &#125;&#125; SinkToMySQL该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法。在 invoke 方法中将数据插入到 MySQL 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.zhisheng.flink.sink;import com.zhisheng.flink.model.Student;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * Desc: * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class SinkToMySQL extends RichSinkFunction&lt;Student&gt; &#123; PreparedStatement ps; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); connection = getConnection(); String sql = \"insert into Student(id, name, password, age) values(?, ?, ?, ?);\"; ps = this.connection.prepareStatement(sql); &#125; @Override public void close() throws Exception &#123; super.close(); //关闭连接和释放资源 if (connection != null) &#123; connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * 每条数据的插入都要调用一次 invoke() 方法 * * @param value * @param context * @throws Exception */ @Override public void invoke(Student value, Context context) throws Exception &#123; //组装数据，执行插入操作 ps.setInt(1, value.getId()); ps.setString(2, value.getName()); ps.setString(3, value.getPassword()); ps.setInt(4, value.getAge()); ps.executeUpdate(); &#125; private static Connection getConnection() &#123; Connection con = null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8\", \"root\", \"root123456\"); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \"+ e.getMessage()); &#125; return con; &#125;&#125; Flink 程序这里的 source 是从 kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.zhisheng.flink;import com.alibaba.fastjson.JSON;import com.zhisheng.flink.model.Student;import com.zhisheng.flink.sink.SinkToMySQL;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.sink.PrintSinkFunction;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;import java.util.Properties;/** * Desc: * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Main3 &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; JSON.parseObject(string, Student.class)); //Fastjson 解析字符串成 student 对象 student.addSink(new SinkToMySQL()); //数据 sink 到 mysql env.execute(\"Flink add sink\"); &#125;&#125; 结果运行 Flink 程序，然后再运行 KafkaUtils2.java 工具类，这样就可以了。 如果数据插入成功了，那么我们查看下我们的数据库： 数据库中已经插入了 100 条我们从 Kafka 发送的数据了。证明我们的 SinkToMySQL 起作用了。是不是很简单？ 项目结构怕大家不知道我的项目结构，这里发个截图看下： 最后本文主要利用一个 demo，告诉大家如何自定义 Sink Function，将从 Kafka 的数据 Sink 到 MySQL 中，如果你项目中有其他的数据来源，你也可以换成对应的 Source，也有可能你的 Sink 是到其他的地方或者其他不同的方式，那么依旧是这个套路：继承 RichSinkFunction 抽象类，重写 invoke 方法。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 如何自定义 Data Source ？","date":"2018-10-29T16:00:00.000Z","path":"2018/10/30/flink-create-source/","text":"前言在 《从0到1学习Flink》—— Data Source 介绍 文章中，我给大家介绍了 Flink Data Source 以及简短的介绍了一下自定义 Data Source，这篇文章更详细的介绍下，并写一个 demo 出来让大家理解。 Flink Kafka source准备工作我们先来看下 Flink 从 Kafka topic 中获取数据的 demo，首先你需要安装好了 FLink 和 Kafka 。 运行启动 Flink、Zookepeer、Kafka， 好了，都启动了！ maven 依赖1234567891011121314151617181920212223242526272829303132333435363738&lt;!--flink java--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!--日志--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--flink kafka connector--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!--alibaba fastjson--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.51&lt;/version&gt;&lt;/dependency&gt; 测试发送数据到 kafka topic实体类，Metric.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.zhisheng.flink.model;import java.util.Map;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Metric &#123; public String name; public long timestamp; public Map&lt;String, Object&gt; fields; public Map&lt;String, String&gt; tags; public Metric() &#123; &#125; public Metric(String name, long timestamp, Map&lt;String, Object&gt; fields, Map&lt;String, String&gt; tags) &#123; this.name = name; this.timestamp = timestamp; this.fields = fields; this.tags = tags; &#125; @Override public String toString() &#123; return \"Metric&#123;\" + \"name='\" + name + '\\'' + \", timestamp='\" + timestamp + '\\'' + \", fields=\" + fields + \", tags=\" + tags + '&#125;'; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public long getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(long timestamp) &#123; this.timestamp = timestamp; &#125; public Map&lt;String, Object&gt; getFields() &#123; return fields; &#125; public void setFields(Map&lt;String, Object&gt; fields) &#123; this.fields = fields; &#125; public Map&lt;String, String&gt; getTags() &#123; return tags; &#125; public void setTags(Map&lt;String, String&gt; tags) &#123; this.tags = tags; &#125;&#125; 往 kafka 中写数据工具类：KafkaUtils.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import com.alibaba.fastjson.JSON;import com.zhisheng.flink.model.Metric;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.HashMap;import java.util.Map;import java.util.Properties;/** * 往kafka中写数据 * 可以使用这个main函数进行测试一下 * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class KafkaUtils &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"metric\"; // kafka topic，Flink 程序中需要和这个统一 public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //key 序列化 props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //value 序列化 KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); Metric metric = new Metric(); metric.setTimestamp(System.currentTimeMillis()); metric.setName(\"mem\"); Map&lt;String, String&gt; tags = new HashMap&lt;&gt;(); Map&lt;String, Object&gt; fields = new HashMap&lt;&gt;(); tags.put(\"cluster\", \"zhisheng\"); tags.put(\"host_ip\", \"101.147.022.106\"); fields.put(\"used_percent\", 90d); fields.put(\"max\", 27244873d); fields.put(\"used\", 17244873d); fields.put(\"init\", 27244873d); metric.setTags(tags); metric.setFields(fields); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, JSON.toJSONString(metric)); producer.send(record); System.out.println(\"发送数据: \" + JSON.toJSONString(metric)); producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; while (true) &#123; Thread.sleep(300); writeToKafka(); &#125; &#125;&#125; 运行： 如果出现如上图标记的，即代表能够不断的往 kafka 发送数据的。 Flink 程序Main.java 123456789101112131415161718192021222324252627282930313233343536package com.zhisheng.flink;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;import java.util.Properties;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Main &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); //key 反序列化 props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); //value 反序列化 DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"metric\", //kafka topic new SimpleStringSchema(), // String 序列化 props)).setParallelism(1); dataStreamSource.print(); //把从 kafka 读取到的数据打印在控制台 env.execute(\"Flink add data source\"); &#125;&#125; 运行起来： 看到没程序，Flink 程序控制台能够源源不断的打印数据呢。 自定义 Source上面就是 Flink 自带的 Kafka source，那么接下来就模仿着写一个从 MySQL 中读取数据的 Source。 首先 pom.xml 中添加 MySQL 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt;&lt;/dependency&gt; 数据库建表如下： 12345678DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(25) COLLATE utf8_bin DEFAULT NULL, `password` varchar(25) COLLATE utf8_bin DEFAULT NULL, `age` int(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 插入数据： 12INSERT INTO `student` VALUES ('1', 'zhisheng01', '123456', '18'), ('2', 'zhisheng02', '123', '17'), ('3', 'zhisheng03', '1234', '18'), ('4', 'zhisheng04', '12345', '16');COMMIT; 新建实体类：Student.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.zhisheng.flink.model;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Student &#123; public int id; public String name; public String password; public int age; public Student() &#123; &#125; public Student(int id, String name, String password, int age) &#123; this.id = id; this.name = name; this.password = password; this.age = age; &#125; @Override public String toString() &#123; return \"Student&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", password='\" + password + '\\'' + \", age=\" + age + '&#125;'; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 新建 Source 类 SourceFromMySQL.java，该类继承 RichSourceFunction ，实现里面的 open、close、run、cancel 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 package com.zhisheng.flink.source;import com.zhisheng.flink.model.Student;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.source.RichSourceFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class SourceFromMySQL extends RichSourceFunction&lt;Student&gt; &#123; PreparedStatement ps; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); connection = getConnection(); String sql = \"select * from Student;\"; ps = this.connection.prepareStatement(sql); &#125; /** * 程序执行完毕就可以进行，关闭连接和释放资源的动作了 * * @throws Exception */ @Override public void close() throws Exception &#123; super.close(); if (connection != null) &#123; //关闭连接和释放资源 connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * DataStream 调用一次 run() 方法用来获取数据 * * @param ctx * @throws Exception */ @Override public void run(SourceContext&lt;Student&gt; ctx) throws Exception &#123; ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) &#123; Student student = new Student( resultSet.getInt(\"id\"), resultSet.getString(\"name\").trim(), resultSet.getString(\"password\").trim(), resultSet.getInt(\"age\")); ctx.collect(student); &#125; &#125; @Override public void cancel() &#123; &#125; private static Connection getConnection() &#123; Connection con = null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8\", \"root\", \"root123456\"); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \"+ e.getMessage()); &#125; return con; &#125;&#125; Flink 程序： 12345678910111213141516171819package com.zhisheng.flink;import com.zhisheng.flink.source.SourceFromMySQL;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Main2 &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(new SourceFromMySQL()).print(); env.execute(\"Flink add data sourc\"); &#125;&#125; 运行 Flink 程序，控制台日志中可以看见打印的 student 信息。 RichSourceFunction从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，那么来了解一下： 一个抽象类，继承自 AbstractRichFunction。为实现一个 Rich SourceFunction 提供基础能力。该类的子类有三个，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。 MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。 MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。 ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。 最后本文主要讲了下 Flink 使用 Kafka Source 的使用，并提供了一个 demo 教大家如何自定义 Source，从 MySQL 中读取数据，当然你也可以从其他地方读取，实现自己的数据源 source。可能平时工作会比这个更复杂，需要大家灵活应对！ 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Data Sink 介绍","date":"2018-10-28T16:00:00.000Z","path":"2018/10/29/flink-sink/","text":"前言再上一篇文章中 《从0到1学习Flink》—— Data Source 介绍 讲解了 Flink Data Source ，那么这里就来讲讲 Flink Data Sink 吧。 首先 Sink 的意思是： 大概可以猜到了吧！Data sink 有点把数据存储下来（落库）的意思。 如上图，Source 就是数据的来源，中间的 Compute 其实就是 Flink 干的事情，可以做一系列的操作，操作完后就把计算后的数据结果 Sink 到某个地方。（可以是 MySQL、ElasticSearch、Kafka、Cassandra 等）。这里我说下自己目前做告警这块就是把 Compute 计算后的结果 Sink 直接告警出来了（发送告警消息到钉钉群、邮件、短信等），这个 sink 的意思也不一定非得说成要把数据存储到某个地方去。其实官网用的 Connector 来形容要去的地方更合适，这个 Connector 可以有 MySQL、ElasticSearch、Kafka、Cassandra RabbitMQ 等。 Flink Data Sink前面文章 《从0到1学习Flink》—— Data Source 介绍 介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。 看下源码有哪些呢？ 可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。 SinkFunction 从上图可以看到 SinkFunction 接口有 invoke 方法，它有一个 RichSinkFunction 抽象类。 上面的那些自带的 Sink 可以看到都是继承了 RichSinkFunction 抽象类，实现了其中的方法，那么我们要是自己定义自己的 Sink 的话其实也是要按照这个套路来做的。 这里就拿个较为简单的 PrintSinkFunction 源码来讲下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@PublicEvolvingpublic class PrintSinkFunction&lt;IN&gt; extends RichSinkFunction&lt;IN&gt; &#123; private static final long serialVersionUID = 1L; private static final boolean STD_OUT = false; private static final boolean STD_ERR = true; private boolean target; private transient PrintStream stream; private transient String prefix; /** * Instantiates a print sink function that prints to standard out. */ public PrintSinkFunction() &#123;&#125; /** * Instantiates a print sink function that prints to standard out. * * @param stdErr True, if the format should print to standard error instead of standard out. */ public PrintSinkFunction(boolean stdErr) &#123; target = stdErr; &#125; public void setTargetToStandardOut() &#123; target = STD_OUT; &#125; public void setTargetToStandardErr() &#123; target = STD_ERR; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext(); // get the target stream stream = target == STD_OUT ? System.out : System.err; // set the prefix if we have a &gt;1 parallelism prefix = (context.getNumberOfParallelSubtasks() &gt; 1) ? ((context.getIndexOfThisSubtask() + 1) + \"&gt; \") : null; &#125; @Override public void invoke(IN record) &#123; if (prefix != null) &#123; stream.println(prefix + record.toString()); &#125; else &#123; stream.println(record.toString()); &#125; &#125; @Override public void close() &#123; this.stream = null; this.prefix = null; &#125; @Override public String toString() &#123; return \"Print to \" + (target == STD_OUT ? \"System.out\" : \"System.err\"); &#125;&#125; 可以看到它就是实现了 RichSinkFunction 抽象类，然后实现了 invoke 方法，这里 invoke 方法就是把记录打印出来了就是，没做其他的额外操作。 如何使用？1SingleOutputStreamOperator.addSink(new PrintSinkFunction&lt;&gt;(); 这样就可以了，如果是其他的 Sink Function 的话需要换成对应的。 使用这个 Function 其效果就是打印从 Source 过来的数据，和直接 Source.print() 效果一样。 下篇文章我们将讲解下如何自定义自己的 Sink Function，并使用一个 demo 来教大家，让大家知道这个套路，且能够在自己工作中自定义自己需要的 Sink Function，来完成自己的工作需求。 最后本文主要讲了下 Flink 的 Data Sink，并介绍了常见的 Data Sink，也看了下源码的 SinkFunction，介绍了一个简单的 Function 使用, 告诉了大家自定义 Sink Function 的套路，下篇文章带大家写个。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/29/flink-sink/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Data Source 介绍","date":"2018-10-27T16:00:00.000Z","path":"2018/10/28/flink-sources/","text":"前言Data Sources 是什么呢？就字面意思其实就可以知道：数据来源。 Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即实时的处理些实时数据流，实时的产生数据流结果，只要数据源源不断的过来，Flink 就能够一直计算下去，这个 Data Sources 就是数据的来源地。 Flink 中你可以使用 StreamExecutionEnvironment.addSource(sourceFunction) 来为你的程序添加数据来源。 Flink 已经提供了若干实现好了的 source functions，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source， FlinkStreamExecutionEnvironment 中可以使用以下几个已实现的 stream sources， 总的来说可以分为下面几大类： 基于集合1、fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。 2、fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。 3、fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。 12345678StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;Event&gt; input = env.fromElements( new Event(1, \"barfoo\", 1.0), new Event(2, \"start\", 2.0), new Event(3, \"foobar\", 3.0), ...); 4、fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。 5、generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。 基于文件1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。 123final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;String&gt; text = env.readTextFile(\"file:///path/to/file\"); 2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。 3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS_ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。 12345final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;MyEvent&gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo); 实现: 在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。 重要注意： 如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。 如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。 基于 Socket：socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。 12345678StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env .socketTextStream(\"localhost\", 9999) // 监听 localhost 的 9999 端口过来的数据 .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); 这个在 《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 文章里用的就是基于 Socket 的 Word Count 程序。 自定义：addSource - 添加一个新的 source function。例如，你可以 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 以从 Apache Kafka 读取数据 说下上面几种的特点吧： 1、基于集合：有界数据集，更偏向于本地测试用 2、基于文件：适合监听文件修改并读取其内容 3、基于 Socket：监听主机的 host port，从 Socket 中获取数据 4、自定义 addSource：大多数的场景数据都是无界的，会源源不断的过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。 123456789StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;KafkaEvent&gt; input = env .addSource( new FlinkKafkaConsumer011&lt;&gt;( parameterTool.getRequired(\"input-topic\"), //从参数中获取传进来的 topic new KafkaEventSchema(), parameterTool.getProperties()) .assignTimestampsAndWatermarks(new CustomWatermarkExtractor())); Flink 目前支持如下图里面常见的 Source： 如果你想自己自定义自己的 Source 呢？ 那么你就需要去了解一下 SourceFunction 接口了，它是所有 stream source 的根接口，它继承自一个标记接口（空接口）Function。 SourceFunction 定义了两个接口方法： 1、run ： 启动一个 source，即对接一个外部数据源然后 emit 元素形成 stream（大部分情况下会通过在该方法里运行一个 while 循环的形式来产生 stream）。 2、cancel ： 取消一个 source，也即将 run 中的循环 emit 元素的行为终止。 正常情况下，一个 SourceFunction 实现这两个接口方法就可以了。其实这两个接口方法也固定了一种实现模板。 比如，实现一个 XXXSourceFunction，那么大致的模板是这样的：(直接拿 FLink 源码的实例给你看看) 最后本文主要讲了下 Flink 的常见 Source 有哪些并且简单的提了下如何自定义 Source。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/28/flink-sources/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 配置文件详解","date":"2018-10-26T16:00:00.000Z","path":"2018/10/27/flink-config/","text":"前面文章我们已经知道 Flink 是什么东西了，安装好 Flink 后，我们再来看下安装路径下的配置文件吧。 安装目录下主要有 flink-conf.yaml 配置、日志的配置文件、zk 配置、Flink SQL Client 配置。 flink-conf.yaml基础配置123456789101112131415161718192021# jobManager 的IP地址jobmanager.rpc.address: localhost# JobManager 的端口号jobmanager.rpc.port: 6123# JobManager JVM heap 内存大小jobmanager.heap.size: 1024m# TaskManager JVM heap 内存大小taskmanager.heap.size: 1024m# 每个 TaskManager 提供的任务 slots 数量大小taskmanager.numberOfTaskSlots: 1# 程序默认并行计算的个数parallelism.default: 1# 文件系统来源# fs.default-scheme 高可用性配置1234567891011# 可以选择 &apos;NONE&apos; 或者 &apos;zookeeper&apos;.# high-availability: zookeeper# 文件系统路径，让 Flink 在高可用性设置中持久保存元数据# high-availability.storageDir: hdfs:///flink/ha/# zookeeper 集群中仲裁者的机器 ip 和 port 端口号# high-availability.zookeeper.quorum: localhost:2181# 默认是 open，如果 zookeeper security 启用了该值会更改成 creator# high-availability.zookeeper.client.acl: open 容错和检查点 配置1234567891011# 用于存储和检查点状态# state.backend: filesystem# 存储检查点的数据文件和元数据的默认目录# state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints# savepoints 的默认目标目录(可选)# state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints# 用于启用/禁用增量 checkpoints 的标志# state.backend.incremental: false web 前端配置12345678# 基于 Web 的运行时监视器侦听的地址.#jobmanager.web.address: 0.0.0.0# Web 的运行时监视器端口rest.port: 8081# 是否从基于 Web 的 jobmanager 启用作业提交# jobmanager.web.submit.enable: false 高级配置123456789101112131415# io.tmp.dirs: /tmp# 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存# taskmanager.memory.preallocate: false# 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类# classloader.resolve-order: child-first# 用于网络缓冲区的 JVM 内存的分数。 这决定了 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度。 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值。 另请注意，“taskmanager.network.memory.min”和“taskmanager.network.memory.max”可能会覆盖此分数# taskmanager.network.memory.fraction: 0.1# taskmanager.network.memory.min: 67108864# taskmanager.network.memory.max: 1073741824 Flink 集群安全配置1234567891011# 指示是否从 Kerberos ticket 缓存中读取# security.kerberos.login.use-ticket-cache: true# 包含用户凭据的 Kerberos 密钥表文件的绝对路径# security.kerberos.login.keytab: /path/to/kerberos/keytab# 与 keytab 关联的 Kerberos 主体名称# security.kerberos.login.principal: flink-user# 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）# security.kerberos.login.contexts: Client,KafkaClient Zookeeper 安全配置12345# 覆盖以下配置以提供自定义 ZK 服务名称# zookeeper.sasl.service-name: zookeeper# 该配置必须匹配 &quot;security.kerberos.login.contexts&quot; 中的列表（含有一个）# zookeeper.sasl.login-context-name: Client HistoryServer12345678910111213141516# 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer# 将已完成的作业上传到的目录# jobmanager.archive.fs.dir: hdfs:///completed-jobs/# 基于 Web 的 HistoryServer 的地址# historyserver.web.address: 0.0.0.0# 基于 Web 的 HistoryServer 的端口号# historyserver.web.port: 8082# 以逗号分隔的目录列表，用于监视已完成的作业# historyserver.archive.fs.dir: hdfs:///completed-jobs/# 刷新受监控目录的时间间隔（以毫秒为单位）# historyserver.archive.fs.refresh-interval: 10000 查看下另外两个配置 slaves / master 2、slaves里面是每个 worker 节点的 IP/Hostname，每一个 worker 结点之后都会运行一个 TaskManager，一个一行。 1localhost 3、mastershost:port 1localhost:8081 4、zoo.cfg123456789101112131415161718# 每个 tick 的毫秒数tickTime=2000# 初始同步阶段可以采用的 tick 数initLimit=10# 在发送请求和获取确认之间可以传递的 tick 数syncLimit=5# 存储快照的目录# dataDir=/tmp/zookeeper# 客户端将连接的端口clientPort=2181# ZooKeeper quorum peersserver.1=localhost:2888:3888# server.2=host:peer-port:leader-port 5、日志配置Flink 在不同平台下运行的日志文件 1234567log4j-cli.propertieslog4j-console.propertieslog4j-yarn-session.propertieslog4j.propertieslogback-console.xmllogback-yarn.xmllogback.xml sql-client-defaults.yaml12345678910111213141516171819202122232425execution: # 'batch' or 'streaming' execution type: streaming # allow 'event-time' or only 'processing-time' in sources time-characteristic: event-time # interval in ms for emitting periodic watermarks periodic-watermarks-interval: 200 # 'changelog' or 'table' presentation of results result-mode: changelog # parallelism of the program parallelism: 1 # maximum parallelism max-parallelism: 128 # minimum idle state retention in ms min-idle-state-retention: 0 # maximum idle state retention in ms max-idle-state-retention: 0 deployment: # general cluster communication timeout in ms response-timeout: 5000 # (optional) address from cluster to gateway gateway-address: \"\" # (optional) port from cluster to gateway gateway-port: 0 Flink sql client ：你可以从官网这里了解 https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html 总结本文拿安装目录文件下的配置文件讲解了下 Flink 目录下的所有配置。 你也可以通过官网这里学习更多：https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html 关注我本篇文章地址是：http://www.54tianzhisheng.cn/2018/10/27/flink-config/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Apache Flink 介绍","date":"2018-10-12T16:00:00.000Z","path":"2018/10/13/flink-introduction/","text":"前言Flink 是一种流式计算框架，为什么我会接触到 Flink 呢？ 因为我目前在负责的是监控平台的告警部分，负责采集到的监控数据会直接往 kafka 里塞，然后告警这边需要从 kafka topic 里面实时读取到监控数据，并将读取到的监控数据做一些 聚合/转换/计算 等操作，然后将计算后的结果与告警规则的阈值进行比较，然后做出相应的告警措施（钉钉群、邮件、短信、电话等）。画了个简单的图如下： 目前告警这块的架构是这样的结构，刚进公司那会的时候，架构是所有的监控数据直接存在 ElasticSearch 中，然后我们告警是去 ElasticSearch 中搜索我们监控指标需要的数据，幸好 ElasticSearch 的搜索能力够强大。但是你有没有发现一个问题，就是所有的监控数据从采集、采集后的数据做一些 计算/转换/聚合、再通过 Kafka 消息队列、再存进 ElasticSearch 中，再而去 ElasticSearch 中查找我们的监控数据，然后做出告警策略。整个流程对监控来说看起来很按照常理，但是对于告警来说，如果中间某个环节出了问题，比如 Kafka 消息队列延迟、监控数据存到 ElasticSearch 中写入时间较长、你的查询姿势写的不对等原因，这都将导致告警从 ElasticSearch 查到的数据是有延迟的。也许是 30 秒、一分钟、或者更长，这样对于告警来说这无疑将导致告警的消息没有任何的意义。 为什么这么说呢？为什么需要监控告警平台呢？无非就是希望我们能够尽早的发现问题，把问题给告警出来，这样开发和运维人员才能够及时的处理解决好线上的问题，以免给公司造成巨大的损失。 更何况现在还有更多的公司在做那种提前预警呢！这种又该如何做呢？需要用大数据和机器学习的技术去分析周期性的历史数据，然后根据这些数据可以整理出来某些监控指标的一些周期性（一天/七天/一月/一季度/一年）走势图，这样就大概可以绘图出来。然后根据这个走势图，可以将当前时间点的监控指标的数据使用量和走势图进行对比，在快要达到我们告警规则的阈值时，这时就可以提前告一个预警出来，让运维提前知道预警，然后提前查找问题，这样就能够提早发现问题所在，避免损失，将损失降到最小！当然，这种也是我打算做的，应该可以学到不少东西的。 于是乎，我现在就在接触流式计算框架 Flink，类似的还有常用的 Spark 等。 自己也接触了 Flink 一段时间了，这块中文资料目前书籍是只有一本很薄的，英文书籍也是三本不超过。 我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以关注我的公众号：zhisheng，然后回复关键字：Flink 即可无条件获取到。 另外这里也推荐一些博客可以看看： 1、官网：https://flink.apache.org/ 2、GitHub: https://github.com/apache/flink 3、https://blog.csdn.net/column/details/apacheflink.html 4、https://blog.csdn.net/lmalds/article/category/6263085 5、http://wuchong.me/ 6、https://blog.csdn.net/liguohuabigdata/article/category/7279020 下面的介绍可能也有不少参考以上所有的资料，感谢他们！在介绍 Flink 前，我们先看看 数据集类型 和 数据运算模型 的种类。 数据集类型有哪些呢： 无穷数据集：无穷的持续集成的数据集合 有界数据集：有限不会改变的数据集合 那么那些常见的无穷数据集有哪些呢？ 用户与客户端的实时交互数据 应用实时产生的日志 金融市场的实时交易记录 … 数据运算模型有哪些呢： 流式：只要数据一直在产生，计算就持续地进行 批处理：在预先定义的时间内运行计算，当完成时释放计算机资源 Flink 它可以处理有界的数据集、也可以处理无界的数据集、它可以流式的处理数据、也可以批量的处理数据。 Flink 是什么 ？ 上面三张图转自 云邪 成都站 《Flink 技术介绍与未来展望》，侵删。 从下至上，Flink 整体结构 从下至上： 1、部署：Flink 支持本地运行、能在独立集群或者在被 YARN 或 Mesos 管理的集群上运行， 也能部署在云上。 2、运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。 3、API：DataStream、DataSet、Table、SQL API。 4、扩展库：Flink 还包括用于复杂事件处理，机器学习，图形处理和 Apache Storm 兼容性的专用代码库。 Flink 数据流编程模型抽象级别Flink 提供了不同的抽象级别以开发流式或批处理应用。 最底层提供了有状态流。它将通过 过程函数（Process Function）嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。 DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换 / 计算。 Table API 是以 表 为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。 你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。 Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。 Flink 程序与数据流结构 Flink 应用程序结构就是如上图所示： 1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。 为什么选择 Flink？Flink 是一个开源的分布式流式处理框架： ①提供准确的结果，甚至在出现无序或者延迟加载的数据的情况下。 ②它是状态化的容错的，同时在维护一次完整的的应用状态时，能无缝修复错误。 ③大规模运行，在上千个节点运行时有很好的吞吐量和低延迟。 更早的时候，我们讨论了数据集类型（有界 vs 无穷）和运算模型（批处理 vs 流式）的匹配。Flink 的流式计算模型启用了很多功能特性，如状态管理，处理无序数据，灵活的视窗，这些功能对于得出无穷数据集的精确结果是很重要的。 Flink 保证状态化计算强一致性。”状态化“意味着应用可以维护随着时间推移已经产生的数据聚合或者，并且 Filnk 的检查点机制在一次失败的事件中一个应用状态的强一致性。 Flink 支持流式计算和带有事件时间语义的视窗。事件时间机制使得那些事件无序到达甚至延迟到达的数据流能够计算出精确的结果。 除了提供数据驱动的视窗外，Flink 还支持基于时间，计数，session 等的灵活视窗。视窗能够用灵活的触发条件定制化从而达到对复杂的流传输模式的支持。Flink 的视窗使得模拟真实的创建数据的环境成为可能。 Flink 的容错能力是轻量级的，允许系统保持高并发，同时在相同时间内提供强一致性保证。Flink 以零数据丢失的方式从故障中恢复，但没有考虑可靠性和延迟之间的折衷。 Flink 能满足高并发和低延迟（计算大量数据很快）。下图显示了 Apache Flink 与 Apache Storm 在完成流数据清洗的分布式任务的性能对比。 Flink 保存点提供了一个状态化的版本机制，使得能以无丢失状态和最短停机时间的方式更新应用或者回退历史数据。 Flink 被设计成能用上千个点在大规模集群上运行。除了支持独立集群部署外，Flink 还支持 YARN 和Mesos 方式部署。 Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为1； 自己的内存管理 Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。 丰富的库 Flink 拥有丰富的库来进行机器学习，图形处理，关系数据处理等。 由于其架构，很容易执行复杂的事件处理和警报。 分布式运行flink 作业提交架构流程可见下图： 1、Program Code：我们编写的 Flink 应用程序代码 2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户 3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件 4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 最后本文主要讲了我接触到 Flink 的缘由，然后从数据集类型和数据运算模型开始讲起，接着介绍了下 Flink 是什么、Flink 的整体架构、提供的 API、Flink 的优点所在以及 Flink 的分布式作业运行的方式。水文一篇，希望你能够对 Flink 稍微有一点概念了。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门","date":"2018-09-17T16:00:00.000Z","path":"2018/09/18/flink-install/","text":"准备工作1、安装查看 Java 的版本号，推荐使用 Java 8。 安装 Flink2、在 Mac OS X 上安装 Flink 是非常方便的。推荐通过 homebrew 来安装。 1brew install apache-flink 3、检查安装： 1flink --version 结果： 1Version: 1.6.0, Commit ID: ff472b4 4、启动 flink 1234zhisheng@zhisheng  /usr/local/Cellar/apache-flink/1.6.0/libexec/bin  ./start-cluster.shStarting cluster.Starting standalonesession daemon on host zhisheng.Starting taskexecutor daemon on host zhisheng. 接着就可以进入 web 页面(http://localhost:8081/) 查看 demo1、新建一个 maven 项目 创建一个 SocketTextStreamWordCount 文件，加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.zhisheng.flink;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;/** * Created by zhisheng_tian on 2018/9/18 */public class SocketTextStreamWordCount &#123; public static void main(String[] args) throws Exception &#123; //参数检查 if (args.length != 2) &#123; System.err.println(\"USAGE:\\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;\"); return; &#125; String hostname = args[0]; Integer port = Integer.parseInt(args[1]); // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据 DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port); //计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new LineSplitter()) .keyBy(0) .sum(1); sum.print(); env.execute(\"Java WordCount from SocketTextStream Example\"); &#125; public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; String[] tokens = s.toLowerCase().split(\"\\\\W+\"); for (String token: tokens) &#123; if (token.length() &gt; 0) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 接着进入工程目录，使用以下命令打包。 1mvn clean package -Dmaven.test.skip=true 然后我们开启监听 9000 端口: 1nc -l 9000 最后进入 flink 安装目录 bin 下执行以下命令跑程序： 1flink run -c com.zhisheng.flink.SocketTextStreamWordCount /Users/zhisheng/IdeaProjects/flink/word-count/target/original-word-count-1.0-SNAPSHOT.jar 127.0.0.1 9000 注意换成你自己项目的路径。 执行完上述命令后，我们可以在 webUI 中看到正在运行的程序： 我们可以在 nc 监听端口中输入 text，比如： 然后我们通过 tail 命令看一下输出的 log 文件，来观察统计结果。进入目录 apache-flink/1.6.0/libexec/log，执行以下命令: 1tail -f flink-zhisheng-taskexecutor-0-zhisheng.out 注意：切换成你自己的路径和查看自己的目录。 总结本文描述了如何在 Mac 电脑上安装 Flink，及运行它。接着通过一个简单的 Flink 程序来介绍如何构建及运行Flink 程序。 关注我转载请注明地址：http://www.54tianzhisheng.cn/2018/09/18/flink-install 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号（zhisheng）了，你可以回复关键字：Flink 即可无条件获取到。另外也可以加我微信 你可以加我的微信：yuanblog_tzs，探讨技术！ 更多私密资料请加入知识星球！ 专栏介绍扫码下面专栏二维码可以订阅该专栏 首发地址：http://www.54tianzhisheng.cn/2019/11/15/flink-in-action/ 专栏地址：https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Go 并发——实现协程同步的几种方式","date":"2018-08-29T16:00:00.000Z","path":"2018/08/30/go-sync/","text":"前言Java 中有一系列的线程同步的方法，go 里面有 goroutine（协程），先看下下面的代码执行的结果是什么呢？ 123456789101112131415package mainimport ( \"fmt\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;()&#125; 执行以上代码很可能看不到输出。 因为有可能这两个协程还没得到执行，主协程就已经结束了，而主协程结束时会结束所有其他协程，所以导致代码运行的结果什么都没有。 估计不少新接触 go 的童鞋都会对此郁闷😒，可能会问那么该如何等待主协程中创建的协程执行完毕之后再结束主协程呢？ 下面说几种可以解决的方法： Sleep 一段时间在 main 方法退出之前 sleep 一段时间就可能会出现结果了，如下代码： 123456789101112131415161718package mainimport ( \"fmt\" \"time\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;() time.Sleep(time.Second * 1) // 睡眠1秒，等待上面两个协程结束&#125; 这两个简单的协程执行消耗的时间很短的，所以你会发现现在就有结果出现了。 12Goroutine 1Goroutine 2 为什么上面我要说 “可能会出现” ？ 因为 sleep 这个时间目前是设置的 1s，如果我这两个协程里面执行了很复杂的逻辑操作（时间大于 1s），那么就会发现依旧也是无结果打印出来的。 那么就可以发现这种方式得到问题所在了：我们无法确定需要睡眠多久 上面那种方式有问题，go 里面其实也可以用管道来实现同步的。 管道实现同步那么用管道怎么实现同步呢？show code： 123456789101112131415161718192021222324252627282930package mainimport ( \"fmt\")func main() &#123; ch := make(chan struct&#123;&#125;) count := 2 // count 表示活动的协程个数 go func() &#123; fmt.Println(\"Goroutine 1\") ch &lt;- struct&#123;&#125;&#123;&#125; // 协程结束，发出信号 &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") ch &lt;- struct&#123;&#125;&#123;&#125; // 协程结束，发出信号 &#125;() for range ch &#123; // 每次从ch中接收数据，表明一个活动的协程结束 count-- // 当所有活动的协程都结束时，关闭管道 if count == 0 &#123; close(ch) &#125; &#125;&#125; 这种方式是一种比较完美的解决方案， goroutine / channel 它们也是在 go 里面经常搭配在一起的一对。 sync.WaitGroup其实 go 里面也提供了更简单的方式 —— 使用 sync.WaitGroup。 WaitGroup 顾名思义，就是用来等待一组操作完成的。WaitGroup 内部实现了一个计数器，用来记录未完成的操作个数，它提供了三个方法： Add() 用来添加计数 Done() 用来在操作结束时调用，使计数减一 Wait() 用来等待所有的操作结束，即计数变为 0，该函数会在计数不为 0 时等待，在计数为 0 时立即返回 继续 show code： 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"sync\")func main() &#123; var wg sync.WaitGroup wg.Add(2) // 因为有两个动作，所以增加2个计数 go func() &#123; fmt.Println(\"Goroutine 1\") wg.Done() // 操作完成，减少一个计数 &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") wg.Done() // 操作完成，减少一个计数 &#125;() wg.Wait() // 等待，直到计数为0&#125; 你会发现也是可以看到运行结果的，是不是发现这种方式是很简单的。 总结多看别人写的代码；多想想为啥要这样写；多查自己不理解的地方；多写 demo 测试；多写文章总结。 关注我 本文地址为：http://www.54tianzhisheng.cn/2018/08/30/go-sync/ ，转载请注明原文出处！","tags":[{"name":"GO","slug":"GO","permalink":"http://www.54tianzhisheng.cn/tags/GO/"}]},{"title":"教你如何在 IDEA 远程 Debug ElasticSearch","date":"2018-08-13T16:00:00.000Z","path":"2018/08/14/idea-remote-debug-elasticsearch/","text":"前提之前在源码阅读环境搭建文章中写过我遇到的一个问题迟迟没有解决，也一直困扰着我。问题如下，在启动的时候解决掉其他异常和报错后，最后剩下这个错误一直解决不了： 12345678910111213141516171819202122[2018-08-01T09:44:27,370][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?] at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?] at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?] at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?] at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?] ... 15 more 网上的解决办法也试了很多遍，包括自己也在 GitHub issue 提问了，也没能解决。然后后面自己分享文章在掘金也发现有人和我有同样的问题。 下面讲讲另一种可以让你继续看源码的方法。 远程 Debug前提条件是你之前已经把项目导入进 IDEA 了，如果你还没了解，请看之前的文章，这里不重复了。 启动一个实例在你 git 拉取下的代码，切换你要阅读的分支代码后，执行下面这条命令启动一个 debug 的实例： 1./gradlew run --debug-jvm 启动等会后，就可以看到启动好后的端口号为 8000 了。 配置 IDEA新建一个远程的 debug： 配置如下图： 接下来点击 OK 就好了。 然后点击下面的 debug 图标： 启动后如下： 这时就可以发现是可以把整个流程全启动了，也不会报什么错误！ 流程全启动后，你会发现终端的日志都打印出来了（注意：这时不是打印在你的 IDEA 控制台） 总结遇到问题，多思考，多搜索，多想办法解决！这样才能够不断的提升你解决问题的能力！ 关注我最后转载请务必注明文章出处为： http://www.54tianzhisheng.cn/2018/08/14/idea-remote-debug-elasticsearch/ 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程（下）","date":"2018-08-11T16:00:00.000Z","path":"2018/08/12/es-code03/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/12/es-code03/ 前提上篇文章写完了 ES 流程启动的一部分，main 方法都入口，以及创建 Elasticsearch 运行的必须环境以及相关配置，接着就是创建该环境的节点了。 Node 的创建看下新建节点的代码：(代码比较多，这里是比较关键的地方，我就把注释直接写在代码上面了，实在不好拆开这段代码，300 多行代码) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278public Node(Environment environment) &#123; this(environment, Collections.emptyList()); //执行下面的代码 &#125;protected Node(final Environment environment, Collection&lt;Class&lt;? extends Plugin&gt;&gt; classpathPlugins) &#123; final List&lt;Closeable&gt; resourcesToClose = new ArrayList&lt;&gt;(); // register everything we need to release in the case of an error boolean success = false; &#123;// use temp logger just to say we are starting. we can't use it later on because the node name might not be set Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(environment.settings())); logger.info(\"initializing ...\"); &#125; try &#123; originalSettings = environment.settings(); Settings tmpSettings = Settings.builder().put(environment.settings()) .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build();// create the node environment as soon as possible, to recover the node id and enable logging try &#123; nodeEnvironment = new NodeEnvironment(tmpSettings, environment); //1、创建节点环境,比如节点名称,节点ID,分片信息,存储元,以及分配内存准备给节点使用 resourcesToClose.add(nodeEnvironment); &#125; catch (IOException ex) &#123; throw new IllegalStateException(\"Failed to create node environment\", ex); &#125; final boolean hadPredefinedNodeName = NODE_NAME_SETTING.exists(tmpSettings); final String nodeId = nodeEnvironment.nodeId(); tmpSettings = addNodeNameIfNeeded(tmpSettings, nodeId); final Logger logger = Loggers.getLogger(Node.class, tmpSettings);// this must be captured after the node name is possibly added to the settings final String nodeName = NODE_NAME_SETTING.get(tmpSettings); if (hadPredefinedNodeName == false) &#123; logger.info(\"node name derived from node ID [&#123;&#125;]; set [&#123;&#125;] to override\", nodeId, NODE_NAME_SETTING.getKey()); &#125; else &#123; logger.info(\"node name [&#123;&#125;], node ID [&#123;&#125;]\", nodeName, nodeId); &#125; //2、打印出JVM相关信息 final JvmInfo jvmInfo = JvmInfo.jvmInfo(); logger.info(\"version[&#123;&#125;], pid[&#123;&#125;], build[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;], OS[&#123;&#125;/&#123;&#125;/&#123;&#125;], JVM[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;]\", Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()), jvmInfo.pid(), Build.CURRENT.flavor().displayName(), Build.CURRENT.type().displayName(), Build.CURRENT.shortHash(), Build.CURRENT.date(), Constants.OS_NAME, Constants.OS_VERSION, Constants.OS_ARCH,Constants.JVM_VENDOR,Constants.JVM_NAME, Constants.JAVA_VERSION,Constants.JVM_VERSION); logger.info(\"JVM arguments &#123;&#125;\", Arrays.toString(jvmInfo.getInputArguments())); //检查当前版本是不是 pre-release 版本（Snapshot）， warnIfPreRelease(Version.CURRENT, Build.CURRENT.isSnapshot(), logger); 。。。 this.pluginsService = new PluginsService(tmpSettings, environment.configFile(), environment.modulesFile(), environment.pluginsFile(), classpathPlugins); //3、利用PluginsService加载相应的模块和插件 this.settings = pluginsService.updatedSettings(); localNodeFactory = new LocalNodeFactory(settings, nodeEnvironment.nodeId());// create the environment based on the finalized (processed) view of the settings// this is just to makes sure that people get the same settings, no matter where they ask them from this.environment = new Environment(this.settings, environment.configFile()); Environment.assertEquivalent(environment, this.environment); final List&lt;ExecutorBuilder&lt;?&gt;&gt; executorBuilders = pluginsService.getExecutorBuilders(settings); //线程池 final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0])); resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS)); // adds the context to the DeprecationLogger so that it does not need to be injected everywhere DeprecationLogger.setThreadContext(threadPool.getThreadContext()); resourcesToClose.add(() -&gt; DeprecationLogger.removeThreadContext(threadPool.getThreadContext())); final List&lt;Setting&lt;?&gt;&gt; additionalSettings = new ArrayList&lt;&gt;(pluginsService.getPluginSettings()); //额外配置 final List&lt;String&gt; additionalSettingsFilter = new ArrayList&lt;&gt;(pluginsService.getPluginSettingsFilter()); for (final ExecutorBuilder&lt;?&gt; builder : threadPool.builders()) &#123; //4、加载一些额外配置 additionalSettings.addAll(builder.getRegisteredSettings()); &#125; client = new NodeClient(settings, threadPool);//5、创建一个节点客户端 //6、缓存一系列模块,如NodeModule,ClusterModule,IndicesModule,ActionModule,GatewayModule,SettingsModule,RepositioriesModule，scriptModule，analysisModule final ResourceWatcherService resourceWatcherService = new ResourceWatcherService(settings, threadPool); final ScriptModule scriptModule = new ScriptModule(settings, pluginsService.filterPlugins(ScriptPlugin.class)); AnalysisModule analysisModule = new AnalysisModule(this.environment, pluginsService.filterPlugins(AnalysisPlugin.class)); // this is as early as we can validate settings at this point. we already pass them to ScriptModule as well as ThreadPool so we might be late here already final SettingsModule settingsModule = new SettingsModule(this.settings, additionalSettings, additionalSettingsFilter);scriptModule.registerClusterSettingsListeners(settingsModule.getClusterSettings()); resourcesToClose.add(resourceWatcherService); final NetworkService networkService = new NetworkService( getCustomNameResolvers(pluginsService.filterPlugins(DiscoveryPlugin.class))); List&lt;ClusterPlugin&gt; clusterPlugins = pluginsService.filterPlugins(ClusterPlugin.class); final ClusterService clusterService = new ClusterService(settings, settingsModule.getClusterSettings(), threadPool, ClusterModule.getClusterStateCustomSuppliers(clusterPlugins)); clusterService.addStateApplier(scriptModule.getScriptService()); resourcesToClose.add(clusterService); final IngestService ingestService = new IngestService(settings, threadPool, this.environment, scriptModule.getScriptService(), analysisModule.getAnalysisRegistry(), pluginsService.filterPlugins(IngestPlugin.class)); final DiskThresholdMonitor listener = new DiskThresholdMonitor(settings, clusterService::state, clusterService.getClusterSettings(), client); final ClusterInfoService clusterInfoService = newClusterInfoService(settings, clusterService, threadPool, client,listener::onNewInfo); final UsageService usageService = new UsageService(settings); ModulesBuilder modules = new ModulesBuilder();// plugin modules must be added here, before others or we can get crazy injection errors... for (Module pluginModule : pluginsService.createGuiceModules()) &#123; modules.add(pluginModule); &#125; final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool, clusterInfoService); ClusterModule clusterModule = new ClusterModule(settings, clusterService, clusterPlugins, clusterInfoService); modules.add(clusterModule); IndicesModule indicesModule = new IndicesModule(pluginsService.filterPlugins(MapperPlugin.class)); modules.add(indicesModule); SearchModule searchModule = new SearchModule(settings, false, pluginsService.filterPlugins(SearchPlugin.class)); CircuitBreakerService circuitBreakerService = createCircuitBreakerService(settingsModule.getSettings(), settingsModule.getClusterSettings()); resourcesToClose.add(circuitBreakerService); modules.add(new GatewayModule()); PageCacheRecycler pageCacheRecycler = createPageCacheRecycler(settings); BigArrays bigArrays = createBigArrays(pageCacheRecycler, circuitBreakerService); resourcesToClose.add(bigArrays); modules.add(settingsModule); List&lt;NamedWriteableRegistry.Entry&gt; namedWriteables = Stream.of( NetworkModule.getNamedWriteables().stream(), indicesModule.getNamedWriteables().stream(), searchModule.getNamedWriteables().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedWriteables().stream()), ClusterModule.getNamedWriteables().stream()) .flatMap(Function.identity()).collect(Collectors.toList()); final NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry(namedWriteables); NamedXContentRegistry xContentRegistry = new NamedXContentRegistry(Stream.of( NetworkModule.getNamedXContents().stream(), searchModule.getNamedXContents().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedXContent().stream()), ClusterModule.getNamedXWriteables().stream()).flatMap(Function.identity()).collect(toList())); modules.add(new RepositoriesModule(this.environment, pluginsService.filterPlugins(RepositoryPlugin.class), xContentRegistry)); final MetaStateService metaStateService = new MetaStateService(settings, nodeEnvironment, xContentRegistry); final IndicesService indicesService = new IndicesService(settings, pluginsService, nodeEnvironment, xContentRegistry,analysisModule.getAnalysisRegistry(), clusterModule.getIndexNameExpressionResolver(), indicesModule.getMapperRegistry(), namedWriteableRegistry,threadPool, settingsModule.getIndexScopedSettings(), circuitBreakerService, bigArrays, scriptModule.getScriptService(),client, metaStateService); Collection&lt;Object&gt; pluginComponents = pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.createComponents(client, clusterService, threadPool, resourceWatcherService,scriptModule.getScriptService(), xContentRegistry, environment, nodeEnvironment,namedWriteableRegistry).stream()).collect(Collectors.toList()); ActionModule actionModule = new ActionModule(false, settings, clusterModule.getIndexNameExpressionResolver(), settingsModule.getIndexScopedSettings(), settingsModule.getClusterSettings(), settingsModule.getSettingsFilter(),threadPool, pluginsService.filterPlugins(ActionPlugin.class), client, circuitBreakerService, usageService); modules.add(actionModule); //7、获取RestController,用于处理各种Elasticsearch的rest命令,如_cat,_all,_cat/health,_clusters等rest命令(Elasticsearch称之为action) final RestController restController = actionModule.getRestController(); final NetworkModule networkModule = new NetworkModule(settings, false, pluginsService.filterPlugins(NetworkPlugin.class),threadPool, bigArrays, pageCacheRecycler, circuitBreakerService, namedWriteableRegistry, xContentRegistry,networkService, restController); Collection&lt;UnaryOperator&lt;Map&lt;String, MetaData.Custom&gt;&gt;&gt; customMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getCustomMetaDataUpgrader) .collect(Collectors.toList()); Collection&lt;UnaryOperator&lt;Map&lt;String, IndexTemplateMetaData&gt;&gt;&gt; indexTemplateMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getIndexTemplateMetaDataUpgrader) .collect(Collectors.toList()); Collection&lt;UnaryOperator&lt;IndexMetaData&gt;&gt; indexMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getIndexMetaDataUpgrader).collect(Collectors.toList()); final MetaDataUpgrader metaDataUpgrader = new MetaDataUpgrader(customMetaDataUpgraders, indexTemplateMetaDataUpgraders); final MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(settings, xContentRegistry, indicesModule.getMapperRegistry(), settingsModule.getIndexScopedSettings(), indexMetaDataUpgraders); final GatewayMetaState gatewayMetaState = new GatewayMetaState(settings, nodeEnvironment, metaStateService, metaDataIndexUpgradeService, metaDataUpgrader); new TemplateUpgradeService(settings, client, clusterService, threadPool, indexTemplateMetaDataUpgraders); final Transport transport = networkModule.getTransportSupplier().get(); Set&lt;String&gt; taskHeaders = Stream.concat( pluginsService.filterPlugins(ActionPlugin.class).stream().flatMap(p -&gt; p.getTaskHeaders().stream()), Stream.of(\"X-Opaque-Id\") ).collect(Collectors.toSet()); final TransportService transportService = newTransportService(settings, transport, threadPool, networkModule.getTransportInterceptor(), localNodeFactory, settingsModule.getClusterSettings(), taskHeaders); final ResponseCollectorService responseCollectorService = new ResponseCollectorService(this.settings, clusterService); final SearchTransportService searchTransportService = new SearchTransportService(settings, transportService, SearchExecutionStatsCollector.makeWrapper(responseCollectorService)); final Consumer&lt;Binder&gt; httpBind; final HttpServerTransport httpServerTransport; if (networkModule.isHttpEnabled()) &#123; httpServerTransport = networkModule.getHttpServerTransportSupplier().get(); httpBind = b -&gt; &#123;b.bind(HttpServerTransport.class).toInstance(httpServerTransport); &#125;; &#125; else &#123; httpBind = b -&gt; &#123; b.bind(HttpServerTransport.class).toProvider(Providers.of(null)); &#125;; httpServerTransport = null; &#125; final DiscoveryModule discoveryModule = new DiscoveryModule(this.settings, threadPool, transportService, namedWriteableRegistry,networkService, clusterService.getMasterService(), clusterService.getClusterApplierService(),clusterService.getClusterSettings(), pluginsService.filterPlugins(DiscoveryPlugin.class),clusterModule.getAllocationService()); this.nodeService = new NodeService(settings, threadPool, monitorService, discoveryModule.getDiscovery(),transportService, indicesService, pluginsService, circuitBreakerService, scriptModule.getScriptService(),httpServerTransport, ingestService, clusterService, settingsModule.getSettingsFilter(), responseCollectorService,searchTransportService); final SearchService searchService = newSearchService(clusterService, indicesService, threadPool, scriptModule.getScriptService(), bigArrays, searchModule.getFetchPhase(),responseCollectorService); final List&lt;PersistentTasksExecutor&lt;?&gt;&gt; tasksExecutors = pluginsService .filterPlugins(PersistentTaskPlugin.class).stream() .map(p -&gt; p.getPersistentTasksExecutor(clusterService, threadPool, client)) .flatMap(List::stream) .collect(toList()); final PersistentTasksExecutorRegistry registry = new PersistentTasksExecutorRegistry(settings, tasksExecutors); final PersistentTasksClusterService persistentTasksClusterService = new PersistentTasksClusterService(settings, registry, clusterService); final PersistentTasksService persistentTasksService = new PersistentTasksService(settings, clusterService, threadPool, client);//8、绑定处理各种服务的实例,这里是最核心的地方,也是Elasticsearch能处理各种服务的核心. modules.add(b -&gt; &#123; b.bind(Node.class).toInstance(this); b.bind(NodeService.class).toInstance(nodeService); b.bind(NamedXContentRegistry.class).toInstance(xContentRegistry); b.bind(PluginsService.class).toInstance(pluginsService); b.bind(Client.class).toInstance(client); b.bind(NodeClient.class).toInstance(client); b.bind(Environment.class).toInstance(this.environment); b.bind(ThreadPool.class).toInstance(threadPool); b.bind(NodeEnvironment.class).toInstance(nodeEnvironment); b.bind(ResourceWatcherService.class).toInstance(resourceWatcherService);b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService); b.bind(BigArrays.class).toInstance(bigArrays); b.bind(ScriptService.class).toInstance(scriptModule.getScriptService()); b.bind(AnalysisRegistry.class).toInstance(analysisModule.getAnalysisRegistry()); b.bind(IngestService.class).toInstance(ingestService); b.bind(UsageService.class).toInstance(usageService); b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry); b.bind(MetaDataUpgrader.class).toInstance(metaDataUpgrader); b.bind(MetaStateService.class).toInstance(metaStateService); b.bind(IndicesService.class).toInstance(indicesService); b.bind(SearchService.class).toInstance(searchService); b.bind(SearchTransportService.class).toInstance(searchTransportService);b.bind(SearchPhaseController.class).toInstance(new SearchPhaseController(settings, searchService::createReduceContext)); b.bind(Transport.class).toInstance(transport); b.bind(TransportService.class).toInstance(transportService); b.bind(NetworkService.class).toInstance(networkService); b.bind(UpdateHelper.class).toInstance(new UpdateHelper(settings, scriptModule.getScriptService()));b.bind(MetaDataIndexUpgradeService.class).toInstance(metaDataIndexUpgradeService); b.bind(ClusterInfoService.class).toInstance(clusterInfoService); b.bind(GatewayMetaState.class).toInstance(gatewayMetaState); b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); &#123; RecoverySettings recoverySettings = new RecoverySettings(settings, settingsModule.getClusterSettings()); processRecoverySettings(settingsModule.getClusterSettings(), recoverySettings); b.bind(PeerRecoverySourceService.class).toInstance(new PeerRecoverySourceService(settings, transportService,indicesService, recoverySettings)); b.bind(PeerRecoveryTargetService.class).toInstance(new PeerRecoveryTargetService(settings, threadPool,transportService, recoverySettings, clusterService)); &#125; httpBind.accept(b); pluginComponents.stream().forEach(p -&gt; b.bind((Class) p.getClass()).toInstance(p));b.bind(PersistentTasksService.class).toInstance(persistentTasksService); b.bind(PersistentTasksClusterService.class).toInstance(persistentTasksClusterService);b.bind(PersistentTasksExecutorRegistry.class).toInstance(registry); &#125;); injector = modules.createInjector(); // TODO hack around circular dependencies problems in AllocationServiceclusterModule.getAllocationService().setGatewayAllocator(injector.getInstance(GatewayAllocator.class)); List&lt;LifecycleComponent&gt; pluginLifecycleComponents = pluginComponents.stream() .filter(p -&gt; p instanceof LifecycleComponent) .map(p -&gt; (LifecycleComponent) p).collect(Collectors.toList()); //9、利用Guice将各种模块以及服务(xxxService)注入到Elasticsearch环境中pluginLifecycleComponents.addAll(pluginsService.getGuiceServiceClasses().stream() .map(injector::getInstance).collect(Collectors.toList())); resourcesToClose.addAll(pluginLifecycleComponents); this.pluginLifecycleComponents = Collections.unmodifiableList(pluginLifecycleComponents); client.initialize(injector.getInstance(new Key&lt;Map&lt;GenericAction, TransportAction&gt;&gt;() &#123;&#125;), () -&gt; clusterService.localNode().getId(), transportService.getRemoteClusterService()); if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; //如果elasticsearch.yml文件中配置了http.enabled参数(默认为true),则会初始化RestHandlers logger.debug(\"initializing HTTP handlers ...\"); actionModule.initRestHandlers(() -&gt; clusterService.state().nodes()); //初始化RestHandlers, 解析集群命令,如_cat/,_cat/health &#125; //10、初始化工作完成 logger.info(\"initialized\"); success = true; &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to bind service\", ex); &#125; finally &#123; if (!success) &#123; IOUtils.closeWhileHandlingException(resourcesToClose); &#125; &#125;&#125; 上面代码真的很多，这里再说下上面这么多代码主要干了什么吧：（具体是哪行代码执行的如下流程，上面代码中也标记了） 1、创建节点环境,比如节点名称,节点 ID,分片信息,存储元,以及分配内存准备给节点使用 2、打印出 JVM 相关信息 3、利用 PluginsService 加载相应的模块和插件，具体哪些模块可以去 modules 目录下查看 4、加载一些额外的配置参数 5、创建一个节点客户端 6、缓存一系列模块,如NodeModule,ClusterModule,IndicesModule,ActionModule,GatewayModule,SettingsModule,RepositioriesModule，scriptModule，analysisModule 7、获取 RestController，用于处理各种 Elasticsearch 的 rest 命令,如 _cat, _all, _cat/health, _clusters 等 rest命令 8、绑定处理各种服务的实例 9、利用 Guice 将各种模块以及服务(xxxService)注入到 Elasticsearch 环境中 10、初始化工作完成（打印日志） JarHell 报错解释前一篇阅读源码环境搭建的文章写过用 JDK 1.8 编译 ES 源码是会遇到如下异常： 1org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: jar hell! 这里说下就是 setup 方法中的如下代码导致的 1234567try &#123; // look for jar hell final Logger logger = ESLoggerFactory.getLogger(JarHell.class); JarHell.checkJarHell(logger::debug);&#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e);&#125; 所以你如果是用 JDK 1.8 编译的，那么就需要把所有的有这块的代码给注释掉就可以编译成功的。 我自己试过用 JDK 10 编译是没有出现这里报错的。 正式启动 ES 节点回到上面 Bootstrap 中的静态 init 方法中，接下来就是正式启动 elasticsearch 节点了： 123456INSTANCE.start(); //调用下面的 start 方法private void start() throws NodeValidationException &#123; node.start(); //正式启动 Elasticsearch 节点 keepAliveThread.start();&#125; 接下来看看这个 start 方法里面的 node.start() 方法源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120public Node start() throws NodeValidationException &#123; if (!lifecycle.moveToStarted()) &#123; return this; &#125; Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info(\"starting ...\"); pluginLifecycleComponents.forEach(LifecycleComponent::start); //1、利用Guice获取上述注册的各种模块以及服务 //Node 的启动其实就是 node 里每个组件的启动，同样的，分别调用不同的实例的 start 方法来启动这个组件, 如下： injector.getInstance(MappingUpdatedAction.class).setClient(client); injector.getInstance(IndicesService.class).start(); injector.getInstance(IndicesClusterStateService.class).start(); injector.getInstance(SnapshotsService.class).start(); injector.getInstance(SnapshotShardsService.class).start(); injector.getInstance(RoutingService.class).start(); injector.getInstance(SearchService.class).start(); nodeService.getMonitorService().start(); final ClusterService clusterService = injector.getInstance(ClusterService.class); final NodeConnectionsService nodeConnectionsService = injector.getInstance(NodeConnectionsService.class); nodeConnectionsService.start(); clusterService.setNodeConnectionsService(nodeConnectionsService); injector.getInstance(ResourceWatcherService.class).start(); injector.getInstance(GatewayService.class).start(); Discovery discovery = injector.getInstance(Discovery.class); clusterService.getMasterService().setClusterStatePublisher(discovery::publish); // Start the transport service now so the publish address will be added to the local disco node in ClusterService TransportService transportService = injector.getInstance(TransportService.class); transportService.getTaskManager().setTaskResultsService(injector.getInstance(TaskResultsService.class)); transportService.start(); assert localNodeFactory.getNode() != null; assert transportService.getLocalNode().equals(localNodeFactory.getNode()) : \"transportService has a different local node than the factory provided\"; final MetaData onDiskMetadata; try &#123; // we load the global state here (the persistent part of the cluster state stored on disk) to // pass it to the bootstrap checks to allow plugins to enforce certain preconditions based on the recovered state. if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) &#123;//根据配置文件看当前节点是master还是data节点 onDiskMetadata = injector.getInstance(GatewayMetaState.class).loadMetaState(); &#125; else &#123; onDiskMetadata = MetaData.EMPTY_META_DATA; &#125; assert onDiskMetadata != null : \"metadata is null but shouldn't\"; // this is never null &#125; catch (IOException e) &#123; throw new UncheckedIOException(e); &#125; validateNodeBeforeAcceptingRequests(new BootstrapContext(settings, onDiskMetadata), transportService.boundAddress(), pluginsService .filterPlugins(Plugin .class) .stream() .flatMap(p -&gt; p.getBootstrapChecks().stream()).collect(Collectors.toList())); //2、将当前节点加入到一个集群簇中去,并启动当前节点 clusterService.addStateApplier(transportService.getTaskManager()); // start after transport service so the local disco is known discovery.start(); // start before cluster service so that it can set initial state on ClusterApplierService clusterService.start(); assert clusterService.localNode().equals(localNodeFactory.getNode()) : \"clusterService has a different local node than the factory provided\"; transportService.acceptIncomingRequests(); discovery.startInitialJoin(); // tribe nodes don't have a master so we shouldn't register an observer s final TimeValue initialStateTimeout = DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings); if (initialStateTimeout.millis() &gt; 0) &#123; final ThreadPool thread = injector.getInstance(ThreadPool.class); ClusterState clusterState = clusterService.state(); ClusterStateObserver observer = new ClusterStateObserver(clusterState, clusterService, null, logger, thread.getThreadContext()); if (clusterState.nodes().getMasterNodeId() == null) &#123; logger.debug(\"waiting to join the cluster. timeout [&#123;&#125;]\", initialStateTimeout); final CountDownLatch latch = new CountDownLatch(1); observer.waitForNextChange(new ClusterStateObserver.Listener() &#123; @Override public void onNewClusterState(ClusterState state) &#123; latch.countDown(); &#125; @Override public void onClusterServiceClose() &#123; latch.countDown(); &#125; @Override public void onTimeout(TimeValue timeout) &#123; logger.warn(\"timed out while waiting for initial discovery state - timeout: &#123;&#125;\", initialStateTimeout); latch.countDown(); &#125; &#125;, state -&gt; state.nodes().getMasterNodeId() != null, initialStateTimeout); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; throw new ElasticsearchTimeoutException(\"Interrupted while waiting for initial discovery state\"); &#125; &#125; &#125; if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; injector.getInstance(HttpServerTransport.class).start(); &#125; if (WRITE_PORTS_FILE_SETTING.get(settings)) &#123; if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; HttpServerTransport http = injector.getInstance(HttpServerTransport.class); writePortsFile(\"http\", http.boundAddress()); &#125; TransportService transport = injector.getInstance(TransportService.class); writePortsFile(\"transport\", transport.boundAddress()); &#125; logger.info(\"started\"); pluginsService.filterPlugins(ClusterPlugin.class).forEach(ClusterPlugin::onNodeStarted); return this;&#125; 上面代码主要是： 1、利用 Guice 获取上述注册的各种模块以及服务，然后启动 node 里每个组件（分别调用不同的实例的 start 方法来启动） 2、打印日志（启动节点完成） 总结这篇文章主要把大概启动流程串通了，讲了下 node 节点的创建和正式启动 ES 节点了。因为篇幅较多所以拆开成两篇，先不扣细节了，后面流程启动文章写完后我们再单一的扣细节。 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程（上）","date":"2018-08-10T16:00:00.000Z","path":"2018/08/11/es-code02/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/11/es-code02/ 前提上篇文章写了 ElasticSearch 源码解析 —— 环境搭建 ，其中里面说了启动 打开 server 模块下的 Elasticsearch 类：org.elasticsearch.bootstrap.Elasticsearch，运行里面的 main 函数就可以启动 ElasticSearch 了，这篇文章讲讲启动流程，因为篇幅会很多，所以分了两篇来写。 启动流程main 方法入口可以看到入口其实是一个 main 方法，方法里面先是检查权限，然后是一个错误日志监听器（确保在日志配置之前状态日志没有出现 error），然后是 Elasticsearch 对象的创建，然后调用了静态方法 main 方法（18 行），并把创建的对象和参数以及 Terminal 默认值传进去。静态的 main 方法里面调用 elasticsearch.main 方法。 1234567891011121314151617181920public static void main(final String[] args) throws Exception &#123; //1、入口 // we want the JVM to think there is a security manager installed so that if internal policy decisions that would be based on the // presence of a security manager or lack thereof act as if there is a security manager present (e.g., DNS cache policy) System.setSecurityManager(new SecurityManager() &#123; @Override public void checkPermission(Permission perm) &#123; // grant all permissions so that we can later set the security manager to the one that we want &#125; &#125;); LogConfigurator.registerErrorListener(); // final Elasticsearch elasticsearch = new Elasticsearch(); int status = main(args, elasticsearch, Terminal.DEFAULT); //2、调用Elasticsearch.main方法 if (status != ExitCodes.OK) &#123; exit(status); &#125;&#125;static int main(final String[] args, final Elasticsearch elasticsearch, final Terminal terminal) throws Exception &#123; return elasticsearch.main(args, terminal); //3、command main&#125; 因为 Elasticsearch 类是继承了 EnvironmentAwareCommand 类，EnvironmentAwareCommand 类继承了 Command 类，但是 Elasticsearch 类并没有重写 main 方法，所以上面调用的 elasticsearch.main 其实是调用了 Command 的 main 方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839/** Parses options for this command from args and executes it. */public final int main(String[] args, Terminal terminal) throws Exception &#123; if (addShutdownHook()) &#123; //利用Runtime.getRuntime().addShutdownHook方法加入一个Hook，在程序退出时触发该Hook shutdownHookThread = new Thread(() -&gt; &#123; try &#123; this.close(); &#125; catch (final IOException e) &#123; try ( StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw)) &#123; e.printStackTrace(pw); terminal.println(sw.toString()); &#125; catch (final IOException impossible) &#123; // StringWriter#close declares a checked IOException from the Closeable interface but the Javadocs for StringWriter // say that an exception here is impossible throw new AssertionError(impossible); &#125; &#125; &#125;); Runtime.getRuntime().addShutdownHook(shutdownHookThread); &#125; beforeMain.run(); try &#123; mainWithoutErrorHandling(args, terminal);//4、mainWithoutErrorHandling &#125; catch (OptionException e) &#123; printHelp(terminal); terminal.println(Terminal.Verbosity.SILENT, \"ERROR: \" + e.getMessage()); return ExitCodes.USAGE; &#125; catch (UserException e) &#123; if (e.exitCode == ExitCodes.USAGE) &#123; printHelp(terminal); &#125; terminal.println(Terminal.Verbosity.SILENT, \"ERROR: \" + e.getMessage()); return e.exitCode; &#125; return ExitCodes.OK;&#125; 上面代码一开始利用一个勾子函数，在程序退出时触发该 Hook，该方法主要代码是 mainWithoutErrorHandling() 方法，然后下面的是 catch 住方法抛出的异常，方法代码如下： 12345678910111213141516/*** Executes the command, but all errors are thrown. */void mainWithoutErrorHandling(String[] args, Terminal terminal) throws Exception &#123; final OptionSet options = parser.parse(args); if (options.has(helpOption)) &#123; printHelp(terminal); return; &#125; if (options.has(silentOption)) &#123; terminal.setVerbosity(Terminal.Verbosity.SILENT); &#125; else if (options.has(verboseOption)) &#123; terminal.setVerbosity(Terminal.Verbosity.VERBOSE); &#125; else &#123; terminal.setVerbosity(Terminal.Verbosity.NORMAL); &#125; execute(terminal, options);//5、执行 EnvironmentAwareCommand 中的 execute()，（重写了command里面抽象的execute方法）&#125; 上面的代码从 3 ～ 14 行是解析传进来的参数并配置 terminal，重要的 execute() 方法，执行的是 EnvironmentAwareCommand 中的 execute() （重写了 Command 类里面的抽象 execute 方法），从上面那个继承图可以看到 EnvironmentAwareCommand 继承了 Command，重写的 execute 方法代码如下： 1234567891011121314151617181920212223@Overrideprotected void execute(Terminal terminal, OptionSet options) throws Exception &#123; final Map&lt;String, String&gt; settings = new HashMap&lt;&gt;(); for (final KeyValuePair kvp : settingOption.values(options)) &#123; if (kvp.value.isEmpty()) &#123; throw new UserException(ExitCodes.USAGE, \"setting [\" + kvp.key + \"] must not be empty\"); &#125; if (settings.containsKey(kvp.key)) &#123; final String message = String.format( Locale.ROOT, \"setting [%s] already set, saw [%s] and [%s]\", kvp.key, settings.get(kvp.key), kvp.value); throw new UserException(ExitCodes.USAGE, message); &#125; settings.put(kvp.key, kvp.value); &#125; //6、根据我们ide配置的 vm options 进行设置path.data、path.home、path.logs putSystemPropertyIfSettingIsMissing(settings, \"path.data\", \"es.path.data\"); putSystemPropertyIfSettingIsMissing(settings, \"path.home\", \"es.path.home\"); putSystemPropertyIfSettingIsMissing(settings, \"path.logs\", \"es.path.logs\"); execute(terminal, options, createEnv(terminal, settings));//7、先调用 createEnv 创建环境 //9、执行elasticsearch的execute方法，elasticsearch中重写了EnvironmentAwareCommand中的抽象execute方法&#125; 方法前面是根据传参去判断配置的，如果配置为空，就会直接跳到执行 putSystemPropertyIfSettingIsMissing 方法，这里会配置三个属性：path.data、path.home、path.logs 设置 es 的 data、home、logs 目录，它这里是根据我们 ide 配置的 vm options 进行设置的，这也是为什么我们上篇文章说的配置信息，如果不配置的话就会直接报错。下面看看 putSystemPropertyIfSettingIsMissing 方法代码里面怎么做到的： 12345678910111213141516/** Ensure the given setting exists, reading it from system properties if not already set. */private static void putSystemPropertyIfSettingIsMissing(final Map&lt;String, String&gt; settings, final String setting, final String key) &#123; final String value = System.getProperty(key);//获取key（es.path.data）找系统设置 if (value != null) &#123; if (settings.containsKey(setting)) &#123; final String message = String.format( Locale.ROOT, \"duplicate setting [%s] found via command-line [%s] and system property [%s]\", setting, settings.get(setting), value); throw new IllegalArgumentException(message); &#125; else &#123; settings.put(setting, value); &#125; &#125;&#125; 执行这三个方法后： 跳出此方法，继续看会发现 execute 方法调用了方法， 1execute(terminal, options, createEnv(terminal, settings)); 这里我们先看看 createEnv(terminal, settings) 方法： 1234567protected Environment createEnv(final Terminal terminal, final Map&lt;String, String&gt; settings) throws UserException &#123; final String esPathConf = System.getProperty(\"es.path.conf\");//8、读取我们 vm options 中配置的 es.path.conf if (esPathConf == null) &#123; throw new UserException(ExitCodes.CONFIG, \"the system property [es.path.conf] must be set\"); &#125; return InternalSettingsPreparer.prepareEnvironment(Settings.EMPTY, terminal, settings, getConfigPath(esPathConf)); //8、准备环境 prepareEnvironment&#125; 读取我们 ide vm options 中配置的 es.path.conf，同上篇文章也讲了这个一定要配置的，因为 es 启动的时候会加载我们的配置和一些插件。这里继续看下上面代码第 6 行的 prepareEnvironment 方法： 1234567891011121314151617181920212223242526272829303132333435public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties, Path configPath) &#123; // just create enough settings to build the environment, to get the config dir Settings.Builder output = Settings.builder(); initializeSettings(output, input, properties); Environment environment = new Environment(output.build(), configPath); //查看 es.path.conf 目录下的配置文件是不是 yml 格式的，如果不是则抛出一个异常 if (Files.exists(environment.configFile().resolve(\"elasticsearch.yaml\"))) &#123; throw new SettingsException(\"elasticsearch.yaml was deprecated in 5.5.0 and must be renamed to elasticsearch.yml\"); &#125; if (Files.exists(environment.configFile().resolve(\"elasticsearch.json\"))) &#123; throw new SettingsException(\"elasticsearch.json was deprecated in 5.5.0 and must be converted to elasticsearch.yml\"); &#125; output = Settings.builder(); // start with a fresh output Path path = environment.configFile().resolve(\"elasticsearch.yml\"); if (Files.exists(path)) &#123; try &#123; output.loadFromPath(path); //加载文件并读取配置文件内容 &#125; catch (IOException e) &#123; throw new SettingsException(\"Failed to load settings from \" + path.toString(), e); &#125; &#125; // re-initialize settings now that the config file has been loaded initializeSettings(output, input, properties); //再一次初始化设置 finalizeSettings(output, terminal); environment = new Environment(output.build(), configPath); // we put back the path.logs so we can use it in the logging configuration file output.put(Environment.PATH_LOGS_SETTING.getKey(), environment.logsFile().toAbsolutePath().normalize().toString()); return new Environment(output.build(), configPath);&#125; 准备的环境如上图，通过构建的环境查看配置文件 elasticsearch.yml 是不是以 yml 结尾，如果是 yaml 或者 json 结尾的则抛出异常（在 5.5.0 版本其他两种格式过期了，只能使用 yml 格式），然后加载该配置文件并读取里面的内容（KV结构）。 跳出 createEnv 方法，我们继续看 execute 方法吧。 EnvironmentAwareCommand 类的 execute 方法代码如下： 1protected abstract void execute(Terminal terminal, OptionSet options, Environment env) throws Exception; 这是个抽象方法，那么它的实现方法在 Elasticsearch 类中，代码如下： 1234567891011121314151617181920212223242526272829303132333435@Overrideprotected void execute(Terminal terminal, OptionSet options, Environment env) throws UserException &#123; if (options.nonOptionArguments().isEmpty() == false) &#123; throw new UserException(ExitCodes.USAGE, \"Positional arguments not allowed, found \" + options.nonOptionArguments()); &#125; if (options.has(versionOption)) &#123; final String versionOutput = String.format( Locale.ROOT, \"Version: %s, Build: %s/%s/%s/%s, JVM: %s\", Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()), Build.CURRENT.flavor().displayName(), Build.CURRENT.type().displayName(), Build.CURRENT.shortHash(), Build.CURRENT.date(), JvmInfo.jvmInfo().version()); terminal.println(versionOutput); return; &#125; final boolean daemonize = options.has(daemonizeOption); final Path pidFile = pidfileOption.value(options); final boolean quiet = options.has(quietOption); // a misconfigured java.io.tmpdir can cause hard-to-diagnose problems later, so reject it immediately try &#123; env.validateTmpFile(); &#125; catch (IOException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125; try &#123; init(daemonize, pidFile, quiet, env); //10、初始化 &#125; catch (NodeValidationException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125;&#125; 上面代码里主要还是看看 init(daemonize, pidFile, quiet, env); 初始化方法吧。 12345678910void init(final boolean daemonize, final Path pidFile, final boolean quiet, Environment initialEnv) throws NodeValidationException, UserException &#123; try &#123; Bootstrap.init(!daemonize, pidFile, quiet, initialEnv); //11、执行 Bootstrap 中的 init 方法 &#125; catch (BootstrapException | RuntimeException e) &#123; // format exceptions to the console in a special way // to avoid 2MB stacktraces from guice, etc. throw new StartupException(e); &#125;&#125; init 方法Bootstrap 中的静态 init 方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101static void init( final boolean foreground, final Path pidFile, final boolean quiet, final Environment initialEnv) throws BootstrapException, NodeValidationException, UserException &#123; // force the class initializer for BootstrapInfo to run before // the security manager is installed BootstrapInfo.init(); INSTANCE = new Bootstrap(); //12、创建一个 Bootstrap 实例 final SecureSettings keystore = loadSecureSettings(initialEnv);//如果注册了安全模块则将相关配置加载进来 final Environment environment = createEnvironment(foreground, pidFile, keystore, initialEnv.settings(), initialEnv.configFile()); //干之前干过的事情 try &#123; LogConfigurator.configure(environment); //13、log 配置环境 &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; if (environment.pidFile() != null) &#123; try &#123; PidFile.create(environment.pidFile(), true); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; &#125; final boolean closeStandardStreams = (foreground == false) || quiet; try &#123; if (closeStandardStreams) &#123; final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; closeSystOut(); &#125; // fail if somebody replaced the lucene jars checkLucene(); //14、检查Lucene版本// install the default uncaught exception handler; must be done before security is initialized as we do not want to grant the runtime permission setDefaultUncaughtExceptionHandler Thread.setDefaultUncaughtExceptionHandler( new ElasticsearchUncaughtExceptionHandler(() -&gt; Node.NODE_NAME_SETTING.get(environment.settings()))); INSTANCE.setup(true, environment); //15、调用 setup 方法 try &#123; // any secure settings must be read during node construction IOUtils.close(keystore); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; INSTANCE.start(); //26、调用 start 方法 if (closeStandardStreams) &#123; closeSysError(); &#125; &#125; catch (NodeValidationException | RuntimeException e) &#123; // disable console logging, so user does not see the exception twice (jvm will show it already) final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (foreground &amp;&amp; maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; Logger logger = Loggers.getLogger(Bootstrap.class); if (INSTANCE.node != null) &#123; logger = Loggers.getLogger(Bootstrap.class, Node.NODE_NAME_SETTING.get(INSTANCE.node.settings())); &#125; // HACK, it sucks to do this, but we will run users out of disk space otherwise if (e instanceof CreationException) &#123; // guice: log the shortened exc to the log file ByteArrayOutputStream os = new ByteArrayOutputStream(); PrintStream ps = null; try &#123; ps = new PrintStream(os, false, \"UTF-8\"); &#125; catch (UnsupportedEncodingException uee) &#123; assert false; e.addSuppressed(uee); &#125; new StartupException(e).printStackTrace(ps); ps.flush(); try &#123; logger.error(\"Guice Exception: &#123;&#125;\", os.toString(\"UTF-8\")); &#125; catch (UnsupportedEncodingException uee) &#123; assert false; e.addSuppressed(uee); &#125; &#125; else if (e instanceof NodeValidationException) &#123; logger.error(\"node validation exception\\n&#123;&#125;\", e.getMessage()); &#125; else &#123; // full exception logger.error(\"Exception\", e); &#125; // re-enable it if appropriate, so they can see any logging during the shutdown process if (foreground &amp;&amp; maybeConsoleAppender != null) &#123; Loggers.addAppender(rootLogger, maybeConsoleAppender); &#125; throw e; &#125;&#125; 该方法主要有： 1、创建 Bootstrap 实例 2、如果注册了安全模块则将相关配置加载进来 3、创建 Elasticsearch 运行的必须环境以及相关配置, 如将 config、scripts、plugins、modules、logs、lib、bin 等配置目录加载到运行环境中 4、log 配置环境，创建日志上下文 5、检查是否存在 PID 文件，如果不存在，创建 PID 文件 6、检查 Lucene 版本 7、调用 setup 方法（用当前环境来创建一个节点） setup 方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private void setup(boolean addShutdownHook, Environment environment) throws BootstrapException &#123; Settings settings = environment.settings();//根据环境得到配置 try &#123; spawner.spawnNativeControllers(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; initializeNatives( environment.tmpFile(), BootstrapSettings.MEMORY_LOCK_SETTING.get(settings), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.get(settings), BootstrapSettings.CTRLHANDLER_SETTING.get(settings)); // initialize probes before the security manager is installed initializeProbes(); if (addShutdownHook) &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to stop node\", ex); &#125; &#125; &#125;); &#125; try &#123; // look for jar hell final Logger logger = ESLoggerFactory.getLogger(JarHell.class); JarHell.checkJarHell(logger::debug); &#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e); &#125; // Log ifconfig output before SecurityManager is installed IfConfig.logIfNecessary(); // install SM after natives, shutdown hooks, etc. try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; node = new Node(environment) &#123; //16、新建节点 @Override protected void validateNodeBeforeAcceptingRequests( final BootstrapContext context, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(context, boundTransportAddress, checks); &#125; &#125;;&#125; 上面代码最后就是 Node 节点的创建，这篇文章就不讲 Node 的创建了，下篇文章会好好讲一下 Node 节点的创建和正式启动 ES 节点的。 总结这篇文章主要先把大概启动流程串通，因为篇幅较多所以拆开成两篇，先不扣细节了，后面流程启动文章写完后我们再单一的扣细节。 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建","date":"2018-08-04T16:00:00.000Z","path":"2018/08/05/es-code01/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/05/es-code01/ 软件环境1、Intellij Idea:2018.2版本 2、Elasticsearch 源码版本: 6.3.2 3、JDK:10.0.2 4、Gradle : 建议 4.5 及以上版本 5、Macbook Pro 2017 安装 ElasticSearch 去 https://www.elastic.co/downloads/past-releases 这里找到 ElasticSearch 6.3.2 版本，下载后然后解压就好了。（注意：这个版本需要和下面的源码版本一致） 下载源码从 https://github.com/elastic/elasticsearch 上下载相应版本的源代码，这里建议用 git clone ，这样的话后面你可以随意切换到 ElasticSearch 的其他版本去。 1git clone git@github.com:elastic/elasticsearch.git 我们看下有哪些版本的： 1git tag 找到了目前源码版本最新的版本的稳定版为：v6.3.2 切换到该版本： 1git checkout v6.3.2 于是就可以切换到该稳定版本了。接下来不要直接导入到 IDEA/Eclipse 中。 编译GitHub 这里已经有描述如何导入 IDEA/Eclipse 中： 1234567891011121314151617181920JDK 10 is required to build Elasticsearch. You must have a JDK 10 installation with the environment variable JAVA_HOME referencing the path to Java home for your JDK 10 installation. By default, tests use the same runtime as JAVA_HOME. However, since Elasticsearch, supports JDK 8 the build supports compiling with JDK 10 and testing on a JDK 8 runtime; to do this, set RUNTIME_JAVA_HOME pointing to the Java home of a JDK 8 installation. Note that this mechanism can be used to test against other JDKs as well, this is not only limited to JDK 8.Note: It is also required to have JAVA7_HOME, JAVA8_HOME and JAVA10_HOME available so that the tests can pass.Warning: do not use sdkman for Java installations which do not have proper jrunscript for jdk distributions.Elasticsearch uses the Gradle wrapper for its build. You can execute Gradle using the wrapper via the gradlew script in the root of the repository.Configuring IDEs And Running TestsEclipse users can automatically configure their IDE: ./gradlew eclipse then File: Import: Existing Projects into Workspace. Select the option Search for nested projects. Additionally you will want to ensure that Eclipse is using 2048m of heap by modifying eclipse.ini accordingly to avoid GC overhead errors.IntelliJ users can automatically configure their IDE: ./gradlew idea then File-&gt;New Project From Existing Sources. Point to the root of the source directory, select Import project from external model-&gt;Gradle, enable Use auto-import. In order to run tests directly from IDEA 2017.2 and above, it is required to disable the IDEA run launcher in order to avoid idea_rt.jar causing &quot;jar hell&quot;. This can be achieved by adding the -Didea.no.launcher=true JVM option. Alternatively, idea.no.launcher=true can be set in the idea.properties file which can be accessed under Help &gt; Edit Custom Properties (this will require a restart of IDEA). For IDEA 2017.3 and above, in addition to the JVM option, you will need to go to Run-&gt;Edit Configurations-&gt;...-&gt;Defaults-&gt;JUnit and verify that the Shorten command line setting is set to user-local default: none. You may also need to remove ant-javafx.jar from your classpath if that is reported as a source of jar hell.To run an instance of elasticsearch from the source code run ./gradlew runThe Elasticsearch codebase makes heavy use of Java asserts and the test runner requires that assertions be enabled within the JVM. This can be accomplished by passing the flag -ea to the JVM on startup.For IntelliJ, go to Run-&gt;Edit Configurations...-&gt;Defaults-&gt;JUnit-&gt;VM options and input -ea.For Eclipse, go to Preferences-&gt;Java-&gt;Installed JREs and add -ea to VM Arguments. 上面说了下如何编译 Elasticsearch 和如何在 ide 中配置好环境。下面说下步骤吧：（这里我只是演示在 IDEA 中如何导入） 1、在我们下载的 Elasticsearch 根目录下执行命令：(执行已经写好的脚本 gradlew) 1./gradlew idea 请注意版本和我的一致，早的版本可能没有该执行脚本，需要执行 gradle idea 命令 最后结果如下： 2、导入 IDEA idea 中 File -&gt; New Project From Existing Sources 选择你下载的 Elasticsearch 根目录，然后点 open ，之后 Import project from external model -&gt; Gradle , 选中 Use auto-import, 然后就可以了。 导入进去后，gradle 又会编译一遍，需要等一会，好了之后如下： 运行打开 server 模块下的 Elasticsearch 类：org.elasticsearch.bootstrap.Elasticsearch，运行里面的 main 函数。 1、报错如下： 1ERROR: the system property [es.path.conf] must be set 我们在运行的配置 vm options 如下：（后面启动流程会写为什么会报这个错误） 1-Des.path.conf=&quot;/usr/local/elasticsearch-6.3.2/config&quot; 2、再次运行，报错如下： 12345678910Exception in thread &quot;main&quot; java.lang.IllegalStateException: path.home is not configured at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:103) at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:94) at org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:86) at org.elasticsearch.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:95) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 我们在运行的配置 vm options 如下：（后面启动流程会写为什么会报这个错误） 1-Des.path.home=&quot;/usr/local/elasticsearch-6.3.2&quot; 3、再次运行，报错如下： 123456789101112131415161718192021222324252018-08-01 09:38:03,974 main ERROR Could not register mbeans java.security.AccessControlException: access denied (&quot;javax.management.MBeanTrustPermission&quot; &quot;register&quot;) at java.base/java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) at java.base/java.lang.SecurityManager.checkPermission(SecurityManager.java:371) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanTrustPermission(DefaultMBeanServerInterceptor.java:1805) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:318) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522) at org.apache.logging.log4j.core.jmx.Server.register(Server.java:389) at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:167) at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:556) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:261) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:206) at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:220) at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:197) at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:171) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:140) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:119) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:294) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 我们在运行的配置 vm options 如下： 1-Dlog4j2.disable.jmx=true 4、如果你用的是 JDK 1.8 编译的应该还会报这个错误 123456789101112131415161718192021222324[2018-08-01T11:02:24,663][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: jar hell!class: jdk.packager.services.UserJvmOptionsServicejar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/ant-javafx.jarjar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/packager.jar at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.IllegalStateException: jar hell!class: jdk.packager.services.UserJvmOptionsServicejar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/ant-javafx.jarjar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/packager.jar at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:273) ~[main/:?] at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:190) ~[main/:?] at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:86) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:198) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] ... 6 more2018-08-01 11:02:24,713 Thread-2 ERROR No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property &apos;log4j2.debug&apos; to show Log4j2 internal initialization logging. 有两个解决方法就是， （1）、把源码中有关使用了 JarHell.checkJarHell 代码的地方全部注释掉就好了 （2）、换成 JDK 10 编译 两种方法我都试了是可行的，建议直接换第二种方案吧！ 5、然后再启动的话，应该没问题了,出现下面日志：（网上很多人在这步就好了） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[elasticsearch] Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.[elasticsearch] [2018-08-04T16:42:26,073][INFO ][o.e.n.Node ] [node-0] initializing ...[elasticsearch] [2018-08-04T16:42:26,185][INFO ][o.e.e.NodeEnvironment ] [node-0] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [109.3gb], net total_space [233.4gb], types [apfs][elasticsearch] [2018-08-04T16:42:26,187][INFO ][o.e.e.NodeEnvironment ] [node-0] heap size [494.9mb], compressed ordinary object pointers [true][elasticsearch] [2018-08-04T16:42:26,190][INFO ][o.e.n.Node ] [node-0] node name [node-0], node ID [o9SuMXP-R7uvJLtE3h37Rw][elasticsearch] [2018-08-04T16:42:26,191][INFO ][o.e.n.Node ] [node-0] version[6.3.2-SNAPSHOT], pid[61499], build[default/zip/053779d/2018-08-04T08:39:59.714654Z], OS[Mac OS X/10.13.5/x86_64], JVM[&quot;Oracle Corporation&quot;/Java HotSpot(TM) 64-Bit Server VM/10.0.2/10.0.2+13][elasticsearch] [2018-08-04T16:42:26,191][INFO ][o.e.n.Node ] [node-0] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/elasticsearch.URRKTybG, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Djava.locale.providers=COMPAT, -XX:UseAVX=2, -ea, -esa, -Xms512m, -Xmx512m, -Des.path.home=/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT, -Des.path.conf=/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT/config, -Des.distribution.flavor=default, -Des.distribution.type=zip][elasticsearch] [2018-08-04T16:42:26,191][WARN ][o.e.n.Node ] [node-0] version [6.3.2-SNAPSHOT] is a pre-release version of Elasticsearch and is not suitable for production[elasticsearch] [2018-08-04T16:42:28,808][INFO ][o.e.p.PluginsService ] [node-0] loaded module [aggs-matrix-stats][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [analysis-common][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [ingest-common][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-expression][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-mustache][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-painless][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [mapper-extras][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [parent-join][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [percolator][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [rank-eval][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [reindex][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [repository-url][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [transport-netty4][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [tribe][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-core][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-deprecation][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-graph][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-logstash][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-ml][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-monitoring][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-rollup][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-security][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-sql][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-upgrade][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-watcher][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] no plugins loaded[elasticsearch] [2018-08-04T16:42:32,722][INFO ][o.e.x.s.a.s.FileRolesStore] [node-0] parsed [0] roles from file [/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT/config/roles.yml][elasticsearch] [2018-08-04T16:42:33,358][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/61517] [Main.cc@109] controller (64 bit): Version 6.3.2-SNAPSHOT (Build 903094f295d249) Copyright (c) 2018 Elasticsearch BV[elasticsearch] [2018-08-04T16:42:33,783][DEBUG][o.e.a.ActionModule ] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[elasticsearch] [2018-08-04T16:42:34,110][INFO ][o.e.d.DiscoveryModule ] [node-0] using discovery type [zen][elasticsearch] [2018-08-04T16:42:34,971][INFO ][o.e.n.Node ] [node-0] initialized[elasticsearch] [2018-08-04T16:42:34,971][INFO ][o.e.n.Node ] [node-0] starting ...[elasticsearch] [2018-08-04T16:42:35,217][INFO ][o.e.t.TransportService ] [node-0] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[elasticsearch] [2018-08-04T16:42:38,291][INFO ][o.e.c.s.MasterService ] [node-0] zen-disco-elected-as-master ([0] nodes joined)[, ], reason: new_master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125;[elasticsearch] [2018-08-04T16:42:38,295][INFO ][o.e.c.s.ClusterApplierService] [node-0] new_master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125;, reason: apply cluster state (from master [master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125; committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)[, ]]])[elasticsearch] [2018-08-04T16:42:38,317][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [node-0] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[elasticsearch] [2018-08-04T16:42:38,319][INFO ][o.e.n.Node ] [node-0] started[elasticsearch] [2018-08-04T16:42:38,358][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [node-0] Failed to clear cache for realms [[]][elasticsearch] [2018-08-04T16:42:38,413][INFO ][o.e.g.GatewayService ] [node-0] recovered [0] indices into cluster_state[elasticsearch] [2018-08-04T16:42:38,597][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.watch-history-7] for index patterns [.watcher-history-7*][elasticsearch] [2018-08-04T16:42:38,660][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.watches] for index patterns [.watches*][elasticsearch] [2018-08-04T16:42:38,707][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.triggered_watches] for index patterns [.triggered_watches*][elasticsearch] [2018-08-04T16:42:38,771][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-logstash] for index patterns [.monitoring-logstash-6-*][elasticsearch] [2018-08-04T16:42:38,836][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-es] for index patterns [.monitoring-es-6-*][elasticsearch] [2018-08-04T16:42:38,878][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-alerts] for index patterns [.monitoring-alerts-6][elasticsearch] [2018-08-04T16:42:38,926][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-beats] for index patterns [.monitoring-beats-6-*][elasticsearch] [2018-08-04T16:42:38,970][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-kibana] for index patterns [.monitoring-kibana-6-*][elasticsearch] [2018-08-04T16:42:39,055][INFO ][o.e.l.LicenseService ] [node-0] license [79704513-d3c4-4535-8276-beeb146765de] mode [basic] - valid 6、但是我出现了下面这个问题，一直困扰着我呢，我是直接跳过去的。 12345678910111213141516171819202122[2018-08-01T09:44:27,370][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?] at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?] at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?] at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?] at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?] ... 15 more 遇到的这个问题，我在 GitHub 求助信息如下： https://github.com/elastic/elasticsearch/issues/30774 但是并没有解决我的问题，这里暂时先记录下来！我自己也跟了下源码，定位到错误信息是怎么产生的，但是没有解决方案！ 后面写了篇文章：教你如何在 IDEA 远程 Debug ElasticSearch 或许可以帮你解决上面问题带给你的困扰！ 更新后面有一个读者提醒了我一下，他自己也遇到这个问题，然后他的解决方案挺好的，完美解决我的问题。这里做个记录： 解决方法： 打开 IDEA Edit Configurations ，给 Include dependencies with Provided scope 打上勾即可解决，很简单吧！！ 继续RUN，又来一个 EXceptin： 1234567891011121314151617[2018-08-23T01:13:38,551][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;) at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) ~[?:?] at java.security.AccessController.checkPermission(AccessController.java:895) ~[?:?] at java.lang.SecurityManager.checkPermission(SecurityManager.java:335) ~[?:?] at java.lang.SecurityManager.checkCreateClassLoader(SecurityManager.java:397) ~[?:?]...Exception: java.security.AccessControlException thrown from the UncaughtExceptionHandler in thread &quot;Thread-2&quot; 第一种： 在 config 目录下新建 java.policy 文件，填入下面内容: 123grant &#123; permission java.lang.RuntimePermission &quot;createClassLoader&quot;;&#125;; 然后在 VM options 加入 java.security.policy 的设置，指向该文件即可 1-Djava.security.policy=/usr/local/elasticsearch-6.3.2/config/java.policy 第二种： 就是在 %JAVA_HOME%/conf/security 目录下（JDK10是这个路径，之前的版本不确定），我的目录是 /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/conf/security，打开 java.policy 文件，在 grant 中加入下面这句，赋予权限。 12//for es 6.3.2permission java.lang.RuntimePermission &quot;createClassLoader&quot;; 再 RUN，这次可终于运行起来了！！！ 再次感谢下读者，他的文章地址是：http://laijianfeng.org/2018/08/%E6%95%99%E4%BD%A0%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95Elasticsearch-6-3-2%E6%BA%90%E7%A0%81/ 总结折腾的路上少不了各种错误烦扰你，学会解决问题！ 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡为什么要看 ElasticSearch 源码？","date":"2018-08-03T16:00:00.000Z","path":"2018/08/04/why-see-es-code/","text":"前提人工智能、大数据快速发展的今天，对于 TB 甚至 PB 级大数据的快速检索已然成为刚需，大型企业早已淹没在系统生成的浩瀚数据流当中。大数据技术业已集中在如何存储和处理这些海量的数据上。Elasticsearch 作为开源领域的后起之秀，从2010年至今得到飞跃式的发展。 Elasticsearch 以其开源、分布式、RESTFul API 三大优势，已经成为当下风口中“会飞的猪”。 在我的电脑本地写了几篇 ElasticSearch 的源码解析了，回过头来想想应该也写一篇为何我会去看它的源码？ 为什么呢？下面我讲讲自己从接触搜索到现在看源码的过程！ 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/24/why-see-es-code/ 第一次接触搜索搜索，我们首先想到的是搜索引擎：Google、百度，这个就算是接触的最早的了。 我自己项目里面接触搜索是大二暑假那时练习的一个项目，里面用了 Solr，然后当时自己也稍微了解了下，并用在了项目里面。 第二次接触搜索从第一次项目里面用到了搜索，后面自己对这方面就比较感兴趣。再一次接触搜索是实习的时候进公司。第一件事情就是被老大叫的去学习搭建 Elasticsearch 集群，于是乎，电脑就装了三个虚拟机，Elasticsearch 就一个个的装了起来了。也记录了博客下来：Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程，当时搭建的时候 ES 的版本才刚从 2.x 升级到 5.x 呢，截止本文时间 2018.08.04，现在 ES 版本已经是到 7.0 了，这版本升级是真的的快，这也说明了 ES 的活跃度很高，背后的开发工程师维护也快，侧面突出要去看它源码的重要性。 当时自己在本地测试搭建集群后，给分配了另外一个任务就是去了解 ES 中的自带分词、英文分词、中文分词的相同与差异、以及自己建立分词需要注意的点。于是乎：当时在公司 wiki 贡献了这篇文章：Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法。这篇文章几乎已经把市面上所有的分词都写进去了，包括他们的相同点、不同点、如何使用、如何自定义分词器。 然后还有就是我同组的一个同学，她的任务就是 2.x 升级到 5.x 中 mapping 的大改变有哪些？后面我也看了她总结的文档，很详细！ 在这次接触了 ES 后，因为我自己本地已经有环境了，所以自己测试了一些功能，给 ES 安装插件（IK、x-pack、支持 sql 的、），后面自己也去测试 ES 的索引、文档、REST API。 第三次接触搜索由于是自己对其感兴趣，所以后面就去找了些相关的视频，比如：中华石衫的《Elasticsearch 顶尖高手系列-高手进阶篇》几个系列视频教程个人感觉还是不错的，看完这几个系列估计入门肯定是没有问题的。版权原因，不提供下载链接。 另外就是《Elasticsearch 权威指南》翻译的版本，翻译还没有全，可以去看看，讲得很详细的，市面上应该还没有哪本书讲的有这么清楚，如果英文不错的可以直接啃英文吧。 还有就是官网的文档了，非常非常详细，还有 demo，2.x 版本的是有中文的官方文档，可以凑合着看。 学习新东西，要学会先看官方文档，何况 Elasticsearch 的官方文档这么详细呢！ 第四次接触搜索后面实习的时候，又分配了公司中间件监控的两个模块：Elasticsearch 和 HBase 组件的监控。于是乎，再次有机会接触 Elasticsearch 了，这次主要还是利用 Elasticsearch 自带的 REST API ：_cluster/health 、_cluster/stats、_nodes、_nodes/stats 去获取到集群的健康信息、节点信息（内存、CPU、网络、JVM等信息）。为了做这个项目自己当时也去找了网上很多类似的文章参考常用的监控指标和他们是怎么做监控的。我当时的任务主要还是采集信息，然后存到公司大项目的 influxdb 中，最后用 grafana 展示出来，后面我组的运维大佬给我看了监控大盘，界面挺酷炫的，哈哈哈，牛逼！ 当时写的两篇博客： 1、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 2、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 取之网络，还之网络，希望给后面做类似任务的小伙伴给点参考意见！ 再就是自己搭建 ELK（ElasticSearch, Logstash, Kibana）日志分析平台，然后玩了下！ 搭建环境博客：Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 第五次接触搜索后面就没怎么接触 ElasticSearch 了，一直忙着其他的东西。 实习辞职后，毕业出来找工作的那段日子，自己又花了一星期稍微过了一遍 《Elasticsearch 权威指南》 这本书，话说还帮我面试挺过不少关呢，哈哈哈！因为我项目里写了 Elasticsearch 的监控，如果你对 Elasticsearch 其他的不熟悉，面试官稍微问些其他关于这方面的，那就不知道就有点尴尬😅了，所以还是准备了下。看完之后应付面试没多大的问题。 第六次接触看起来我接触了 Elasticsearch 很久了，其实真正项目里面是没有用到 Elasticsearch 做过项目的，没有用到 Elasticsearch 的搜索做什么项目，于是自己当时找工作其实也打算找到工作后看能不能自己做个项目或者公司项目里面用用 Elasticsearch 呢？ 结果在新公司新项目里，很快就用到了。只不过这次不是 Java 项目里面用了，而是和 GoLang 整合。不过 API 都差不多，多熟悉几次就很快上手了，关键还是要懂 Elasticsearch 如何构造 DSL 查询语句，这样再转换成 GO 里面的 API 就快了。 还有就是公司里刚好有个中科院研究生大佬，他就写过 Elasticsearch 这块的书籍《从 lucene 到 Elasticsearch 全文检索实战》，另外他的 CSDN 博客也很火，阅读量很高，感兴趣的可以买本书支持下。 中途自己遇到 Elasticsearch 实在不会的问题也会主动去找大佬咨询，然后大佬耐心教教我这个渣渣菜鸡，在文章这里感谢下大佬这段时间的关照。 萌生阅读源码的想法既然接触了这么久的 Elasticsearch ，项目里用过，书籍也看过，虽然还不是很熟，但是如果看看它的源码是不是会让我对它的印象更深呢？ 说干就干，晚上回家就从 GitHub clone 了源码在本地，那时刚好回家，就在火车上直接用 VS code 看了会源码，也没有在 ide 里 debug 起来。 写这篇文章的时候已经把 Elasticsearch 的整个启动流程（加载读取配置、加载插件等）、如何支持 REST API 看了下，后面会在下班后回家继续阅读源码，继续分享我的源码解析的。 有想法就去干，不尝试下，怎么知道适不适合你？ 总结其实阅读源码的主要原因还是自己感兴趣；另外就是这东西现在项目里确实也用到了，如果我对源码熟悉的话可能会对我的理解会更加透彻点；还有就是 Elasticsearch 确实火，公司几乎都用的，所以学习下还是有必要的。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的蚂蚁金服面试经历(二)","date":"2018-07-30T16:00:00.000Z","path":"2018/07/31/alipay02/","text":"蚂蚁金服电话二面（85 分钟）1、自我介绍、工作经历、技术栈 2、项目中你学到了什么技术？（把三项目具体描述了很久） 3、微服务划分的粒度 4、微服务的高可用怎么保证的？ 5、常用的负载均衡，该怎么用，你能说下吗？ 6、网关能够为后端服务带来哪些好处？ 7、Spring Bean 的生命周期 8、xml 中配置的 init、destroy 方法怎么可以做到调用具体的方法？ 9、反射的机制 10、Object 类中的方法 11、hashcode 和 equals 方法常用地方 12、对象比较是否相同 13、hashmap put 方法存放的时候怎么判断是否是重复的 14、Object toString 方法常用的地方，为什么要重写该方法 15、Set 和 List 区别？ 16、ArrayList 和 LinkedList 区别 17、如果存取相同的数据，ArrayList 和 LinkedList 谁占用空间更大？ 18、Set 存的顺序是有序的吗？ 19、常见 Set 的实现有哪些？ 20、TreeSet 对存入对数据有什么要求呢？ 21、HashSet 的底层实现呢 22、TreeSet 底层源码有看过吗？ 23、HashSet 是不是线程安全的？为什么不是线程安全的？ 24、Java 中有哪些线程安全的 Map？ 25、Concurrenthashmap 是怎么做到线程安全的？ 26、HashTable 你了解过吗？ 27、如何保证线程安全问题？ 28、synchronized、lock 29、volatile 的原子性问题？为什么 i++ 这种不支持原子性？从计算机原理的设计来讲下不能保证原子性的原因 30、happens before 原理 31、cas 操作 32、lock 和 synchronized 的区别？ 33、公平锁和非公平锁 34、Java 读写锁 35、读写锁设计主要解决什么问题？ 36、你项目除了写 Java 代码，还有前端代码，那你知道前端有哪些框架吗？ 37、MySQL 分页查询语句 38、MySQL 事务特性和隔离级别 39、不可重复读会出现在什么场景？ 40、sql having 的使用场景 41、前端浏览器地址的一个 http 请求到后端整个流程是怎么样？能够说下吗？ 42、http 默认端口，https 默认端口 43、DNS 你知道是干嘛的吗？ 44、你们开发用的 ide 是啥？你能说下 idea 的常用几个快捷键吧？ 45、代码版本管理你们用的是啥？ 46、git rebase 和 merge 有什么区别？ 47、你们公司加班多吗？ 48、后面一起聊 high 了，之间扯了些蛋，哈哈哈 相关文章：1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺 3、秋招第三站 —— 内推阿里（一面） 4、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 5、面试过阿里等互联网大公司，我知道了这些套路 6、渣渣菜鸡的有赞面试经历（一） 7、渣渣菜鸡的蚂蚁金服面试经历（一） 最后本地地址：http://www.54tianzhisheng.cn/2018/07/31/alipay02 ，转载请授权，否则禁止转载！ 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，已经分享在我的知识星球，如果感兴趣，可以加入我的知识星球！","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"渣渣菜鸡的蚂蚁金服面试经历(一)","date":"2018-07-29T16:00:00.000Z","path":"2018/07/30/alipay01/","text":"蚂蚁金服电话一面1、自我介绍、自己做的项目和技术领域 2、项目中的监控：那个监控指标常见的哪些？ 3、微服务涉及到的技术以及需要注意的问题有哪些？ 4、注册中心你了解了哪些？ 5、consul 的可靠性你了解吗？ 6、consul 的机制你有没有具体深入过？有没有和其他的注册中心对比过？ 7、项目用 Spring 比较多，有没有了解 Spring 的原理？AOP 和 IOC 的原理 8、Spring Boot除了自动配置，相比传统的 Spring 有什么其他的区别？ 9、Spring Cloud 有了解多少？ 10、Spring Bean 的生命周期 11、HashMap 和 hashTable 区别？ 12、Object 的 hashcode 方法重写了，equals 方法要不要改？ 13、Hashmap 线程不安全的出现场景 14、线上服务 CPU 很高该怎么做？有哪些措施可以找到问题 15、JDK 中有哪几个线程池？顺带把线程池讲了个遍 16、SQL 优化的常见方法有哪些 17、SQL 索引的顺序，字段的顺序 18、查看 SQL 是不是使用了索引？（有什么工具） 19、TCP 和 UDP 的区别？TCP 数据传输过程中怎么做到可靠的？ 20、说下你知道的排序算法吧 21、查找一个数组的中位数？ 22、你有什么问题想问我的吗？ 相关文章：1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺 3、秋招第三站 —— 内推阿里（一面） 4、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 5、面试过阿里等互联网大公司，我知道了这些套路 6、渣渣菜鸡的有赞面试经历（一） 最后本地地址：http://www.54tianzhisheng.cn/2018/07/30/alipay01 ，转载请授权，否则禁止转载！ 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，已经分享在我的知识星球，如果感兴趣，可以加入我的知识星球！","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"渣渣菜鸡的有赞面试经历（一）","date":"2018-07-11T16:00:00.000Z","path":"2018/07/12/youzan/","text":"出去面试的话还是得好好准备，不然会被虐的有点惨！ 有赞（框架组）四月份面试有赞的时候，自己还在实习，所以也没有复习，是在 Boss 直聘上投的，当时看到了有赞的 2018 届春招，就投了下，然后不知道怎么就被推到了框架组，结果后面就感觉自己被虐的可惨了。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/07/12/youzan/ 电话一面好像是清明节还是五一劳动节来着，我还在睡觉，就接到一面面试官的电话，说现在有时间吗，能够接受下电话面试吗？我勒个去，今天过节、我被电话吵醒的，现在人都没清醒、这面试那肯定得一面就挂了，所以就老实得说现在还是不方便呢，约了周一上午 10 点面试。 周一 10 点面试官准时打电话过来了！ 以下是面试的问题： 1、自我介绍 2、Map 的底层结构？（HashMap） 3、线程安全的 Map （concurrentHashMap）简单的说了下这两 1。7 和 1.8 的区别，本想问下要不要深入的讲下（源码级别），结果面试官说不用了。 4、项目 MySQL 的数据量和并发量有多大？ 5、你对数据库了解多少？ 6、你说下数据库的索引实现和非主键的二级索引 7、项目用的是 SpringBoot ，你能说下 Spring Boot 与 Spring 的区别吗？ 8、SpringBoot 的自动配置是怎么做的？ 9、MyBatis 定义的接口，怎么找到实现的？ 10、Java 内存结构 11、对象是否可 GC？ 12、Minor GC 和 Full GC 13、垃圾回收算法 14、垃圾回收器 G1 15、项目里用过 ElasticSearch 和 Hbase，有深入了解他们的调优技巧吗？ 16、Spring RestTemplate 的具体实现 17、描述下网页一个 Http 请求，到后端的整个请求过程 18、多线程的常用方法和接口类及线程池的机制 19、总结我的 Java 基础还是不错，但是一些主流的框架源码还是处在使用的状态，需要继续去看源码 20、死锁 21、自己研究比较新的技术，说下成果！ 22、你有什么想问的？我就问了下公司那边的情况，这个自由发挥！ 最后我知道有二面的面试机会了。 10 来分钟不到，就再次打电话过来约了明早上午 10 点的视频面试。 视频二面二面面试官先打电话过来，然后加了个微信，开始微信视频面试 这个面试我也不太记得具体面试题目了，下面写的是大概方向的： 1、HashMap，源码级别的问了，包括为什么线程不安全 2、死锁 3、Synchronized 和 ReentrantLock 锁机制，怎么判断重入锁的，会不会是死锁？ 4、进程和线程的区别？ 5、进程之间如何保证同步？ 6、分布式锁 7、对象 GC 8、垃圾回收算法 9、JVM 参数 10、OOM 出现的有哪些场景？为什么会发生？ 11、JVM 内存结构说下吧 12、堆和栈的共享问题？ 13、有比较过 Http 和 RPC 吗？ 14、HttpClient 你说说里面的具体实现吧？（涉及了哪些东西） 15、那要你设计一个高性能的 Http ，你会怎么设计？ 二面微信视频面试只记得这么多了。 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，后面会一篇一篇分享在我的知识星球的，如果感兴趣，可以加入我的知识星球！ 知识星球更多面经文章： 1、蚂蚁金服电话一面 2、蚂蚁金服电话二面——后面聊的有点high 3、club factory 面经分享 4、作为面试官得到的经验 5、史上最强最全面经合集 6、公司需要什么样的人 7、如何介绍项目","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"20 个案例教你在 Java 8 中如何处理日期和时间?","date":"2018-06-19T16:00:00.000Z","path":"2018/06/20/java-8-date/","text":"前言前面一篇文章写了《SimpleDateFormat 如何安全的使用？》, 里面介绍了 SimpleDateFormat 如何处理日期／时间，以及如何保证线程安全，及其介绍了在 Java 8 中的处理时间／日期默认就线程安全的 DateTimeFormatter 类。那么 Java 8 中该怎么样处理生活中常见的一些日期／时间呢？比如：计算一周后的日期；计算一年前或一年后的日期；检查闰年等。 接下来创建了 20 个基于任务的实例来学习 Java 8 的新特性。从最简单创建当天的日期开始，然后创建时间及时区，接着模拟一个日期提醒应用中的任务——计算重要日期的到期天数，例如生日、纪念日、账单日、保费到期日、信用卡过期日等。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/06/20/java-8-date/ 示例 1、在 Java 8 中获取今天的日期Java 8 中的 LocalDate 用于表示当天日期。和 java.util.Date 不同，它只有日期，不包含时间。当你仅需要表示日期时就用这个类。 12LocalDate now = LocalDate.now();System.out.println(now); 结果是： 12018-06-20 上面的代码创建了当天的日期，不含时间信息。打印出的日期格式非常友好，不像老的 Date 类打印出一堆没有格式化的信息。 示例 2、在 Java 8 中获取年、月、日信息LocalDate 类提供了获取年、月、日的快捷方法，其实例还包含很多其它的日期属性。通过调用这些方法就可以很方便的得到需要的日期信息，不用像以前一样需要依赖 java.util.Calendar 类了 12345LocalDate now = LocalDate.now();int year = now.getYear();int monthValue = now.getMonthValue();int dayOfMonth = now.getDayOfMonth();System.out.printf(\"year = %d, month = %d, day = %d\", year, monthValue, dayOfMonth); 结果是： 1year = 2018, month = 6, day = 20 示例 3、在 Java 8 中处理特定日期在第一个例子里，我们通过静态工厂方法 now() 非常容易地创建了当天日期，你还可以调用另一个有用的工厂方法LocalDate.of() 创建任意日期， 该方法需要传入年、月、日做参数，返回对应的 LocalDate 实例。这个方法的好处是没再犯老 API 的设计错误，比如年度起始于 1900，月份是从 0 开始等等。日期所见即所得，就像下面这个例子表示了 6 月 20 日，没有任何隐藏机关。 12LocalDate date = LocalDate.of(2018, 06, 20);System.out.println(date); 可以看到创建的日期完全符合预期，与写入的 2018 年 6 月 20 日完全一致。 示例 4、在 Java 8 中判断两个日期是否相等现实生活中有一类时间处理就是判断两个日期是否相等。你常常会检查今天是不是个特殊的日子，比如生日、纪念日或非交易日。这时就需要把指定的日期与某个特定日期做比较，例如判断这一天是否是假期。下面这个例子会帮助你用 Java 8 的方式去解决，你肯定已经想到了，LocalDate 重载了 equal 方法，请看下面的例子： 12345LocalDate now = LocalDate.now();LocalDate date = LocalDate.of(2018, 06, 20);if (date.equals(now)) &#123; System.out.println(\"同一天\");&#125; 这个例子中我们比较的两个日期相同。注意，如果比较的日期是字符型的，需要先解析成日期对象再作判断。 示例 5、在 Java 8 中检查像生日这种周期性事件Java 中另一个日期时间的处理就是检查类似每月账单、结婚纪念日、EMI日或保险缴费日这些周期性事件。如果你在电子商务网站工作，那么一定会有一个模块用来在圣诞节、感恩节这种节日时向客户发送问候邮件。Java 中如何检查这些节日或其它周期性事件呢？答案就是 MonthDay 类。这个类组合了月份和日，去掉了年，这意味着你可以用它判断每年都会发生事件。和这个类相似的还有一个 YearMonth 类。这些类也都是不可变并且线程安全的值类型。下面我们通过 MonthDay 来检查周期性事件： 123456789LocalDate now = LocalDate.now();LocalDate dateOfBirth = LocalDate.of(2018, 06, 20);MonthDay birthday = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth());MonthDay currentMonthDay = MonthDay.from(now);if (currentMonthDay.equals(birthday)) &#123; System.out.println(\"Happy Birthday\");&#125; else &#123; System.out.println(\"Sorry, today is not your birthday\");&#125; 结果：（注意：获取当前时间可能与你看的时候不对，所以这个结果可能和你看的时候运行结果不一样） 1Happy Birthday 只要当天的日期和生日匹配，无论是哪一年都会打印出祝贺信息。你可以把程序整合进系统时钟，看看生日时是否会受到提醒，或者写一个单元测试来检测代码是否运行正确。 示例 6、在 Java 8 中获取当前时间与 Java 8 获取日期的例子很像，获取时间使用的是 LocalTime 类，一个只有时间没有日期的 LocalDate 近亲。可以调用静态工厂方法 now() 来获取当前时间。默认的格式是 hh:mm:ss:nnn。 12LocalTime localTime = LocalTime.now();System.out.println(localTime); 结果： 113:35:56.155 可以看到当前时间就只包含时间信息，没有日期。 示例 7、如何在现有的时间上增加小时通过增加小时、分、秒来计算将来的时间很常见。Java 8 除了不变类型和线程安全的好处之外，还提供了更好的plusHours() 方法替换 add()，并且是兼容的。注意，这些方法返回一个全新的 LocalTime 实例，由于其不可变性，返回后一定要用变量赋值。 1234LocalTime localTime = LocalTime.now();System.out.println(localTime);LocalTime localTime1 = localTime.plusHours(2);//增加2小时System.out.println(localTime1); 结果： 1213:41:20.72115:41:20.721 可以看到，新的时间在当前时间 13:41:20.721 的基础上增加了 2 个小时。 示例 8、如何计算一周后的日期和上个例子计算两小时以后的时间类似，这个例子会计算一周后的日期。LocalDate 日期不包含时间信息，它的 plus()方法用来增加天、周、月，ChronoUnit 类声明了这些时间单位。由于 LocalDate 也是不变类型，返回后一定要用变量赋值。 1234LocalDate now = LocalDate.now();LocalDate plusDate = now.plus(1, ChronoUnit.WEEKS);System.out.println(now);System.out.println(plusDate); 结果： 122018-06-202018-06-27 可以看到新日期离当天日期是 7 天，也就是一周。你可以用同样的方法增加 1 个月、1 年、1 小时、1 分钟甚至一个世纪，更多选项可以查看 Java 8 API 中的 ChronoUnit 类。 示例 9、计算一年前或一年后的日期继续上面的例子，上个例子中我们通过 LocalDate 的 plus() 方法增加天数、周数或月数，这个例子我们利用 minus() 方法计算一年前的日期。 12345LocalDate now = LocalDate.now();LocalDate minusDate = now.minus(1, ChronoUnit.YEARS);LocalDate plusDate1 = now.plus(1, ChronoUnit.YEARS);System.out.println(minusDate);System.out.println(plusDate1); 结果： 122017-06-202019-06-20 示例 10、使用 Java 8 的 Clock 时钟类Java 8 增加了一个 Clock 时钟类用于获取当时的时间戳，或当前时区下的日期时间信息。以前用到System.currentTimeInMillis() 和 TimeZone.getDefault() 的地方都可用 Clock 替换。 1234Clock clock = Clock.systemUTC();Clock clock1 = Clock.systemDefaultZone();System.out.println(clock);System.out.println(clock1); 结果： 12SystemClock[Z]SystemClock[Asia/Shanghai] 示例 11、如何用 Java 判断日期是早于还是晚于另一个日期另一个工作中常见的操作就是如何判断给定的一个日期是大于某天还是小于某天？在 Java 8 中，LocalDate 类有两类方法 isBefore() 和 isAfter() 用于比较日期。调用 isBefore() 方法时，如果给定日期小于当前日期则返回 true。 12345678LocalDate tomorrow = LocalDate.of(2018,6,20);if(tomorrow.isAfter(now))&#123; System.out.println(\"Tomorrow comes after today\");&#125;LocalDate yesterday = now.minus(1, ChronoUnit.DAYS);if(yesterday.isBefore(now))&#123; System.out.println(\"Yesterday is day before today\");&#125; 在 Java 8 中比较日期非常方便，不需要使用额外的 Calendar 类来做这些基础工作了。 示例 12、在 Java 8 中处理时区Java 8 不仅分离了日期和时间，也把时区分离出来了。现在有一系列单独的类如 ZoneId 来处理特定时区，ZoneDateTime 类来表示某时区下的时间。这在 Java 8 以前都是 GregorianCalendar 类来做的。 1234ZoneId america = ZoneId.of(\"America/New_York\");LocalDateTime localtDateAndTime = LocalDateTime.now();ZonedDateTime dateAndTimeInNewYork = ZonedDateTime.of(localtDateAndTime, america );System.out.println(dateAndTimeInNewYork); 示例 13、如何表示信用卡到期这类固定日期，答案就在 YearMonth与 MonthDay 检查重复事件的例子相似，YearMonth 是另一个组合类，用于表示信用卡到期日、FD 到期日、期货期权到期日等。还可以用这个类得到 当月共有多少天，YearMonth 实例的 lengthOfMonth() 方法可以返回当月的天数，在判断 2 月有 28 天还是 29 天时非常有用。 1234YearMonth currentYearMonth = YearMonth.now();System.out.printf(\"Days in month year %s: %d%n\", currentYearMonth, currentYearMonth.lengthOfMonth());YearMonth creditCardExpiry = YearMonth.of(2018, Month.FEBRUARY);System.out.printf(\"Your credit card expires on %s %n\", creditCardExpiry); 结果： 12Days in month year 2018-06: 30Your credit card expires on 2018-02 示例 14、如何在 Java 8 中检查闰年LocalDate 类有一个很实用的方法 isLeapYear() 判断该实例是否是一个闰年。 示例 15、计算两个日期之间的天数和月数有一个常见日期操作是计算两个日期之间的天数、周数或月数。在 Java 8 中可以用 java.time.Period 类来做计算。下面这个例子中，我们计算了当天和将来某一天之间的月数。 123LocalDate date = LocalDate.of(2019, Month.MARCH, 20);Period period = Period.between(now, date);System.out.println(&quot;离下个时间还有&quot; + period.getMonths() + &quot; 个月&quot;); 示例 16、包含时差信息的日期和时间在 Java 8 中，ZoneOffset 类用来表示时区，举例来说印度与 GMT 或 UTC 标准时区相差 +05:30，可以通过ZoneOffset.of() 静态方法来获取对应的时区。一旦得到了时差就可以通过传入 LocalDateTime 和 ZoneOffset 来创建一个 OffSetDateTime 对象。 1234LocalDateTime datetime = LocalDateTime.of(2014, Month.JANUARY, 14,19,30);ZoneOffset offset = ZoneOffset.of(\"+05:30\");OffsetDateTime date = OffsetDateTime.of(datetime, offset); System.out.println(\"Date and Time with timezone offset in Java : \" + date); 示例 17、在 Java 8 中获取当前的时间戳如果你还记得 Java 8 以前是如何获得当前时间戳，那么现在你终于解脱了。Instant 类有一个静态工厂方法 now() 会返回当前的时间戳，如下所示： 12Instant timestamp = Instant.now();System.out.println(timestamp); 结果： 12018-06-20T06:35:24.881Z 时间戳信息里同时包含了日期和时间，这和 java.util.Date 很像。实际上 Instant 类确实等同于 Java 8 之前的 Date类，你可以使用 Date 类和 Instant 类各自的转换方法互相转换，例如：Date.from(Instant) 将 Instant 转换成java.util.Date，Date.toInstant() 则是将 Date 类转换成 Instant 类。 示例 18、在 Java 8 中如何使用预定义的格式化工具去解析或格式化日期在 Java 8 以前的世界里，日期和时间的格式化非常诡异，唯一的帮助类 SimpleDateFormat 也是非线程安全的，而且用作局部变量解析和格式化日期时显得很笨重。幸好线程局部变量能使它在多线程环境中变得可用，不过这都是过去时了。Java 8 引入了全新的日期时间格式工具，线程安全而且使用方便。它自带了一些常用的内置格式化工具。 参见我上一篇文章： 《SimpleDateFormat 如何安全的使用？》 示例 19、如何在 Java 中使用自定义格式化工具解析日期尽管内置格式化工具很好用，有时还是需要定义特定的日期格式。可以调用 DateTimeFormatter 的 ofPattern() 静态方法并传入任意格式返回其实例，格式中的字符和以前代表的一样，M 代表月，m 代表分。如果格式不规范会抛出 DateTimeParseException 异常，不过如果只是把 M 写成 m 这种逻辑错误是不会抛异常的。 参见我上一篇文章： 《SimpleDateFormat 如何安全的使用？》 示例 20、在 Java 8 中如何把日期转换成字符串上两个主要是从字符串解析日期。现在我们反过来，把 LocalDateTime 日期实例转换成特定格式的字符串。这是迄今为止 Java 日期转字符串最为简单的方式了。下面的例子将返回一个代表日期的格式化字符串。和前面类似，还是需要创建 DateTimeFormatter 实例并传入格式，但这回调用的是 format() 方法，而非 parse() 方法。这个方法会把传入的日期转化成指定格式的字符串。 123456789LocalDateTime arrivalDate = LocalDateTime.now();try &#123; DateTimeFormatter format = DateTimeFormatter.ofPattern(\"MMMdd yyyy hh:mm a\"); String landing = arrivalDate.format(format); System.out.printf(\"Arriving at : %s %n\", landing);&#125;catch (DateTimeException ex) &#123; System.out.printf(\"%s can't be formatted!%n\", arrivalDate); ex.printStackTrace();&#125; Java 8 日期时间 API 的重点通过这些例子，你肯定已经掌握了 Java 8 日期时间 API 的新知识点。现在来回顾一下这个优雅 API 的使用要点： 1）提供了 javax.time.ZoneId 获取时区。 2）提供了 LocalDate 和 LocalTime 类。 3）Java 8 的所有日期和时间 API 都是不可变类并且线程安全，而现有的 Date 和 Calendar API 中的 java.util.Date 和SimpleDateFormat 是非线程安全的。 4）主包是 java.time, 包含了表示日期、时间、时间间隔的一些类。里面有两个子包 java.time.format 用于格式化， java.time.temporal 用于更底层的操作。 5）时区代表了地球上某个区域内普遍使用的标准时间。每个时区都有一个代号，格式通常由区域/城市构成（Asia/Tokyo），在加上与格林威治或 UTC 的时差。例如：东京的时差是 +09:00。 6）OffsetDateTime 类实际上组合了 LocalDateTime 类和 ZoneOffset 类。用来表示包含和格林威治或 UTC 时差的完整日期（年、月、日）和时间（时、分、秒、纳秒）信息。 7）DateTimeFormatter 类用来格式化和解析时间。与 SimpleDateFormat 不同，这个类不可变并且线程安全，需要时可以给静态常量赋值。 DateTimeFormatter 类提供了大量的内置格式化工具，同时也允许你自定义。在转换方面也提供了 parse() 将字符串解析成日期，如果解析出错会抛出 DateTimeParseException。DateTimeFormatter 类同时还有format() 用来格式化日期，如果出错会抛出 DateTimeException异常。 8）再补充一点，日期格式“MMM d yyyy”和“MMM dd yyyy”有一些微妙的不同，第一个格式可以解析“Jan 2 2014”和“Jan 14 2014”，而第二个在解析“Jan 2 2014”就会抛异常，因为第二个格式里要求日必须是两位的。如果想修正，你必须在日期只有个位数时在前面补零，就是说“Jan 2 2014”应该写成 “Jan 02 2014”。 相关文章SimpleDateFormat 如何安全的使用？","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"SimpleDateFormat 如何安全的使用？","date":"2018-06-18T16:00:00.000Z","path":"2018/06/19/SimpleDateFormat/","text":"前言为什么会写这篇文章？因为这些天在看《阿里巴巴开发手册详尽版》，没看过的可以关注微信公众号：zhisheng，回复关键字：阿里巴巴开发手册详尽版 就可以获得。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/06/19/SimpleDateFormat/ 在看的过程中有这么一条： 【强制】SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为 static，必须加锁，或者使用 DateUtils 工具类。 看到这条我立马就想起了我实习的时候有个项目里面就犯了这个错误，记得当时是这样写的： 1private static final SimpleDateFormat df = new SimpleDateFormat(\"yyyyMMddHHmmss\"); 所以才认真的去研究下这个 SimpleDateFormat，所以才有了这篇文章。 它是谁？想必大家对 SimpleDateFormat 并不陌生。SimpleDateFormat 是 Java 中一个非常常用的类，他是以区域敏感的方式格式化和解析日期的具体类。 它允许格式化 (date -&gt; text)、语法分析 (text -&gt; date)和标准化。 SimpleDateFormat 允许以任何用户指定的日期-时间格式方式启动。 但是，建议使用 DateFormat 中的 getTimeInstance、 getDateInstance 或 getDateTimeInstance 方法来创建一个日期-时间格式。 这几个方法会返回一个默认的日期／时间格式。 你可以根据需要用 applyPattern 方法修改格式方式。 日期时间格式日期和时间格式由 日期和时间模式字符串 指定。在 日期和时间模式字符串 中，未加引号的字母 ‘A’ 到 ‘Z’ 和 ‘a’ 到 ‘z’ 被解释为模式字母，用来表示日期或时间字符串元素。文本可以使用单引号 (‘) 引起来，以免进行解释。所有其他字符均不解释，只是在格式化时将它们简单复制到输出字符串。 简单的讲：这些 A ——Z，a —— z 这些字母(不被单引号包围的)会被特殊处理替换为对应的日期时间，其他的字符串还是原样输出。 日期和时间模式(注意大小写，代表的含义是不同的)如下： 怎么使用？日期／时间格式模版样例：（给的时间是：2001-07-04 12:08:56 U.S. Pacific Time time zone） 使用方法： 1234567891011121314151617181920212223import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/19 */public class FormatDateTime &#123; public static void main(String[] args) &#123; SimpleDateFormat myFmt = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒\"); SimpleDateFormat myFmt1 = new SimpleDateFormat(\"yy/MM/dd HH:mm\"); SimpleDateFormat myFmt2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//等价于now.toLocaleString() SimpleDateFormat myFmt3 = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒 E \"); SimpleDateFormat myFmt4 = new SimpleDateFormat(\"一年中的第 D 天 一年中第w个星期 一月中第W个星期 在一天中k时 z时区\"); Date now = new Date(); System.out.println(myFmt.format(now)); System.out.println(myFmt1.format(now)); System.out.println(myFmt2.format(now)); System.out.println(myFmt3.format(now)); System.out.println(myFmt4.format(now)); System.out.println(now.toGMTString()); System.out.println(now.toLocaleString()); System.out.println(now.toString()); &#125;&#125; 结果是： 123456782018年06月19日 23时10分05秒18/06/19 23:102018-06-19 23:10:052018年06月19日 23时10分05秒 星期二一年中的第 170 天 一年中第25个星期 一月中第4个星期 在一天中23时 CST时区19 Jun 2018 15:10:05 GMT2018-6-19 23:10:05Tue Jun 19 23:10:05 CST 2018 使用方法很简单，就是先自己定义好时间／日期模版，然后调用 format 方法（传入一个时间 Date 参数）。 上面的是日期转换成自己想要的字符串格式。下面反过来，将字符串类型装换成日期类型： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/19 */public class StringFormatDate &#123; public static void main(String[] args) &#123; String time1 = \"2018年06月19日 23时10分05秒\"; String time2 = \"18/06/19 23:10\"; String time3 = \"2018-06-19 23:10:05\"; String time4 = \"2018年06月19日 23时10分05秒 星期二\"; SimpleDateFormat myFmt = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒\"); SimpleDateFormat myFmt1 = new SimpleDateFormat(\"yy/MM/dd HH:mm\"); SimpleDateFormat myFmt2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//等价于now.toLocaleString() SimpleDateFormat myFmt3 = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒 E\"); Date date1 = null; try &#123; date1 = myFmt.parse(time1); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date1); Date date2 = null; try &#123; date2 = myFmt1.parse(time2); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date2); Date date3 = null; try &#123; date3 = myFmt2.parse(time3); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date3); Date date4 = null; try &#123; date4 = myFmt3.parse(time4); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date4); &#125;&#125; 结果是： 1234Tue Jun 19 23:10:05 CST 2018Tue Jun 19 23:10:00 CST 2018Tue Jun 19 23:10:05 CST 2018Tue Jun 19 23:10:05 CST 2018 这个转换方法也很简单。但是不要高兴的太早，主角不在这。 线程不安全 在 SimpleDateFormat 类的 JavaDoc 中，描述了该类不能够保证线程安全，建议为每个线程创建单独的日期／时间格式实例，如果多个线程同时访问一个日期／时间格式，它必须在外部进行同步。那么在多线程环境下调用 format() 和 parse() 方法应该使用同步代码来避免问题。下面我们通过一个具体的场景来一步步的深入学习和理解SimpleDateFormat 类。 1、每个线程创建单独的日期／时间格式实例 大量的创建 SimpleDateFormat 实例对象，然后再丢弃这个对象，占用大量的内存和 JVM 空间。 2、创建一个静态的 SimpleDateFormat 实例，在使用时直接使用这个实例进行操作（我当时就是这么干的😄） 123private static final SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");Date date = new Date();df.format(date); 当然，这个方法的确很不错，在大部分的时间里面都会工作得很好，但一旦在生产环境中一定负载情况下时，这个问题就出来了。他会出现各种不同的情况，比如转化的时间不正确，比如报错，比如线程被挂死等等。我们看下面的测试用例，拿事实说话： 1234567891011121314151617import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/20 */public class DateUtils &#123; private static final SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; return sdf.format(date); &#125; public static Date parse(String strDate) throws ParseException &#123; return sdf.parse(strDate); &#125;&#125; 12345678910111213141516171819202122232425262728import java.text.ParseException;/** * Created by zhisheng_tian on 2018/6/20 */public class DateUtilsTest &#123; public static class TestSimpleDateFormatThreadSafe extends Thread &#123; @Override public void run() &#123; while (true) &#123; try &#123; this.join(2000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; System.out.println(this.getName() + \":\" + DateUtils.parse(\"2018-06-20 01:18:20\")); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; new TestSimpleDateFormatThreadSafe().start(); &#125; &#125;&#125; 运行结果如下： 1234567891011121314151617181920212223242526Exception in thread \"Thread-0\" Exception in thread \"Thread-1\" java.lang.NumberFormatException: For input string: \"\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:601) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2051) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at com.zhisheng.demo.date.DateUtils.parse(DateUtils.java:19) at com.zhisheng.demo.date.DateUtilsTest$TestSimpleDateFormatThreadSafe.run(DateUtilsTest.java:19)java.lang.NumberFormatException: For input string: \".1818\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:578) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2051) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:2162) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at com.zhisheng.demo.date.DateUtils.parse(DateUtils.java:19) at com.zhisheng.demo.date.DateUtilsTest$TestSimpleDateFormatThreadSafe.run(DateUtilsTest.java:19)Thread-2:Sat Jun 20 01:18:20 CST 2201Thread-2:Wed Jun 20 01:18:20 CST 2018Thread-2:Wed Jun 20 01:18:20 CST 2018Thread-2:Wed Jun 20 01:18:20 CST 2018 说明：Thread-1和Thread-0报java.lang.NumberFormatException: multiple points错误，直接挂死，没起来；Thread-2 虽然没有挂死，但输出的时间是有错误的，比如我们输入的时间是：2018-06-20 01:18:20 ，当会输出：Sat Jun 20 01:18:20 CST 2201 这样的灵异事件。 Why?为什么会出现线程不安全的问题呢？ 下面我们通过看 JDK 源码来看看为什么 SimpleDateFormat 和 DateFormat 类不是线程安全的真正原因： SimpleDateFormat 继承了 DateFormat，在 DateFormat 中定义了一个 protected 属性的 Calendar 类的对象：calendar。只是因为 Calendar 类的概念复杂，牵扯到时区与本地化等等，JDK 的实现中使用了成员变量来传递参数，这就造成在多线程的时候会出现错误。 在 SimpleDateFormat 中的 format 方法源码中： 123456789101112131415161718192021222324252627282930313233@Overridepublic StringBuffer format(Date date, StringBuffer toAppendTo,FieldPosition pos) &#123; pos.beginIndex = pos.endIndex = 0; return format(date, toAppendTo, pos.getFieldDelegate());&#125;// Called from Format after creating a FieldDelegateprivate StringBuffer format(Date date, StringBuffer toAppendTo,FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo;&#125; calendar.setTime(date) 这条语句改变了 calendar，稍后，calendar 还会用到（在 subFormat 方法里），而这就是引发问题的根源。想象一下，在一个多线程环境下，有两个线程持有了同一个 SimpleDateFormat 的实例，分别调用format 方法： 12345线程 1 调用 format 方法，改变了 calendar 这个字段。线程 1 中断了。线程 2 开始执行，它也改变了 calendar。线程 2 中断了。线程 1 回来了 此时，calendar 已然不是它所设的值，而是走上了线程 2 设计的道路。如果多个线程同时争抢 calendar 对象，则会出现各种问题，时间不对，线程挂死等等。 分析一下 format 的实现，我们不难发现，用到成员变量 calendar，唯一的好处，就是在调用 subFormat 时，少了一个参数，却带来了许多的问题。其实，只要在这里用一个局部变量，一路传递下去，所有问题都将迎刃而解。 这个问题背后隐藏着一个更为重要的问题–无状态：无状态方法的好处之一，就是它在各种环境下，都可以安全的调用。衡量一个方法是否是有状态的，就看它是否改动了其它的东西，比如全局变量，比如实例的字段。format 方法在运行过程中改动了 SimpleDateFormat 的 calendar 字段，所以，它是有状态的。 这也同时提醒我们在开发和设计系统的时候注意下一下三点: 1.自己写公用类的时候，要对多线程调用情况下的后果在注释里进行明确说明 2.多线程环境下，对每一个共享的可变变量都要注意其线程安全性 3.我们的类和方法在做设计的时候，要尽量设计成无状态的 解决方法1、需要的时候创建新实例 说明：在需要用到 SimpleDateFormat 的地方新建一个实例，不管什么时候，将有线程安全问题的对象由共享变为局部私有都能避免多线程问题，不过也加重了创建对象的负担。在一般情况下，这样其实对性能影响比不是很明显的。 2、使用同步：同步 SimpleDateFormat 对象 1234567891011121314151617181920import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class DateSyncUtil &#123; private static SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; synchronized(sdf) &#123; return sdf.format(date); &#125; &#125; public static Date parse(String strDate) throws ParseException &#123; synchronized(sdf) &#123; return sdf.parse(strDate); &#125; &#125;&#125; 说明：当线程较多时，当一个线程调用该方法时，其他想要调用此方法的线程就要 block 等待，多线程并发量大的时候会对性能有一定的影响。 3、使用 ThreadLocal 12345678910111213141516171819202122import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class ConcurrentDateUtil &#123; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; public static Date parse(String dateStr) throws ParseException &#123; return threadLocal.get().parse(dateStr); &#125; public static String format(Date date) &#123; return threadLocal.get().format(date); &#125;&#125; 说明：使用 ThreadLocal, 也是将共享变量变为独享，线程独享肯定能比方法独享在并发环境中能减少不少创建对象的开销。如果对性能要求比较高的情况下，一般推荐使用这种方法。 Java 8 中的解决办法Java 8 提供了新的日期时间 API，其中包括用于日期时间格式化的 DateTimeFormatter，它与 SimpleDateFormat 最大的区别在于：DateTimeFormatter 是线程安全的，而 SimpleDateFormat 并不是线程安全。 DateTimeFormatter 如何使用： 解析日期 123String dateStr= \"2018年06月20日\";DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日\"); LocalDate date= LocalDate.parse(dateStr, formatter); 日期转换为字符串 123LocalDateTime now = LocalDateTime.now(); DateTimeFormatter format = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日 hh:mm a\");String nowStr = now .format(format); 由 DateTimeFormatter 的静态方法 ofPattern() 构建日期格式，LocalDateTime 和 LocalDate 等一些表示日期或时间的类使用 parse 和 format 方法把日期和字符串做转换。 使用新的 API，整个转换过程都不需要考虑线程安全的问题。 总结SimpleDateFormat 是线程不安全的类，多线程环境下注意线程安全问题，如果是 Java 8 ，建议使用 DateTimeFormatter 代替 SimpleDateFormat。 参考资料http://www.cnblogs.com/peida/archive/2013/05/31/3070790.html 相关文章20 个案例教你在 Java 8 中如何处理日期和时间?","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"苦逼的毕业论文经历","date":"2018-05-25T16:00:00.000Z","path":"2018/05/26/paper/","text":"背景最近一直在学校忙毕业论文的事，抱歉了，很长一段时间没更新文章了，今天星期六，昨天星期五，幸好所有资料都在周末前交齐了，昨晚还到参加班上的聚会，也喝了不少酒，但幸好没醉，不然今天肯定不会写下这篇文章的。这篇文章就把从接触到毕业论文指导老师到现在的这半年时间有关毕业论文的事都讲讲，希望能够留下点回忆！ 毕设小分队成立学校会给每个毕业导师安排带几个学生（7～8 人左右），但是在学校发出表格之前，谁也不知道自己将会由哪个导师带，甚至你会发现就算学校发出表格后看到自己导师名字后，自己都不知道这导师是谁？是男是女？（当某个导师的名字比较中性的时候）到底好不好相处？性格咋样？他的联系方式（手机 或者 QQ），WTF，一个都不知道？苦逼了，后面也是通过问辅导员才知道这些情况。后面我们几个人中就有一个带头建了个 QQ 群（毕设小分队），并把指导老师拉进群了。 就这样，我们终于开始了毕设小分队之旅！ 实习时的毕设状态2017.12月那时是还在公司实习的，到了毕业设计选题的时候，我们导师还好，给我们每个人很自由的选择，你可以自拟题目，也可以从导师那里挑选题目。不过我看了下老师给的题目，大都是深度学习、机器学习做的相关图像识别、推荐系统、搜索系统、人脸识别、网络流量监控、文本情感分析等高大上的课题。臣妾做不到，毕竟这都是研究生才会研究点方向，老师会发出来这些题目可能也是和她自己在研究学习的相关知识有关（因为我后来给导师检查我的系统的时候就是导师带的一个研究生检查的，期间那个学长和我说他最近就在做文本情感分析相关的一个项目，因为看到我项目中的中文分词器，所以还向我请教了下，我于是把我原来写过的一篇中英文分词器找出来给他一个个解释），所以导师希望在我们这里也能够有人去研究下这方面的知识，拓展下视野。但是由于在公司实习，工作之外腾出的时间确实有限，没这么多时间去学习这些特别新的东西，而且还要做出一定的成果出来，这还是有点挑战性的。导师说你可以自拟题目，或者把自己在公司做的项目优化后拿来做毕设项目。于是，我选择的是拿自己以前写的项目来当毕业论文项目了，因为感觉项目也还行，拿来做毕设完全可以了，所以后面自己也就比较轻松了。 毕设流程把自己的毕设题目报给老师后，老师会根据你的题目审核是否可以拿来当毕设。然后再要求你把毕业设计的开题报告书（论文题目、目的、意义、怎么做、做出什么效果、目标之类的）写详细上交上去。要经过导师和学校的审批后才能够正式开题。开题后确定后几乎就可以开始写自己的项目了。机智的我，那时比较轻松，哈哈哈，工作之外都撸代码和博客呢。差不多过了一个月后，导师就开始在群里问我们系统完成的怎么样了？然后要我们上交论文任务书，学校的这种材料有时候要的很急，不得不吐槽下，很头疼，自己有时候都一下子交不了。又过了一段时间就需要把论文初稿交上去，真是苦逼，那时还不知道论文的模版到底是咋样？该怎么写本科论文呢？搞得我自己那段时间花了周末两天和一天的上班的时间才勉强交了个一般的初稿，初稿中的各种图画的我是想哭。真多，还麻烦。幸好在那段时间公司的任务还不忙，所以在有时间在实习上班的时候也稍微完成下自己的初稿。 今年 3 ～ 4 月的时候，导师也在不断的叫我们把学校该交的实习每周报告交给她。这个实习还有实习申请表、实习鉴定表材料（要求盖章），这两个资料也是很重要的，如果后面没有这个材料都不能参加答辩的。 在快五月的时候，导师那时就天天晚上深夜在毕设小分队群里催同学们，你们的论文和系统都完成的咋样了？然后又催了要交论文中期报告（这个应该也蛮重要的，我们班两个同学好像就因为这个导致有个论文中期报告警告，吓得他们后来答辩的时候都怕自己挂了）。 过完劳动节后，再过了几天后，我就辞职了。辞职后在苏州玩了几天，期间也是在不断的修改自己的论文和优化些格式问题。 后来又浪回家了，在家呆了几天，也是改论文改到深夜，那段时间自己已经开始在把自己的论文初稿拿到些第三方免费的查重去查重网站去查重，然后根据查重后的结果，将一些重复的地方东改改，西凑凑，或者用自己的话写一遍。这样就可以减少点重复语句。 再然后就是 18 号赶回来学校了，19，20号刚好周末，又是不眠之夜，那两天不断查重，向别人请教，怎么加字数，自己一开始的字数好像还是不够的，学校要求的字数有点高，其他学校都是 8000 字，我们却要求 1.5 w，真是能扯啊，怎么可以扯出这么多字来。通过某人的经验之谈，我成功的扯了不少字，还不带重复的，😄。 然后 21 号周一拿给老师检查的时候就是看了下格式（老师不在办公室，她带的研究生给检查的），说你全文首行咋都没缩进呢（写博客写习惯了，谁还缩进啊），行间距比较大，图的标号错误，主要还是检查格式。当晚，又好好改了下，并和我同学讨论了下他的指导导师的要求有哪些，并也做了相应的修改。（因为我的答辩导师就有我那同学的指导老师）记得当晚好像改论文改到三点，幸好有人陪着我一边聊天一边改，不然早困了，睡了。 22 号一早就醒来了，然后就去打印店打印论文。这里想给的大家一个建议就是：如果你也要打印论文，最好早点去，因为毕业季，学校打印店几乎都是爆满，打印论文的非常多，还有学弟学妹打印各种考试卷子、资料啥的，反正人很多，估计要排队。指导老师也不是一直在办公室的，他们也是要上课的。所以最好在导师在办公室的时候能赶到，这样就不会错过了。这次老师检查论文就比较仔细了，论文一行行的找内容，看是否通顺？是否有不合适的地方？标点符号是不是多了或者少了？格式是不是还有问题？所以呢，这次又很惨，要改的地方很多，还包括流程图和 E-R 图要改的。苦逼了，下午回去租房宾馆的时候就开始拼命的改。改的差不多了，因为第二天要答辩了，所以就在看看自己的系统是否能够跑起来，有没有什么bug，结果还真发现几个小错误和一个大错误，小错误很快修复完善了，有一个 Redis 存数据再取出来的时候数据变化问题当然debug了很久没找到原因。找到深夜一点多，没解决，放弃了，第二天不演示这块。当晚怕自己明天项目启动的时候其他环境要一个个起，需要耗费不少时间，于是自己简单的写了个脚本，一键将自己项目的环境启动起来，这样就可以直接运行项目展示给答辩老师，节省时间，尤其是关键场合，怕掉链子。 23 号早上很早就去把自己的实习申请表、实习鉴定表、实习 20 周日志、实习终结、论文初稿这些资料打印，然后拿给导师查看和打分。（又找出问题来了，苦逼，记得那个早上现场在导师办公室用电脑改好后，检查完才拿去打印再拿回来给导师检查的，来回跑来跑去的真的很急，出了不少汗）注意：越是关键时刻，千万要顺着导师来，他说改哪里就改哪里，千万别刚，我一个同学的论文改的导师都发火了，差点没让我那同学参加答辩，直接进入二辩的。 答辩的时候先每个人介绍下自己的实习经历，听到不少牛逼的同学，进了不少厉害的公司，有的同学还当上了公司的项目组组长了，真厉害，一年的实习时间就混的这么好了。瞬间发现差距很大。 然后就是每个人的答辩了，我第三个，还好，答辩的过程问了我的问题和演示系统的时候都表现的蛮好的。就现场还问了我有个功能的代码是哪？（估计是想看下是不是自己写的项目）后面也看到有的同学项目竟然起不来的，或者回答不出老师的问题的。答辩过程中，答辩老师又给论文找了不少问题，又要苦逼的修改了。 当天就出了待定名单和要二辩的名单，速度还是很快。还好我没有，开森。本以为答辩好了，就可以松下一口气了，后面改好了论文后拿去给导师检查，她都说 OK 了，结果我就去把论文拿去胶装了，注意胶装顺序别搞错了。第二天把胶装好的论文和各种资料一起交给老师的时候，结果又挑出一个目录的问题，WTF，搞得我后面改好后去打印店重新胶装了遍（封皮从原来的上面扯下来胶装）。昨天签完字了，把论文的初稿、终稿、答辩记录、实习材料等材料一起装进档案袋了，这才安心了。 于是当晚就去参加了班上的毕业聚会。 感受这段时间真的是很累，每天熬夜到很晚，第二天早上很早就自然醒了。瘦了不少，牛仔裤的皮带我感觉都缩了一圈，右手的几个手指天天在电脑的触摸板上滑来滑去的，都脱了一层皮了，现在放上去都有点痛，打印论文好像打印了7份，烧钱啊。发现我的记忆还是可以的，这么多事竟然还能记得，搞得整篇文章有点像在记流水账，哈哈哈。反正也是记录下自己的论文答辩这段时间的经历。论文的格式很重要，不再像写博客那样随意，论文是需要以一种严谨的态度去对待的。 写着写着忘了说论文的查重了。一般你一开始最好先用第三方的免费查重下，然后修改。 我当时用的是：http://xueshu.baidu.com/usercenter/papercheck/ 这个地方有几个是可以第一次免费查重的，如果次数用完了，记得换个百度账号就又有好些次查重的机会。如果你实在是不放心，可以去学校的打印店问下是否有论文查重的，也是查的知网的库，和学校知网查重的区别不大，我比较自信，没花这个钱，毕竟好贵，😯我穷，哈哈哈。学校规定的是不能高于 30% 的查重率，建议还是自己把查重降低到 20% 以下然后再提交到学校知网去，不然超过的话是需要二次查重的。 还有一个感受就是：多和你的导师交流，遇到不会或者拿不准的最好在群里问他，然后他说的错误，你要及时更改好过来，说你问题的时候要学会脸上挂着笑容，嘻嘻嘻就过去了，然后记住该问题的错误，回去立马改好，及时拿材料给老师查阅修改好的论文。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","date":"2018-04-29T16:00:00.000Z","path":"2018/04/30/springboot_SpringApplication/","text":"SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/30/springboot_SpringApplication/ 前言在 Spring Boot 项目的启动类中常见代码如下： 123456@SpringBootApplicationpublic class SpringbotApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbotApplication.class, args); &#125;&#125; 其中也就两个比较引人注意的地方： @SpringBootApplication SpringApplication.run() 对于第一个注解 @SpringBootApplication，我已经在博客 Spring Boot 2.0系列文章(六)：Spring Boot 2.0中SpringBootApplication注解详解 中详细的讲解了。接下来就是深入探究第二个了 SpringApplication.run() 。 换个姿势上面的姿势太简单了，只一行代码就完事了。 1SpringApplication.run(SpringbotApplication.class, args); 其实是支持做一些个性化的设置，接下来我们换个姿势瞧瞧： 123456789@SpringBootApplicationpublic class SpringbotApplication &#123; public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(SpringbotApplication.class); // 自定义应用程序的配置 //app.setXxx() app.run(args) &#125;&#125; 没错，就是通过一个构造函数，然后设置相关的属性，从而达到定制化服务。有哪些属性呢？ 属性对应的 get／set 方法 看到没，还很多呢！ 举个例子：你想把 Spring Boot 项目的默认 Banner 换成你自己的，就需要在这里如下： 123456789101112131415161718public static void main(String[] args) &#123;// SpringApplication.run(Springboot2Application.class, args); SpringApplication application = new SpringApplication(Springboot2Application.class); application.setBanner((environment, sourceClass, out) -&gt; &#123; //这里打印一个logo System.out.println(\" _ _ _\\n\" + \" | | (_) | |\\n\" + \" ____| |__ _ ___ | |__ ___ _ __ __ _\\n\" + \"|_ /| '_ \\\\ | |/ __|| '_ \\\\ / _ \\\\| '_ \\\\ / _` |\\n\" + \" / / | | | || |\\\\__ \\\\| | | || __/| | | || (_| |\\n\" + \"/___||_| |_||_||___/|_| |_| \\\\___||_| |_| \\\\__, |\\n\" + \" __/ |\\n\" + \" |___/\\n\"); &#125;); application.setBannerMode(Banner.Mode.CONSOLE); //你还可以干其他的定制化初始设置 application.run(args);&#125; 现在重启项目，你就会发现，控制台的 logo 已经换成你自己的了。 当然了，你可能会觉得这样写有点复杂，嗯嗯，确实，这样硬编码在代码里确实不太友好。你还可以在src/main/resources路径下新建一个banner.txt文件，banner.txt中填写好需要打印的字符串内容即可。 从该类中可以看到在 Spring Boot 2 中引入了个新的 WebApplicationType 和 WebEnvironment。 确实，这也是 Spring Boot 2 中比较大的特性，它是支持响应式编程的。我之前在文章 Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 中也介绍过，以后有机会会介绍它的，这里我先卖个关子。 SpringApplication 初始化SpringApplication.run() 的实现才是我们要深入探究的主角，该方法代码如下： 123456789//静态方法，可用于使用默认配置运行 SpringApplicationpublic static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) &#123; return run(new Class&lt;?&gt;[] &#123; primarySource &#125;, args);&#125;public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args);&#125; 在这个静态方法中，创建 SpringApplication 对象，并调用该对象的 run 方法。 123456789101112131415public SpringApplication(Class&lt;?&gt;... primarySources) &#123; this(null, primarySources);&#125;//创建一个 SpringApplication 实例，应用上下文会根据指定的主要资源加载 beans ，实例在调用 run 方法之前可以定制化@SuppressWarnings(&#123; \"unchecked\", \"rawtypes\" &#125;)public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = deduceWebApplicationType(); setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125; 首先是进入单个参数的构造方法，然后进入两参数的构造方法（ResourceLoader 为 null），然后进行初始化。 1、deduceWebApplicationType() : 推断应用的类型 ，创建的是一个 SERVLET 应用还是 REACTIVE应用或者是 NONE 1234567891011121314151617private static final String REACTIVE_WEB_ENVIRONMENT_CLASS = \"org.springframework.web.reactive.DispatcherHandler\";private static final String MVC_WEB_ENVIRONMENT_CLASS = \"org.springframework.web.servlet.DispatcherServlet\";private static final String[] WEB_ENVIRONMENT_CLASSES = &#123; \"javax.servlet.Servlet\", \"org.springframework.web.context.ConfigurableWebApplicationContext\" &#125;;private WebApplicationType deduceWebApplicationType() &#123; if (ClassUtils.isPresent(REACTIVE_WEB_ENVIRONMENT_CLASS, null) &amp;&amp; !ClassUtils.isPresent(MVC_WEB_ENVIRONMENT_CLASS, null)) &#123; return WebApplicationType.REACTIVE; //该程序是 REACTIVE 程序 &#125; for (String className : WEB_ENVIRONMENT_CLASSES) &#123; if (!ClassUtils.isPresent(className, null)) &#123; return WebApplicationType.NONE; //该程序为 NONE &#125; &#125; return WebApplicationType.SERVLET; //默认返回是 SERVLET 程序&#125; 2、setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class))：初始化 classpath 下的所有的可用的 ApplicationContextInitializer。 1）、getSpringFactoriesInstances() 123456789101112131415161718192021222324252627282930313233private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type) &#123; return getSpringFactoriesInstances(type, new Class&lt;?&gt;[] &#123;&#125;);&#125;//获取所有的 Spring 工厂实例private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type,Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); //获取所有 Spring Factories 的名字 List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); //Spring 工厂实例排序 return instances;&#125;//根据读取到的名字创建对象（Spring 工厂实例）private &lt;T&gt; List&lt;T&gt; createSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, ClassLoader classLoader, Object[] args, Set&lt;String&gt; names) &#123; List&lt;T&gt; instances = new ArrayList&lt;&gt;(names.size()); for (String name : names) &#123; try &#123; Class&lt;?&gt; instanceClass = ClassUtils.forName(name, classLoader); Assert.isAssignable(type, instanceClass); Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(parameterTypes); T instance = (T) BeanUtils.instantiateClass(constructor, args); instances.add(instance); &#125; catch (Throwable ex) &#123; throw new IllegalArgumentException( \"Cannot instantiate \" + type + \" : \" + name, ex); &#125; &#125; return instances;&#125; 上面的 SpringFactoriesLoader.loadFactoryNames() ，是从 META-INF/spring.factories 的资源文件中，读取 key 为org.springframework.context.ApplicationContextInitializer 的 value。 而 spring.factories 的部分内容如下： 可以看到，最近的得到的，是 ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer 这四个类的名字。 2）、setInitializers()： 12345public void setInitializers( Collection&lt;? extends ApplicationContextInitializer&lt;?&gt;&gt; initializers) &#123; this.initializers = new ArrayList&lt;&gt;(); this.initializers.addAll(initializers);&#125; 所以，这里 setInitializers() 所得到的成员变量 initializers 就被初始化为ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer 这四个类的对象组成的 list。 3、setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class))：初始化 classpath 下的所有的可用的 ApplicationListener。 1）、getSpringFactoriesInstances() 和上面的类似，但是它是从 META-INF/spring.factories 的资源文件中，获取到 key 为 org.springframework.context.ApplicationListener 的 value。 2）、setListeners()： 1234public void setListeners(Collection&lt;? extends ApplicationListener&lt;?&gt;&gt; listeners) &#123; this.listeners = new ArrayList&lt;&gt;(); this.listeners.addAll(listeners);&#125; 所以，这里 setListeners() 所得到的成员变量 listeners 就被初始化为 ClearCachesApplicationListener，ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener ，ConfigFileApplicationListener，DelegatingApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener，LiquibaseServiceLocatorApplicationListener 这九个类的对象组成的 list。 4、deduceMainApplicationClass() ：根据调用栈，推断出 main 方法的类名 1234567891011121314private Class&lt;?&gt; deduceMainApplicationClass() &#123; try &#123; StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) &#123; if (\"main\".equals(stackTraceElement.getMethodName())) &#123; return Class.forName(stackTraceElement.getClassName()); &#125; &#125; &#125; catch (ClassNotFoundException ex) &#123; // Swallow and continue &#125; return null;&#125; run 方法背后的秘密上面看完了构造方法后，已经初始化了一个 SpringApplication 对象，接下来调用其 run 方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445//运行 Spring 应用程序，创建并刷新一个新的 ApplicationContextpublic ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context; &#125; 可变个数参数 args 即是我们整个应用程序的入口 main 方法的参数。StopWatch 是来自 org.springframework.util 的工具类，可以用来方便的记录程序的运行时间。 再来看看 1.5.12 与 2.0.1 版本的 run 方法 有什么不一样的地方？ 接下来好好分析上面新版本（2.0.1）的 run 方法的代码并配合比较旧版本（1.5.12）。 1、configureHeadlessProperty()：设置 headless 模式 1234567private static final String SYSTEM_PROPERTY_JAVA_AWT_HEADLESS = \"java.awt.headless\";private boolean headless = true;private void configureHeadlessProperty() &#123; System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty( SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless)));&#125; 实际上是就是设置系统属性 java.awt.headless，该属性会被设置为 true。 2、getRunListeners()：加载 SpringApplicationRunListener 对象 12345678910111213141516171819 //TODO: xxxSpringApplicationRunListeners listeners = getRunListeners(args);//初始化监听器listeners.starting();try &#123; prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); listeners.started(context); callRunners(context, applicationArguments);&#125;try &#123; listeners.running(context);&#125;private SpringApplicationRunListeners getRunListeners(String[] args) &#123; Class&lt;?&gt;[] types = new Class&lt;?&gt;[] &#123; SpringApplication.class, String[].class &#125;; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances( SpringApplicationRunListener.class, types, this, args));&#125; 上面的 getRunListeners() 中也利用 SpringFactoriesLoader 加载 META-INF/spring.factories 中 key 为 SpringApplicationRunListener 的值，然后再将获取到的值作为参数传递到 SpringApplicationRunListeners 的构造方法中去创建对象。 3、new DefaultApplicationArguments(args) ：获取启动时传入参数 args（main 方法传进来的参数） 并初始化为 ApplicationArguments 对象。 12345public DefaultApplicationArguments(String[] args) &#123; Assert.notNull(args, \"Args must not be null\"); this.source = new Source(args); this.args = args;&#125; 4、prepareEnvironment(listeners, applicationArguments)：根据 listeners 和 applicationArguments 配置SpringBoot 应用的环境。 1234567891011121314151617181920212223242526272829303132333435363738private ConfigurableEnvironment prepareEnvironment( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); listeners.environmentPrepared(environment); bindToSpringApplication(environment); if (this.webApplicationType == WebApplicationType.NONE) &#123; environment = new EnvironmentConverter(getClassLoader()) .convertToStandardEnvironmentIfNecessary(environment); &#125; ConfigurationPropertySources.attach(environment); return environment;&#125;//如果 environment 不为空，直接 get 到，否则创建private ConfigurableEnvironment getOrCreateEnvironment() &#123; if (this.environment != null) &#123; return this.environment; &#125; if (this.webApplicationType == WebApplicationType.SERVLET) &#123; return new StandardServletEnvironment(); &#125; return new StandardEnvironment();&#125;//配置环境protected void configureEnvironment(ConfigurableEnvironment environment,String[] args) &#123; configurePropertySources(environment, args);//配置要使用的PropertySources configureProfiles(environment, args);//配置要使用的Profiles&#125;//将环境绑定到 SpringApplicationprotected void bindToSpringApplication(ConfigurableEnvironment environment) &#123; try &#123; Binder.get(environment).bind(\"spring.main\", Bindable.ofInstance(this)); &#125; catch (Exception ex) &#123; throw new IllegalStateException(\"Cannot bind to SpringApplication\", ex); &#125;&#125; 5、configureIgnoreBeanInfo(environment)：根据环境信息配置要忽略的 bean 信息 1234567891011public static final String IGNORE_BEANINFO_PROPERTY_NAME = \"spring.beaninfo.ignore\";private void configureIgnoreBeanInfo(ConfigurableEnvironment environment) &#123; if (System.getProperty( CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME) == null) &#123; Boolean ignore = environment.getProperty(\"spring.beaninfo.ignore\", Boolean.class, Boolean.TRUE); System.setProperty(CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME, ignore.toString()); &#125;&#125; 6、printBanner(environment)：打印标志，上面我已经说过了。 12345678910111213private Banner printBanner(ConfigurableEnvironment environment) &#123; if (this.bannerMode == Banner.Mode.OFF) &#123; //如果设置为 off，不打印 Banner return null; &#125; ResourceLoader resourceLoader = this.resourceLoader != null ? this.resourceLoader : new DefaultResourceLoader(getClassLoader()); SpringApplicationBannerPrinter bannerPrinter = new SpringApplicationBannerPrinter( resourceLoader, this.banner); if (this.bannerMode == Mode.LOG) &#123; return bannerPrinter.print(environment, this.mainApplicationClass, logger); &#125; return bannerPrinter.print(environment, this.mainApplicationClass, System.out);&#125; 7、createApplicationContext()：根据应用类型来确定该 Spring Boot 项目应该创建什么类型的 ApplicationContext ，默认情况下，如果没有明确设置的应用程序上下文或应用程序上下文类，该方法会在返回合适的默认值。 1234567891011121314151617181920212223242526public static final String DEFAULT_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext\";public static final String DEFAULT_REACTIVE_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext\";public static final String DEFAULT_CONTEXT_CLASS = \"org.springframework.context.annotation.AnnotationConfigApplicationContext\";protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; //根据应用程序的类型来初始化容器 case SERVLET: //servlet 应用程序 contextClass = Class.forName(DEFAULT_WEB_CONTEXT_CLASS); break; case REACTIVE: //reactive 应用程序 contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: //默认 contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext,please specify an ApplicationContextClass\",ex); &#125; &#125; //最后通过Spring的工具类 BeanUtils 初始化容器类 bean return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);&#125; 来看看在 1.5.12 中是怎么样的？ 8、exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context) 1234567891011private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names);//根据类型 key 为 SpringBootExceptionReporter 去加载 AnnotationAwareOrderComparator.sort(instances);//对实例排序 return instances;&#125; 这里也是通过 SpringFactoriesLoader 加载 META-INF/spring.factories 中 key 为 SpringBootExceptionReporter 的全类名的 value 值。 9、prepareContext(context, environment, listeners, applicationArguments, printedBanner)：完成整个容器的创建与启动以及 bean 的注入功能。 12345678910111213141516171819202122232425262728//装配 Contextprivate void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; //将之前准备好的 environment 设置给创建好的 ApplicationContext 使用 context.setEnvironment(environment); //1、 postProcessApplicationContext(context); //2、 applyInitializers(context); listeners.contextPrepared(context); if (this.logStartupInfo) &#123;//启动日志 logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // Add boot specific singleton beans context.getBeanFactory().registerSingleton(\"springApplicationArguments\", applicationArguments); if (printedBanner != null) &#123; context.getBeanFactory().registerSingleton(\"springBootBanner\", printedBanner); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); //3、 load(context, sources.toArray(new Object[0])); listeners.contextLoaded(context);&#125; 1）、postProcessApplicationContext(context) 12345678910111213141516171819public static final String CONFIGURATION_BEAN_NAME_GENERATOR = \"org.springframework.context.annotation.internalConfigurationBeanNameGenerator\";protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.beanNameGenerator != null) &#123; context.getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context) .setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context) .setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125;&#125; 该方法对 context 进行了预设置，设置了 ResourceLoader 和 ClassLoader，并向 bean 工厂中添加了一个beanNameGenerator 。 2）、applyInitializers(context) 12345678protected void applyInitializers(ConfigurableApplicationContext context) &#123; for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument( initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; 在刷新之前将任何 ApplicationContextInitializer 应用于上下文 3)、load(context, sources.toArray(new Object[0])) 主要是加载各种 beans 到 ApplicationContext 对象中。 1234567891011121314protected void load(ApplicationContext context, Object[] sources) &#123; BeanDefinitionLoader loader = createBeanDefinitionLoader( //2 getBeanDefinitionRegistry(context), sources);// 1 if (this.beanNameGenerator != null) &#123; loader.setBeanNameGenerator(this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; loader.setResourceLoader(this.resourceLoader); &#125; if (this.environment != null) &#123; loader.setEnvironment(this.environment); &#125; loader.load();//3&#125; (1)、getBeanDefinitionRegistry(context) 获取 bean 定义注册表 12345678910private BeanDefinitionRegistry getBeanDefinitionRegistry(ApplicationContext context) &#123; if (context instanceof BeanDefinitionRegistry) &#123; return (BeanDefinitionRegistry) context; &#125; if (context instanceof AbstractApplicationContext) &#123; return (BeanDefinitionRegistry) ((AbstractApplicationContext) context) .getBeanFactory(); &#125; throw new IllegalStateException(\"Could not locate BeanDefinitionRegistry\");&#125; (2)、createBeanDefinitionLoader() 通过 BeanDefinitionLoader 的构造方法把参数（注册表、资源）传进去，然后创建 BeanDefinitionLoader。 (3)、load() 把资源全部加载。 10、refreshContext(context) 12345678910111213141516private void refreshContext(ConfigurableApplicationContext context) &#123; refresh(context);//1 if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; catch (AccessControlException ex) &#123; // Not allowed in some environments. &#125; &#125;&#125;//刷新底层的 ApplicationContextprotected void refresh(ApplicationContext applicationContext) &#123; Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh();&#125; refreshContext(context) 方法又调用了 refresh(context)。在调用了 refresh(context) 方法之后，调用了 registerShutdownHook 方法。继续看它的 refresh 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); //1 // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; 。。。 // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 到这里，我们就看见重点了，仔细看上的注释，正在做各种初始化工作，而今天我们关注的重点就是方法 finishBeanFactoryInitialization(beanFactory)。该方法进行了非懒加载 beans 的初始化工作。现在我们进入该方法内部，一探究竟。 看上图方法中的最后一步，调用了 beanFactory 的 preInstantiateSingletons() 方法。此处的 beanFactory 是哪个类的实例对象呢？ 可以看到 ConfigurableListableBeanFactory 接口的实现类只有 DefaultListableBeanFactory，我们看下实现类中的 preInstantiateSingletons 方法是怎么做的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public void preInstantiateSingletons() throws BeansException &#123; // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... for (String beanName : beanNames) &#123; RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; if (isFactoryBean(beanName)) &#123; Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean)&#123; isEagerInit = AccessController.doPrivileged((PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; else &#123; getBean(beanName); &#125; &#125; &#125; // Trigger post-initialization callback for all applicable beans... for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 从上面的代码中可以看到很多调用了 getBean(beanName) 方法，跟踪此方法进去后，最终发现 getBean 调用了AbstractBeanFactory 类的 doGetBean(xxx) 方法，doGetBean(xxx) 方法中有这么一段代码： 但是 createBean() 方法并没有得到实现，实现类在 AbstractAutowireCapableBeanFactory 中。这才是创建 bean 的核心方法。 不知不觉，代码看的越来越深，感觉思维都差点回不去 run 方法了，切回大脑的上下文线程到 run 方法去。 11、afterRefresh(context, applicationArguments)：在上下文刷新后调用该方法，其内部没有做任何操作。 发现没做任何操作了之后，就觉得有点奇怪，所以把当前版本和 1.5.12 对比了下，发现： 在 1.5.12 中的 afterRefresh() 方法中调用了 callRunners() 方法，但是在 2.0.1 版本中的 run 方法中调用了 callRunners () 方法: 这里不得不说 SpringApplicationRunListeners 在 2.0.1 中的改变： 可以发现在 run 方法中，SpringApplicationRunListeners 监听器的状态花生了变化，这也是通过对比不同版本的代码才知道的区别，所以说我们看源码需要多对比着看。 so，我们来看下这个 SpringApplicationRunListener 这个接口： started 状态：The context has been refreshed and the application has started but CommandLineRunner and ApplicationRunner have not been called running 状态：Called immediately before the run method finishes, when the application context has been refreshed and all CommandLineRunner and ApplicationRunners have been called. 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文从源码级别分析了 Spring Boot 应用程序的启动过程，着重看了 SpringApplication 类中的构造函数的初始化和其 run 方法内部实现，并把涉及到的流程代码都过了一遍。 感悟：有时候跟代码跟着跟着，发现越陷越深，好难跳出来！后面还需多向别人请教阅读源码的技巧！ 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！ 自己本人能力有限，源码看的不多，上面如有不对的还请留言交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"分布式锁看这篇就够了","date":"2018-04-23T16:00:00.000Z","path":"2018/04/24/Distributed_lock/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/24/Distributed_lock/ 什么是锁？ 在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。 而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。 不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。 除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。 什么是分布式？分布式的 CAP 理论告诉我们: 任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。 目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。 分布式场景 此处主要指集群模式下，多个相同服务同时开启. 在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。 分布式与单机情况下最大的不同在于其不是多线程而是多进程。 多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。 什么是分布式锁？ 当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑） 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。 我们需要怎样的分布式锁？ 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条） 这把锁最好是一把公平锁（根据业务需求考虑要不要这条） 有高可用的获取锁和释放锁功能 获取锁和释放锁的性能要好 基于数据库做分布式锁 基于乐观锁 基于表主键唯一做分布式锁利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。 上面这种简单的实现有以下几个问题： 这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。 在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。 当然，我们也可以有其他方式解决上面的问题。 数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。 非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。 比较好的办法是在程序中生产主键进行防重。 基于表字段版本号做分布式锁这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。 基于悲观锁 基于数据库排他锁做分布式锁在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。)。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。 我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。 这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。 阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。 但是还是无法直接解决数据库单点和可重入问题。 这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。 还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。 优缺点优点：简单，易于理解 缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱） 基于 Redis 做分布式锁基于 redis 的 setnx()、expire() 方法做分布式锁setnx()setnx 的含义就是 SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0。 expire()expire 设置过期时间，要注意的是 setnx 命令不能设置 key 的超时时间，只能通过 expire() 来对 key 设置。 使用步骤1、setnx(lockkey, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功 2、expire() 命令对 lockkey 设置超时时间，为的是避免死锁问题。 3、执行完业务代码后，可以通过 delete 命令删除 key。 这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。比如，如果在第一步 setnx 执行成功后，在 expire() 命令执行成功前，发生了宕机的现象，那么就依然会出现死锁的问题，所以如果要对其进行完善的话，可以使用 redis 的 setnx()、get() 和 getset() 方法来实现分布式锁。 基于 redis 的 setnx()、get()、getset()方法做分布式锁这个方案的背景主要是在 setnx() 和 expire() 的方案上针对可能存在的死锁问题，做了一些优化。 getset()这个命令主要有两个参数 getset(key，newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果： getset(key, “value1”) 返回 null 此时 key 的值会被设置为 value1 getset(key, “value2”) 返回 value1 此时 key 的值会被设置为 value2 依次类推！ 使用步骤 setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。 get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。 计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。 判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import cn.com.tpig.cache.redis.RedisService;import cn.com.tpig.utils.SpringUtils;//redis分布式锁public final class RedisLockUtil &#123; private static final int defaultExpire = 60; private RedisLockUtil() &#123; // &#125; /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock(String key, int expire) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long status = redisService.setnx(key, \"1\"); if(status == 1) &#123; redisService.expire(key, expire); return true; &#125; return false; &#125; public static boolean lock(String key) &#123; return lock2(key, defaultExpire); &#125; /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock2(String key, int expire) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long value = System.currentTimeMillis() + expire; long status = redisService.setnx(key, String.valueOf(value)); if(status == 1) &#123; return true; &#125; long oldExpireTime = Long.parseLong(redisService.get(key, \"0\")); if(oldExpireTime &lt; System.currentTimeMillis()) &#123; //超时 long newExpireTime = System.currentTimeMillis() + expire; long currentExpireTime = Long.parseLong(redisService.getSet(key, String.valueOf(newExpireTime))); if(currentExpireTime == oldExpireTime) &#123; return true; &#125; &#125; return false; &#125; public static void unLock1(String key) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); redisService.del(key); &#125; public static void unLock2(String key) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long oldExpireTime = Long.parseLong(redisService.get(key, \"0\")); if(oldExpireTime &gt; System.currentTimeMillis()) &#123; redisService.del(key); &#125; &#125;&#125; 123456789101112131415public void drawRedPacket(long userId) &#123; String key = \"draw.redpacket.userid:\" + userId; boolean lock = RedisLockUtil.lock2(key, 60); if(lock) &#123; try &#123; //领取操作 &#125; finally &#123; //释放锁 RedisLockUtil.unLock(key); &#125; &#125; else &#123; new RuntimeException(\"重复领取奖励\"); &#125;&#125; 基于 Redlock 做分布式锁Redlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立的 Redis 节点（通常情况下 N 可以设置成 5）。 算法的步骤如下： 1、客户端获取当前时间，以毫秒为单位。 2、客户端尝试获取 N 个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N 个节点以相同的 key 和 value 获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是 10s，那么接口超时大概设置 5-50ms。这样可以在有 redis 节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过 3 个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。使用 Redlock 算法，可以保证在挂掉最多 2 个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于 redis 的高效性能，分布式缓存锁性能并不比数据库锁差。 但是，有一位分布式的专家写了一篇文章《How to do distributed locking》，质疑 Redlock 的正确性。 https://mp.weixin.qq.com/s/1bPLk_VZhZ0QYNZS8LkviA https://blog.csdn.net/jek123456/article/details/72954106 优缺点优点： 性能高 缺点： 失效时间设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。 基于 redisson 做分布式锁redisson 是 redis 官方的分布式锁组件。GitHub 地址：https://github.com/redisson/redisson 上面的这个问题 ——&gt; 失效时间设置多长时间为好？这个问题在 redisson 的做法是：每获得一个锁时，只设置一个很短的超时时间，同时起一个线程在每次快要到超时时间时去刷新锁的超时时间。在释放锁的同时结束这个线程。 基于 ZooKeeper 做分布式锁zookeeper 锁相关基础知识 zk 一般由多个节点构成（单数），采用 zab 一致性协议。因此可以将 zk 看成一个单点结构，对其修改数据其内部自动将所有节点数据进行修改而后才提供查询服务。 zk 的数据以目录树的形式，每个目录称为 znode， znode 中可存储数据（一般不超过 1M），还可以在其中增加子节点。 子节点有三种类型。序列化节点，每在该节点下增加一个节点自动给该节点的名称上自增。临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除。最后就是普通节点。 Watch 机制，client 可以监控每个节点的变化，当产生变化会给 client 产生一个事件。 zk 基本锁 原理：利用临时节点与 watch 机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch/lock 节点，有删除操作后再去争锁。临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。 缺点：所有取锁失败的进程都监听父节点，很容易发生羊群效应，即当释放锁后所有等待进程一起来创建节点，并发量很大。 zk 锁优化 原理：上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。 步骤： 在 /lock 节点下创建一个有序临时节点 (EPHEMERAL_SEQUENTIAL)。 判断创建的节点序号是否最小，如果是最小则获取锁成功。不是则取锁失败，然后 watch 序号比本身小的前一个节点。 当取锁失败，设置 watch 后则等待 watch 事件到来后，再次判断是否序号最小。 取锁成功则执行代码，最后释放锁（删除该节点）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class DistributedLock implements Lock, Watcher&#123; private ZooKeeper zk; private String root = \"/locks\";//根 private String lockName;//竞争资源的标志 private String waitNode;//等待前一个锁 private String myZnode;//当前锁 private CountDownLatch latch;//计数器 private int sessionTimeout = 30000; private List&lt;Exception&gt; exception = new ArrayList&lt;Exception&gt;(); /** * 创建分布式锁,使用前请确认config配置的zookeeper服务可用 * @param config 127.0.0.1:2181 * @param lockName 竞争资源标志,lockName中不能包含单词lock */ public DistributedLock(String config, String lockName)&#123; this.lockName = lockName; // 创建一个与服务器的连接 try &#123; zk = new ZooKeeper(config, sessionTimeout, this); Stat stat = zk.exists(root, false); if(stat == null)&#123; // 创建根节点 zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); &#125; &#125; catch (IOException e) &#123; exception.add(e); &#125; catch (KeeperException e) &#123; exception.add(e); &#125; catch (InterruptedException e) &#123; exception.add(e); &#125; &#125; /** * zookeeper节点的监视器 */ public void process(WatchedEvent event) &#123; if(this.latch != null) &#123; this.latch.countDown(); &#125; &#125; public void lock() &#123; if(exception.size() &gt; 0)&#123; throw new LockException(exception.get(0)); &#125; try &#123; if(this.tryLock())&#123; System.out.println(\"Thread \" + Thread.currentThread().getId() + \" \" +myZnode + \" get lock true\"); return; &#125; else&#123; waitForLock(waitNode, sessionTimeout);//等待锁 &#125; &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; &#125; public boolean tryLock() &#123; try &#123; String splitStr = \"_lock_\"; if(lockName.contains(splitStr)) throw new LockException(\"lockName can not contains \\\\u000B\"); //创建临时子节点 myZnode = zk.create(root + \"/\" + lockName + splitStr, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(myZnode + \" is created \"); //取出所有子节点 List&lt;String&gt; subNodes = zk.getChildren(root, false); //取出所有lockName的锁 List&lt;String&gt; lockObjNodes = new ArrayList&lt;String&gt;(); for (String node : subNodes) &#123; String _node = node.split(splitStr)[0]; if(_node.equals(lockName))&#123; lockObjNodes.add(node); &#125; &#125; Collections.sort(lockObjNodes); System.out.println(myZnode + \"==\" + lockObjNodes.get(0)); if(myZnode.equals(root+\"/\"+lockObjNodes.get(0)))&#123; //如果是最小的节点,则表示取得锁 return true; &#125; //如果不是最小的节点，找到比自己小1的节点 String subMyZnode = myZnode.substring(myZnode.lastIndexOf(\"/\") + 1); waitNode = lockObjNodes.get(Collections.binarySearch(lockObjNodes, subMyZnode) - 1); &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; return false; &#125; public boolean tryLock(long time, TimeUnit unit) &#123; try &#123; if(this.tryLock())&#123; return true; &#125; return waitForLock(waitNode,time); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; private boolean waitForLock(String lower, long waitTime) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(root + \"/\" + lower,true); //判断比自己小一个数的节点是否存在,如果不存在则无需等待锁,同时注册监听 if(stat != null)&#123; System.out.println(\"Thread \" + Thread.currentThread().getId() + \" waiting for \" + root + \"/\" + lower); this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; return true; &#125; public void unlock() &#123; try &#123; System.out.println(\"unlock \" + myZnode); zk.delete(myZnode,-1); myZnode = null; zk.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; public void lockInterruptibly() throws InterruptedException &#123; this.lock(); &#125; public Condition newCondition() &#123; return null; &#125; public class LockException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public LockException(String e)&#123; super(e); &#125; public LockException(Exception e)&#123; super(e); &#125; &#125;&#125; 优缺点优点： 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 缺点： 性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。还需要对 ZK的原理有所了解。 基于 Consul 做分布式锁DD 写过类似文章，其实主要利用 Consul 的 Key / Value 存储 API 中的 acquire 和 release 操作来实现。 文章地址：http://blog.didispace.com/spring-cloud-consul-lock-and-semphore/ 使用分布式锁的注意事项1、注意分布式锁的开销 2、注意加锁的粒度 3、加锁的方式 总结无论你身处一个什么样的公司，最开始的工作可能都需要从最简单的做起。不要提阿里和腾讯的业务场景 qps 如何大，因为在这样的大场景中你未必能亲自参与项目，亲自参与项目未必能是核心的设计者，是核心的设计者未必能独自设计。希望大家能根据自己公司业务场景，选择适合自己项目的方案。 参考资料http://www.hollischuang.com/archives/1716 http://www.spring4all.com/question/158 https://www.cnblogs.com/PurpleDream/p/5559352.html http://www.cnblogs.com/PurpleDream/p/5573040.html https://www.cnblogs.com/suolu/p/6588902.html","tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.54tianzhisheng.cn/tags/数据库/"},{"name":"Redis","slug":"Redis","permalink":"http://www.54tianzhisheng.cn/tags/Redis/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://www.54tianzhisheng.cn/tags/分布式锁/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://www.54tianzhisheng.cn/tags/Zookeeper/"}]},{"title":"Spring Boot 2.0系列文章(六)：Spring Boot 2.0中SpringBootApplication注解详解","date":"2018-04-18T16:00:00.000Z","path":"2018/04/19/SpringBootApplication-annotation/","text":"SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/19/SpringBootApplication-annotation/ 概述许多 Spring Boot 开发者喜欢他们的应用程序使用自动配置、组件扫描、并能够在他们的 “Application” 类上定义额外的配置。 可以使用一个 @SpringBootApplication 注解来启用这些功能。 123456789import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; @SpringBootApplication跟进去 @SpringBootApplication 注解可以发现下图： 其中标注的三个注解正能解决我们上面所说的三种功能，它们是： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 该接口除了上面三个注解外，还有四个方法如下： Class&lt;?&gt;[] exclude() default {}:根据 class 来排除，排除特定的类加入 spring 容器，传入参数 value 类型是 class 类型。 String[] excludeName() default {}:根据 class name 来排除，排除特定的类加入 spring 容器，传入参数 value 类型是 class 的全类名字符串数组。 String[] scanBasePackages() default {}:指定扫描包，参数是包名的字符串数组。 Class&lt;?&gt;[] scanBasePackageClasses() default {}:扫描特定的包，参数类似是 Class 类型数组。 就拿 scanBasePackages 来举个例子： 1@SpringBootApplication(scanBasePackages = &#123;\"com.zhisheng.controller\",\"com.zhisheng.model\"&#125;) 将不需要的 bean 排除在 spring 容器中，如何操作？看看官方的代码怎么用的： @SpringBootConfiguration @SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到srping容器中，并且实例名就是方法名。 虽说现在已经推荐使用 Spring Boot 里面的 @SpringBootConfiguration 注解，为了探个究竟，我们还是继续研究下 @Configuration 注解。 @Configuration@Configuration 标注在类上，相当于把该类作为 spring 的 xml 配置文件中的 &lt;beans&gt;，作用为：配置 spring 容器(应用上下文) 123&lt;beans&gt;&lt;/beans&gt; @Bean@Bean 标注在方法上(返回某个实例的方法)，等价于 spring 的 xml 配置文件中的&lt;bean&gt;，作用为：注册 bean 对象 可以看看这篇文章：https://www.ibm.com/developerworks/cn/webservices/ws-springjava/index.html @ComponentScan可以通过该注解指定扫描某些包下包含如下注解的均自动注册为 spring beans： @Component、@Service、 @Repository、 @Controller、@Entity 等等 例如： 1@ComponentScan(basePackages = &#123;\"com.zhisheng.controller\",\"com.zhisheng.model\"&#125;) 以前是在 xml 配置文件中设置如下标签：&lt;context:component-scan&gt;（用来扫描包配置） 除了可以使用 @ComponentScan 注解来加载我们的 bean，还可以在 Application 类中使用 @Import 指定该类。 例如： 1@Import(&#123;ConsulConfig.class, Log4jEndPointConfiguration.class&#125;) //直接 imoport 要引入的类 @EnableAutoConfiguration@EnableAutoConfiguration的作用启动自动的配置，@EnableAutoConfiguration注解的意思就是Springboot根据你添加的 jar 包来配置你项目的默认配置，比如根据spring-boot-starter-web ，来判断你的项目是否需要添加了webmvc和tomcat，就会自动的帮你配置 web 项目中所需要的默认配置。简单点说就是它会根据定义在 classpath 下的类，自动的给你生成一些 Bean，并加载到 Spring 的 Context 中。 可以看到 import 引入了 AutoConfigurationImportSelector 类。该类使用了 Spring Core 包的 SpringFactoriesLoader 类的 loadFactoryNamesof() 方法。 AutoConfigurationImportSelector 类实现了 DeferredImportSelector 接口，并实现了 selectImports 方法，用来导出Configuration 类。 1List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); 导出的类是通过 SpringFactoriesLoader.loadFactoryNames() 读取了 ClassPath 下面的 META-INF/spring.factories 文件。 这个文件内容大致如下。 后面继续会写自动配置方面的博客，请继续关注！ 如果你发现自动装配的 Bean 不是你想要的，你也可以 disable 它。比如说，我不想要自动装配 Database 的那些Bean： 1@EnableAutoConfiguration(exclude = &#123;DataSourceAutoConfiguration.class&#125;) 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文主要讲了 SpringBootApplication 注解，然后展开写了其包含的三个注解 SpringBootConfiguration、ComponentScan、EnableAutoConfiguration 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！ 自己本人能力有限，源码看的不多，上面如有不对的还请留言交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览","date":"2018-04-17T16:00:00.000Z","path":"2018/04/18/spring_boot2_project/","text":"SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/15/springboot2_code/ 项目结构 结构分析： Spring-boot-project 核心代码，代码量很多（197508 行） Spring-boot-samples 一些样例 demo，代码量不多（9685 行），蛮有用的 Spring-boot-samples-invoker 里面无代码 Spring-boot-tests 测试代码（1640 行） spring-boot-projectSpring-boot-project 下面有很多模块，如下： Spirng-boot 该模块 47760 行代码（含测试代码），Spring boot 主要的库，提供了支持 Spring Boot 其他部分的功能，其中包括了： 在SpringApplication类，提供静态便捷方法，可以很容易写一个独立的 Spring 应用程序。它唯一的工作就是创造并更新一个合适的 SpringApplicationContext 带有可选容器的嵌入式 Web 应用程序（Tomcat，Jetty 或 Undertow） 一流的外部配置支持 便捷ApplicationContext初始化程序，包括对敏感日志记录默认值的支持 spring-boot-actuator 该模块 18398 行代码（含测试代码），spring-boot-actuator 模块它完全是一个用于暴露自身信息的模块，提供了一个监控和管理生产环境的模块，可以使用 http、jmx、ssh、telnet 等管理和监控应用。审计（Auditing）、 健康（health）、数据采集（metrics gathering）会自动加入到应用里面。 spring-boot-actuator-autoconfigure 该模块 16721 行代码（含测试代码），Spring Boot Actuator 提供了额外的自动配置功能，可以在生产环境中实现可即时部署和支持的功能，从而装饰你的应用。例如，如果您正在编写 JSON Web 服务，那么它将提供服务器，安全性，日志记录，外部配置，管理端点，审计抽象等等功能。如果您想关闭内置功能，或者扩展或替换它们，它也会变得非常简单。 spring-boot-autoconfigure 该模块 51100 行代码（含测试代码）， Spring Boot 可以根据类路径的内容配置大部分常用应用程序。单个@EnableAutoConfiguration注释会触发 Spring上下文的自动配置。 自动配置尝试推断用户可能需要哪些 bean。例如，如果 HSQLDB在类路径中，并且用户尚未配置任何数据库连接，则他们可能需要定义内存数据库。当用户开始定义他们自己的 bean 时，自动配置将永远远离。 spring-boot-cli 该模块 9346 行代码（含测试代码），Spring 命令行应用程序编译并运行 Groovy 源代码，使得可以编写少量代码就能运行应用程序。Spring CLI 也可以监视文件，当它们改变时自动重新编译并重新启动。 spring-boot-dependencies 该模块里面没有源码，只有所有依赖和插件的版本号信息。 spring-boot-devtools 该模块 9418 行代码（含测试代码），spring-boot-devtools 模块来使 Spring Boot 应用支持热部署，提高开发者的开发效率，无需手动重启 Spring Boot 应用。 spring-boot-docs 该模块 671 行代码，springboot 参考文件。 spring-boot-parent 该模块是其他项目的 parent，该模块的父模块是 spring-boot-dependencies。 spring-boot-properties-migrator 该模块有 495 行代码，在 Spring Boot 2.0 中，许多配置属性被重新命名/删除，开发人员需要更新application.properties/ application.yml相应的配置。为了帮助你解决这一问题，Spring Boot 发布了一个新spring-boot-properties-migrator模块。一旦作为该模块作为依赖被添加到你的项目中，它不仅会分析应用程序的环境，而且还会在启动时打印诊断信息，而且还会在运行时为您暂时迁移属性。在您的应用程序迁移期间，这个模块是必备的，完成迁移后，请确保从项目的依赖关系中删除此模块。 spring-boot-starters Starter POMs 是由很多方便的依赖集合组成，如果你需要使用某种技术，通过添加少量的jar就可以把相关的依赖加入到项目中去。 虽然你看得到有这么多 starter，但是却没有一行 Java 代码，意不意外？ 这确实是 Spring Boot 自动配置的关键之处，后面我可以讲讲。 spring-boot-test测试代码！有 10980 行代码。 spring-boot-test-autoconfigure自动配置的测试代码，有 6063 行代码。 spring-boot-tools spring-boot-antlib Spring Boot AntLib 模块为 Apache Ant 提供了基本的 Spring Boot 支持。 您可以使用该模块创建可执行文件夹。 要使用该模块，您需要在 build.xml 中声明一个额外的 spring-boot 命名空间，如以下示例所示： 12345&lt;project xmlns:ivy=\"antlib:org.apache.ivy.ant\" xmlns:spring-boot=\"antlib:org.springframework.boot.ant\" name=\"myapp\" default=\"build\"&gt; ...&lt;/project&gt; 您需要记住使用 -lib 选项启动 Ant，如以下示例所示： 1ant -lib &lt;folder containing spring-boot-antlib-2.1.0.BUILD-SNAPSHOT.jar&gt; Spring-boot-autoconfigure-processor spring boot 自动配置的核心类 Spring-boot-configuration-metadata Spring boot 配置元数据 Spring-boot-configuration-processor spring boot 配置的核心 Spring-boot-gradle-plugin Spring Boot Gradle 插件在 Gradle 中提供了 Spring Boot 支持，可以打包成可执行 jar 或 war ，运行 Spring Boot 应用程序，并使用 spring-boot-dependencies 提供的依赖关系管理。 它需要 Gradle 4.0 或更高版本。 Spring-boot-maven-plugin Spring Boot Maven Plugin 在 Maven 中提供了 Spring Boot 支持，让您可以打包成可执行 jar 或 war 应用，并“就地”运行应用程序。 要使用它，你必须使用 Maven 3.2（或更高版本）。 Spring-boot-loader spring-boot-load 模块通过自定义 jar 包结构，自定义类加载器，优雅的实现了嵌套 jar 资源的加载，通过打包时候重新设置启动类和组织 jar 结构，通过运行时设置自定义加载器来实现嵌套 jar 资源加载。 Spring-boot-loader-tools spring-boot-load 模块的工具模块 Spring-boot-test-support 测试 spring-boot-samples 样例 demo 比较多，大家看源码的时候可以拿这些现成 demo 测试。 spring-boot-tests 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文主要分析了下 Spring boot 项目源码结构。包含 Spring boot 核心源码、样例 demo、测试。分析了项目的整体结构后，后面才能够有的放矢的去读源码。 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建","date":"2018-04-14T16:00:00.000Z","path":"2018/04/15/springboot2_code/","text":"SpringBoot 系列文章 前提前几天面试的时候，被问过 Spring Boot 的自动配置源码怎么实现的，没看过源码的我只能投降👦了。 这不，赶紧来补补了，所以才有了这篇文章的出现，Spring Boot 2. 0 源码阅读环境的搭建中还遇到点问题，被坑死了，还好解决了，感谢群里的小伙伴！ 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/15/springboot2_code/ 项目下载从 https://github.com/spring-projects/spring-boot/releases 可以看到所有版本的下载地址，我这里选择的是 Spring Boot 2 中最新的 v2.0.1.RELEASE 版本，下载后，然后解压。获取代码之前，请先确保你的 JDK 版本是 1.8 以上哦。 项目编译进入 spring-boot-2.0.1.RELEASE 的目录下，执行下面的命令。 跳过测试用例编译1sudo mvn clean install -DskipTests -Pfast //跳过测试用例 跳过测试用例可以加快编译的速度。 先看下运行成功的效果： 只花了 6 分多钟就好了。 全量编译1sudo mvn -f spring-boot-project -Pfull clean install 全量编译竟然报错，一波未平，一波又起！ 看网上的解决方法是：在项目的 pom.xml 文件中的 &lt;properties&gt; 添加 &lt;javadocExecutable&gt; 123&lt;properties&gt; &lt;javadocExecutable&gt;$&#123;java.home&#125;/../bin/javadoc&lt;/javadocExecutable&gt;&lt;/properties&gt; 此方法虽然管用，但是只是临时的，需要对每个项目都进行添加。 问题产生的原因应该是，mvn 拿到的 JAVA_HOME 位置应该是 ${JAVA_HOME}/jre 而不是 jdk 位置。 后面又看官方的 README 上面写的执行命令： 1sudo mvn clean install 执行后也是有各种报错，尝试了很久解决，最后花了好几个小时才到下面这图： 太折腾人了，太麻烦了！ 暂时就不全量编译了，我们就直接把现在 跳过测试用例编译 后的项目导入到 IDEA 中去。 导入项目工程 导入后将那些测试的 module 标记为 maven 项目，然后后面自己再根据测试用例去跟源码吧。 导入后项目没出现报错，美滋滋，后面源码可以看起来。 遇到的坑在这之前，我自己创建项目 Spring Boot 2 项目都是失败的，maven 运行项目（mvn clean install）报错如下： 通过上图可以发现报错的罪魁祸首是由于找不到 org.yaml.snakeyaml 1.19 的包，这个依赖死活下不下来，苦逼了😢。 一开始以为是公司配的 maven setting.xml 文件有问题（公司私服有问题），导致我这个 org.yaml.snakeyaml 1.19 的包一直下载不来。后来我叫群里的好友帮忙测试下能不能创建 Spring Boot 2 项目，结果他们都行的。我就换成了他们阿里云镜像的 setting 文件，结果在我这还是不行的。真是醉了，我干脆直接叫他把 maven 本地仓库中的 org.yaml.snakeyaml 1.19 整个包都发给我，结果再次创建 Spring Boot 2 项目就能成功了。美滋滋😄！ 然后就蹭着现在环境 OK，开始搭建我的 Spring Boot 2 源码阅读环境！ 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 最后源码不骗人，多看看！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变","date":"2018-04-12T16:00:00.000Z","path":"2018/04/13/Spring_Boot_2.0_Configuration_Changelog/","text":"SpringBoot 系列文章 前提好久没更新文章了，本来打算在毕业之前不更新了，这里，对不住了，我又更新了。😝😝 之前翻译了两篇 Spring Boot 2.0 的文章，Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 和 Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 今天就继续详细探究 Spring Boot 2.0 里面的改变。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/13/Spring_Boot_2.0_Configuration_Changelog/ 配置改变配置属性在 1.5.10.RELEASE 和 2.0.0.RELEASE 两个版本之间的改变： 启用键下面的表是 2.0.0.RELEASE 版本中的弃用键： Key Replacement（替代） 原因 spring.datasource.hikari.initialization-fail-fast spring.main.web-environment spring.main.web-application-type 新键下面的表是 2.0.0.RELEASE 版本中新的键： Key Default value（默认值） 描述 logging.file.max-history 0 要保存的归档日志文件的最大值 logging.file.max-size 10MB 日志文件最大容量 logging.pattern.dateformat yyyy-MM-dd HH:mm:ss.SSS 日志的日期格式 management.endpoint.auditevents.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.auditevents.enabled true 是否启用 auditevents 端点 management.endpoint.beans.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.beans.enabled true 是否启用 bean 端点 management.endpoint.conditions.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.conditions.enabled true 是否启用 conditions 端点 management.endpoint.configprops.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.configprops.enabled true 是否启用 configprops 端点 management.endpoint.configprops.keys-to-sanitize password,secret,key,token, .credentials.,vcap_services Keys that should be sanitized management.endpoint.env.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.env.enabled true 是否启用 env 端点 management.endpoint.env.keys-to-sanitize password,secret,key,token, .credentials.,vcap_services Keys that should be sanitized. management.endpoint.flyway.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.flyway.enabled true 是否启用 flyway 端点 management.endpoint.health.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.health.enabled true 是否启用 health 端点 management.endpoint.health.roles 角色用于确定用户是否有权显示详细信息 management.endpoint.health.show-details never 何时显示完整的健康详情 management.endpoint.heapdump.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.heapdump.enabled true 是否启用 heapdump 端点 management.endpoint.httptrace.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.httptrace.enabled true 是否启用 httptrace 端点 management.endpoint.info.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.info.enabled true 是否启用 info 端点 management.endpoint.jolokia.config Jolokia 设置 management.endpoint.jolokia.enabled true 是否启用 jolokia 端点 management.endpoint.liquibase.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.liquibase.enabled true 是否启用 liquibase 端点 management.endpoint.logfile.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.logfile.enabled true 是否启用 logfile 端点 management.endpoint.logfile.external-file 要访问的外部日志文件 management.endpoint.loggers.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.loggers.enabled true 是否启用 loggers 端点 management.endpoint.mappings.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.mappings.enabled true 是否启用 mappings 端点 management.endpoint.metrics.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.metrics.enabled true 是否启用 metrics 端点 management.endpoint.prometheus.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.prometheus.enabled true 是否启用 prometheus 端点 management.endpoint.scheduledtasks.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.scheduledtasks.enabled true 是否启用 scheduledtasks 端点 management.endpoint.sessions.enabled true 是否启用 sessions 端点 management.endpoint.shutdown.enabled false 是否启用 shutdown 端点 management.endpoint.threaddump.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.threaddump.enabled true 是否启用 threaddump 端点 management.endpoints.enabled-by-default 是否启用或者关闭所有的端点 management.endpoints.jmx.domain org.springframework.boot 端点 JMX 域名 management.endpoints.jmx.exposure.exclude 应排除的端点 ID management.endpoints.jmx.exposure.include * 应包含的端点 ID 或全部 * management.endpoints.jmx.static-names 追加到所有表示端点的 MBean 的ObjectName 的静态属性. management.endpoints.jmx.unique-names false 是否确保 ObjectNames 在发生冲突时被修改 management.endpoints.web.base-path /actuator Web 端点的基本路径 management.endpoints.web.cors.allow-credentials 是否支持凭证 management.endpoints.web.cors.allowed-headers Comma-separated list of headers to allow in a request. ‘*’ allows all headers. management.endpoints.web.cors.allowed-methods Comma-separated list of methods to allow. ‘*’ allows all methods. management.endpoints.web.cors.allowed-origins Comma-separated list of origins to allow. ‘*’ allows all origins. management.endpoints.web.cors.exposed-headers Comma-separated list of headers to include in a response. management.endpoints.web.cors.max-age 1800s How long the response from a pre-flight request can be cached by clients. management.endpoints.web.exposure.exclude Endpoint IDs that should be excluded. management.endpoints.web.exposure.include health,info Endpoint IDs that should be included or ‘*’ for all. management.endpoints.web.path-mapping Mapping between endpoint IDs and the path that should expose them. management.health.influxdb.enabled true Whether to enable InfluxDB health check. management.health.neo4j.enabled true Whether to enable Neo4j health check. management.health.status.http-mapping Mapping of health statuses to HTTP status codes. management.metrics.binders.files.enabled true Whether to enable files metrics. management.metrics.binders.integration.enabled true Whether to enable Spring Integration metrics. management.metrics.binders.jvm.enabled true Whether to enable JVM metrics. management.metrics.binders.logback.enabled true Whether to enable Logback metrics. management.metrics.binders.processor.enabled true Whether to enable processor metrics. management.metrics.binders.uptime.enabled true Whether to enable uptime metrics. management.metrics.distribution.percentiles Specific computed non-aggregable percentiles to ship to the backend for meter IDs starting-with the specified name. management.metrics.distribution.percentiles-histogram Whether meter IDs starting-with the specified name should be publish percentile histograms. management.metrics.distribution.sla Specific SLA boundaries for meter IDs starting-with the specified name. management.metrics.enable Whether meter IDs starting-with the specified name should be enabled. management.metrics.export.atlas.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.atlas.config-refresh-frequency 10s Frequency for refreshing config settings from the LWC service. management.metrics.export.atlas.config-time-to-live 150s Time to live for subscriptions from the LWC service. management.metrics.export.atlas.config-uri http://localhost:7101/ lwc/api/v1/expressions/local-dev URI for the Atlas LWC endpoint to retrieve current subscriptions. management.metrics.export.atlas.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.atlas.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.atlas.eval-uri http://localhost:7101/ lwc/api/v1/evaluate URI for the Atlas LWC endpoint to evaluate the data for a subscription. management.metrics.export.atlas.lwc-enabled false Whether to enable streaming to Atlas LWC. management.metrics.export.atlas.meter-time-to-live 15m Time to live for meters that do not have any activity. management.metrics.export.atlas.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.atlas.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.atlas.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.atlas.uri http://localhost:7101/ api/v1/publish URI of the Atlas server. management.metrics.export.datadog.api-key Datadog API key. management.metrics.export.datadog.application-key Datadog application key. management.metrics.export.datadog.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.datadog.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.datadog.descriptions true Whether to publish descriptions metadata to Datadog. management.metrics.export.datadog.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.datadog.host-tag instance Tag that will be mapped to “host” when shipping metrics to Datadog. management.metrics.export.datadog.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.datadog.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.datadog.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.datadog.uri https://app.datadoghq.com URI to ship metrics to. management.metrics.export.ganglia.addressing-mode multicast UDP addressing mode, either unicast or multicast. management.metrics.export.ganglia.duration-units milliseconds Base time unit used to report durations. management.metrics.export.ganglia.enabled true Whether exporting of metrics to Ganglia is enabled. management.metrics.export.ganglia.host localhost Host of the Ganglia server to receive exported metrics. management.metrics.export.ganglia.port 8649 Port of the Ganglia server to receive exported metrics. management.metrics.export.ganglia.protocol-version 3.1 Ganglia protocol version. management.metrics.export.ganglia.rate-units seconds Base time unit used to report rates. management.metrics.export.ganglia.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.ganglia.time-to-live 1 Time to live for metrics on Ganglia. management.metrics.export.graphite.duration-units milliseconds Base time unit used to report durations. management.metrics.export.graphite.enabled true Whether exporting of metrics to Graphite is enabled. management.metrics.export.graphite.host localhost Host of the Graphite server to receive exported metrics. management.metrics.export.graphite.port 2004 Port of the Graphite server to receive exported metrics. management.metrics.export.graphite.protocol pickled Protocol to use while shipping data to Graphite. management.metrics.export.graphite.rate-units seconds Base time unit used to report rates. management.metrics.export.graphite.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.graphite.tags-as-prefix `` For the default naming convention, turn the specified tag keys into part of the metric prefix. management.metrics.export.influx.auto-create-db true Whether to create the Influx database if it does not exist before attempting to publish metrics to it. management.metrics.export.influx.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.influx.compressed true Whether to enable GZIP compression of metrics batches published to Influx. management.metrics.export.influx.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.influx.consistency one Write consistency for each point. management.metrics.export.influx.db mydb Tag that will be mapped to “host” when shipping metrics to Influx. management.metrics.export.influx.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.influx.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.influx.password Login password of the Influx server. management.metrics.export.influx.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.influx.retention-policy Retention policy to use (Influx writes to the DEFAULT retention policy if one is not specified). management.metrics.export.influx.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.influx.uri http://localhost:8086 URI of the Influx server. management.metrics.export.influx.user-name Login user of the Influx server. management.metrics.export.jmx.enabled true Whether exporting of metrics to JMX is enabled. management.metrics.export.jmx.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.newrelic.account-id New Relic account ID. management.metrics.export.newrelic.api-key New Relic API key. management.metrics.export.newrelic.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.newrelic.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.newrelic.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.newrelic.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.newrelic.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.newrelic.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.newrelic.uri https://insights-collector .newrelic.com URI to ship metrics to. management.metrics.export.prometheus.descriptions true Whether to enable publishing descriptions as part of the scrape payload to Prometheus. management.metrics.export.prometheus.enabled true Whether exporting of metrics to Prometheus is enabled. management.metrics.export.prometheus.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.signalfx.access-token SignalFX access token. management.metrics.export.signalfx.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.signalfx.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.signalfx.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.signalfx.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.signalfx.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.signalfx.source Uniquely identifies the app instance that is publishing metrics to SignalFx. management.metrics.export.signalfx.step 10s Step size (i.e. reporting frequency) to use. management.metrics.export.signalfx.uri https://ingest.signalfx.com URI to ship metrics to. management.metrics.export.simple.enabled true Whether, in the absence of any other exporter, exporting of metrics to an in-memory backend is enabled. management.metrics.export.simple.mode cumulative Counting mode. management.metrics.export.simple.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.statsd.enabled true Whether exporting of metrics to StatsD is enabled. management.metrics.export.statsd.flavor datadog StatsD line protocol to use. management.metrics.export.statsd.host localhost Host of the StatsD server to receive exported metrics. management.metrics.export.statsd.max-packet-length 1400 Total length of a single payload should be kept within your network’s MTU. management.metrics.export.statsd.polling-frequency 10s How often gauges will be polled. management.metrics.export.statsd.port 8125 Port of the StatsD server to receive exported metrics. management.metrics.export.statsd.publish-unchanged-meters true Whether to send unchanged meters to the StatsD server. management.metrics.export.statsd.queue-size 2147483647 Maximum size of the queue of items waiting to be sent to the StatsD server. management.metrics.export.wavefront.api-token API token used when publishing metrics directly to the Wavefront API host. management.metrics.export.wavefront.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.wavefront.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.wavefront.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.wavefront.global-prefix Global prefix to separate metrics originating from this app’s white box instrumentation from those originating from other Wavefront integrations when viewed in the Wavefront UI. management.metrics.export.wavefront.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.wavefront.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.wavefront.source Unique identifier for the app instance that is the source of metrics being published to Wavefront. management.metrics.export.wavefront.step 10s Step size (i.e. reporting frequency) to use. management.metrics.export.wavefront.uri https://longboard.wavefront.com URI to ship metrics to. management.metrics.use-global-registry true Whether auto-configured MeterRegistry implementations should be bound to the global static registry on Metrics. management.metrics.web.client.max-uri-tags 100 Maximum number of unique URI tag values allowed. management.metrics.web.client.requests-metric-name http.client.requests Name of the metric for sent requests. management.metrics.web.server.auto-time-requests true Whether requests handled by Spring MVC or WebFlux should be automatically timed. management.metrics.web.server.requests-metric-name http.server.requests Name of the metric for received requests. management.server.add-application-context-header false Add the “X-Application-Context” HTTP header in each response. management.server.address Network address to which the management endpoints should bind. management.server.port Management endpoint HTTP port (uses the same port as the application by default). management.server.servlet.context-path Management endpoint context-path (for instance,/management). management.server.ssl.ciphers management.server.ssl.client-auth management.server.ssl.enabled management.server.ssl.enabled-protocols management.server.ssl.key-alias management.server.ssl.key-password management.server.ssl.key-store management.server.ssl.key-store-password management.server.ssl.key-store-provider management.server.ssl.key-store-type management.server.ssl.protocol management.server.ssl.trust-store management.server.ssl.trust-store-password management.server.ssl.trust-store-provider management.server.ssl.trust-store-type management.trace.http.enabled true Whether to enable HTTP request-response tracing. management.trace.http.include request-headers,response-headers, cookies,errors Items to be included in the trace. server.error.include-exception false Include the “exception” attribute. server.http2.enabled server.jetty.accesslog.append false Append to log. server.jetty.accesslog.date-format dd/MMM/yyyy:HH:mm:ss Z Timestamp format of the request log. server.jetty.accesslog.enabled false Enable access log. server.jetty.accesslog.extended-format false Enable extended NCSA format. server.jetty.accesslog.file-date-format Date format to place in log file name. server.jetty.accesslog.filename Log filename. server.jetty.accesslog.locale Locale of the request log. server.jetty.accesslog.log-cookies false Enable logging of the request cookies. server.jetty.accesslog.log-latency false Enable logging of request processing time. server.jetty.accesslog.log-server false Enable logging of the request hostname. server.jetty.accesslog.retention-period 31 Number of days before rotated log files are deleted. server.jetty.accesslog.time-zone GMT Timezone of the request log. server.servlet.application-display-name application Display name of the application. server.servlet.context-parameters Servlet context init parameters. server.servlet.context-path Context path of the application. server.servlet.jsp.class-name server.servlet.jsp.init-parameters server.servlet.jsp.registered server.servlet.path / Path of the main dispatcher servlet. server.servlet.session.cookie.comment server.servlet.session.cookie.domain server.servlet.session.cookie.http-only server.servlet.session.cookie.max-age server.servlet.session.cookie.name server.servlet.session.cookie.path server.servlet.session.cookie.secure server.servlet.session.persistent server.servlet.session.store-dir server.servlet.session.timeout server.servlet.session.tracking-modes server.tomcat.max-http-header-size 0 Maximum size, in bytes, of the HTTP message header. server.tomcat.resource.cache-ttl Time-to-live of the static resource cache. server.tomcat.use-relative-redirects Whether HTTP 1.1 and later location headers generated by a call to sendRedirect will use relative or absolute redirects. server.undertow.eager-filter-init true Whether servlet filters should be initialized on startup. spring.banner.charset UTF-8 Banner file encoding. spring.banner.image.height Height of the banner image in chars (default based on image height). spring.banner.image.invert false Whether images should be inverted for dark terminal themes. spring.banner.image.location classpath:banner.gif Banner image file location (jpg or png can also be used). spring.banner.image.margin 2 Left hand image margin in chars. spring.banner.image.width 76 Width of the banner image in chars. spring.banner.location classpath:banner.txt Banner text resource location. spring.batch.initialize-schema embedded Database schema initialization mode. spring.cache.redis.cache-null-values true Allow caching null values. spring.cache.redis.key-prefix Key prefix. spring.cache.redis.time-to-live Entry expiration. spring.cache.redis.use-key-prefix true Whether to use the key prefix when writing to Redis. spring.config.additional-location Config file locations used in addition to the defaults. spring.data.cassandra.connect-timeout Socket option: connection time out. spring.data.cassandra.pool.heartbeat-interval 30s Heartbeat interval after which a message is sent on an idle connection to make sure it’s still alive. spring.data.cassandra.pool.idle-timeout 120s Idle timeout before an idle connection is removed. spring.data.cassandra.pool.max-queue-size 256 Maximum number of requests that get queued if no connection is available. spring.data.cassandra.pool.pool-timeout 5000ms Pool timeout when trying to acquire a connection from a host’s pool. spring.data.cassandra.read-timeout Socket option: read time out. spring.data.cassandra.repositories.type auto Type of Cassandra repositories to enable. spring.data.couchbase.repositories.type auto Type of Couchbase repositories to enable. spring.data.mongodb.repositories.type auto Type of Mongo repositories to enable. spring.data.neo4j.auto-index none Auto index mode. spring.data.web.pageable.default-page-size 20 Default page size. spring.data.web.pageable.max-page-size 2000 Maximum page size to be accepted. spring.data.web.pageable.one-indexed-parameters false Whether to expose and assume 1-based page number indexes. spring.data.web.pageable.page-parameter page Page index parameter name. spring.data.web.pageable.prefix `` General prefix to be prepended to the page number and page size parameters. spring.data.web.pageable.qualifier-delimiter _ Delimiter to be used between the qualifier and the actual page number and size properties. spring.data.web.pageable.size-parameter size Page size parameter name. spring.data.web.sort.sort-parameter sort Sort parameter name. spring.datasource.hikari.initialization-fail-timeout spring.datasource.hikari.metrics-tracker-factory spring.datasource.hikari.scheduled-executor spring.datasource.hikari.scheduled-executor-service spring.datasource.hikari.schema spring.datasource.initialization-mode embedded Initialize the datasource with available DDL and DML scripts. spring.devtools.restart.log-condition-evaluation-delta true Whether to log the condition evaluation delta upon restart. spring.flyway.baseline-description spring.flyway.baseline-on-migrate spring.flyway.baseline-version spring.flyway.check-location true Whether to check that migration scripts location exists. spring.flyway.clean-disabled spring.flyway.clean-on-validation-error spring.flyway.dry-run-output spring.flyway.enabled true 是否启用 flyway. spring.flyway.encoding spring.flyway.error-handlers spring.flyway.group spring.flyway.ignore-future-migrations spring.flyway.ignore-missing-migrations spring.flyway.init-sqls SQL statements to execute to initialize a connection immediately after obtaining it. spring.flyway.installed-by spring.flyway.locations spring.flyway.mixed spring.flyway.out-of-order spring.flyway.password 如果您想让 Flyway 创建自己的DataSource，可以使用 JDBC 密码 spring.flyway.placeholder-prefix spring.flyway.placeholder-replacement spring.flyway.placeholder-suffix spring.flyway.placeholders spring.flyway.repeatable-sql-migration-prefix spring.flyway.schemas spring.flyway.skip-default-callbacks spring.flyway.skip-default-resolvers spring.flyway.sql-migration-prefix spring.flyway.sql-migration-separator spring.flyway.sql-migration-suffix spring.flyway.sql-migration-suffixes spring.flyway.table spring.flyway.target spring.flyway.undo-sql-migration-prefix spring.flyway.url 要迁移的数据库的 JDBC URL spring.flyway.user 登录要迁移数据库的用户名 spring.flyway.validate-on-migrate spring.gson.date-format 序列化 Date 对象时使用的格式 spring.gson.disable-html-escaping Whether to disable the escaping of HTML characters such as ‘&lt;’, ‘&gt;’, etc. spring.gson.disable-inner-class-serialization Whether to exclude inner classes during serialization. spring.gson.enable-complex-map-key-serialization Whether to enable serialization of complex map keys (i.e. non-primitives). spring.gson.exclude-fields-without-expose-annotation Whether to exclude all fields from consideration for serialization or deserialization that do not have the “Expose” annotation. spring.gson.field-naming-policy Naming policy that should be applied to an object’s field during serialization and deserialization. spring.gson.generate-non-executable-json Whether to generate non executable JSON by prefixing the output with some special text. spring.gson.lenient Whether to be lenient about parsing JSON that doesn’t conform to RFC 4627. spring.gson.long-serialization-policy Serialization policy for Long and long types. spring.gson.pretty-printing Whether to output serialized JSON that fits in a page for pretty printing. spring.gson.serialize-nulls Whether to serialize null fields. spring.influx.password Influx 登录用户名密码 spring.influx.url InfluxDB 数据库 URL spring.influx.user Influx 登录用户名 spring.integration.jdbc.initialize-schema embedded Database schema initialization mode. spring.integration.jdbc.schema classpath:org/springframework/ integration/jdbc/schema-@@platform@@.sql Path to the SQL file to use to initialize the database schema. spring.jdbc.template.fetch-size -1 Number of rows that should be fetched from the database when more rows are needed. spring.jdbc.template.max-rows -1 Maximum number of rows. spring.jdbc.template.query-timeout Query timeout. spring.jpa.mapping-resources Mapping resources (equivalent to “mapping-file” entries in persistence.xml). spring.jta.atomikos.datasource.concurrent-connection-validation spring.jta.atomikos.properties.allow-sub-transactions true Specify whether sub-transactions are allowed. spring.jta.atomikos.properties.default-max-wait-time-on-shutdown How long should normal shutdown (no-force) wait for transactions to complete. spring.jta.atomikos.properties.recovery.delay 10000ms Delay between two recovery scans. spring.jta.atomikos.properties.recovery.forget-orphaned-log-entries-delay 86400000ms Delay after which recovery can cleanup pending (‘orphaned’) log entries. spring.jta.atomikos.properties.recovery.max-retries 5 Number of retry attempts to commit the transaction before throwing an exception. spring.jta.atomikos.properties.recovery.retry-interval 10000ms Delay between retry attempts. spring.kafka.admin.client-id ID to pass to the server when making requests. spring.kafka.admin.fail-fast false Whether to fail fast if the broker is not available on startup. spring.kafka.admin.properties Additional admin-specific properties used to configure the client. spring.kafka.admin.ssl.key-password Password of the private key in the key store file. spring.kafka.admin.ssl.keystore-location Location of the key store file. spring.kafka.admin.ssl.keystore-password Store password for the key store file. spring.kafka.admin.ssl.truststore-location Location of the trust store file. spring.kafka.admin.ssl.truststore-password Store password for the trust store file. spring.kafka.consumer.properties Additional consumer-specific properties used to configure the client. spring.kafka.consumer.ssl.key-password Password of the private key in the key store file. spring.kafka.consumer.ssl.keystore-location Location of the key store file. spring.kafka.consumer.ssl.keystore-password Store password for the key store file. spring.kafka.consumer.ssl.truststore-location Location of the trust store file. spring.kafka.consumer.ssl.truststore-password Store password for the trust store file. spring.kafka.jaas.control-flag required Control flag for login configuration. spring.kafka.jaas.enabled false Whether to enable JAAS configuration. spring.kafka.jaas.login-module com.sun.security.auth .module.Krb5LoginModule Login module. spring.kafka.jaas.options Additional JAAS options. spring.kafka.listener.client-id Prefix for the listener’s consumer client.id property. spring.kafka.listener.idle-event-interval Time between publishing idle consumer events (no data received). spring.kafka.listener.log-container-config Whether to log the container configuration during initialization (INFO level). spring.kafka.listener.monitor-interval Time between checks for non-responsive consumers. spring.kafka.listener.no-poll-threshold Multiplier applied to “pollTimeout” to determine if a consumer is non-responsive. spring.kafka.listener.type single Listener type. spring.kafka.producer.properties Additional producer-specific properties used to configure the client. spring.kafka.producer.ssl.key-password Password of the private key in the key store file. spring.kafka.producer.ssl.keystore-location Location of the key store file. spring.kafka.producer.ssl.keystore-password Store password for the key store file. spring.kafka.producer.ssl.truststore-location Location of the trust store file. spring.kafka.producer.ssl.truststore-password Store password for the trust store file. spring.kafka.producer.transaction-id-prefix When non empty, enables transaction support for producer. spring.ldap.anonymous-read-only false Whether read-only operations should use an anonymous environment. spring.liquibase.change-log classpath:/db/changelog/ db.changelog-master.yaml Change log configuration path. spring.liquibase.check-change-log-location true Whether to check that the change log location exists. spring.liquibase.contexts Comma-separated list of runtime contexts to use. spring.liquibase.default-schema Default database schema. spring.liquibase.drop-first false Whether to first drop the database schema. spring.liquibase.enabled true Whether to enable Liquibase support. spring.liquibase.labels Comma-separated list of runtime labels to use. spring.liquibase.parameters Change log parameters. spring.liquibase.password Login password of the database to migrate. spring.liquibase.rollback-file File to which rollback SQL is written when an update is performed. spring.liquibase.url JDBC URL of the database to migrate. spring.liquibase.user Login user of the database to migrate. spring.main.web-application-type Flag to explicitly request a specific type of web application. spring.messages.cache-duration Loaded resource bundle files cache duration. spring.messages.use-code-as-default-message false Whether to use the message code as the default message instead of throwing a “NoSuchMessageException”. spring.mvc.contentnegotiation.favor-parameter false Whether a request parameter (“format” by default) should be used to determine the requested media type. spring.mvc.contentnegotiation.favor-path-extension false Whether the path extension in the URL path should be used to determine the requested media type. spring.mvc.contentnegotiation.media-types Map file extensions to media types for content negotiation. spring.mvc.contentnegotiation.parameter-name Query parameter name to use when “favor-parameter” is enabled. spring.mvc.pathmatch.use-registered-suffix-pattern false Whether suffix pattern matching should work only against extensions registered with “spring.mvc.contentnegotiation.media-types.*”. spring.mvc.pathmatch.use-suffix-pattern false Whether to use suffix pattern match (“.*”) when matching patterns to requests. spring.quartz.jdbc.initialize-schema embedded Database schema initialization mode. spring.quartz.jdbc.schema classpath:org/quartz/impl/ jdbcjobstore/ tables_@@platform@@.sql Path to the SQL file to use to initialize the database schema. spring.quartz.job-store-type memory Quartz job store type. spring.quartz.properties Additional Quartz Scheduler properties. spring.rabbitmq.listener.direct.acknowledge-mode Acknowledge mode of container. spring.rabbitmq.listener.direct.auto-startup true Whether to start the container automatically on startup. spring.rabbitmq.listener.direct.consumers-per-queue Number of consumers per queue. spring.rabbitmq.listener.direct.default-requeue-rejected Whether rejected deliveries are re-queued by default. spring.rabbitmq.listener.direct.idle-event-interval How often idle container events should be published. spring.rabbitmq.listener.direct.prefetch Number of messages to be handled in a single request. spring.rabbitmq.listener.direct.retry.enabled false Whether publishing retries are enabled. spring.rabbitmq.listener.direct.retry.initial-interval 1000ms Duration between the first and second attempt to deliver a message. spring.rabbitmq.listener.direct.retry.max-attempts 3 Maximum number of attempts to deliver a message. spring.rabbitmq.listener.direct.retry.max-interval 10000ms Maximum duration between attempts. spring.rabbitmq.listener.direct.retry.multiplier 1 Multiplier to apply to the previous retry interval. spring.rabbitmq.listener.direct.retry.stateless true Whether retries are stateless or stateful. spring.rabbitmq.listener.type simple Listener container type. spring.rabbitmq.ssl.key-store-type PKCS12 Key store type. spring.rabbitmq.ssl.trust-store-type JKS Trust store type. spring.rabbitmq.template.exchange `` Name of the default exchange to use for send operations. spring.rabbitmq.template.routing-key `` Value of a default routing key to use for send operations. spring.reactor.stacktrace-mode.enabled false Whether Reactor should collect stacktrace information at runtime. spring.redis.jedis.pool.max-active 8 Maximum number of connections that can be allocated by the pool at a given time. spring.redis.jedis.pool.max-idle 8 Maximum number of “idle” connections in the pool. spring.redis.jedis.pool.max-wait -1ms Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. spring.redis.jedis.pool.min-idle 0 Target for the minimum number of idle connections to maintain in the pool. spring.redis.lettuce.pool.max-active 8 Maximum number of connections that can be allocated by the pool at a given time. spring.redis.lettuce.pool.max-idle 8 Maximum number of “idle” connections in the pool. spring.redis.lettuce.pool.max-wait -1ms Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. spring.redis.lettuce.pool.min-idle 0 Target for the minimum number of idle connections to maintain in the pool. spring.redis.lettuce.shutdown-timeout 100ms Shutdown timeout. spring.resources.cache.cachecontrol.cache-private Indicate that the response message is intended for a single user and must not be stored by a shared cache. spring.resources.cache.cachecontrol.cache-public Indicate that any cache may store the response. spring.resources.cache.cachecontrol.max-age Maximum time the response should be cached, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.must-revalidate Indicate that once it has become stale, a cache must not use the response without re-validating it with the server. spring.resources.cache.cachecontrol.no-cache Indicate that the cached response can be reused only if re-validated with the server. spring.resources.cache.cachecontrol.no-store Indicate to not cache the response in any case. spring.resources.cache.cachecontrol.no-transform Indicate intermediaries (caches and others) that they should not transform the response content. spring.resources.cache.cachecontrol.proxy-revalidate Same meaning as the “must-revalidate” directive, except that it does not apply to private caches. spring.resources.cache.cachecontrol.s-max-age Maximum time the response should be cached by shared caches, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.stale-if-error Maximum time the response may be used when errors are encountered, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.stale-while-revalidate Maximum time the response can be served after it becomes stale, in seconds if no duration suffix is not specified. spring.resources.cache.period Cache period for the resources served by the resource handler. spring.security.filter.dispatcher-types async,error,request Security filter chain dispatcher types. spring.security.filter.order -100 Security filter chain order. spring.security.oauth2.client.provider OAuth provider details. spring.security.oauth2.client.registration OAuth client registrations. spring.security.user.name user Default user name. spring.security.user.password Password for the default user name. spring.security.user.roles Granted roles for the default user name. spring.servlet.multipart.enabled true Whether to enable support of multipart uploads. spring.servlet.multipart.file-size-threshold 0 Threshold after which files are written to disk. spring.servlet.multipart.location Intermediate location of uploaded files. spring.servlet.multipart.max-file-size 1MB Max file size. spring.servlet.multipart.max-request-size 10MB Max request size. spring.servlet.multipart.resolve-lazily false Whether to resolve the multipart request lazily at the time of file or parameter access. spring.session.jdbc.cleanup-cron 0 * * * * * Cron expression for expired session cleanup job. spring.session.jdbc.initialize-schema embedded Database schema initialization mode. spring.session.mongodb.collection-name sessions Collection name used to store sessions. spring.session.redis.cleanup-cron 0 * * * * * Cron expression for expired session cleanup job. spring.session.servlet.filter-dispatcher-types async,error,request Session repository filter dispatcher types. spring.session.servlet.filter-order Session repository filter order. spring.thymeleaf.enable-spring-el-compiler false Enable the SpringEL compiler in SpringEL expressions. spring.thymeleaf.reactive.chunked-mode-view-names Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set. spring.thymeleaf.reactive.full-mode-view-names Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set. spring.thymeleaf.reactive.max-chunk-size 0 Maximum size of data buffers used for writing to the response, in bytes. spring.thymeleaf.reactive.media-types Media types supported by the view technology. spring.thymeleaf.servlet.content-type text/html Content-Type value written to HTTP responses. spring.webflux.date-format Date format to use. spring.webflux.static-path-pattern /** Path pattern used for static resources. spring.webservices.wsdl-locations Comma-separated list of locations of WSDLs and accompanying XSDs to be exposed as beans. Key Replacement（替代） 原因 banner.charset spring.banner.charset banner.image.height spring.banner.image.height banner.image.invert spring.banner.image.invert banner.image.location spring.banner.image.location banner.image.margin spring.banner.image.margin banner.image.width spring.banner.image.width banner.location spring.banner.location endpoints.actuator.enabled actuator 端点不再可用 endpoints.actuator.path actuator 端点不再可用 endpoints.actuator.sensitive actuator 端点不再可用 endpoints.auditevents.enabled management.endpoint. auditevents.enabled endpoints.auditevents.path management.endpoints.web.path-mapping.auditevents endpoints.auditevents.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.autoconfig.enabled management.endpoint. conditions.enabled endpoints.autoconfig.id 端点标识符不再可定制 endpoints.autoconfig.path management.endpoints.web.path-mapping.conditions endpoints.autoconfig.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.beans.enabled management.endpoint.beans.enabled endpoints.beans.id 端点标识符不再可定制 endpoints.beans.path management.endpoints.web.path-mapping.beans endpoints.beans.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.configprops.enabled management.endpoint. configprops.enabled endpoints.configprops.id 端点标识符不再可定制 endpoints.configprops.keys-to-sanitize management.endpoint. configprops.keys-to-sanitize endpoints.configprops.path management.endpoints.web.path-mapping.configprops endpoints.configprops.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.cors.allow-credentials management.endpoints. web.cors.allow-credentials endpoints.cors.allowed-headers management.endpoints. web.cors.allowed-headers endpoints.cors.allowed-methods management.endpoints. web.cors.allowed-methods endpoints.cors.allowed-origins management.endpoints. web.cors.allowed-origins endpoints.cors.exposed-headers management.endpoints. web.cors.exposed-headers endpoints.cors.max-age management.endpoints. web.cors.max-age endpoints.docs.curies.enabled docs 端点不再可用 endpoints.docs.enabled docs 端点不再可用 endpoints.docs.path docs 端点不再可用 endpoints.docs.sensitive docs 端点不再可用 endpoints.dump.enabled management.endpoint. threaddump.enabled endpoints.dump.id 端点标识符不再可定制 endpoints.dump.path management.endpoints.web.path-mapping.dump endpoints.dump.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.enabled management.endpoints. enabled-by-default endpoints.env.enabled management.endpoint.env.enabled endpoints.env.id 端点标识符不再可定制 endpoints.env.keys-to-sanitize management.endpoint. env.keys-to-sanitize endpoints.env.path management.endpoints. web.path-mapping.env endpoints.env.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.flyway.enabled management.endpoint.flyway.enabled endpoints.flyway.id 端点标识符不再可定制 endpoints.flyway.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.health.enabled management.endpoint.health.enabled endpoints.health.id 端点标识符不再可定制 endpoints.health.mapping management.health.status.http-mapping endpoints.health.path management.endpoints.web.path-mapping.health endpoints.health.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.health.time-to-live management.endpoint. health.cache.time-to-live endpoints.heapdump.enabled management.endpoint.heapdump.enabled endpoints.heapdump.path management.endpoints.web.path-mapping.heapdump endpoints.heapdump.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.hypermedia.enabled Actuator 中的 Hypermedia 不再可用 endpoints.info.enabled management.endpoint.info.enabled endpoints.info.id 端点标识符不再可定制 endpoints.info.path management.endpoints.web.path-mapping.info endpoints.info.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.jmx.domain management.endpoints.jmx.domain endpoints.jmx.enabled management.endpoints. jmx.exposure.exclude endpoints.jmx.static-names management.endpoints. jmx.static-names endpoints.jmx.unique-names management.endpoints. jmx.unique-names endpoints.jolokia.enabled management.endpoint. jolokia.enabled endpoints.jolokia.path management.endpoints.web.path-mapping.jolokia endpoints.jolokia.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.liquibase.enabled management.endpoint. liquibase.enabled endpoints.liquibase.id 端点标识符不再可定制 endpoints.liquibase.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.logfile.enabled management.endpoint. logfile.enabled endpoints.logfile.external-file management.endpoint. logfile.external-file endpoints.logfile.path management.endpoints.web.path-mapping.logfile endpoints.logfile.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.loggers.enabled management.endpoint. loggers.enabled endpoints.loggers.id 端点标识符不再可定制 endpoints.loggers.path management.endpoints.web.path-mapping.loggers endpoints.loggers.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.mappings.enabled management.endpoint. mappings.enabled endpoints.mappings.id 端点标识符不再可定制 endpoints.mappings.path management.endpoints.web.path-mapping.mappings endpoints.mappings.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.metrics.enabled management.endpoint.metrics.enabled endpoints.metrics.filter.counter-submissions Metrics support 现在使用千分尺 endpoints.metrics.filter.enabled Metrics support 现在使用千分尺 endpoints.metrics.filter.gauge-submissions Metrics support 现在使用千分尺 endpoints.metrics.id 端点标识符不再可定制 endpoints.metrics.path management.endpoints.web.path-mapping.metrics endpoints.metrics.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.shutdown.enabled management.endpoint. shutdown.enabled endpoints.shutdown.id 端点标识符不再可定制 endpoints.shutdown.path management.endpoints.web.path-mapping.shutdown endpoints.shutdown.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.trace.enabled management.endpoint. httptrace.enabled endpoints.trace.filter.enabled management.trace.http.enabled endpoints.trace.id 端点标识符不再可定制 endpoints.trace.path management.endpoints.web.path-mapping.httptrace endpoints.trace.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 error.path Path of the error controller. flyway.baseline-description spring.flyway.baseline-description flyway.baseline-on-migrate spring.flyway.baseline-on-migrate flyway.baseline-version spring.flyway.baseline-version flyway.check-location spring.flyway.check-location flyway.clean-on-validation-error spring.flyway. clean-on-validation-error flyway.enabled spring.flyway.enabled flyway.encoding spring.flyway.encoding flyway.ignore-failed-future-migration flyway.init-sqls spring.flyway.init-sqls flyway.locations spring.flyway.locations flyway.out-of-order spring.flyway.out-of-order flyway.password spring.flyway.password flyway.placeholder-prefix spring.flyway.placeholder-prefix flyway.placeholder-replacement spring.flyway.placeholder-replacement flyway.placeholder-suffix spring.flyway.placeholder-suffix flyway.placeholders spring.flyway.placeholders flyway.schemas spring.flyway.schemas flyway.sql-migration-prefix spring.flyway.sql-migration-prefix flyway.sql-migration-separator spring.flyway.sql-migration-separator flyway.sql-migration-suffix spring.flyway.sql-migration-suffixes flyway.table spring.flyway.table flyway.target spring.flyway.target flyway.url spring.flyway.url flyway.user spring.flyway.user flyway.validate-on-migrate spring.flyway.validate-on-migrate jolokia.config management.endpoint.jolokia.config liquibase.change-log spring.liquibase.change-log liquibase.check-change-log-location spring.liquibase.check-change-log-location liquibase.contexts spring.liquibase.contexts liquibase.default-schema spring.liquibase.default-schema liquibase.drop-first spring.liquibase.drop-first liquibase.enabled spring.liquibase.enabled liquibase.labels spring.liquibase.labels liquibase.parameters spring.liquibase.parameters liquibase.password spring.liquibase.password liquibase.rollback-file spring.liquibase.rollback-file liquibase.url spring.liquibase.url liquibase.user spring.liquibase.user management.add-application-context-header management.server.add-application-context-header management.address management.server.address management.context-path management.server. servlet.context-path management.port management.server.port management.security.enabled 现在提供全局 security 自动配置。 management.security.roles security 自动配置不再可定制 management.security.sessions security 自动配置不再可定制 management.shell.auth.jaas.domain CRaSH 支持不再可用 management.shell.auth.key.path CRaSH 支持不再可用 management.shell.auth.simple.user.name CRaSH 支持不再可用 management.shell.auth.simple.user.password CRaSH 支持不再可用 management.shell.auth.spring.roles CRaSH 支持不再可用 management.shell.auth.type CRaSH 支持不再可用 management.shell.ssh.auth-timeout CRaSH 支持不再可用 management.shell.ssh.enabled CRaSH 支持不再可用 management.shell.ssh.idle-timeout CRaSH 支持不再可用 management.shell.ssh.key-path CRaSH 支持不再可用 management.shell.ssh.port CRaSH 支持不再可用 management.shell.telnet.enabled CRaSH 支持不再可用 management.shell.telnet.port CRaSH 支持不再可用 management.ssl.ciphers management.server.ssl.ciphers management.ssl.client-auth management.server.ssl.client-auth management.ssl.enabled management.server.ssl.enabled management.ssl.enabled-protocols management.server.ssl.enabled-protocols management.ssl.key-alias management.server.ssl.key-alias management.ssl.key-password management.server.ssl.key-password management.ssl.key-store management.server.ssl.key-store management.ssl.key-store-password management.server.ssl.key-store-password management.ssl.key-store-provider management.server.ssl.key-store-provider management.ssl.key-store-type management.server.ssl.key-store-type management.ssl.protocol management.server.ssl.protocol management.ssl.trust-store management.server.ssl.trust-store management.ssl.trust-store-password management.server.ssl.trust-store-password management.ssl.trust-store-provider management.server.ssl.trust-store-provider management.ssl.trust-store-type management.server.ssl.trust-store-type management.trace.include management.trace.http.include security.basic.authorize-mode security 自动配置不再可定制 security.basic.enabled security 自动配置不再可定制 security.basic.path security 自动配置不再可定制 security.basic.realm security 自动配置不再可定制 security.enable-csrf security 自动配置不再可定制 security.filter-dispatcher-types spring.security. filter.dispatcher-types security.filter-order spring.security.filter.order security.headers.cache security 自动配置不再可定制 security.headers.content-security-policy security 自动配置不再可定制 security.headers.content-security-policy-mode security 自动配置不再可定制 security.headers.content-type security 自动配置不再可定制 security.headers.frame security 自动配置不再可定制 security.headers.hsts security 自动配置不再可定制 security.headers.xss security 自动配置不再可定制 security.ignored security 自动配置不再可定制 security.oauth2. authorization.check-token-access Spring Security 访问规则用于检查令牌端点（例如，SpEL表达式，如“isAuthenticated（）”） security.oauth2.authorization.realm 客户端身份验证的领域名称 security.oauth2.authorization.token-key-access Spring Security访问规则用于检查令牌端点（例如，SpEL表达式，如“isAuthenticated（）”） security.oauth2.client.access-token-uri security.oauth2.client.access-token-validity-seconds security.oauth2.client.additional-information security.oauth2.client.authentication-scheme security.oauth2.client.authorities security.oauth2.client.authorized-grant-types security.oauth2.client.auto-approve-scopes security.oauth2.client.client-authentication-scheme security.oauth2.client.client-id security.oauth2.client.client-secret security.oauth2.client.grant-type security.oauth2.client.id security.oauth2.client.pre- established-redirect-uri security.oauth2.client.refresh-token-validity-seconds security.oauth2.client.registered-redirect-uri security.oauth2.client.resource-ids security.oauth2.client.scope security.oauth2.client.token-name security.oauth2.client.use-current-uri security.oauth2.client.user-authorization-uri security.oauth2.resource.filter-order 0 用于验证令牌的过滤器链的顺序。 security.oauth2.resource.id 资源的标识符 security.oauth2.resource.jwk.key-set-uri 获取验证密钥以验证 JWT 令牌的 URI security.oauth2.resource.jwt.key-uri JWT 令牌的 URI security.oauth2.resource.jwt.key-value JWT 令牌的验证密钥 security.oauth2.resource.prefer-token-info true 使用令牌信息，可以设置为 false以使用用户信息 security.oauth2.resource.service-id resource security.oauth2.resource.token-info-uri token decoding 端点的 URI security.oauth2.resource.token-type 使用 userInfoUri 时要发送的令牌类型。 security.oauth2.resource.user-info-uri 用户端点的 URI security.oauth2.sso.filter-order 如果不提供显式的WebSecurityConfigurerAdapter，则应用过滤器顺序（在这种情况下，可以改为提供顺序）。 security.oauth2.sso.login-path /login 登录页面的路径，即触发重定向到 OAuth2 授权服务器的页面。 security.require-ssl security 自动配置已不再可定制 security.sessions security 自动配置已不再可定制 security.user.name spring.security.user.name security.user.password spring.security.user.password security.user.role spring.security.user.roles server.context-parameters server.servlet.context-parameters server.context-path server.servlet.context-path server.display-name server.servlet. application-display-name server.jsp-servlet.class-name server.servlet.jsp.class-name server.jsp-servlet.init-parameters server.servlet.jsp.init-parameters server.jsp-servlet.registered server.servlet.jsp.registered server.servlet-path server.servlet.path server.session.cookie.comment server.servlet.session.cookie.comment server.session.cookie.domain server.servlet.session.cookie.domain server.session.cookie.http-only server.servlet. session.cookie.http-only server.session.cookie.max-age server.servlet.session.cookie.max-age server.session.cookie.name server.servlet.session.cookie.name server.session.cookie.path server.servlet.session.cookie.path server.session.cookie.secure server.servlet.session.cookie.secure server.session.persistent server.servlet.session.persistent server.session.store-dir server.servlet.session.store-dir server.session.timeout server.servlet.session.timeout server.session.tracking-modes server.servlet.session.tracking-modes spring.activemq.pool.configuration.block-if-session-pool-is-full spring.activemq.pool.configuration.block-if-session-pool-is-full-timeout spring.activemq.pool.configuration.connection-factory spring.activemq.pool.configuration.create-connection-on-startup spring.activemq.pool.configuration.expiry-timeout spring.activemq.pool.configuration.idle-timeout spring.activemq.pool.configuration.max-connections spring.activemq.pool.configuration.maximum-active-session-per-connection spring.activemq.pool.configuration.properties spring.activemq.pool.configuration.reconnect-on-exception spring.activemq.pool.configuration.time-between-expiration-check-millis spring.activemq.pool.configuration.use-anonymous-producers spring.application.index 应用程序上下文 ID 默认情况下是唯一的 spring.batch.initializer.enabled spring.batch.initialize-schema spring.cache.guava.spec 用于创建缓存的规范 spring.cache.hazelcast.config 用于初始化 Hazelcast 的配置文件的位置 spring.data.cassandra.connect-timeout-millis spring.data. cassandra.connect-timeout spring.data.cassandra.read-timeout-millis spring.data.cassandra.read-timeout spring.data.cassandra.repositories.enabled spring.data.cassandra. repositories.type spring.data.couchbase.repositories.enabled spring.data.couchbase. repositories.type spring.data.mongodb.repositories.enabled spring.data.mongodb. repositories.type spring.data.neo4j.compiler 从 Neo4j 3 开始不再支持 spring.datasource.dbcp.access-to-underlying-connection-allowed spring.datasource.dbcp.connection-init-sqls spring.datasource.dbcp.default-auto-commit spring.datasource.dbcp.default-catalog spring.datasource.dbcp.default-read-only spring.datasource.dbcp.default-transaction-isolation spring.datasource.dbcp.driver-class-name spring.datasource.dbcp.initial-size spring.datasource.dbcp.log-abandoned spring.datasource.dbcp.login-timeout spring.datasource.dbcp.max-active spring.datasource.dbcp.max-idle spring.datasource.dbcp.max-open-prepared-statements spring.datasource.dbcp.max-wait spring.datasource.dbcp.min-evictable-idle-time-millis spring.datasource.dbcp.min-idle spring.datasource.dbcp.num-tests-per-eviction-run spring.datasource.dbcp.password spring.datasource.dbcp.pool-prepared-statements spring.datasource.dbcp.remove-abandoned spring.datasource.dbcp.remove-abandoned-timeout spring.datasource.dbcp.test-on-borrow spring.datasource.dbcp.test-on-return spring.datasource.dbcp.test-while-idle spring.datasource.dbcp.time-between-eviction-runs-millis spring.datasource.dbcp.url spring.datasource.dbcp.username spring.datasource.dbcp.validation-query spring.datasource.dbcp.validation-query-timeout spring.datasource.hikari.connection-customizer-class-name spring.datasource.initialize spring.datasource. initialization-mode spring.devtools.remote.debug.enabled 远程 debug 已不再支持 spring.devtools.remote.debug.local-port 远程 debug 已不再支持 spring.http.multipart.enabled spring.servlet.multipart.enabled spring.http.multipart.file-size-threshold spring.servlet.multipart.file-size-threshold spring.http.multipart.location spring.servlet.multipart.location spring.http.multipart.max-file-size spring.servlet.multipart.max-file-size spring.http.multipart.max-request-size spring.servlet. multipart.max-request-size spring.http.multipart.resolve-lazily spring.servlet.multipart.resolve-lazily spring.jpa.hibernate.naming.strategy Hibernate 4 的自动配置已不再提供 spring.jta.atomikos.properties.console-log-level warn spring.messages.cache-seconds spring.messages.cache-duration spring.metrics.export.aggregate.key-pattern Metrics support 现在使用千分尺 spring.metrics.export.aggregate.prefix Metrics support 现在使用千分尺 spring.metrics.export.delay-millis Metrics support 现在使用千分尺 spring.metrics.export.enabled Metrics support 现在使用千分尺 spring.metrics.export.excludes Metrics support 现在使用千分尺 spring.metrics.export.includes Metrics support 现在使用千分尺 spring.metrics.export.redis.key Metrics support 现在使用千分尺 spring.metrics.export.redis.prefix Metrics support 现在使用千分尺 spring.metrics.export.send-latest Metrics support 现在使用千分尺 spring.metrics.export.statsd.host management.metrics. export.statsd.host spring.metrics.export.statsd.port management.metrics. export.statsd.port spring.metrics.export.statsd.prefix Metrics support 现在使用千分尺 spring.metrics.export.triggers Metrics support 现在使用千分尺 spring.mobile.devicedelegatingviewresolver.enable-fallback false 启用对回退解决方案的支持 spring.mobile.devicedelegatingviewresolver.enabled false 启用 device 视图解析器 spring.mobile.devicedelegatingviewresolver.mobile-prefix mobile/ 用于查看移动设备名称的前缀 spring.mobile.devicedelegatingviewresolver.mobile-suffix `` 附加后缀以查看移动设备的名称 spring.mobile.devicedelegatingviewresolver.normal-prefix `` 用于查看普通设备名称的前缀. spring.mobile.devicedelegatingviewresolver.normal-suffix `` 附加后缀以查看普通设备的名称 spring.mobile.devicedelegatingviewresolver.tablet-prefix tablet/ 前缀预设为查看平板电脑设备的名称 spring.mobile.devicedelegatingviewresolver.tablet-suffix `` 附加后缀以查看平板电脑设备的名称 spring.mobile.sitepreference.enabled true 启用 SitePreferenceHandler. spring.mvc.media-types spring.mvc. contentnegotiation.media-types spring.rabbitmq.listener.acknowledge-mode spring.rabbitmq.listener.auto-startup spring.rabbitmq.listener.concurrency spring.rabbitmq.listener.default-requeue-rejected spring.rabbitmq.listener.idle-event-interval spring.rabbitmq.listener.max-concurrency spring.rabbitmq.listener.prefetch spring.rabbitmq.listener.retry.enabled false 是否启用发布重试 spring.rabbitmq.listener.retry.initial-interval 1000 第一次和第二次尝试发布或传递讯息的时间间隔 spring.rabbitmq.listener.retry.max-attempts 3 尝试发布或传递邮件的最大次数 spring.rabbitmq.listener.retry.max-interval 10000 尝试之间的最大间隔 spring.rabbitmq.listener.retry.multiplier 1 于先前重试间隔的倍数 spring.rabbitmq.listener.retry.stateless true 无论重试是无状态还是有状态 spring.rabbitmq.listener.transaction-size spring.redis.pool.max-active spring.redis.jedis.pool.max-idle spring.redis.pool.max-idle spring.redis.jedis.pool.max-idle spring.redis.pool.max-wait spring.redis.jedis.pool.max-wait spring.redis.pool.min-idle spring.redis.jedis.pool.min-idle spring.resources.cache-period spring.resources.cache.period spring.sendgrid.password 不再支持使用用户名和密码 ( 使用 spring.sendgrid.api-key 代替 ) spring.sendgrid.username 不再支持使用用户名和密码 ( 使用 spring.sendgrid.api-key 代替 ) spring.session.jdbc.initializer.enabled spring.session. jdbc.initialize-schema spring.session.mongo.collection-name spring.session.mongodb.collection-name spring.social.auto-connection-views false 为支持的生产者启用连接状态视图 spring.social.facebook.app-id Application id. spring.social.facebook.app-secret Application secret. spring.social.linkedin.app-id Application id. spring.social.linkedin.app-secret Application secret. spring.social.twitter.app-id Application id. spring.social.twitter.app-secret Application secret. spring.thymeleaf.content-type spring.thymeleaf. servlet.content-type 相关文章相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 最后英文参考：https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Configuration-Changelog","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"写这么多系列博客，怪不得找不到女朋友","date":"2018-03-26T16:00:00.000Z","path":"2018/03/27/blogs/","text":"前提好几周没更新博客了，对不断支持我博客的童鞋们说声：“抱歉了！”。自己这段时间确实比较忙，而且还在抽空完成学校的毕业设计。今天晚上抽空把大学期间写过的博客弄一个系列文章合集，算是对大学这四年的一个总结，证明自己没白过。 熟悉我的人都知道我写博客的时间比较早，而且坚持的时间也比较久，一直到现在也是一直保持着更新状态。最早最早开始写博客是在 CSDN 上写的，然后在简书也写过一段时间，后来放弃了简书转战了掘金，以下图片是自己在掘金这一年的成果，快 1.5 万关注了，哈哈哈。 去年过年前还收到掘金送来的专属礼物，真是激动，感谢掘金，希望越办越好！ 细数文章后，发现自己在实习的这段时间写的博客也挺多的，而且质量还比较高，经常上开发者头条、掘金等平台的首页推荐。在此，感谢实习期间组内大佬们的各种帮助！ 这里再次说下写博客的好处： 很好的用来总结自己所学的知识 遇到那么一群也写博客的大佬，有共同话题聊了 面试加分（在简历上放上自己的个人网站链接，面试官就可以更好的了解你，知道你所学知识的深度和广度） 不要小看你的每一篇不起眼博客，用一个蚂蚁金服大佬跟我说的话叫做：厚积薄发！ 不多说了，如果想和我交流的可以加我 qq 群：528776268 和我的微信：zhisheng_tian 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/27/blogs/ 系列文章合集Spring Boot 系列文章1、Spring Boot系列文章（一）：SpringBoot Kafka 整合使用 2、Spring Boot系列文章（二）：SpringBoot Admin 使用指南 3、Spring Boot系列文章（三）：SpringBoot RabbitMQ 整合使用 4、Spring Boot系列文章（四）：SpringBoot ActiveMQ 整合使用 5、Spring Boot系列文章（五）：SpringBoot RabbitMQ 整合进阶版 6、Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控 7、更多请期待 Spring Boot 2.0 系列文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、后面绝对有更多文章出现的 Docker 系列文章1、Docker系列文章（一）：基于 Harbor 搭建 Docker 私有镜像仓库 2、Docker系列文章（二）：Mac 安装 Docker 及常用命令 3、同样，后面也会持续更新 ElasticSearch 系列文章1、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、 Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 3、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 4、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 5、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 6、也有更多深入的文章 搭建博客系列文章1、利用Github Page 搭建个人博客网站 2、Github pages + Hexo 博客 yilia 主题使用畅言评论系统 3、Hexo + yilia 搭建博客可能会遇到的所有疑问 4、Hexo + yilia 主题实现文章目录 5、这个系列看情况，可能还会有 Java 系列文章1、关于String s = new String(“xyz”); 创建几个对象的问题 2、《Java 多线程编程核心技术》学习笔记及总结 3、从对象深入分析 Java 中实例变量和类变量的区别 4、深度探究Java 中 finally 语句块 5、解决jdk1.8中发送邮件失败（handshake_failure）问题 6、深入分析 Java Web 中的中文编码问题 7、奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True? 8、Java读取文件 9、HashMap、Hashtable、HashSet 和 ConcurrentHashMap 的比较 10、Java连接Oracle数据库的三种连接方式 11、Java NIO 系列教程 12、《疯狂 Java 突破程序员基本功的 16 课》读书笔记 13、详细深入分析 Java ClassLoader 工作机制 14、详解 Filter 过滤器 15、Java IO流学习超详细总结（图文并茂） 16、通过源码详解 Servlet 17、Java 性能调优需要格外注意的细节 18、Java 线程池艺术探索 19、JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解 20、这个必须的持续更新下去 Maven 系列文章1、Centos7 搭建最新 Nexus3 Maven 私服 2、Maven 中 dependencies 与 dependencyManagement 的区别 Kafka 系列文章1、Kafka 安装及快速入门 2、Spring Boot系列文章（一）：SpringBoot Kafka 整合使用 Mybatis 系列文章1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、通过项目逐步深入了解Mybatis&lt;二&gt; 3、通过项目逐步深入了解Mybatis&lt;三&gt; 4、通过项目逐步深入了解Mybatis（四)/) 5、MyBatis的foreach语句详解 6、期待它的源码解析文章吗？ Nginx 系列文章1、Ubuntu16.10 安装 Nginx 2、Nginx 基本知识快速入门 Python 爬虫系列文章1、Python爬虫实战之爬取百度贴吧帖子 2、Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子 3、Python爬虫实战之爬取糗事百科段子 4、这个估计得等有机会再次学 Python 时再写 RocketMQ 系列文章1、RocketMQ系列文章（一）：RocketMQ 初探 2、RocketMQ系列文章（二）：RocketMQ 安装及快速入门 3、RocketMQ系列文章（三）：RocketMQ 简单的消息示例 4、Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控 Spring MVC 系列文章1、Spring MVC系列文章（一）：Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统 Demo 2、Spring MVC系列文章（二）：Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑 3、Spring MVC系列文章（三）：看透 Spring MVC 源代码分析与实践 —— 网站基础知识 4、Spring MVC系列文章（四）：看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 5、Spring MVC系列文章（五）：看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 6、通过项目逐步深入了解Spring MVC（一） Netty 系列文章1、Netty系列文章（一）：Netty 源码阅读之初始环境搭建 2、这个系列迟早会更新的。 前端系列文章1、Bootstrap入门需掌握的知识点（一） 2、Bootstrap入门需掌握的知识点（二） 3、使用 CodeMirror 打造属于自己的在线代码编辑器 4、AJAX 学习 5、前端渣渣这个也要慢慢学习这块 面试经验系列1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺（一面二面） 3、秋招第三站 —— 内推阿里（一面） 4、 面试过阿里等互联网大公司，我知道了这些套路 5、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 其他还有一些其他方面的技术文章，算不是系列文章，比较零散，还有就是一些随笔文章，就不把它们放在合集里了。 总结用自己的一句话：坑要一个个填，路要一步步走！前人栽树，后人乘凉，学会感恩！ 建了个不错的微信群，如果有感兴趣的可以加我微信，对我回复 加群 ，然后会拉你进群交流。","tags":[{"name":"博客合集","slug":"博客合集","permalink":"http://www.54tianzhisheng.cn/tags/博客合集/"}]},{"title":"Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解","date":"2018-03-05T16:00:00.000Z","path":"2018/03/06/SpringBoot2-new-features/","text":"背景在 3 月 1 号，Spring Boot2.0.0.RELEASE正式发布，这是 Spring Boot1.0 发布 4 年之后第一次重大修订，因此有多的新功能和特性值得大家期待！下面带大家了解下 Spring Boot 2.0 中的新特性。 SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/06/SpringBoot2-new-features/ 从 Spring Boot 1.5 升级由于 Spring Boot 2.0 的改变幅度有点大，所以升级现有的程序可能会比平常更大一些。 如果你还在考虑是否要升级，这里推荐 DD 的博客文章：Spring Boot 2.0 正式发布，升还是不升呢？ 如果要升级可以参考我的另外一篇文章：Spring Boot 2.0 迁移指南 如果您目前正在运行较早版本的 Spring Boot，我们强烈建议您在迁移到 Spring Boot 2.0 之前先升级到 Spring Boot 1.5。 新的和值得注意的特性小技巧：检查 配置更改日志 来获取配置更改的完整描述。 起码 JDK 8 和支持 JDK 9Spring Boot 2.0 要求 Java 8 作为最低版本。许多现有的 API 已更新，以利用 Java 8 的特性，例如：接口上的默认方法，函数回调以及新的 API，如javax.time。如果您当前正在使用 Java 7 或更早版本，则在开发 Spring Boot 2.0 应用程序之前，您需要升级您的 JDK。 Spring Boot 2.0 通过了在 JDK 9 下的测试，可以在 JDK 9 下正常运行，。我们所有的 jar 包都在模块系统兼容性的清单中附带了自动模块名称条目。 第三方库的升级Spring Boot 2.0 建立在 Spring Framework 5 之上，并且需要 Spring Framework 5 。你可以通过 What’s New in Spring Framework 5.x 了解 Spring 5 的新特性。并在继续之前查看其升级指南 Upgrading to Spring Framework 5.x 。 我们已尽可能升级到其他第三方库的最新稳定版本。 本版本中一些显着的依赖性升级包括： Tomcat 8.5 Flyway 5 Hibernate 5.2 Thymeleaf 3 Reactive SpringSpring 产品组合中的许多项目现在都为开发反应式应用程序提供一流的支持。反应性应用程序是完全异步和非阻塞的。它们旨在用于事件循环执行模型（而不是更传统的每个请求线程执行模型）。Spring 框架参考文档中的“Web 反应堆栈”部分为这个主题提供了一个很好的入门。 Spring Boot 2.0 通过自动配置和启动器 POM 完全支持反应式应用。Spring Boot 的内部本身也在必要时进行了更新，以提供反应性的反应（最明显的是我们的嵌入式服务器支持）。 Spring WebFlux＆WebFlux.fnSpring WebFlux 是 Spring MVC 的完全非阻塞反应式替代方案。Spring Boot 为基于注释的 Spring WebFlux 应用程序以及 WebFlux.fn 提供了自动配置，WebFlux.fn 提供了更实用的样式 API。 要开始，请添加 spring-boot-starter-webflux 到 POM，它将提供由嵌入式 Netty 服务器支持的 Spring WebFlux。 Reactive Spring Data在底层技术支持的情况下，Spring Data 还为反应式应用程序提供支持。目前 Cassandra，MongoDB，Couchbase 和 Redis 都有反应式 API 支持。 Spring Boot 包含针对这些技术的特殊 starter-POMs，可为您提供启动所需的一切。例如，spring-boot-starter-data-mongodb-reactive包括对反应性 mongo 驱动程序和项目反应堆的依赖性。 Reactive Spring SecuritySpring Boot 2.0 可以充分利用 Spring Security 5.0 来保护您的反应式应用程序。当 Spring Security 位于类路径中时，会为 WebFlux 应用程序提供自动配置。 使用 WebFlux 的 Spring Security 访问规则可以通过SecurityWebFilterChain。如果你之前整合过 Spring MVC 和 Spring Security，应该会感到非常熟悉。有关更多详细信息，请参阅 Spring Boot 参考文档和 Spring Security 文档。 嵌入式 Netty 服务器由于 WebFlux 不依赖于 Servlet API，我们现在可以首次为 Netty 作为嵌入式服务器提供支持。该spring-boot-starter-webflux 启动 POM 将拉取 Netty 4.1 和 Ractor Netty 。 注意：您只能将 Netty 用作反应式服务器。不提供阻止 servlet API 支持。 HTTP/2 支持为 Tomcat，Undertow 和 Jetty 提供 HTTP / 2 支持。支持取决于所选的 Web 服务器和应用程序环境（因为 JDK 8 不支持该协议）。 如何配置 HTTP／2，请参考 官方文档 。 配置属性的绑定在 Spring Boot 2.0 中，用于绑定Environment属性的机制@ConfigurationProperties已经完全彻底修改。我们借此机会收紧了松散绑定的规则，并修复了 Spring Boot 1.x 中的许多不一致之处。 新的BinderAPI 也可以@ConfigurationProperties直接在你自己的代码之外使用。例如，下面将结合到List的PersonName对象： 123List&lt;PersonName&gt; people = Binder.get(environment) .bind(\"my.property\", Bindable.listOf(PersonName.class)) .orElseThrow(IllegalStateException::new); 配置源可以像这样在 YAML 中表示： 123456my: property: - first-name: zhisheng last-name: tian - first-name: zhisheng last-name: tian 有关更新绑定规则的更多信息，请参阅此Wiki页面。 配置起源YAML 文件和被 Spring Boot 加载的 Properties 文件现在包含Origin信息，可帮助您跟踪项目从何处加载的信息。有些 Spring Boot 特性利用了这个信息可以在适当的时候展示出来。 例如，BindException绑定失败时抛出的类是一个OriginProvider。这意味着原始信息可以很好地从故障分析器中显示出来。 另一个例子是env执行器端点，当它有可用时包含了原始信息。下面的代码片断显示该spring.security.user.name属性来自 jar 包中的 application.properties 文件的第 1行，第 27 列。 123456789&#123; \"name\": \"applicationConfig: [classpath:/application.properties]\", \"properties\": &#123; \"spring.security.user.name\": &#123; \"value\": \"user\", \"origin\": \"class path resource [application.properties]:1:27\" &#125; &#125;&#125; 转换器支持Binding 利用了一个新的 ApplicationConversionService 类，它提供了一些对属性绑定特别有用的额外转换器。最引人注目的是转换器的Duration类型和分隔字符串。 该Duration转换器允许在任一 ISO-8601 格式中指定的持续时间，或作为一个简单的字符串（例如10m，10 分钟）。现有的属性已更改为始终使用Duration。该@DurationUnit注释通过设置如果没有指定所使用的单元确保向后兼容性。例如，Spring Boot 1.5 中需要秒数的属性现在必须@DurationUnit(ChronoUnit.SECONDS)确保一个简单的值，例如10实际使用的值10s。 分隔字符串转换允许您将简单绑定String到Collection或Array不必分割逗号。例如，LDAP base-dn 属性用 @Delimiter(Delimiter.NONE)，所以 LDAP DN（通常包含逗号）不会被错误解释。 Gradle 插件Spring Boot 的 Gradle 插件已在很大程度上进行了重新编写，以实现许多重大改进。您可以在其参考文献和 API 文档中阅读关于插件功能的更多信息。 Spring Boot 现在需要 Gradle 4.x. 如果您要升级使用 Gradle 的项目，请查看迁移指南。 KotlinSpring Boot 2.0 现在包含对 Kotlin 1.2.x 的支持，并提供了runApplication ，一个使用 Kotlin 运行 Spring Boot 应用程序的方法。我们还公开和利用了 Kotlin 对其他 Spring 项目（如Spring Framework，Spring Data 和 Reactor）已添加到其最近版本中的支持。 有关更多信息，请参阅参考文档的Kotlin支持部分。 Actuator 改进在 Spring Boot 2.0 中 Actuator endpoints 有很大的改进。所有 HTTP Actuator endpoints 现在都在该/actuator路径下公开，并且生成的 JSON 有效负载得到了改进。 我们现在也不会在默认情况下暴露很多端点。如果您要升级现有的 Spring Boot 1.5 应用程序，请务必查看迁移指南并特别注意该management.endpoints.web.exposure.include属性。 Actuator JSONSpring Boot 2.0 改进了从许多端点返回的 JSON 有效负载。 现在许多端点都具有更精确地反映底层数据的 JSON。例如，/actuator/conditions终端（/autoconfig在Spring Boot 1.5中）现在有一个顶级contexts密钥来将结果分组ApplicationContext。 现在还使用 Spring REST Docs 生成了广泛的 REST API 文档，并随每个版本发布。 Jersey and WebFlux 支持除了支持 Spring MVC 和 JMX，您现在可以在开发 Jersey 或 WebFlux 应用程序时访问执行器端点。Jersey 支持通过自定义 Jersey 提供Resource，WebFlux 使用自定义HandlerMapping。 Hypermedia links该/actuator端点现在提供了一个 HAL 格式的响应提供链接到所有活动端点（即使你没有 Spring HATEOAS 在classpath）。 Actuator @Endpoints为了支持 Spring MVC，JMX，WebFlux 和 Jersey，我们为 Actuator @Endpoints 开发了一种新的编程模型。该@Endpoint注解可以与@ReadOperation，@WriteOperation 和 @DeleteOperation 组合使用开发 endpoints。 您还可以使用@EndpointWebExtension或@EndpointJmxExtension编写技术特定的增强功能到 endpoints。详细信息请参阅更新的参考文档。 MicrometerSpring Boot 2.0 不再提供自己的指标 API。相反，我们依靠 micrometer.io 来满足所有应用程序监视需求。 Micrometer 包括尺寸指标的支持，当与尺寸监测系统配对时，尺寸指标可以有效访问特定的指定度量标准，并且可以在其尺寸范围内向下钻取。 指标可以输出到各种系统和开箱即用的 Spring Boot 2.0，为 Atlas，Datadog，Ganglia，Graphite，Influx，JMX，New Relic，Prometheus，SignalFx，StatsD 和 Wavefront 提供支持。另外还可以使用简单的内存中度量标准。 集成随 JVM 指标（包括 CPU，内存，线程和 GC），Logback，Tomcat，Spring MVC＆提供RestTemplate。 有关更多详细信息，请参阅参考文档的更新“指标”部分。 数据支持除了上面提到的 Reactive Spring Data 支持外，在数据领域还进行了其他一些更新和改进。 HikariCPSpring Boot 2.0 中的默认数据库池技术已从 Tomcat Pool 切换到 HikariCP。我们发现 Hakari 提供了卓越的性能，我们的许多用户更喜欢 Tomcat Pool。 初始化数据库初始化逻辑在 Spring Boot 2.0 中已经合理化。Spring Batch，Spring Integration，Spring Session 和 Quartz的初始化现在仅在使用嵌入式数据库时才会默认发生。该enabled属性已被替换为更具表现力枚举。例如，如果你想一直执行 Spring Batch 的初始化，您可以设置spring.batch.initialize-schema=always。 如果 Flyway 或 Liquibase 正在管理您的 DataSource 的模式，并且您正在使用嵌入式数据库，Spring Boot 现在会自动关闭 Hibernate 的自动 DDL 功能。 JOOQSpring Boot 2.0 现在基于 DataSource 自动检测 JOOQ 方言（类似于为 JPA 方言所做的）。@JooqTest是新引入的注解用来简化那些只有 JOOQ 必须被使用的测试。 JdbcTemplateSpring Boot 自动配置的 JdbcTemplate 现在可以通过 spring.jdbc.template 属性进行自定义。此外，NamedParameterJdbcTemplate自动配置的内容会重用JdbcTemplate。 Spring Data Web 配置Spring Boot 公开了一个新的spring.data.web配置名称空间，可以轻松配置分页和排序。 InfluxDBSpring Boot 现在自动配置开源时间序列数据库 InfluxDB。要启用 InfluxDB 支持，您需要设置一个spring.influx.url属性，并将其包含influxdb-java在您的类路径中。 Flyway/Liquibase 灵活配置如果仅提供自定义url或user属性，则 Flyway 和 Liquibase 的自动配置现在将重用标准数据源属性，而不是忽略它们。这使您可以创建一个自定义的数据源，仅用于所需信息的迁移。 Hibernate现在支持自定义 Hibernate 命名策略。对于高级场景，现在可以在上下文中定义ImplicitNamingStrategy或PhysicalNamingStrategy使用常规 bean。 现在也可以通过公开HibernatePropertiesCustomizerbean 来更加细致地定制 Hibernate 使用的属性。 MongoDB 客户端自定义现在可以通过定义一个类型的 bean 来为 Spring Boot 自动配置的 Mongo 客户端应用高级定制MongoClientSettingsBuilderCustomizer。 Redis现在可以使用spring.cache.redis.*属性配置 Redis 的缓存默认值。 Web除了上面提到的 WebFlux 和 WebFlux.fn 支持之外，还在开发 Web 应用程序时进行了以下改进。 上下文路径记录当使用嵌入式容器时，当您的应用程序启动时，上下文路径将与 HTTP 端口一起记录。例如，嵌入式 Tomcat 现在看起来像这样： 1Tomcat 在端口上启动：8080（http），其上下文路径为 &apos;/foo&apos; Web过滤器初始化Web 过滤器现在在所有支持的容器上急切地初始化。 ThymeleafThymeleaf 初始化现在包括thymeleaf-extras-java8time，提供javax.time类型支持。 JSON 支持新的spring-boot-starter-json起始者收集必要的位以读取和写入 JSON。它不仅提供了jackson-databind与Java8 工作时，也是有用的模块：jackson-datatype-jdk8，jackson-datatype-jsr310和jackson-module-parameter-names。这个新的起动器现在被用于jackson-databind之前定义的地方。 如果您更喜欢 Jackson 之外的其他产品，我们对 GSON 的支持在 Spring Boot 2.0 已经大大提高。我们还引入了对 JSON-B 的支持（包括 JSON-B 测试支持）。 Quartz自动配置支持目前包含了 Quartz Scheduler。我们还添加了新的spring-boot-starter-quartz 初始化 POM。 您可以使用内存JobStores中或完整的基于 JDBC 的存储。所有JobDetail，Calendar并Trigger从你的 Spring应用程序上下文豆将自动注册Scheduler。 有关更多详细信息，请阅读参考文档的新“Quartz Scheduler”部分。 测试对 Spring Boot 2.0 中提供的测试支持进行了一些补充和调整： @WebFluxTest已添加新注释以支持 WebFlux 应用程序的“切片”测试。 Converter和GenericConverter豆类现在自动扫描@WebMvcTest和@WebFluxTest。 @AutoConfigureWebTestClient已经添加了一个注释来提供一个WebTestClientbean 供测试使用。注释会自动应用于@WebFluxTest测试。 增加了一个新的ApplicationContextRunner测试实用程序，可以很容易地测试您的自动配置。我们已将大部分内部测试套件移至此新模型。详细信息请参阅更新的文档。 其它除了上面列出的变化外，还有很多小的调整和改进，包括： @ConditionalOnBean现在在确定是否满足条件时使用逻辑AND而不是逻辑OR。 无条件类现在包含在自动配置报告中。 该springCLI 应用程序现在包括encodepassword可用于创建 Spring Security 的兼容散列密码命令。 计划任务（即 @EnableScheduling）可以使用scheduledtasks执行器端点进行审查。 该loggers驱动器终端现在允许你重新设置一个记录器级别为它的默认。 Spring Session 用户现在可以通过sessions执行器端点查找和删除会话。 使用spring-boot-starter-parent现在基于 Maven 的应用程序-parameters默认使用标志。 我们的构建现在使用 concourse 的 CI 和我们的项目 POM 文件已被重构，使它们更简单的。 动画 ASCII 艺术最后，为了好玩，Spring Boot 2.0 现在支持动画 GIF 横幅。 参考资料https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Release-Notes 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南","date":"2018-03-05T16:00:00.000Z","path":"2018/03/06/SpringBoot2-Migration-Guide/","text":"前提希望本文档将帮助您把应用程序迁移到 Spring Boot 2.0。 SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/06/SpringBoot2-Migration-Guide/ 在你开始之前首先，Spring Boot 2.0 需要 Java 8 或更高版本。不再支持 Java 6 和 7 了。 在 Spring Boot 2.0 中，许多配置属性被重新命名/删除，开发人员需要更新application.properties/ application.yml相应的配置。为了帮助你解决这一问题，Spring Boot 发布了一个新spring-boot-properties-migrator模块。一旦作为该模块作为依赖被添加到你的项目中，它不仅会分析应用程序的环境，而且还会在启动时打印诊断信息，而且还会在运行时为您暂时迁移属性。在您的应用程序迁移期间，这个模块是必备的： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-properties-migrator&lt;/artifactId&gt;&lt;/dependency&gt; 注意：完成迁移后，请确保从项目的依赖关系中删除此模块。 构建您的 Spring Boot 应用程序Spring Boot Maven 插件为了保持了一致性，并且避免与其他插件发生冲突，现在暴露的插件配置属性都以一个spring-boot前缀开始。 例如，以下命令prod使用命令行启用配置文件 1mvn spring-boot:run -Dspring-boot.run.profiles=prod Surefire 默认值以前的 include/exclude 模式已与最新的 Surefire 默认设置保持一致。如果依赖于此插件，需要相应地更新插件配置。之前对应的配置如下： 12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*Tests.java&lt;/include&gt; &lt;include&gt;**/*Test.java&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/Abstract*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt;&lt;/plugin&gt; PS: 如果您使用 JUnit 5，则应将 Surefire 降级到 2.19.1。该**/*Tests.java版本不包含此模式，因此如果您依赖该模式，请确保将其添加到您的配置中。 Spring Boot Gradle 插件Spring Boot 的 Gradle 插件在很大程度上已被重写，有了重大的改进。您可以在其参考文献和API文档中阅读关于插件功能的更多信息。 依赖管理Spring Boot 的 Gradle 插件不再自动应用依赖管理插件。相反，Spring Boot 的插件现在可以通过导入正确版本的spring-boot-dependencies BOM 来应用依赖管理插件。当依赖管理被配置的时候，这一点会让你有更多的控制权。 对于大多数应用程序，使用应用依赖管理插件就足够了： 12apply plugin: &apos;org.springframework.boot&apos;apply plugin: &apos;io.spring.dependency-management&apos; // &lt;-- add this to your build.gradle 注意：依赖管理插件仍然是 spring-boot-gradle-plugin 的传递依赖项，所以不需要在 buildscript 配置中将其列为类路径依赖项。 建立可执行的 Jars 和 Wars bootRepackage 任务已经被替换成 bootJar 和 bootWar 任务，分别用于构建可执行的 jar 包和 war包。 配置更新BootRun，BootJar和BootWar任务现在都使用mainClassName的属性来配置主类的名称。这使得三个特定于引导的任务相互一致，并将其与 Gradle 自己的应用程序插件进行对齐。 Spring Boot 特性默认动态代理策略Spring Boot 默认使用 CGLIB 做动态代理代理(基于类的动态代理)，包括对 AOP 的支持。如果你需要基于接口的动态代理，你需要将spring.aop.proxy-target-class 设置为false。 SpringApplicationWeb 环境Spring Boot 应用程序现在可以在更多模式下运行，因此spring.main.web-environment现在不推荐使用，spring.main.web-application-type属性可以提供更多的支持。 如果您想确保应用程序不启动 Web 服务器，则必须将该属性更改为： 1spring.main.web-application-type=none 注意：可以通过 SpringApplication 的 setWebApplicationType 方法实现。 Spring Boot 应用程序事件更改我们已经添加了一个新事件ApplicationStartedEvent。 ApplicationStartedEvent在上下文刷新之后但在任何应用程序和命令行参数被调用之前发送。 ApplicationReadyEvent在任何应用程序和命令行参数被调用后发送。它表示应用程序已准备好为请求提供服务。 请参阅更新的参考文档。 Banner在我们限制 Spring Boot 使用的根名称空间的数量的过程中，与标志相关的属性已被重定位到spring.banner。 外部化配置轻松的绑定有关宽松绑定的规则已经收紧。我们假设一个现有的acme.my-project.my-name属性： 所有前缀必须是 kebab格式（小写，连字符分隔）acme.myProject或acme.my_project无效 - 您必须acme.my-project在此处使用。 属性名称可以使用 kebab-case（my-name），camel-case（myName）或 snake-case（my_name）。 环境属性（来自操作系统环境变量）必须使用通常的大写下划线格式，下划线只能用于分隔键的各个部分ACME_MYPROJECT_MYNAME。 这种新的放松绑定具有以下几个优点： 无需担心密钥的结构@ConditionalOnProperty：只要密钥是以规范格式定义的，支持的松散变体就可以透明地工作。如果您正在使用该prefix属性，则现在只需使用name或value属性即可放置完整密钥。 RelaxedPropertyResolver不再可以Environment自动处理：env.getProperty(&quot;com.foo.my-bar&quot;)将找到一个com.foo.myBar属性。 该org.springframework.boot.bind软件包不再可用，并被新的宽松绑定规则所取代。特别是，RelaxedDataBinder朋友已被新的BinderAPI 取代。以下样品MyProperties从app.acme前缀中进行绑定。 123MyProperties target = Binder.get(environment) .bind(\"app.acme\", MyProperties.class) .orElse(null); 由于现在内置了轻松绑定，因此只要使用其中一种支持的格式，就可以请求任何属性而不必关心案例： 123FlagType flagType = Binder.get(environment) .bind(\"acme.app.my-flag\", FlagType.class) .orElse(FlagType.DEFAULT); @ConfigurationProperties 验证如果您想打开验证，现在必须为您的@ConfigurationProperties对象添加注释@Validated。 配置位置spring.config.location配置的方式已被修复; 它提前将一个位置添加到默认位置列表中，现在它将替换默认位置。如果你是按照以前的方式进行处理，现在应该使用它spring.config.additional-location进行替换。 开发 Web 应用程序嵌入式容器包装结构为了支持响应式用例，嵌入式容器包结构已经被大幅度的重构。 EmbeddedServletContainer已被重新命名为，WebServer并且该org.springframework.boot.context.embedded包已被重新定位到org.springframework.boot.web.embedded。例如，如果您使用TomcatEmbeddedServletContainerFactory回调接口定制嵌入式 Tomcat 容器，则应该使用TomcatServletWebServerFactory。 特定于 Servlet 的服务器属性许多server.* 属性 ( Servlet 特有的) 已经转移到server.servlet： 旧的属性 新的属性 server.context-parameters.* server.servlet.context-parameters.* server.context-path server.servlet.context-path server.jsp.class-name server.servlet.jsp.class-name server.jsp.init-parameters.* server.servlet.jsp.init-parameters.* server.jsp.registered server.servlet.jsp.registered server.servlet-path server.servlet.path Web Starter 作为传递依赖以前有几个 Spring Boot starter 是依赖于 Spring MVC 而传递的spring-boot-starter-web。在 Spring WebFlux 新的支持下，spring-boot-starter-mustache，spring-boot-starter-freemarker并spring-boot-starter-thymeleaf不再依赖它。开发者有责任选择和添加spring-boot-starter-web或spring-boot-starter-webflux。 模板引擎Mustache 模板曾经的文件扩展名是.html，现在的扩展名为 .mustache ，与官方规范和大多数 IDE 插件一致。您可以通过更改spring.mustache.suffix配置键来覆盖此新的默认值。 Jackson / JSON 支持在 2.0 中，我们改变了 Jackson 配置的默认值，将 ISO-8601 字符串 写为 JSR-310 日期 。如果你想回到以前的行为，你可以添加spring.jackson.serialization.write-dates-as-timestamps=true到你的配置。 新的spring-boot-starter-json starter 收集了必要的位去读写 JSON。它不仅提供了jackson-databind，而且提供了和 Java8 一起运作的时候相当有用的组件：jackson-datatype-jdk8, jackson-datatype-jsr310 和 jackson-module-parameter-names。如果你曾经手动地依赖这些组件，现在可以依赖这个新的 starter 取代。 Spring MVC 路径匹配默认行为更改我们已决定在 Spring MVC 应用程序中更改后缀路径匹配的默认值（请参阅＃11105）。按照 Spring Framework 中记录的最佳实践，此功能不再默认启用。 如果您的应用程序希望将请求&quot;GET /projects/spring-boot.json&quot;映射到@GetMapping(&quot;/projects/spring-boot&quot;)映射，则此更改会影响您。 有关此更多信息以及如何减轻此更改，请查阅Spring Boot中有关路径匹配和内容协商的参考文档。 Servlet 过滤器Servlet 过滤器的默认调度程序类型现在是DipatcherType.REQUEST; 这使 Spring Boot 的默认值与 Servlet 规范的默认值一致。如果您希望将过滤器映射到其他调度程序类型，请使用FilterRegistrationBean注册您的过滤器。 注意：Spring Security 和 Spring Session 过滤器配置 ASYNC, ERROR以及 REQUEST 调度类型。 RestTemplateBuilder该requestFactory(ClientHttpRequestFactory)方法已被新requestFactory(Supplier&lt;ClientHttpRequestFactory&gt; requestFactorySupplier)方法所取代。Supplier允许构建器生成的每个模板使用它自己的请求工厂，从而避免共享工厂可能导致的副作用。见＃11255。 WebJars 定位器Spring Boot 1.x 使用并提供依赖关系管理org.webjars:webjars-locator。webjars-locator是一个“命名不佳的库……包装webjars-locator-core项目”。org.webjars:webjars-locator应该更新依赖项来org.webjars:webjars-locator-core代替使用。 SecuritySpring Boot 2 极大地简化了默认的安全配置，并使添加定制安全变得简单。Spring Boot 现在具有一种行为，只要您添加自己的 WebSecurityConfigurerAdapter 就会退出，而不是进行多种与安全性相关的自动配置。 如果您使用以下任何属性，则会受到影响： 123456789101112131415security.basic.authorize-modesecurity.basic.enabledsecurity.basic.pathsecurity.basic.realmsecurity.enable-csrfsecurity.headers.cachesecurity.headers.content-security-policysecurity.headers.content-security-policy-modesecurity.headers.content-typesecurity.headers.framesecurity.headers.hstssecurity.headers.xsssecurity.ignoredsecurity.require-sslsecurity.sessions 默认安全安全自动配置不再公开选项，并尽可能使用 Spring Security 默认值。一个明显的副作用是使用 Spring Security 的内容协商进行授权（表单登录）。 默认用户默认情况下，Spring Boot 使用生成的密码配置单个用户。用户可以使用 spring.security.user.* 属性进行配置。要进一步定制用户或添加其他用户，您将不得不公开一个UserDetailsServicebean。 AuthenticationManager Bean如果您想将 Spring Security AuthenticationManager作为 bean 公开，请覆盖authenticationManagerBean您的方法WebSecurityConfigurerAdapter并为其添加注释@Bean。 OAuth2从功能的 Spring Security OAuth 项目 迁移到核心 Spring Security。不再为依赖关系提供依赖管理，Spring Boot 2 通过 Spring Security 5 提供 OAuth 2.0 客户端支持。 如果您依赖尚未迁移的 Spring Security OAuth 功能，则需要在其他 jar 上添加依赖项，请查看文档以获取更多详细信息。我们还继续支持 Spring Boot 1.5，以便旧版应用程序可以继续使用它，直到提供升级路径。 执行器安全执行器不再有单独的安全自动配置（management.security.*属性消失）。sensitive每个端点的标志也没有在安全配置中变得更加明确。如果您依赖于此行为，则需要创建或调整您的安全配置，以保护您选择角色的端点。 例如，假设以下配置： 123endpoints.flyway.sensitive=falseendpoints.info.sensitive=truemanagement.security.roles=MY_ADMIN 12345http .authorizeRequests() .requestMatchers(EndpointRequest.to(\"health\", \"flyway\")).permitAll() .requestMatchers(EndpointRequest.toAnyEndpoint()).hasRole(\"MY_ADMIN\") ... 需要注意的是在2.x，health和info在默认情况下启用（与health默认情况下不显示其细节）。为了与这些新的默认值一致，health已被添加到第一个匹配器。 使用 SQL 数据库配置数据源默认连接池已从 Tomcat 切换到 HikariCP。如果您过去spring.datasource.type在基于 Tomcat 的应用程序中强制使用 Hikari，现在可以删除重写。 特别是，如果你有这样的设置： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;&lt;/dependency&gt; 现在可以这样修改： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; WARN 消息隐含的’打开在视图’从现在起，未明确启用的应用程序spring.jpa.open-in-view将在启动过程中收到警告消息。虽然这种行为是一种友好的默认行为，但如果您没有完全意识到为您做了什么，这可能会导致问题。此消息可确保您了解可在查看呈现期间执行数据库查询。如果你没有问题，你可以明确地配置这个属性来消除警告信息。 JPA 和 Spring Data在 Spring Boot 1.x 中，一些用户正在扩展HibernateJpaAutoConfiguration以将高级自定义应用于自动配置EntityManagerFactory。为了防止发生这种错误的用例，Spring Boot 2 中不再可能扩展它。 为了支持这些用例，现在可以定义一个HibernatePropertiesCustomizerbean，它可以完全控制 Hibernate 属性，包括注册在上下文中声明为 bean 的 Hibernate 拦截器的能力。 FlywayFlyway 配置键被移动到spring命名空间（即spring.flyway） 升级到 Spring Boot 2 将会将 Flyway 升级3.x到5.x。为确保模式升级顺利进行，请按照以下说明操作： 首先将您的1.5.xSpring Boot 应用程序升级到 Flyway 4，请参阅Maven和Gradle的说明。 一旦您的架构升级到了 Flyway 4，升级到 Spring Boot 2 并再次运行迁移以将您的应用程序移植到 Flyway 5。 LiquibaseLiquibase 配置键被移动到spring命名空间（即spring.liquibase） 数据库初始化基本DataSource初始化现在仅针对嵌入式数据源启用，并将在您使用生产数据库时立即关闭。新的spring.datasource.initialization-mode（替换spring.datasource.initialize）提供更多的控制。 更新默认的’创建 - 删除’处理spring.jpa.hibernate.ddl-auto 属性默认为只有在没有使用 Liquibase 或 Flyway 等模式管理器时才使用嵌入式数据库进行创建。一旦检测到模式管理器，默认更改为 none。 整合 NoSQLRedis现在使用的是 Lettuce 而不是 Jedis 作为 Redis 驱动程序spring-boot-starter-redis。如果您使用更高级别的Spring Data 构造，则应该发现变化是透明的。我们仍然支持 Jedis，如果您愿意，通过排除 io.lettuce：lettuce-core并添加 redis.clients：jedis，则可以自由切换依赖项。 ElasticsearchElasticsearch 已经升级到 6.0+。与 Elastic 宣布嵌入式 Elasticsearch 不再受支持一致，自动配置NodeClient已被删除。TransportClient可以通过使用spring.data.elasticsearch.cluster-nodes提供要连接的一个或多个节点的地址来自动配置。 高速缓存用于缓存的专用 Hazelcast 自动配置。 无法自动配置常规HazelcastInstance和专用HazelcastInstance缓存。因此，该spring.cache.hazelcast.config属性已不再可用。 批量在启动时执行批处理作业的 CommandLineRunner 的顺序为 0。 测试Mockito 1.xMockito 1.x 不再支持@MockBean和@SpyBean。如果你不用spring-boot-starter-test来管理你的依赖关系，你应该升级到 Mockito 2.x. Spring Boot ActuatorSpring Boot 2 为 Actuator 带来了重要变化，无论是内部还是面向用户，请查阅参考指南中的更新部分和新的Actuator API文档。 您应该期望编程模型，配置密钥和某些端点的响应格式发生变化。Actuator 现在在 Spring MVC，Spring WebFlux 和Jersey 上得到本地支持。 构建Actuator 的代码分为两个模块：现有的spring-boot-actuator和新的spring-boot-actuator-autoconfigure。如果您使用原始模块（spring-boot-actuator）导入 actuator，请考虑使用spring-boot-starter-actuator启动器替代它。 Keys 的配置结构Endpoints 基础配置 key 已经统一： 旧的属性 新的属性 endpoints.&lt;id&gt;.* management.endpoint.&lt;id&gt;.* endpoints.cors.* management.endpoints.web.cors.* endpoints.jmx.* management.endpoints.jmx.* management.address management.server.address management.context-path management.server.servlet.context-path management.ssl.* management.server.ssl.* management.port management.server.port 基本路径所有 endpoints 默认情况下都已移至 /actuator。 我们修改了 management.server.servlet.context-path 的含义：它现在是 server.servlet.context-path 的端点管理的等价替代（只有在设置了 management.server.port 时才有效）。另外，您还可以使用新的单独属性 management.endpoints.web.base-path 为管理端点设置基本路径。 例如，如果你设置management.server.servlet.context-path=/management和management.endpoints.web.base-path=/application，你就可以在下面的路径到达终点健康：/management/application/health。 如果你想恢复 1.x 的行为（即具有/health代替/actuator/health），设置以下属性： 1management.endpoints.web.base-path=/ 审计事件 API 更改AuditEventRepository 现在有一个包含所有可选参数的单一方法。 Endpoints要通过 HTTP 使执行器端点可用，它需要同时启用和公开。默认： 无论您的应用程序中是否存在和配置 Spring Security，只有端点/health和/info端点都是暴露的。 所有端点，但/shutdown已启用。 您可以按如下方式公开所有端点： 1management.endpoints.web.exposure.include=* 您可以通过以下方式显式启用/shutdown端点： 1management.endpoint.shutdown.enabled=true 要公开所有（已启用）网络端点除env端点之外： 12management.endpoints.web.exposure.include=*management.endpoints.web.exposure.exclude=env Endpoint changes 1.x 端点 2.0 端点（改变） /actuator 不再可用。 但是，在 management.endpoints.web.base-path 的根目录中有一个映射，它提供了到所有暴露端点的链接。 /auditevents 该after参数不再需要 /autoconfig 重命名为 /conditions /docs 不再可用 /health 现在有一个 management.endpoint.health.show-details 选项 never, always, when-authenticated，而不是依靠 sensitive 标志来确定 health 端点是否必须显示全部细节。 默认情况下，/actuator/health公开并且不显示细节。 /trace 重命名为 /httptrace 端点属性已更改如下： endpoints.&lt;id&gt;.enabled 已经转移到了 management.endpoint.&lt;id&gt;.enabled endpoints.&lt;id&gt;.id 没有替换（端点的 ID 不再可配置） endpoints.&lt;id&gt;.sensitive没有替代品（请参见执行器安全） endpoints.&lt;id&gt;.path 已经转移到了 management.endpoints.web.path-mapping.&lt;id&gt; 端点格式/actuator/mappings 端点大改变JSON 格式已经更改为现在正确地包含有关上下文层次结构，多个DispatcherServlets，部署的 Servlet 和 Servlet 过滤器的信息。详情请参阅＃9979。 Actuator API 文档的相关部分提供了一个示例文档。 /actuator/httptrace 端点大改变响应的结构已经过改进，以反映端点关注跟踪 HTTP 请求 - 响应交换的情况。 迁移自定义端点如果您有自定义执行器端点，请查看专用博客文章。该团队还撰写了一个 wiki 页面，介绍如何将现有的执行器端点迁移到新的基础架构。 MetricsSpring Boot 自己的指标已被支持取代，包括自动配置，用于 icrometer 和 dimensional 指标。 设置 icrometer如果您的 Spring Boot 2.0 应用程序已依赖于 Actuator，则 icrometer 已在此处并自动配置。如果您希望将度量标准导出到 Prometheus，Atlas 或 Datadog 等外部注册表，Micrometer 将为许多注册表提供依赖关系; 您可以使用spring.metrics.*属性配置您的应用程序以导出到特定的注册表。 迁移定制计数器/量表您可以通过以下方式创建各种指标，而不是在应用程序代码中注入CounterService或GaugeService的实例： 注入MeterRegistry和调用方法。 直接调用静态方法Counter featureCounter = Metrics.counter(&quot;feature&quot;);。 开发者工具热拔插由于 Spring Loaded 项目被搁置，它在 Spring Boot 的支持已被删除。我们建议使用 Devtools。 Devtools 远程调试隧道已经从 Devtools 中删除了对通过 HTTP 进行隧道远程调试的支持。 已删除的功能以下功能不再可用： CRaSH 支持 Spring Mobile 的自动配置和依赖关系管理。 Spring Social 的自动配置和依赖关系管理。 依赖关系管理commons-digester。 依赖版本以下库的最低支持版本已更改： Elasticsearch 5.6 Gradle 4 Hibernate 5.2 Jetty 9.4 Spring Framework 5 Spring Security 5 Tomcat 8.5 参考资料https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Migration-Guide 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"《深入理解 Java 内存模型》读书笔记","date":"2018-02-27T16:00:00.000Z","path":"2018/02/28/Java-Memory-Model/","text":"前提《深入理解 Java 内存模型》程晓明著，该书在以前看过一遍，现在学的东西越多，感觉那块越重要，于是又再细看一遍，于是便有了下面的读书笔记总结。全书页数虽不多，内容讲得挺深的。细看的话，也是挺花时间的，看完收获绝对挺大的。也建议 Java 开发者都去看看。里面主要有 Java 内存模型的基础、重排序、顺序一致性、Volatile 关键字、锁、final。本文参考书中内容。 关注我如果你想查看这本书可以关注我的公众号: zhisheng ，然后里面回复关键字 JMM 可以查看我分享的百度云链接。 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/02/28/Java-Memory-Model/ 基础并发编程的模型分类在并发编程需要处理的两个关键问题是：线程之间如何通信 和 线程之间如何同步。 通信通信 是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存 和 消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。 在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步同步 是指程序用于控制不同线程之间操作发生相对顺序的机制。 在共享内存的并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。 Java 内存模型的抽象在 Java 中，所有实例域、静态域 和 数组元素存储在堆内存中，堆内存在线程之间共享。局部变量、方法定义参数 和 异常处理器参数 不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java 线程之间的通信由 Java 内存模型（JMM）控制。JMM 决定了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程与主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每一个线程都有一个自己私有的本地内存，本地内存中存储了该变量以读／写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。 JMM 抽象示意图： 从上图来看，如果线程 A 和线程 B 要通信的话，要如下两个步骤： 1、线程 A 需要将本地内存 A 中的共享变量副本刷新到主内存去 2、线程 B 去主内存读取线程 A 之前已更新过的共享变量 步骤示意图： 举个例子： 本地内存 A 和 B 有主内存共享变量 X 的副本。假设一开始时，这三个内存中 X 的值都是 0。线程 A 正执行时，把更新后的 X 值（假设为 1）临时存放在自己的本地内存 A 中。当线程 A 和 B 需要通信时，线程 A 首先会把自己本地内存 A 中修改后的 X 值刷新到主内存去，此时主内存中的 X 值变为了 1。随后，线程 B 到主内存中读取线程 A 更新后的共享变量 X 的值，此时线程 B 的本地内存的 X 值也变成了 1。 整体来看，这两个步骤实质上是线程 A 再向线程 B 发送消息，而这个通信过程必须经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 Java 程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三类： 1、编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 2、指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 3、内存系统的重排序。由于处理器使用缓存和读／写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上面的这些重排序都可能导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！ 举个例子： 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0。具体的原因如下图所示： 处理器 A 和 B 同时把共享变量写入在写缓冲区中（A1、B1），然后再从内存中读取另一个共享变量（A2、B2），最后才把自己写缓冲区中保存的脏数据刷新到内存中（A3、B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。 从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1 -&gt; A2，但内存操作实际发生的顺序却是：A2 -&gt; A1。此时，处理器 A 的内存操作顺序被重排序了。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。 内存屏障指令为了保证内存可见性，Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM 把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保 Load1 数据的装载，之前于 Load2 及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保 Store1 数据对其他处理器可见（刷新到内存），之前于 Store2 及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保 Load1 数据装载，之前于 Store2 及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保 Store1 数据对其他处理器变得可见（指刷新到内存），之前于 Load2 及所有后续装载指令的装载。StoreLoadBarriers 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 happens-beforeJSR-133 内存模型使用 happens-before 的概念来阐述操作之间的内存可见性。在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下： 程序顺序规则：一个线程中的每个操作，happens-before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器的解锁，happens-before 于随后对这个监视器的加锁。 volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。 传递性：如果 A happens-before B，且 B happens-before C，那么 A happens-before C。 注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。 happens-before 与 JMM 的关系如下图所示： 如上图所示，一个 happens-before 规则对应于一个或多个编译器和处理器重排序规则。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1; b = a; 写一个变量之后，再读这个位置。 写后写 a = 1; a = 2; 写一个变量之后，再写这个变量。 读后写 a = b; b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial 语义as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是如果操作之间没有数据依赖关系，这些操作就可能被编译器和处理器重排序。 举个例子： 123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序： 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens-before 的定义我们可以看出，JMM 同样遵从这一目标。 重排序对多线程的影响举例： 123456789101112131415class Demo &#123; int a = 0; boolean flag = false; public void write() &#123; a = 1; //1 flag = true; //2 &#125; public void read() &#123; if(flag) &#123; //3 int i = a * a; //4 &#125; &#125;&#125; 由于操作 1 和 2 没有数据依赖关系，编译器和处理器可以对这两个操作重排序；操作 3 和操作 4 没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 1、当操作 1 和操作 2 重排序时，可能会产生什么效果？ 如上图所示，操作 1 和操作 2 做了重排序。程序执行时，线程 A 首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！ 2、当操作 3 和操作 4 重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。 在程序中，操作 3 和操作 4 存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a * a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作 3 的条件判断为真时，就把该计算结果写入变量 i 中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性顺序一致性内存模型顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下： 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。 举个例子： 假设有两个线程 A 和 B 并发执行。其中 A 线程有三个操作，它们在程序中的顺序是：A1 -&gt; A2 -&gt; A3。B 线程也有三个操作，它们在程序中的顺序是：B1 -&gt; B2 -&gt; B3。 假设这两个线程使用监视器锁来正确同步：A 线程的三个操作执行后释放监视器锁，随后 B 线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图所示： 现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图： 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程 A 和 B 看到的执行顺序都是：B1 -&gt; A1 -&gt; A2 -&gt; B2 -&gt; A3 -&gt; B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在 JMM 中就没有这个保证。未同步程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 同步程序的顺序一致性效果下面我们对前面的示例程序用锁来同步，看看正确同步的程序如何具有顺序一致性。 请看下面的示例代码： 123456789101112131415class demo &#123; int a = 0; boolean flag = false; public synchronized void write() &#123; //获取锁 a = 1; flag = true; &#125; //释放锁 public synchronized void read() &#123; //获取锁 if(flag) &#123; int i = a; &#125; &#125; //释放锁&#125; 上面示例代码中，假设 A 线程执行 write() 方法后，B 线程执行 reade() 方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图： 在顺序一致性模型中，所有操作完全按程序的顺序执行。而在 JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM 会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程 A 在临界区内做了重排序，但由于监视器的互斥执行的特性，这里的线程 B 根本无法“观察”到线程 A 在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 从这里我们可以看到 JMM 在具体实现上的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能的为编译器和处理器的优化打开方便之门。 未同步程序的执行特性未同步程序在 JMM 中的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有下面几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。 JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子 。 第三个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过总线事务来完成的。总线事务包括读事务和写事务。读事务从内存传送数据到处理器，写事务从处理器传递数据到内存，每个事务会读／写内存中一个或多个物理上连续的字。总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I／O 设备执行内存的读／写。 总线的工作机制： 如上图所示，假设处理器 A、B、和 C 同时向总线发起总线事务，这时总线仲裁会对竞争作出裁决，假设总线在仲裁后判定处理器 A 在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器 A 继续它的总线事务，而其它两个处理器则要等待处理器 A 的总线事务完成后才能开始再次执行内存访问。假设在处理器 A 执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器 D 向总线发起了总线事务，此时处理器 D 的这个请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java 语言规范鼓励但不强求 JVM 对 64 位的 long 型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。 当单个内存操作不具有原子性，将可能会产生意想不到后果。请看下面示意图： 如上图所示，假设处理器 A 写一个 long 型变量，同时处理器 B 要读这个 long 型变量。处理器 A 中 64 位的写操作被拆分为两个 32 位的写操作，且这两个 32 位的写操作被分配到不同的写事务中执行。同时处理器 B 中 64 位的读操作被分配到单个的读事务中执行。当处理器 A 和 B 按上图的时序来执行时，处理器 B 将看到仅仅被处理器 A “写了一半“的无效值。 注意，在 JSR -133 之前的旧内存模型中，一个 64 位 long/ double 型变量的读/写操作可以被拆分为两个 32 位的读/写操作来执行。从 JSR -133 内存模型开始（即从JDK5开始），仅仅只允许把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行，任意的读操作在JSR -133中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。 VolatileVolatile 特性举个例子： 123456789101112131415public class VolatileTest &#123; volatile long a = 1L; // 使用 volatile 声明 64 位的 long 型 public void set(long l) &#123; a = l; //单个 volatile 变量的写 &#125; public long get() &#123; return a; //单个 volatile 变量的读 &#125; public void getAndIncreament() &#123; a++; // 复合（多个） volatile 变量的读 /写 &#125;&#125; 假设有多个线程分别调用上面程序的三个方法，这个程序在语义上和下面程序等价： 1234567891011121314151617public class VolatileTest &#123; long a = 1L; // 64 位的 long 型普通变量 public synchronized void set(long l) &#123; //对单个普通变量的写用同一个锁同步 a = l; &#125; public synchronized long get() &#123; //对单个普通变量的读用同一个锁同步 return a; &#125; public void getAndIncreament() &#123; //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 &#125;&#125; 如上面示例程序所示，对一个 volatile 变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个锁来同步，它们之间的执行效果相同。 锁的 happens-before 规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 锁的语义决定了临界区代码的执行具有原子性。这意味着即使是 64 位的 long 型和 double 型变量，只要它是 volatile变量，对该变量的读写就将具有原子性。如果是多个 volatile 操作或类似于 volatile++ 这种复合操作，这些操作整体上不具有原子性。 简而言之，volatile 变量自身具有下列特性： 可见性。对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 原子性：对任意单个 volatile 变量的读/写具有原子性，但类似于 volatile++ 这种复合操作不具有原子性。 volatile 写-读的内存定义 当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个 volatile 变量时，JMM 会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 假设上面的程序 flag 变量用 volatile 修饰 volatile 内存语义的实现下面是 JMM 针对编译器制定的 volatile 重排序规则表： 为了实现 volatile 的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 下面是基于保守策略的 JMM 内存屏障插入策略： 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。 下面是保守策略下，volatile 写操作 插入内存屏障后生成的指令序列示意图： 下面是在保守策略下，volatile 读操作 插入内存屏障后生成的指令序列示意图： 上述 volatile 写操作和 volatile 读操作的内存屏障插入策略非常保守。在实际执行时，只要不改变 volatile 写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 锁锁释放和获取的内存语义当线程释放锁时，JMM 会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM 会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。 锁内存语义的实现借助 ReentrantLock 来讲解，PS： 后面专门讲下这块（ReentrantLock、Synchronized、公平锁、非公平锁、AQS等），可以看看大明哥的博客：http://cmsblogs.com/?p=2210 concurrent 包的实现如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为 volatile； 然后，使用 CAS 的原子条件更新来实现线程之间的同步； 同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic 包中的类），这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下： final对于 final 域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。 写 final 域的重排序规则写 final 域的重排序规则禁止把 final 域的写重排序到构造函数之外。这个规则的实现包含下面2个方面： JMM 禁止编译器把 final 域的写重排序到构造函数之外。 编译器会在 final 域的写之后，构造函数 return 之前，插入一个 StoreStore 屏障。这个屏障禁止处理器把 final 域的写重排序到构造函数之外。 读 final 域的重排序规则在一个线程中，初次读对象引用与初次读该对象包含的 final 域，JMM 禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读 final 域操作的前面插入一个 LoadLoad 屏障。 final 域是引用类型对于引用类型，写 final 域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 总结JMM，处理器内存模型与顺序一致性内存模型之间的关系JMM 是一个语言级的内存模型，处理器内存模型是硬件级的内存模型，顺序一致性内存模型是一个理论参考模型。下面是语言内存模型，处理器内存模型和顺序一致性内存模型的强弱对比示意图： JMM 的设计示意图 JMM 的内存可见性保证Java 程序的内存可见性保证按程序类型可以分为下列三类： 1.单线程程序。单线程程序不会出现内存可见性问题。编译器，runtime 和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。 2.正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同）。这是 JMM 关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。 3.未同步/未正确同步的多线程程序。JMM 为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false）。 下图展示了这三类程序在 JMM 中与在顺序一致性内存模型中的执行结果的异同：","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"JMM","slug":"JMM","permalink":"http://www.54tianzhisheng.cn/tags/JMM/"}]},{"title":"RocketMQ系列文章（三）：RocketMQ 简单的消息示例","date":"2018-02-06T16:00:00.000Z","path":"2018/02/07/rocketmq-example/","text":"使用 RocketMQ 以三种方式发送消息：可靠的同步，可靠的异步和单向传输。 SpringBoot 系列文章 可靠的同步传输应用：可靠的同步传输广泛应用于重要通知消息，短信通知，短信营销系统等。 12345678910111213141516171819202122public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. SendResult sendResult = producer.send(msg); System.out.printf(\"%s%n\", sendResult); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 可靠的异步传输应用：异步传输一般用于响应时间敏感的业务场景。 12345678910111213141516171819202122232425262728293031public class AsyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); //Launch the instance. producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\", \"TagA\", \"OrderID188\", \"Hello world\".getBytes(RemotingHelper.DEFAULT_CHARSET)); producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.printf(\"%-10d OK %s %n\", index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; System.out.printf(\"%-10d Exception %s %n\", index, e); e.printStackTrace(); &#125; &#125;); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 单向传输应用：单向传输用于需要中等可靠性的情况，例如日志收集。 123456789101112131415161718192021public class OnewayProducer &#123; public static void main(String[] args) throws Exception&#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. producer.sendOneway(msg); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 关注我 总结本文是 RocketMQ 的三种发送消息的方式。 转发请注明地址：http://www.54tianzhisheng.cn/2018/02/07/rocketmq-example/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"}]},{"title":"Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控","date":"2018-02-06T16:00:00.000Z","path":"2018/02/07/SpringBoot-RocketMQ/","text":"前提通过前面两篇文章可以简单的了解 RocketMQ 和 安装 RocketMQ ，今天就将 SpringBoot 和 RocketMQ 整合起来使用。 SpringBoot 系列文章 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 3、SpringBoot ActiveMQ 整合使用 4、Kafka 安装及快速入门 5、SpringBoot RabbitMQ 整合进阶版 6、RocketMQ 初探 7、RocketMQ 安装及快速入门 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/02/07/SpringBoot-RocketMQ/ 创建项目在 IDEA 创建一个 SpringBoot 项目，项目结构如下： pom 文件引入 RocketMQ 的一些相关依赖，最后的 pom 文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;rocketmq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;rocketmq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot RocketMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-common&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 配置文件application.properties 中如下： 123456# 消费者的组名apache.rocketmq.consumer.PushConsumer=PushConsumer# 生产者的组名apache.rocketmq.producer.producerGroup=Producer# NameServer地址apache.rocketmq.namesrvAddr=localhost:9876 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.zhisheng.rocketmq.client;import org.apache.rocketmq.client.producer.DefaultMQProducer;import org.apache.rocketmq.common.message.Message;import org.apache.rocketmq.remoting.common.RemotingHelper;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import org.springframework.util.StopWatch;import javax.annotation.PostConstruct;/** * Created by zhisheng_tian on 2018/2/6 */@Componentpublic class RocketMQClient &#123; /** * 生产者的组名 */ @Value(\"$&#123;apache.rocketmq.producer.producerGroup&#125;\") private String producerGroup; /** * NameServer 地址 */ @Value(\"$&#123;apache.rocketmq.namesrvAddr&#125;\") private String namesrvAddr; @PostConstruct public void defaultMQProducer() &#123; //生产者的组名 DefaultMQProducer producer = new DefaultMQProducer(producerGroup); //指定NameServer地址，多个地址以 ; 隔开 producer.setNamesrvAddr(namesrvAddr); try &#123; /** * Producer对象在使用之前必须要调用start初始化，初始化一次即可 * 注意：切记不可以在每次发送消息时，都调用start方法 */ producer.start(); //创建一个消息实例，包含 topic、tag 和 消息体 //如下：topic 为 \"TopicTest\"，tag 为 \"push\" Message message = new Message(\"TopicTest\", \"push\", \"发送消息----zhisheng-----\".getBytes(RemotingHelper.DEFAULT_CHARSET)); StopWatch stop = new StopWatch(); stop.start(); for (int i = 0; i &lt; 10000; i++) &#123; SendResult result = producer.send(message); System.out.println(\"发送响应：MsgId:\" + result.getMsgId() + \"，发送状态:\" + result.getSendStatus()); &#125; stop.stop(); System.out.println(\"----------------发送一万条消息耗时：\" + stop.getTotalTimeMillis()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; producer.shutdown(); &#125; &#125;&#125; 消费者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.zhisheng.rocketmq.server;import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;import org.apache.rocketmq.common.consumer.ConsumeFromWhere;import org.apache.rocketmq.common.message.MessageExt;import org.apache.rocketmq.remoting.common.RemotingHelper;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;/** * Created by zhisheng_tian on 2018/2/6 */@Componentpublic class RocketMQServer &#123; /** * 消费者的组名 */ @Value(\"$&#123;apache.rocketmq.consumer.PushConsumer&#125;\") private String consumerGroup; /** * NameServer 地址 */ @Value(\"$&#123;apache.rocketmq.namesrvAddr&#125;\") private String namesrvAddr; @PostConstruct public void defaultMQPushConsumer() &#123; //消费者的组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(consumerGroup); //指定NameServer地址，多个地址以 ; 隔开 consumer.setNamesrvAddr(namesrvAddr); try &#123; //订阅PushTopic下Tag为push的消息 consumer.subscribe(\"TopicTest\", \"push\"); //设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费 //如果非第一次启动，那么按照上次消费的位置继续消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.registerMessageListener((MessageListenerConcurrently) (list, context) -&gt; &#123; try &#123; for (MessageExt messageExt : list) &#123; System.out.println(\"messageExt: \" + messageExt);//输出消息内容 String messageBody = new String(messageExt.getBody(), RemotingHelper.DEFAULT_CHARSET); System.out.println(\"消费响应：msgId : \" + messageExt.getMsgId() + \", msgBody : \" + messageBody);//输出消息内容 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return ConsumeConcurrentlyStatus.RECONSUME_LATER; //稍后再试 &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; //消费成功 &#125;); consumer.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 启动类123456789101112package com.zhisheng.rocketmq;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class RocketmqApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RocketmqApplication.class, args); &#125;&#125; RocketMQ代码已经都写好了，接下来我们需要将与 RocketMQ 有关的启动起来。 启动 Name Server在前面文章中已经写过怎么启动，http://www.54tianzhisheng.cn/2018/02/06/RocketMQ-install/#%E5%90%AF%E5%8A%A8-NameServer 进入到目录 ： 1cd distribution/target/apache-rocketmq 启动： 123nohup sh bin/mqnamesrv &amp;tail -f ~/logs/rocketmqlogs/namesrv.log //通过日志查看是否启动成功 启动 Broker123nohup sh bin/mqbroker -n localhost:9876 &amp;tail -f ~/logs/rocketmqlogs/broker.log //通过日志查看是否启动成功 然后运行启动类，运行效果如下： 监控RocketMQ有一个对其扩展的开源项目 ocketmq-console ，如今也提交给了 Apache ，地址在：https://github.com/apache/rocketmq-externals/tree/master/rocketmq-console ，官方也给出了其支持的功能的中文文档：https://github.com/apache/rocketmq-externals/blob/master/rocketmq-console/doc/1_0_0/UserGuide_CN.md ， 那么该如何安装？ Docker 安装1、获取 Docker 镜像 1docker pull styletang/rocketmq-console-ng 2、运行，注意将你自己的 NameServer 地址替换下面的 127.0.0.1 1docker run -e &quot;JAVA_OPTS=-Drocketmq.namesrv.addr=127.0.0.1:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; -p 8080:8080 -t styletang/rocketmq-console-ng 非 Docker 安装我们 git clone 一份代码到本地： 123git clone https://github.com/apache/rocketmq-externals.gitcd rocketmq-externals/rocketmq-console/ 需要 jdk 1.7 以上。 执行以下命令： 1mvn spring-boot:run 或者 123mvn clean package -Dmaven.test.skip=truejava -jar target/rocketmq-console-ng-1.0.0.jar 注意： 1、如果你下载依赖缓慢，你可以重新设置 maven 的 mirror 为阿里云的镜像 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 2、如果你使用的 RocketMQ 版本小于 3.5.8，如果您使用 rocketmq &lt; 3.5.8，请在启动 rocketmq-console-ng 时添加 -Dcom.rocketmq.sendMessageWithVIPChannel = false（或者您可以在 ops 页面中更改它） 3、更改 resource / application.properties 中的 rocketmq.config.namesrvAddr（或者可以在ops页面中更改它） 错误解决方法1、Docker 启动项目报错 org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;null&gt; failed 将 Docker 启动命令改成如下以后： 1docker run -e &quot;JAVA_OPTS=-Drocketmq.config.namesrvAddr=127.0.0.1:9876 -Drocketmq.config.isVIPChannel=false&quot; -p 8080:8080 -t styletang/rocketmq-console-ng 报错信息改变了，新的报错信息如下： 123ERROR op=global_exception_handler_print_errororg.apache.rocketmq.console.exception.ServiceException: This date have&apos;t data! 看到网上有人也遇到这个问题，他们都通过自己的方式解决了，但是方法我都试了，不适合我。不得不说，阿里，你能再用心点吗？既然把 RocketMQ 捐给 Apache 了，这些文档啥的都必须更新啊，不要还滞后着呢，不然少不了被吐槽！ 搞了很久这种方法没成功，暂时放弃！mmp 2、非 Docker 安装，只好把源码编译打包了。 1) 注意需要修改如下图中的配置： 1234rocketmq.config.namesrvAddr=localhost:9876 //注意替换你自己的ip#如果你 rocketmq 版本小于 3.5.8 才需设置 `rocketmq.config.isVIPChannel` 为 false，默认是 true, 这个可以在源码中可以看到的rocketmq.config.isVIPChannel= 2) 执行以下命令： 1mvn clean package -Dmaven.test.skip=true 编译成功： 可以看到已经打好了 jar 包： 运行： 1java -jar rocketmq-console-ng-1.0.0.jar 成功，不报错了，开心😄，访问 http://localhost:8080/ 整个监控大概就是这些了。 然后我运行之前的 SpringBoot 整合项目，查看监控信息如下： 总结整篇文章讲述了 SpringBoot 与 RocketMQ 整合和 RocketMQ 监控平台的搭建。 参考文章1、http://www.ymq.io/2018/02/02/spring-boot-rocketmq-example/#%E6%96%B0%E5%8A%A0%E9%A1%B9%E7%9B%AE 2、GitHub 官方 README","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"RocketMQ系列文章（二）：RocketMQ 安装及快速入门","date":"2018-02-05T16:00:00.000Z","path":"2018/02/06/RocketMQ-install/","text":"如果你对 RocketMQ 还没了解，建议先看下上一篇文章：RocketMQ 初探 SpringBoot 系列文章 安装条件 64位操作系统，建议使用 Linux / Unix / Mac; 64位JDK 1.8+; Maven 3.2.x 下载和构建从 https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.2.0/rocketmq-all-4.2.0-source-release.zip 下载 4.2.0 的源码版本，执行以下命令来解压4.2.0源码版本并构建二进制文件。 12345unzip rocketmq-all-4.2.0-source-release.zipcd rocketmq-all-4.2.0/mvn -Prelease-all -DskipTests clean install -U 构建成功如下： 进入到目录 ： 1cd distribution/target/apache-rocketmq 启动 NameServer123nohup sh bin/mqnamesrv &amp;tail -f ~/logs/rocketmqlogs/namesrv.log 结果如下就代表启动成功了： 启动 Broker123nohup sh bin/mqbroker -n localhost:9876 &amp;tail -f ~/logs/rocketmqlogs/broker.log 结果如下就代表启动成功了：从日志中可以看到 broker 注册到了 nameserver 上了（localhost:9876） 发送和接收消息在发送/接收消息之前，我们需要告诉客户名称服务器的位置。RocketMQ 提供了多种方法来实现这一点。为了简单起见，我们使用环境变量NAMESRV_ADDR 发送消息123export NAMESRV_ADDR=localhost:9876sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 接收消息1sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 关闭服务器123sh bin/mqshutdown broker //停止 brokersh bin/mqshutdown namesrv //停止 nameserver 关闭成功后如下： 常用命令上面几个启动和关闭 name server 和 broker 的就不再说了， 查看集群情况 ./mqadmin clusterList -n 127.0.0.1:9876 查看 broker 状态 ./mqadmin brokerStatus -n 127.0.0.1:9876 -b 172.20.1.138:10911 (注意换成你的 broker 地址) 查看 topic 列表 ./mqadmin topicList -n 127.0.0.1:9876 查看 topic 状态 ./mqadmin topicStatus -n 127.0.0.1:9876 -t MyTopic (换成你想查询的 topic) 查看 topic 路由 ./mqadmin topicRoute -n 127.0.0.1:9876 -t MyTopic 关注我 总结本文是 RocketMQ 的安装及快速入门案例。 转发请注明地址：http://www.54tianzhisheng.cn/2018/02/06/RocketMQ-install/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"}]},{"title":"RocketMQ系列文章（一）：RocketMQ 初探","date":"2018-02-04T16:00:00.000Z","path":"2018/02/05/RocketMQ/","text":"介绍RocketMQ 是阿里开源的消息中间件，前不久捐献给了 Apache 。正如官网介绍如下：它是一个开源的分布式消息传递和流式数据平台。 SpringBoot 系列文章 特点如下： 产品发展历史大约经历了三个主要版本迭代 一、Metaq（Metamorphosis）1.x 由开源社区killme2008维护，开源社区非常活跃。 二、Metaq 2.x 于2012年10月份上线，在淘宝内部被广泛使用。 三、RocketMQ 3.x 基于公司内部开源共建原则，RocketMQ 项目只维护核心功能，且去除了所有其他运行时依赖，核心功能最简化。每个 BU 的个性化需求都在 RocketMQ 项目之上进行深度定制。RocketMQ 向其他 BU 提供的仅仅是 Jar 包，例如要定制一个 Broker，那么只需要依赖 rocketmq-broker 这个 jar 包即可，可通过 API 进行交互，如果定制 client，则依赖 rocketmq-client 这个 jar 包，对其提供的 api 进行再封装。 在 RocketMQ 项目基础上衍生的项目如下 com.taobao.metaq v3.0 = RocketMQ + 淘宝个性化需求 为淘宝应用提供消息服务 com.alipay.zpullmsg v1.0 =RocketMQ + 支付宝个性化需求 为支付宝应用提供消息服务 com.alibaba.commonmq v1.0 = Notify + RocketMQ + B2B个性化需求 为 B2B 应用提供消息服务 四、RocketMQ 3.x 目前它的最新版本是 4.2 版本。 概念专业术语Producer 消息生产者，负责产生消息，一般由业务系统负责产生消息。 Consumer 消息消费者，负责消费消息，一般是后台系统负责异步消费。 Push Consumer Consumer 的一种，应用通常向 Consumer 对象注册一个 Listener 接口，一旦收到消息，Consumer 对象立刻回调 Listener 接口方法。 Pull Consumer Consumer 的一种，应用通常主动调用 Consumer 的拉消息方法从 Broker 拉消息，主动权由应用控制。 Producer Group 一类 Producer 的集合名称，这类 Producer 通常发送一类消息，且发送逻辑一致。 Consumer Group 一类 Consumer 的集合名称，这类 Consumer 通常消费一类消息，且消费逻辑一致。 Broker 消息中转角色，负责存储消息，转发消息，一般也称为 Server。在 JMS 规范中称为 Provider。 架构 从这架构图中可以看到它主要由四部分组成：Producer（生产者）、NameServer、Broker、Consumer（消费者）。 Producer生产者支持分布式部署。分布式生产者通过多种负载均衡模式向 Broker 集群发送消息。发送过程支持快速失败并具有低延迟。 NameServer它提供轻量级服务发现和路由，每个 Name Server 记录完整的路由信息，提供相应的读写服务，支持快速存储扩展。主要包括两个功能： 代理管理， NameServer 接受来自 Broker 集群的注册，并提供检测代理是否存在的心跳机制。 路由管理，每个 NameServer 将保存有关代理群集的全部路由信息以及客户端查询的队列信息。 我们知道，RocketMQ客户端（生产者/消费者）将从NameServer查询队列路由信息，但客户端如何找到NameServer地址？ 将NameServer地址列表提供给客户端有四种方法： 编程方式，就像producer.setNamesrvAddr(&quot;ip:port&quot;)。 Java选项，使用rocketmq.namesrv.addr。 环境变量，使用NAMESRV_ADDR。 HTTP 端点。 BrokerBroker 通过提供轻量级的 Topic 和 Queue 机制来照顾消息存储。它们支持 Push 和 Pull 模式，包含容错机制（2个拷贝或者3个拷贝），并且提供了强大的峰值填充和以原始时间顺序累计数千亿条消息的能力。此外，broker 还提供灾难恢复，丰富的指标统计数据和警报机制，而传统的消息传递系统都缺乏这些机制。 如上图：Broker 服务器重要的子模块： 远程处理模块是 broker 的入口，处理来自客户的请求。 Client manager，管理客户（生产者/消费者）并维护消费者的主题订阅。 Store Service，提供简单的 API 来存储或查询物理磁盘中的消息。 HA 服务，提供主代理和从代理之间的数据同步功能。 索引服务，通过指定键为消息建立索引，并提供快速的消息查询。 Consumer消费者也支持 Push 和 Pull 模型中的分布式部署。它还支持群集消费和消息广播。它提供了实时的消息订阅机制，可以满足大多数消费者的需求。 关注我 总结本文是对 RocketMQ 的简单点了解，参考了官网介绍。 转载请注明地址：http://www.54tianzhisheng.cn/2018/02/05/RocketMQ/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"}]},{"title":"Spring Boot系列文章（五）：SpringBoot RabbitMQ 整合进阶版","date":"2018-01-27T16:00:00.000Z","path":"2018/01/28/RabbitMQ/","text":"消息中间件RabbitMQ 是消息中间件的一种, 消息中间件即分布式系统中完成消息的发送和接收的基础软件. 这些软件有很多, 包括 ActiveMQ ( apache 公司的), RocketMQ (阿里巴巴公司的, 现已经转让给 apache), 还有性能极高的 Kafka。 SpringBoot 系列文章 消息中间件的工作过程可以用生产者消费者模型来表示. 即生产者不断的向消息队列发送信息, 而消费者从消息队列中消费信息. 具体过程如下: 从上图可看出, 对于消息队列来说, 生产者,消息队列,消费者 是最重要的三个概念。生产者发消息到消息队列中去, 消费者监听指定的消息队列, 并且当消息队列收到消息之后, 接收消息队列传来的消息, 并且给予相应的处理. 消息队列常用于分布式系统之间互相信息的传递. RabbitMQ 工作原理对于 RabbitMQ 来说, 除了这三个基本模块以外, 还添加了一个模块, 即交换机(Exchange). 它使得生产者和消息队列之间产生了隔离, 生产者将消息发送给交换机,而交换机则根据调度策略把相应的消息转发给对应的消息队列. 那么 RabitMQ 的工作流程如下所示: 说一下交换机: 交换机的主要作用是接收相应的消息并且绑定到指定的队列. 交换机有四种类型, 分别为Direct, topic, headers, Fanout. Direct 是 RabbitMQ 默认的交换机模式,也是最简单的模式.即创建消息队列的时候,指定一个 BindingKey. 当发送者发送消息的时候, 指定对应的 Key. 当 Key 和消息队列的 BindingKey 一致的时候,消息将会被发送到该消息队列中. topic 转发信息主要是依据通配符, 队列和交换机的绑定主要是依据一种模式(通配符+字符串), 而当发送消息的时候, 只有指定的 Key 和该模式相匹配的时候, 消息才会被发送到该消息队列中. headers 也是根据一个规则进行匹配, 在消息队列和交换机绑定的时候会指定一组键值对规则, 而发送消息的时候也会指定一组键值对规则, 当两组键值对规则相匹配的时候, 消息会被发送到匹配的消息队列中. Fanout 是路由广播的形式, 将会把消息发给绑定它的全部队列, 即便设置了 key, 也会被忽略. 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/28/RabbitMQ/ SpringBoot 整合 RabbitMQ（Topic 转发模式）在上一篇文章中，我们也将 SpringBoot 和 RabbitMQ 整合过，不过那是使用 Direct 模式，文章地址是：SpringBoot RabbitMQ 整合使用 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 3、SpringBoot ActiveMQ 整合使用 4、Kafka 安装及快速入门 整合接下来，我要带大家继续整合（Topic 转发模式）： 1、配置文件和 pom.xml 这些还都是一样的，我们不用再修改 2、启动类中创建 Queue 和 Exchange，并把 Queue 按照相应的规则绑定到交换机Queue 上。代码如下图： 1234567891011121314@Beanpublic Queue queue() &#123; return new Queue(\"rpc-queue-zhisheng\");&#125;@Beanpublic TopicExchange exchange() &#123; return new TopicExchange(\"rpc-exchange-zhisheng\");&#125;@Beanpublic Binding binding(Queue queue, TopicExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange).with(\"rpc-zhisheng\");&#125; 这里创建一个 Queue 和 Exchange ，然后绑定。 注意：上面代码中的 with(“rpc-zhisheng”) 这个 “zhisheng” 是 routingkey，RabbitMQ 将会根据这个参数去寻找有没有匹配此规则的队列，如果有，则会把消息发送给它，如果不止有一个，则会把消息分发给所有匹配的队列。 3、消息发送类 1234567891011121314151617181920212223package com.zhisheng.rabbitmq.rpc.client;import org.springframework.amqp.core.TopicExchange;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; @Autowired private TopicExchange exchange; public void send(String message) &#123; rabbitTemplate.convertAndSend(exchange.getName(), \"rpc-zhisheng\", message); &#125;&#125; 这里是发送消息的代码，“rpc-zhisheng” 就是上面我们设置的 routingkey。 4、消息接收端 12345678910111213141516package com.zhisheng.rabbitmq.rpc.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"rpc-queue-zhisheng\") public void receive(String message) &#123; System.out.println(\"--------receive ------- \" + message); &#125;&#125; 5、启动类中注入 发送消息类，然后调用 send 方法 12345678910111213@Autowiredprivate RabbitMQClient client;@PostConstructpublic void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 1000; i++) &#123; client.send(\" zhisheng, --------- send \" + i); &#125; stopWatch.stop(); System.out.println(\"总共耗时：\" + stopWatch.getTotalTimeMillis());&#125; 运行此 SpringBoot 项目，则可以发现结果如下： 这里测试的是匹配一个消息队列的情况，感兴趣的可以测试下匹配多个消息队列的。 SpringBoot 整合 RabbitMQ( Fanout Exchange 形式)Fanout Exchange 形式又叫广播形式。 任何发送到 Fanout Exchange 的消息都会被转发到与该 Exchange 绑定(Binding)的所有 Queue 上。 这种模式需要提前将 Exchange 与 Queue 进行绑定，一个 Exchange 可以绑定多个 Queue，一个 Queue 可以同多个 Exchange 进行绑定 这种模式不需要 RoutingKey 如果接受到消息的 Exchange 没有与任何 Queue 绑定，则消息会被抛弃。 1、消息发送类 12345678910111213141516171819package com.zhisheng.rabbitmq.rpc.client;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; public void send2(String message) &#123; rabbitTemplate.convertAndSend(\"fanout-exchange\", \"\", message); &#125;&#125; 这里可以不设置 routingkey 了。 2、启动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.zhisheng.rabbitmq.rpc;import com.zhisheng.rabbitmq.rpc.client.RabbitMQClient;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.FanoutExchange;import org.springframework.amqp.core.Queue;import org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import javax.annotation.PostConstruct;@SpringBootApplicationpublic class RabbitmqRpcApplication &#123; @Autowired private RabbitMQClient client; @PostConstruct public void init() &#123; client.send2(\"zhisheng ++++++++++ send2 \"); &#125; public static void main(String[] args) &#123; SpringApplication.run(RabbitmqRpcApplication.class, args); &#125; @Bean(name = \"queue\") public Queue queue() &#123; return new Queue(\"rpc.queue\"); &#125; @Bean(name = \"queue2\") public Queue queue2() &#123; return new Queue(\"rpc.queue2\"); &#125; @Bean(name = \"queue3\") public Queue queue3() &#123; return new Queue(\"rpc.queue3\"); &#125; @Bean public FanoutExchange exchange() &#123; return new FanoutExchange(\"fanout-exchange\"); &#125; @Bean public Binding binding(@Qualifier(\"queue\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Binding binding2(@Qualifier(\"queue2\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Binding binding3(@Qualifier(\"queue3\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Jackson2JsonMessageConverter messageConverter() &#123; return new Jackson2JsonMessageConverter(); &#125;&#125; 在启动类中我创建三个 Queue： rpc.queue, rpc.queue2 , rpc.queue3 也创建一个 FanoutExchange，并把这三个 Queue 绑定在同一个交换机 fanout-exchange 上面 注意：这个 fanout-exchange 交换机不知为啥，我自己在应用程序里创建，运行程序会出错，下面讲讲我是怎么解决的。 我是从 RabbitMQ 管理界面直接添加个 exchange 的。 3、消息接收类 123456789101112131415161718192021222324252627package com.zhisheng.rabbitmq.rpc.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"rpc.queue\") public void receive(String message) &#123; System.out.println(\"--------receive ------- \" + message); &#125; @RabbitListener(queues = \"rpc.queue2\") public void receive2(String message) &#123; System.out.println(\"--------receive2 ------- \" + message); &#125; @RabbitListener(queues = \"rpc.queue3\") public void receive3(String message) &#123; System.out.println(\"--------receive3 ------- \" + message); &#125;&#125; 监听每个 Queue，并有一个方法输出对应接收到的消息。 4、运行项目 结果如上，每个队列都打印出自己收到的结果，同时我们看看这三个 Queue 是不是绑定到 Exchange 上呢？ 可以看到三个 Queue 都绑定在 Exchange 上了。 总结RabbitMQ 与 SpringBoot 整合就到这里为止了，后面如果有时间会深度研究 RabbitMQ 的。 还请继续关注我的博客：http://www.54tianzhisheng.cn/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://www.54tianzhisheng.cn/tags/RabbitMQ/"}]},{"title":"Spring Boot系列文章（四）：SpringBoot ActiveMQ 整合使用","date":"2018-01-26T16:00:00.000Z","path":"2018/01/27/SpringBoot-ActiveMQ/","text":"介绍 ActiveMQ它是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现，尽管 JMS 规范出台已经是很久的事情了，但是 JMS 在当今的J2EE应用中间仍然扮演着特殊的地位。—— 摘自百度百科，偷了个懒。 SpringBoot 系列文章 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 安装 ActiveMQ同之前一样，直接在 Docker 里面玩吧。命令也是一行解决： 1docker run -d -p 8161:8161 -p 61616:61616 -e ACTIVEMQ_ADMIN_LOGIN=admin -e ACTIVEMQ_ADMIN_PASSWORD=admin --name activemq webcenter/activemq 简单解释下： 8186: 表示 ActiveMQ 控制台端口号，它和 RabbitMQ 一样都是有控制台的，可以登陆控制台进行操作的 61616 ： 表示 ActiveMQ 所监听的 TCP 端口号，应用程序可通过该端口号与 ActiveMQ 建立 TCP 连接 CTIVEMQ_ADMIN_LOGIN ：登陆控制台的用户名 ACTIVEMQ_ADMIN_PASSWORD ：登陆控制台的密码 执行后，可在浏览器输入 http://localhost:8161/ 查看控制台， 解释下上面图片中控制台这些按钮的基本信息： Home：查看 ActiveMQ 的常见信息 Queues：查看 ActiveMQ 的队列信息 Topics：查看 ActiveMQ 的主题信息 Subscribers：查看主题的订阅者信息 Connections：查看 ActiveMQ 客户端的连接信息 Network：查看 ActiveMQ 的网络信息 Scheduled：查看 ActiveMQ 的定时任务 Send：用于通过表单方式向队列或者主题发送具体的消息 整合IDEA 创建 SpringBoot 项目，因为 SpringBoot 已经内置了对 ActiveMQ 的支持，所以直接引入依赖 spring-boot-starter-activemq 就行。整体项目结构如下： 1、pom.xml 文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;activemq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;activemq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot ActiveMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、配置文件 application.properties 123spring.activemq.broker-url=tcp://localhost:61616spring.activemq.user=adminspring.activemq.password=admin 3、发送消息类 12345678910111213141516package com.zhisheng.activemq.client;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jms.core.JmsTemplate;import org.springframework.stereotype.Component;@Componentpublic class ActiveMQClient &#123; @Autowired private JmsTemplate jmsTemplate; public void send(String message) &#123; jmsTemplate.convertAndSend(\"zhisheng\", message); &#125;&#125; 同样，和 RabbitMQ 类似，不多说了。 4、消息接收类 123456789101112package com.zhisheng.activemq.server;import org.springframework.jms.annotation.JmsListener;import org.springframework.stereotype.Component;@Componentpublic class ActiveMQServer &#123; @JmsListener(destination = \"zhisheng\") public void receive(String message) &#123; System.out.println(\"收到的 message 是：\" + message); &#125;&#125; 5、注意 这个队列是不需要我们提前定义好的，它和 RabbitMQ 不一样，它会在我们需要的时候动态的创建。 运行12345678910111213141516171819202122232425262728293031package com.zhisheng.activemq;import com.zhisheng.activemq.client.ActiveMQClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.util.StopWatch;import javax.annotation.PostConstruct;@SpringBootApplicationpublic class ActivemqApplication &#123; @Autowired ActiveMQClient client; @PostConstruct public void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 10000; i++) &#123; client.send(\"发送消息----zhisheng-----\"); &#125; stopWatch.stop(); System.out.println(\"发送消息耗时: \" + stopWatch.getTotalTimeMillis()); &#125; public static void main(String[] args) &#123; SpringApplication.run(ActivemqApplication.class, args); &#125;&#125; 发送一万条消息运行后需要的时间挺久的：73180 ms 比 RabbitMQ 发送 10000 条消息耗时 215 ms 不知道高出多少倍了，可见其性能并不高的。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/27/SpringBoot-ActiveMQ/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://www.54tianzhisheng.cn/tags/ActiveMQ/"}]},{"title":"Spring Boot系列文章（三）：SpringBoot  RabbitMQ 整合使用","date":"2018-01-25T16:00:00.000Z","path":"2018/01/26/SpringBoot-RabbitMQ/","text":"SpringBoot 系列文章 前提上次写了篇文章，《SpringBoot Kafka 整合使用》，阅读量还挺高的，于是想想还是把其他几种 MQ 也和 SpringBoot 整合使用下。 下面是四种比较流行的 MQ ： 后面都写写和 SpringBoot 整合的文章。 安装 RabbitMQ由于换 Mac 了，所以一些环境就直接在 Mac 搞，但是像安装 RabbitMQ 这些又会把自己电脑系统给搞的太乱，所以能在 Docker 里面安装就安装在 Docker，这次 RabbitMQ 我也直接在 Docker 里安装。 启动 Docker for Mac，如果没安装过的请看我上一篇文章：http://www.54tianzhisheng.cn/2018/01/25/Docker-install/ 当然你也可以在自己的 Linux 服务器或者虚拟机里启动安装 RabbitMQ 。 Docker 安装的话很简单，因为 RabbitMQ 官方已经提供了自己的 Docker 容器，只需要一行命令： 1docker run -d -p 15672:15672 -p 5672:5672 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management 该镜像拥有一个基于 web 的控制台和 Http API。Http API 可以在地址看到如何使用：http://localhost:15672/api/ 讲解下上面命令行： 15672 ：表示 RabbitMQ 控制台端口号，可以在浏览器中通过控制台来执行 RabbitMQ 的相关操作。 5672 : 表示 RabbitMQ 所监听的 TCP 端口号，应用程序可通过该端口与 RabbitMQ 建立 TCP 连接，并完成后续的异步消息通信 RABBITMQ_DEFAULT_USER：用于设置登陆控制台的用户名，这里我设置 admin RABBITMQ_DEFAULT_PASS：用于设置登陆控制台的密码，这里我设置 admin 容器启动成功后，可以在浏览器输入地址：http://localhost:15672/ 访问控制台 登陆后： 简单描述下上图中中控制台的列表的作用： Overview ：用于查看 RabbitMQ 的一些基本信息（消息队列、消息发送速率、节点、端口和上下文信息等） Connections：用于查看 RabbitMQ 客户端的连接信息 Channels：用户查看 RabbitMQ 的通道信息 Exchange：用于查看 RabbitMQ 交换机 Queues：用于查看 RabbitMQ 的队列 Admin：用于管理用户，可增加用户 创建项目在 IDEA 中创建一个 SpringBoot 项目结构： SpringBoot 框架中已经内置了对 RabbitMQ 的支持，如果你看过官方文档的话，就可以看到的，我们需要把依赖 spring-boot-starter-amqp 引入就行。 1、 pom.xml 引入依赖后如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;rabbitmq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;rabbitmq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot RabbitMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、application.properties 配置修改如下： 123spring.rabbitmq.addresses=localhost:5672spring.rabbitmq.username=adminspring.rabbitmq.password=admin 3、消息发送类 RabbitMQClient.java 12345678910111213141516171819package com.zhisheng.rabbitmq.client;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/23 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; public void send(String message) &#123; rabbitTemplate.convertAndSend(\"zhisheng\", message); &#125;&#125; 就这样，发送消息代码就实现了。 这里关键的代码为 rabbitTemplate.convertAndSend() 方法，zhisheng 这个是路由规则（routingKey），它的值表明将消息发送到指定的队列 zhisheng 中去，这里跟了下源码，发现 convertAndSend() 方法最后调用的方法其实是一个 doSend() 方法。 4、消息接收类 12345678910111213141516package com.zhisheng.rabbitmq.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/23 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"zhisheng\") public void receive(String message) &#123; System.out.println(\"收到的 message 是：\" + message); &#125;&#125; 你看，这里就有个 RabbitListener 一直在监听着队列 zhisheng 。 当然这个队列是必须要我们自己在应用程序中创建好，它不会像我之前写的文章 《SpringBoot Kafka 整合使用》 中的 Kafka 一样，Kafka 它会在用到队列的时候动态的创建，不需要我们提前创建好。 那么在 RabbitMQ 中该如何创建队列呢？ 如上图所示：这样我们就创建好了一个 zhisheng 的队列，当程序开始运行时，消息接收类会持续监听队列 zhisheng 中即将到来的消息。 5、运行项目 需要在启动类中注入发送消息的类，并且提供 init 方法，在 init 方法中调用发送消息类的 send() 方法 1234@PostConstructpublic void init() &#123; rabbitMQClient.send(\"发送消息----zhisheng-----\");&#125; 需要注意的是：init() 方法带有 @PostConstruct 注解，被 @PostConstruct 修饰的方法会在构造函数之后执行。 启动项目就可以发现控制台已经接收到消息了。 6、单线程测试性能 看到上面图片中注释掉的代码没？那就是用来测试消息发送的性能的，我发送 10000 条消息看看总共耗时多少。 10000 条消息发送耗时：215ms。 这是在单线程下，下次可以和其他的 MQ 测试对比下，并且也可以在多线程的环境下测试性能。 同时从控制台可以看到发送的速率： 7、多线程测试性能 开了10 个线程，每个线程发送 10000 条消息。 init 方法代码如下： 1234567891011121314151617181920212223242526272829303132333435363738@PostConstruct public void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); int threads = 10; ExecutorService executorService = Executors.newFixedThreadPool(threads); final CountDownLatch start = new CountDownLatch(1); final CountDownLatch end = new CountDownLatch(threads); for (int i = 0; i &lt; threads; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; start.await(); for (int i1 = 0; i1 &lt; 10000; i1++) &#123; rabbitMQClient.send(\"发送消息----zhisheng-----\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; end.countDown(); &#125; &#125;); &#125; start.countDown(); try &#123; end.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; executorService.shutdown(); &#125; stopWatch.stop(); System.out.println(\"发送消息耗时：\" + stopWatch.getTotalTimeMillis()); &#125; 耗时：4063ms 控制台显示如下图： 8、注意 这里测试发送的消息直接是 String 类型的，你也可以测试下 Bean 类，这需要注意需要序列化。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://www.54tianzhisheng.cn/tags/RabbitMQ/"}]},{"title":"Docker系列文章（二）：Mac 安装 Docker 及常用命令","date":"2018-01-24T16:00:00.000Z","path":"2018/01/25/Docker-install/","text":"背景微服务 + 容器，完美的一对！必须得好好学习学习。 安装步骤Mac 下 Docker 的安装真心建议跟着官方的文档走一遍，官网已经讲的很详细了。 https://docs.docker.com/docker-for-mac/install/#what-to-know-before-you-install 使用 Docker for Machttps://docs.docker.com/docker-for-mac/#check-versions-of-docker-engine-compose-and-machine 配置 Docker 加速器Docker 加速器是什么，我需要使用吗？ 使用 Docker 的时候，需要经常从官方获取镜像，但是由于显而易见的网络原因，拉取镜像的过程非常耗时，严重影响使用 Docker 的体验。因此 DaoCloud 推出了加速器工具解决这个难题，通过智能路由和缓存机制，极大提升了国内网络访问 Docker Hub 的速度，目前已经拥有了广泛的用户群体，并得到了 Docker 官方的大力推荐。如果您是在国内的网络环境使用 Docker，那么 Docker 加速器一定能帮助到您。 注册 daocloud，然后在 mac 标签页复制加速器 url。 入门案例跟着下面的文章进行敲一遍，熟悉下 Docker 整个的使用。 https://www.jianshu.com/p/cf6e7248b6c7 Docker 常用命令下面列出些自己常用的命令，目的就是记录下来，以后忘记了，再拿来跟着敲就行！ 12345678910111213141516171819202122232425262728293031323334353637383940414243docker run -i -t &lt;image_name/continar_id&gt; /bin/bash 启动容器并启动bash（交互方式）docker run -d -it image_name 启动容器以后台方式运行(更通用的方式）docker ps 列出当前所有正在运行的containerdocker ps -a 列出所有的containerdocker ps -l 列出最近一次启动的containerdocker images 列出本地所有的镜像docker rmi imagesID 删除指定的镜像iddocker rm CONTAINER ID 删除指定的CONTAINER iddocker diff 镜像名 查看容器的修改部分docker kill CONTAINER ID 杀掉正在运行的容器docker logs 容器ID/name 可以查看到容器主程序的输出docker pull image_name 下载imagedocker push image_name 发布docker镜像docker version 查看docker版本docker info 查看docker系统的信息docker inspect 容器的id 可以查看更详细的关于某一个容器的信息docker run -d image-name 后台运行镜像docker search 镜像名 查找公共的可用镜像docker stop 容器名/容器 ID 终止运行的容器docker restart 容器名/容器 ID 重启容器docker commit 提交，创建个新镜像docker build [OPTIONS] PATH | URL | - 利用 Dockerfile 创建新镜像 关注我 最后转载请注明地址：http://www.54tianzhisheng.cn/2018/01/25/Docker-install/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.54tianzhisheng.cn/tags/Docker/"}]},{"title":"MacBook Pro 初体验","date":"2018-01-23T16:00:00.000Z","path":"2018/01/24/mac/","text":"背景 在 Mac 到手之前就在各种群里看到人说 Mac 多好用，也有很多人鼓吹过 Mac 的好处，最后也坚定我的年前目标了 —— 就是买台 Mac，之前请原谅我这个穷鬼，买不起，现在买了 Mac 后更加得体谅我这个穷鬼了，毕竟在上海这个城市，靠着实习工资买这种奢侈品，不容易啊😄 。废话不多说，如果愿意支持我的，请在文章底部扫描二维码，在此先谢谢了。 如何挑选？MacBook 主要分两系列：MacBook Air 和 MacBook Pro。 Air 的话个人感觉配置不高，如果是开发还是建议买 Pro 系列的。如今买的话，可能还会分 2015 款、2016 款、2017 款。每款中又分 内存大小（8/16g）、硬盘大小（128/256/512g）、CPU、处理器（i5/i7）、是否有TouchBar、显卡等。不同配置对应电脑的型号也是不一样的。下面直接上一张在我的特殊渠道里的报价表截图吧。（想了解的可以找我） 光这型号，不懂的人还真不会挑。 不得不说，苹果电脑真尼玛难挑啊，如果你是土豪，那不用挑了，直接上最高价钱的吧。 然后可以从配置中发现 2016 款和 2017 款变化真心不大，在同等配置下，2017 款几乎比 2016 款价格高个 3000 来块。 然后就是 512 G 硬盘比 256 G 也几乎贵个 2000 多。 16G 那是必须的上啊，标配了，8G 就不说了，太小了。 含 TouchBar，虽然确实用处不大，不过调音两还是不错的。高配都有 TouchBar 的。 出于 qiong ，我买了 2016 款，配置是： 不过现在 2016 款好像停产了。 到手2018.01.11 下午六点快递送到的，很开心。晚上拿回家拆箱，第一件事情就是检查序列号啊，上面的图片打码掉的就是序列号，这个序列号在电脑机身、系统、外牛皮癣盒都有的，可以在官网查询这个序列号，获得电脑的激活日期和剩余保修时间的。再就是查询电脑的电池循环次数了，我的是一次，一般好像是几次之内都是符合的。证明之前没被别人用过，这个数字我也忘记了。 熟悉系统Mac 系统是类 Unix 系统，其实我觉得到和 Ubuntu 系统挺像的，既有图形化界面，也可以命令行操作。熟悉过 Linux 的应该上手很快的。 安装软件可以在 Appstore 里面下载，也可以在一些软件的官网直接下载 mac 版的。安装也挺简单的。如果你不知道有什么软件可以安装，那么我这里给你份 Mac 软件参考列表：https://github.com/jaywcjlove/awesome-mac/blob/master/README-zh.md 当然了，上面的不一定全，具体用到其他的还是的自己去找对应的软件。 还有就是好多软件是收费的，在 Mac 上如果要下载的话，还的费点心思去破解，比如 Office、IDEA、Adobe 系列等，当然也不是鼓吹大家去破解，我们自己用用就行，虽说现在没钱支持，但是有钱的话还是支持下。我一个写博客的知道写博客的不容易，那写软件的更不容易了，能支持一两块也挺好的。 然后就是美化下我们的一些软件，比如我们的终端之类的、尽量使用 Homebrew 安装软件。当然这篇文章不会写这些的，改天专门写篇文章写这个话题。 还有就是在 Mac 上从新打造一个适合自己的新写作环境（软件、Hexo 写博客环境）。 最后体验了 Mac 也有一个多礼拜了，整体效果还是不错的，毕竟花了巨资呢，也算是完成了自己年前的小目标。先 bb 到这里吧。有时间再写点关于 Mac 上的东西，这次写的比较简单，这篇文章也是在 Mac 上写的第一篇文章。","tags":[{"name":"Mac","slug":"Mac","permalink":"http://www.54tianzhisheng.cn/tags/Mac/"}]},{"title":"Spring Boot系列文章（二）：SpringBoot Admin 使用指南","date":"2018-01-16T16:00:00.000Z","path":"2018/01/17/SpringBoot-Admin/","text":"什么是 SpringBoot Admin？Spring Boot Admin 是一个管理和监控你的 Spring Boot 应用程序的应用程序。 这些应用程序通过 Spring Boot Admin Client（通过 HTTP）注册或者使用 Spring Cloud（例如 Eureka）发现。 UI只是 Spring Boot Actuator 端点上的一个 AngularJs 应用程序。 SpringBoot 系列文章 快速开始首先在 IDEA 创建一个 SpringBoot 项目，把它当作 server 端，工程如下： 然后在 pom.xml 中引入依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server-ui&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt; 继续在启动类 SpringbootAdminApplication.java 中引入注解 @EnableAdminServer ，然后运行项目： 访问 http://localhost:8084/ 即可： 此时会发现没有任何应用程序的信息。 接下来我们新建一个 SpringBoot 项目，把它当作客户端程序，工程如下： 在 pom.xml 中添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt; 然后在 application.yml 中设置： spring.boot.admin.url=http:localhost:8094 用于将当前应用注册到 Spring Boot Admin。 还可以设置，spring.boot.admin.client.name: （应用程序的名字）不设置的话会有默认的名字 此时把两个项目运行起来： 点击图中的 detail 按钮：可以看到应用程序的健康值、内存、JVM、GC 等信息。 metrics 信息： 环境 信息： log 信息： JMX 信息： 线程 信息： Trace 追踪信息： 还可以下载 Heapdump 文件。 刚才首页的应用列表后面有个红色的 ×，我们可以将注册上去的应用移除，但是只要你不把程序停掉，它立马又会注册上去。 还有就是应用列表的 version 和 info 上面的图中为空，下面看看怎么把它变出来： 123info.groupId: @project.groupId@info.artifactId: @project.artifactId@info.version: @project.version@ 重新运行客户端程序，刷新页面可以发现： 还可以查询应用程序的事件变化： 客户端应用程序JMX bean管理要在管理界面中与JMX-beans进行交互，您必须在客户端应用程序中包含 Jolokia, pom.xml 加入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.jolokia&lt;/groupId&gt; &lt;artifactId&gt;jolokia-core&lt;/artifactId&gt;&lt;/dependency&gt; 重启客户端程序后，就可以在这里与 JMX 做交互了： 还有很多 SpringBoot Admin 客户端配置选项： http://codecentric.github.io/spring-boot-admin/1.5.6/#spring-boot-admin-client 服务端程序也有些 SpringBoot Admin 服务端程序配置选项： http://codecentric.github.io/spring-boot-admin/1.5.6/#spring-boot-admin-server 官方文档里面还有些关于服务下线消息通知的知识，想了解的可以查看： http://codecentric.github.io/spring-boot-admin/1.5.6/#_notifications 关注我 参考文章http://codecentric.github.io/spring-boot-admin/1.5.6/ 最后转载请注明文章原始地址为：http://www.54tianzhisheng.cn/2018/01/17/SpringBoot-Admin/","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Lombok 看这篇就够了","date":"2018-01-08T16:00:00.000Z","path":"2018/01/09/lombok/","text":"前提自从进公司实习后，项目代码中能用 Lombok 的都用了，毕竟这么好的轮子要充分利用好。也可以减少一些 get/set/toString 方法的编写，虽说 IDEA 的插件可以自动生成 get/set/toString 方法，但是使用 Lombok 可以让代码更简洁。下面看看如何在 IDEA 中如何安装 Lombok： 安装打开 IDEA 的 Settings 面板，并选择 Plugins 选项，然后点击 “Browse repositories” 在输入框输入”lombok”，得到搜索结果，点击安装，然后安装提示重启 IDEA，安装成功; 引入依赖在自己的项目里添加 lombok 的编译支持，在 pom 文件里面添加 dependency 123456&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 怎么使用？在实体类上引入相关的注解就行： 有哪些注解？ @Data @Setter @Getter @Slf4j @AllArgsConstructor @NoArgsConstructor @EqualsAndHashCode @NonNull @Cleanup @ToString @RequiredArgsConstructor @Value @SneakyThrows @Synchronized 注解详解@Data 注解在 类 上；提供类所有属性的 get 和 set 方法，此外还提供了equals、canEqual、hashCode、toString 方法。 @Setter 注解在 属性 上；为单个属性提供 set 方法; 注解在 类 上，为该类所有的属性提供 set 方法， 都提供默认构造方法。 @Getter 注解在 属性 上；为单个属性提供 get 方法; 注解在 类 上，为该类所有的属性提供 get 方法，都提供默认构造方法。 @Slf4j 注解在 类 上；为类提供一个 属性名为 log 的日志对象，提供默认构造方法。 @AllArgsConstructor 注解在 类 上；为类提供一个全参的构造方法，加了这个注解后，类中不提供默认构造方法了。 @NoArgsConstructor 注解在 类 上；为类提供一个无参的构造方法。 @EqualsAndHashCode 注解在 类 上, 可以生成 equals、canEqual、hashCode 方法。 @NonNull 注解在 属性 上，会自动产生一个关于此参数的非空检查，如果参数为空，则抛出一个空指针异常，也会有一个默认的无参构造方法。 @Cleanup 这个注解用在 变量 前面，可以保证此变量代表的资源会被自动关闭，默认是调用资源的 close() 方法，如果该资源有其它关闭方法，可使用 @Cleanup(“methodName”) 来指定要调用的方法，也会生成默认的构造方法 @ToString 这个注解用在 类 上，可以生成所有参数的 toString 方法，还会生成默认的构造方法。 @RequiredArgsConstructor 这个注解用在 类 上，使用类中所有带有 @NonNull 注解的或者带有 final 修饰的成员变量生成对应的构造方法。 @Value 这个注解用在 类 上，会生成含所有参数的构造方法，get 方法，此外还提供了equals、hashCode、toString 方法。 @SneakyThrows 这个注解用在 方法 上，可以将方法中的代码用 try-catch 语句包裹起来，捕获异常并在 catch 中用 Lombok.sneakyThrow(e) 把异常抛出，可以使用 @SneakyThrows(Exception.class) 的形式指定抛出哪种异常，也会生成默认的构造方法。 @Synchronized 这个注解用在 类方法 或者 实例方法 上，效果和 synchronized 关键字相同，区别在于锁对象不同，对于类方法和实例方法，synchronized 关键字的锁对象分别是类的 class 对象和 this 对象，而 @Synchronized 的锁对象分别是 私有静态 final 对象 lock 和 私有 final 对象 lock，当然，也可以自己指定锁对象，此外也提供默认的构造方法。 总结以上注解可根据需要一起搭配使用！ 虽说轮子好，但是我们不仅要知其然，也要知其所以然！ 关注我 最后转载请注明原创地址：http://www.54tianzhisheng.cn/2018/01/07/lombok/","tags":[{"name":"lombok","slug":"lombok","permalink":"http://www.54tianzhisheng.cn/tags/lombok/"}]},{"title":"Spring Boot系列文章（一）：SpringBoot Kafka 整合使用","date":"2018-01-04T16:00:00.000Z","path":"2018/01/05/SpringBoot-Kafka/","text":"前提假设你了解过 SpringBoot 和 Kafka。 SpringBoot 系列文章 1、SpringBoot 如果对 SpringBoot 不了解的话，建议去看看 DD 大佬 的系列博客。 2、Kafka Kafka 的话可以看看我前两天写的博客 ： Kafka 安装及快速入门 学习的话自己开台虚拟机自己手动搭建环境吧，有条件的买服务器。 注意：一定要亲自自己安装实践，接下来我们将这两个进行整合。 创建项目项目整体架构： 使用 IDEA 创建 SpringBoot 项目，这个很简单了，这里不做过多的讲解。 1、pom 文件代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;kafka-learning&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;kafka-learning&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot + kafka&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;1.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 主要引入了 spring-kafka 、lombok 、 gson 依赖。 2、消息实体类 Message.java 如下： 123456789@Datapublic class Message &#123; private Long id; //id private String msg; //消息 private Date sendTime; //时间戳&#125; 3、消息发送类 KafkaSender.java 12345678910111213141516171819@Component@Slf4jpublic class KafkaSender &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; private Gson gson = new GsonBuilder().create(); //发送消息方法 public void send() &#123; Message message = new Message(); message.setId(System.currentTimeMillis()); message.setMsg(UUID.randomUUID().toString()); message.setSendTime(new Date()); log.info(\"+++++++++++++++++++++ message = &#123;&#125;\", gson.toJson(message)); kafkaTemplate.send(\"zhisheng\", gson.toJson(message)); &#125;&#125; 就这样，发送消息代码就实现了。 这里关键的代码为 kafkaTemplate.send() 方法，zhisheng 是 Kafka 里的 topic ，这个 topic 在 Java 程序中是不需要提前在 Kafka 中设置的，因为它会在发送的时候自动创建你设置的 topic， gson.toJson(message) 是消息内容，这里暂时先说这么多了，不详解了，后面有机会继续把里面源码解读写篇博客出来（因为中途碰到坑，老子跟了几遍源码）。 4、消息接收类 KafkaReceiver.java 12345678910111213141516171819@Component@Slf4jpublic class KafkaReceiver &#123; @KafkaListener(topics = &#123;\"zhisheng\"&#125;) public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = kafkaMessage.get(); log.info(\"----------------- record =\" + record); log.info(\"------------------ message =\" + message); &#125; &#125;&#125; 客户端 consumer 接收消息特别简单，直接用 @KafkaListener 注解即可，并在监听中设置监听的 topic ，topics 是一个数组所以是可以绑定多个主题的，上面的代码中修改为 @KafkaListener(topics = {&quot;zhisheng&quot;,&quot;tian&quot;}) 就可以同时监听两个 topic 的消息了。需要注意的是：这里的 topic 需要和消息发送类 KafkaSender.java 中设置的 topic 一致。 5、启动类 KafkaApplication.java 123456789101112131415161718192021@SpringBootApplicationpublic class KafkaApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext context = SpringApplication.run(KafkaApplication.class, args); KafkaSender sender = context.getBean(KafkaSender.class); for (int i = 0; i &lt; 3; i++) &#123; //调用消息发送类中的消息发送方法 sender.send(); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 6、配置文件 application.properties 1234567891011121314151617181920212223242526#============== kafka ===================# 指定kafka 代理地址，可以多个spring.kafka.bootstrap-servers=192.168.153.135:9092#=============== provider =======================spring.kafka.producer.retries=0# 每次批量发送消息的数量spring.kafka.producer.batch-size=16384spring.kafka.producer.buffer-memory=33554432# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer#=============== consumer =======================# 指定默认消费者group idspring.kafka.consumer.group-id=test-consumer-groupspring.kafka.consumer.auto-offset-reset=earliestspring.kafka.consumer.enable-auto-commit=truespring.kafka.consumer.auto-commit-interval=100# 指定消息key和消息体的编解码方式spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.bootstrap-servers 后面设置你安装的 Kafka 的机器 IP 地址和端口号 9092。 如果你只是简单整合下，其他的几个默认就好了。 Kafka 设置在你安装的 Kafka 目录文件下： 启动 zk使用安装包中的脚本启动单节点 Zookeeper 实例： 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 启动 Kafka 服务使用 kafka-server-start.sh 启动 kafka 服务： 1bin/kafka-server-start.sh config/server.properties 启动成功后！ 千万注意： 记得将你的虚拟机或者服务器关闭防火墙或者开启 Kafka 的端口 9092。 运行 出现这就代表整合成功了！ 我们看下 Kafka 中的 topic 列表就 1bin/kafka-topics.sh --list --zookeeper localhost:2181 就会发现刚才我们程序中的 zhisheng 已经自己创建了。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/05/SpringBoot-Kafka/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"}]},{"title":"为什么要重新运营以前的公众号呢？","date":"2018-01-03T16:00:00.000Z","path":"2018/01/04/weixin/","text":"前提老读者可能会发现现在我的公众号已经改名了。（由 “猿blog” 变成 “zhisheng” 了，细心的童鞋会发现不仅名字变了， ID 也变了，但是图片还没改，暂时还没想到好的 logo 图片），下面说说为啥吧！ 听我瞎 BB 上图是两年前公众号群发的第一条信息，那时自己还是在学校，如今已经进入了社会，在公司实习了。记得当初开这个公众号的原因是因为几个年轻人有着梦想，打算一起做点东西，当时一腔热血的自己立马就先申请了个公众号，后来 “东西” 倒是没做，反倒是我自己慢慢的在微信公众号分享一些文章，然后那时自己也写博客（算算自己写博客应该快三年了，坚持真不易啊），所以偶尔也把自己的博客分享在微信公众号上。 但是好景不长，那时的微信公众号排版真尼玛难用的一批，作为一个理工科的男生，本来自己做事一般细心和耐心，无奈，把这么好的一个童鞋都给逼坏了。我现在还是得吐槽下，如今的微信公众号后台排版还是那么差。但是可能因为需求比较多了，所以就有人做出了工具（将 markdown 排版后在将整个样式复制粘贴到微信公众号后台），这样一篇排版还算不错的博客就出来了。 自己早就知道了这么个工具，以前看 DD 的博客的时候就发现了这个工具，但是很久没更新的微信公众号，自己也不怎么想再管理。 有人就要问了？那为啥现在又要开始跟新了呢？ 我只想说：“贱人就是矫情！！！又想瞎折腾下。”，反正自己的博客也在不断的更新，偶尔顺带把文章同步到微信公众号其实也是可以的。在学校的时候时间比较多，那时真的是时间比较多，后悔没好好坚持运营下来。现在工作了，自己工作之外的时间较少，除了学习，偶尔写写博客，娱乐时间比较少，都是大学时宅的。 前段时间被人 “忽悠” 说继续更新公众号，那时刚好也快 2018 年了，自己也想给自己定几个目标，在元旦的那天，想想还是继续更新微信公众号吧，所以你也看得到最近我的更新了，可能最近的更新比较有规律，因为这些文章大部分是之前就已经写好了的，已经发过在我的博客里了。估计把这些文章更新完后，就不会每天都更新我自己的文章了。 定位说下微信公众号的定位吧： 1、我的技术博客应该都会同步在这里的。 2、分享自己平时的随笔文章。（比如这篇。。。） 3、除了技术文章，当然还有平时自己的 奇淫技巧 （包括但不限于写作方式、推荐好用的软件等） 4、分享自己觉得不错的文章（别人的，尽量征得同意，一定会备注原创地址的） 5、如果你也写博客，但是阅读量很小的话，可以考虑自荐。（注：文章我可能会审批，必须要觉得不错的文章） 6、分享一些学习视频和书籍 7、后期可能会搞工作内推 。。。 暂时只想到这些了 问题来了因为工作了，所以时间少，运营这微信公众号可能需要花费我不少工作之外的时间。 如果可以的话，我希望能找到一个能帮我分担点的朋友。 注：无偿的，如果介意的话，下面就不用再看了。 说点简单的要求吧： 1、细心、耐心的 boy or girl 都行 2、起码要知道点编程方面的知识 3、能坚持下来 4、对新技术有敏感的嗅觉 5、最后一点就是你要有点时间了，希望不耽误你学习 再说下能给你带来的 好处 吧： 1、肯定能增加你的运营能力（再去互联网公司投运营岗位会有优势的） 2、本人一开始会亲自教授该怎么做，所以没经验的朋友不用担心 3、可以增加和大牛勾搭的机会，你们懂的。。 4、本人可以亲自传授经验（编程和生活点滴经验） 如果你有意愿的话，请加我 QQ ： 1041218129 聊聊吧 关注我","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Kafka 安装及快速入门","date":"2018-01-03T16:00:00.000Z","path":"2018/01/04/Kafka/","text":"介绍官网：http://kafka.apache.org/ Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Apache Kafka与传统消息系统相比，有以下不同： 它被设计为一个分布式系统，易于向外扩展； 它同时为发布和订阅提供高吞吐量； 它支持多订阅者，当失败时能自动平衡消费者； 它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。 安装 kafka下载地址：https://kafka.apache.org/downloads 1wget http://mirrors.shuosc.org/apache/kafka/1.0.0/kafka_2.11-1.0.0.tgz 解压：123tar -zxvf kafka_2.11-1.0.0.tgzcd /usr/local/kafka_2.11-1.0.0/ 修改 kafka-server 的配置文件 1vim /usr/local/kafka/config/server.properties 修改其中的： 12broker.id=1log.dir=/data/kafka/logs-1 功能验证：1、启动 zk使用安装包中的脚本启动单节点 Zookeeper 实例： 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 2、启动Kafka 服务使用 kafka-server-start.sh 启动 kafka 服务： 1bin/kafka-server-start.sh config/server.properties 3、创建 topic使用 kafka-topics.sh 创建单分区单副本的 topic test： 1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看 topic 列表： 1bin/kafka-topics.sh --list --zookeeper localhost:2181 查询创建的 topic 列表报错： 解决方法: 1vim /etc/hosts 将 host 里的 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 修改为： 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 ip6-localhost ip6-localhost.localdomain localhost6 localhost6.localdomain6 方法参考：zookeeper unable to open socket to localhost/0:0:0:0:0:0:0:1:2181 再次查询就不报错了。 4、产生消息使用 kafka-console-producer.sh 发送消息： 1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 5、消费消息使用 kafka-console-consumer.sh 接收消息并在终端打印： 1bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 打开个新的命令窗口执行上面命令即可查看信息： 6、查看描述 topics 信息1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 结果： 12Topic:test PartitionCount:1 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 1 Replicas: 1 Isr: 1 第一行给出了所有分区的摘要，每个附加行给出了关于一个分区的信息。 由于我们只有一个分区，所以只有一行。 “Leader”: 是负责给定分区的所有读取和写入的节点。 每个节点将成为分区随机选择部分的领导者。 “Replicas”: 是复制此分区日志的节点列表，无论它们是否是领导者，或者即使他们当前处于活动状态。 “Isr”: 是一组“同步”副本。这是复制品列表的子集，当前活着并被引导到领导者。 集群配置Kafka 支持两种模式的集群搭建：可以在单机上运行多个 broker 实例来实现集群，也可在多台机器上搭建集群，下面介绍下如何实现单机多 broker 实例集群，其实很简单，只需要如下配置即可。 单机多broker 集群配置利用单节点部署多个 broker。 不同的 broker 设置不同的 id，监听端口及日志目录。 例如： 1234567cp config/server.properties config/server-2.propertiescp config/server.properties config/server-3.propertiesvim config/server-2.propertiesvim config/server-3.properties 修改 ： 12345broker.id=2listeners = PLAINTEXT://your.host.name:9093log.dir=/data/kafka/logs-2 和 12345broker.id=3listeners = PLAINTEXT://your.host.name:9094log.dir=/data/kafka/logs-3 启动Kafka服务： 123bin/kafka-server-start.sh config/server-2.properties &amp;bin/kafka-server-start.sh config/server-3.properties &amp; 至此，单机多broker实例的集群配置完毕。 多机多 broker 集群配置分别在多个节点按上述方式安装 Kafka，配置启动多个 Zookeeper 实例。 假设三台机器 IP 地址是 ： 192.168.153.135， 192.168.153.136， 192.168.153.137 分别配置多个机器上的 Kafka 服务，设置不同的 broker id，zookeeper.connect 设置如下: 1vim config/server.properties 里面的 zookeeper.connect 修改为： 1zookeeper.connect=192.168.153.135:2181,192.168.153.136:2181,192.168.153.137:2181 使用 Kafka Connect 来导入/导出数据从控制台写入数据并将其写回控制台是一个方便的起点，但您可能想要使用其他来源的数据或将数据从 Kafka 导出到其他系统。对于许多系统，您可以使用 Kafka Connect 来导入或导出数据，而不必编写自定义集成代码。 Kafka Connect 是 Kafka 包含的一个工具，可以将数据导入和导出到 Kafka。它是一个可扩展的工具，运行 连接器，实现与外部系统交互的自定义逻辑。在这个快速入门中，我们将看到如何使用简单的连接器运行 Kafka Connect，这些连接器将数据从文件导入到 Kafka topic，并将数据从 Kafka topic 导出到文件。 首先，我们将通过创建一些种子数据开始测试： 1echo -e &quot;zhisheng\\ntian&quot; &gt; test.txt 接下来，我们将启动两个以独立模式运行的连接器，这意味着它们将在单个本地专用进程中运行。我们提供三个配置文件作为参数。首先是 Kafka Connect 过程的配置，包含常见的配置，例如要连接的 Kafka 代理以及数据的序列化格式。其余的配置文件都指定一个要创建的连接器。这些文件包括唯一的连接器名称，要实例化的连接器类以及连接器所需的任何其他配置。 1bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties Kafka 附带的这些示例配置文件使用您之前启动的默认本地群集配置，并创建两个连接器：第一个是源连接器，用于读取输入文件中的行，并将每个连接生成为 Kafka topic，第二个为连接器它从 Kafka topic 读取消息，并在输出文件中产生每行消息。 在启动过程中，您会看到一些日志消息，其中一些指示连接器正在实例化。Kafka Connect 进程启动后，源连接器应该开始读取 test.txt topic connect-test，并将其生成 topic ，并且接收器连接器应该开始读取 topic 中的消息 connect-test 并将其写入文件 test.sink.txt。我们可以通过检查输出文件的内容来验证通过整个管道传输的数据： 数据存储在 Kafka topic 中 connect-test，因此我们也可以运行控制台使用者来查看 topic 中的数据 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning 连接器继续处理数据，所以我们可以将数据添加到文件中，并看到它在管道中移动： 1234echo zhishengtian&gt;&gt; test.txtecho zhishengtian2&gt;&gt; test.txtecho zhishengtian3&gt;&gt; test.txtecho zhishengtian4&gt;&gt; test.txt 使用 Kafka 流来处理数据Kafka Streams 是用于构建关键任务实时应用程序和微服务的客户端库，输入和/或输出数据存储在 Kafka 集群中。Kafka Streams 结合了在客户端编写和部署标准 Java 和 Scala 应用程序的简单性以及 Kafka 服务器端集群技术的优势，使这些应用程序具有高度可伸缩性，弹性，容错性，分布式等特性。 可参考官网入门案例：http://kafka.apache.org/10/documentation/streams/quickstart 参考1、在CentOS 7上安装Kafka 2、http://kafka.apache.org/10/documentation/streams/quickstart 关注我 最后转载请注明原创地址为：http://www.54tianzhisheng.cn/2018/01/04/Kafka/","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"}]},{"title":"Windows 下安装 Consul","date":"2017-12-26T16:00:00.000Z","path":"2017/12/27/consul-install/","text":"前提从刚工作就开始接触 Consul，中途自己也有两个项目和 Consul 有关，后面有机会再讲讲，网上关于这个的资料还比较少。因为明天有 Consul 的技术分享，所以自己今天下午在官网看了下相关的介绍。 介绍Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件, 由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源。Consul 支持健康检查, 并允许 HTTP 和 DNS 协议调用 API 存储键值对。命令行超级好用的虚拟机管理软件 vgrant 也是 HashiCorp 公司开发的产品。一致性协议采用 Raft 算法, 用来保证服务的高可用， 使用 GOSSIP 协议管理成员和广播消息, 并且支持 ACL 访问控制。 下载安装去官网下载：https://www.consul.io/downloads.html 得到一个 zip 压缩包 在你想要安装的位置解压就行，只有一个 consul.exe 文件（我的解压位置是：D:\\software） 设置环境变量（在 path 中新增一条）： D:\\software cmd 命令窗口启动： 1consul agent -dev consul 自带 UI 界面，打开网址：http://localhost:8500 ，可以看到当前注册的服务界面。 Consul 优势 使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. 相比较而言, zookeeper 采用的是 Paxos, 而 etcd 使用的则是 Raft. 支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等. zookeeper 和 etcd 均不提供多数据中心功能的支持. 支持健康检查. etcd 不提供此功能. 支持 http 和 dns 协议接口. zookeeper 的集成较为复杂, etcd 只支持 http 协议. 官方提供web管理界面, etcd 无此功能. 综合比较, Consul 作为服务注册和配置管理的新星, 比较值得关注和研究. 最后 本文首发于：zhisheng的博客 地址为：http://www.54tianzhisheng.cn/2017/12/27/consul-install/ 转载请注明地址！","tags":[{"name":"Consul","slug":"Consul","permalink":"http://www.54tianzhisheng.cn/tags/Consul/"}]},{"title":"Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建","date":"2017-12-24T16:00:00.000Z","path":"2017/12/25/ELK/","text":"简单介绍ELK（ElasticSearch, Logstash, Kibana），三者组合在一起搭建实时的日志分析平台，目前好多公司都是这套！ Elasticsearch 是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful 风格接口，多数据源，自动搜索负载等。 Logstash 是一个完全开源的工具，他可以对你的日志进行收集、过滤，并将其存储供以后使用（如，搜索）。 Kibana 也是一个开源和免费的工具，它 Kibana 可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 安装 ES。。。这个省略，不 bb 了，以前写过。。。传送门：http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/ 安装 LogstashELK 整套环境搭建版本很关键，最好全统一一个版本，否则出啥问题就不太好找了。这是我见过版本统一最严格的了。而已 ES 版本升了后，其他的都要都要升级，包括其插件。升级代价挺大的，最好一开始就定位好要安装哪个版本！ 在官网下好安装包后传到 Linux 上，这是速度最快的。 1234567891011121314151617181920212223242526在 /usr/local 目录下解压：tar -zxvf logstash-5.5.2.tar.gz进入解压后的目录：cd /usr/local/logstash-5.5.2/bin新增配置文件：vim logstash.conf增加：input&#123; file&#123; path =&gt; [&quot;/var/log/*.log&quot;] &#125;&#125;output&#123; elasticsearch&#123; hosts =&gt; [&quot;192.168.153.135:9200&quot;] index =&gt; &quot;logstash__log&quot; &#125;&#125; Logstash 的启动方式是： 123在 /usr/local/logstash-5.5.2/bin 目录下运行：./logstash -f logstash.conf 安装 Kibana同样，官网下好安装包，上传到 Linux。 1234567891011解压：tar -zxvf kibana-5.5.2-linux-x86_64.tar.gz修改配置文件 kibana-5.5.2/config/kibana.yml 如下：Server.host //配置机器ip/hostnameServer.name //此kibana服务的名称elasticsearch.url //es master节点url Kibana 启动方式： 123在 /usr/local/kibana-5.5.2/bin 目录下运行：./kibana Web界面访问: http://ip:5601 此时需要输入用户名和密码登录,默认分别是 elastic 和 changeme X-PackX-Pack 是一个 Elastic Stack 的扩展，将安全，警报，监控，报告和图形功能包含在一个易于安装的软件包中。 ES 和 Kibana 都可安装。 插件 x-pack-5.5.2.zip 依旧官网下。 ES 安装 X-Pack123cd /usr/local/elasticsearch/bin./elasticsearch-plugin install file:///opt/es/x-pack-5.5.2.zip 如果成功：显示如下 123456789101112131415161718192021222324252627282930[root@node1 bin]# ./elasticsearch-plugin install file:///opt/es/x-pack-5.5.2.zip-&gt; Downloading file:///opt/es/x-pack-5.5.2.zip[=================================================] 100%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.io.FilePermission \\\\.\\pipe\\* read,write* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries* java.lang.RuntimePermission getClassLoader* java.lang.RuntimePermission setContextClassLoader* java.lang.RuntimePermission setFactory* java.security.SecurityPermission createPolicy.JavaPolicy* java.security.SecurityPermission getPolicy* java.security.SecurityPermission putProviderProperty.BC* java.security.SecurityPermission setPolicy* java.util.PropertyPermission * read,write* java.util.PropertyPermission sun.nio.ch.bugLevel write* javax.net.ssl.SSLPermission setHostnameVerifierSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]y@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin forks a native controller @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@This plugin launches a native controller that is not subject to the Javasecurity manager nor to system call filters.Continue with installation? [y/N]y-&gt; Installed x-pack Kibana 安装 X-Pack123cd /usr/local/kibana-5.5.2/bin./kibana-plugin install file:///opt/es/x-pack-5.5.2.zip 安装成功如下： 123456789[root@node1 bin]# ./kibana-plugin install file:///opt/es/x-pack-5.5.2.zipAttempting to transfer from file:///opt/es/x-pack-5.5.2.zipTransferring 159867054 bytes....................Transfer completeRetrieving metadata from plugin archiveExtracting plugin archiveExtraction completeOptimizing and caching browser bundles...Plugin installation complete 启用 x-pack 安全机制分别在 kibana.yml 和 elasticsearch.yml 中加入下行 1xpack.security.enabled: true 这样后，你再打开 ES 的 head 界面和 Kibana 管理界面就需要输入账号密码了。 上图右边是安装 X-Pack 后的，功能多了几个。 最后环境搭建很简单，后面如果有时间的话可以再讲讲在 Kibana 的 Dev Tools 上构建 ES 的 JSON 串来对 ES 进行操作。 我还写过 ES 相关的文章： 1、Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、全文搜索引擎 Elasticsearch 集群搭建入门教程 3、ElasticSearch 集群监控 4、ElasticSearch 单个节点监控 结尾 本文首发于：zhisheng 的博客 转载请注明地址：http://www.54tianzhisheng.cn/2017/12/25/ELK/","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"},{"name":"LogStash","slug":"LogStash","permalink":"http://www.54tianzhisheng.cn/tags/LogStash/"},{"name":"Kibana","slug":"Kibana","permalink":"http://www.54tianzhisheng.cn/tags/Kibana/"}]},{"title":"Hexo + yilia 搭建博客可能会遇到的所有疑问","date":"2017-12-17T16:00:00.000Z","path":"2017/12/18/hexo-yilia/","text":"前提为什么会再次写这篇博客？请看下图： 这是我博客搜索引擎的主要关键字。为什么会有这些关键字呢？ 我猜估计是曾经写了几篇关于搭建博客的文章，被搜索引擎收入了，所以搜索引擎才会将这些流量引导至我的博客，文章如下： 1、利用Github Page 搭建个人博客网站 2、Hexo + yilia 主题实现文章目录 3、Github pages + Hexo 博客 yilia 主题使用畅言评论系统 那还有这么多人搜索这些关键字？说明碰到问题的还有不少，所以才有了这篇文章的诞生！ 问题解答1、hexo yilia 文章目录 这个我以前写过一篇文章：Hexo + yilia 主题实现文章目录 那篇文章写了我那个版本的 yilia 怎么添加文章目录的，但是好像新版本的 yilia 已经自带了这个文章目录功能。所以如果你是使用的新版本的 yilia ，请不要做任何修改！但是前几天有人给我发了个图片，又好像有点区别，如果实在有不同的话，请加群 528776268 找我要我那个主题版本的所有配置文件。再次说明，我前端也不是很擅长，我写那篇文章也是参考其他博客的修改，所以无能为力了。有什么问题，建议直接在 yilia 主题的 GitHub 去找作者聊！ 2、Hexo yilia 随笔 随笔如下： 对此想说的就是，“随笔” 其实就是文章的一个 tags(标签)，如果你想把文章作为随笔的话，请在文章的首部写个 tags 为 “随笔” 的标签。如下图： 注意：- 后面有个空格。 3、yilia 主题分类实现 如果要有多个标签，可以如下图所示： 4、hexo yilia 设置文章显示长度，不展开全文 yilia 主题中可以用 &lt;!-- more --&gt; 截取文章的显示长度，如果你想在哪截取文章，就在那行使用该字符。 5、yilia 添加阅读量 我添加的是 “不蒜子” 计数，它可以区分 pv/uv 的统计方式，统计更精准，满足更多需求。有这个需求的可以去查找下博客怎么添加。（网上有很多这方面的博客） 6、yilia 主题使用 “畅言” 评论系统 参见我以前的文章： Github pages + Hexo 博客 yilia 主题使用畅言评论系统 7、hexo yilia 引入音乐 1&lt;iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"填写音乐链接地址\"&gt;&lt;/iframe&gt; 如下图，可以在网易云音乐里搜到你想要引入的音乐，然后点击如下的 “生成外链播放器” 即可： 8、hexo yilia 引入视频 hexo 支持 html 语法的，所以可以如上图这样引入视频！ 9、hexo yilia 相册 这个抱歉，我自己也没做这方面的功能，暂时不太清楚怎么实现。不过有文章写怎么实现，大家可以搜索下！ 10、hexo yilia 怎么写文章 我一般写文章就是先用本地 markdown 编辑器写好后，然后放在 hexo 的 source/_posts 目录下。 结尾好了，大概就这些问题，我也一一解答了，希望搭建博客的你可以看到这篇文章，让你少走点弯路，如果你也遇到过这些问题，还请你能分享下文章，让更多人避免入坑！ 本文地址是：Hexo + yilia 搭建博客可能会遇到的所有疑问 本文原创，转载请注明原创地址。 最后http://www.54tianzhisheng.cn/2017/12/18/hexo-yilia 这个链接是让推酷爬虫吞掉的，哈哈！","tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.54tianzhisheng.cn/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://www.54tianzhisheng.cn/tags/yilia/"}]},{"title":"谷歌开发者大会收获满满，不去真 “可惜” 了","date":"2017-12-12T16:00:00.000Z","path":"2017/12/13/Google-Developer-Days/","text":"全文图片较多，请在 WiFi 下阅读，土豪请随意！ 前提今年 Google 开发者大会再度来袭，大会将于 12 月 13 日和 14 日在上海举办，主题涵盖机器学习(Machine Learning)、Android、移动网络(Mobile Web)、TensorFlow、Firebase、云服务(Cloud)、AR/VR、设计(Design)以及更多开发者相关内容。 今天我就到走一遭，收获满满，都是用袋子提回来的，哈哈。下图为袋子： 再秀张图代表我去了： 入场 说下今天我参加的会场吧！ 会场主会场开幕 当然是用来用来做开发者开幕大会主题演讲的。相信不少没到现场的也看了直播。 拍了两张李飞飞演讲时的照片： 还有个妹子是讲 TensorFlow 的，全程中文，还贼 6，佩服！！！ 中途演讲还好几个，没拍照了。。。 中途茶歇：去外面看了下。 主会场演讲主题是：《渐进式网页应用：快速、集成、可靠并且具有吸引力》 这次坐的是前排，还拍了照，演讲人技巧很好，边演讲边带有身体动作的，而且还比较诙谐。 午餐胸牌上有13、14 号的午餐券，可以免费吃、免费拿，福利超好。 下午会场下午会场有点多，略略略。。。 都拍了点照，如果想要，可以加群：528776268 找我要、 晚餐诱惑颇大。。 我还喝了杯葡萄酒。。哈哈 另外除了照片，还拍了三个视频 回家吃饱喝足，回家拍了张照 礼物到家了，整理了下今天的礼物： 贴纸一张 小礼品一个 一个可 DIY 的音箱 一个定制的手提电脑包，质量很好。 AndroidThings 最后全文图片较多，谢谢阅读！自己收获也挺多的，明天还有一天，可惜不打算去了，转载请注明地址：http://www.54tianzhisheng.cn/2017/12/13/Google-Developer-Days/ 结尾这个是为了防爬虫写的，哈哈","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"使用 CodeMirror 打造属于自己的在线代码编辑器","date":"2017-12-08T16:00:00.000Z","path":"2017/12/09/CodeMirror/","text":"前提写这个的目的是因为之前项目里用到过 CodeMirror，觉得作为一款在线代码编辑器还是不错，也看到过有些网站用到过在线代码编辑，当然我不知道他们是用什么做的，这里我把公司项目里用到的那部分抽出来，单独写篇博客，并把抽出来的那部分代码提交到 GitHub 去（地址），以防日后可能会再次用到（没准毕业设计里可能用的到）。 简单介绍CodeMirror 是一款在线的支持语法高亮的代码编辑器。官网： http://codemirror.net/ 可能光看官网，第一眼觉得那些在线编辑器有点丑，反正第一眼给我的感觉就是这样子，但是经过自己的细调，也能打造出一款精美的在线代码编辑器。 官网可以把它下载下来。 下载后，解压开得到的文件夹中，lib 下是放的是核心库和核心 css，mode 下放的是各种支持语言的语法定义，theme 目录下是支持的主题样式。一般在开发中，添加 lib 下的引用和 mode 下的引用就够了。 如何使用下面两个是使用 Code Mirror 必须引入的： 12&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/lib/codemirror.css\"/&gt;&lt;script src=\"codemirror-5.31.0/lib/codemirror.js\"&gt;&lt;/script&gt; 接下来要引用的就是在 mode 目录下编辑器中要编辑的语言对应的 js 文件，这里以 Groovy 为例： 12&lt;!--groovy代码高亮--&gt;&lt;script src=\"codemirror-5.31.0/mode/groovy/groovy.js\"&gt;&lt;/script&gt; 如果你想让 Java 代码也支持代码高亮，则需要引入我从网上下载下来的 clike.js（我已经放到我的 GitHub 去了） 12&lt;!--Java代码高亮必须引入--&gt;&lt;script src=\"codemirror-5.31.0/clike.js\"&gt;&lt;/script&gt; 引用的文件用于支持对应语言的语法高亮。 然后前面说了第一次进入 Code Mirror 官网，觉得那些编辑器比较丑，那可能是主题比较丑，我这里推荐一款还不错的主题，只需按照如下引入即可： 12&lt;!--引入css文件，用以支持主题--&gt;&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/theme/dracula.css\"/&gt; 如果你还想让你的编辑器支持代码行折叠，请按照如下进行操作： 123456&lt;!--支持代码折叠--&gt;&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/addon/fold/foldgutter.css\"/&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/foldcode.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/foldgutter.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/brace-fold.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/comment-fold.js\"&gt;&lt;/script&gt; 是不是这样引入就好了呢，当然不是啦 创建编辑器在实际项目中，一般都不会直接把 body 整个内容作为编辑器的容器。而最常用的，是使用 textarea。这里我在 里使用个 textarea， 123&lt;!-- begin code --&gt;&lt;textarea class=\"form-control\" id=\"code\" name=\"code\"&gt;&lt;/textarea&gt;&lt;!-- end code--&gt; 接下来就是创建编辑器了。 123//根据DOM元素的id构造出一个编辑器var editor = CodeMirror.fromTextArea(document.getElementById(\"code\"), &#123;&#125;); 是不是有点单调？ 没错，我还可以在里面给他设置些属性：（充分利用我一开始引入的那些文件） 123456789mode: \"text/groovy\", //实现groovy代码高亮mode: \"text/x-java\", //实现Java代码高亮lineNumbers: true, //显示行号theme: \"dracula\", //设置主题lineWrapping: true, //代码折叠foldGutter: true,gutters: [\"CodeMirror-linenumbers\", \"CodeMirror-foldgutter\"],matchBrackets: true, //括号匹配//readOnly: true, //只读 如果需要查看更多属性，可以去官网查找，目前我只用到这些属性！ 下面也列举些吧： indentUnit: integer缩进单位，值为空格数，默认为2 。 smartIndent: boolean自动缩进，设置是否根据上下文自动缩进（和上一行相同的缩进量）。默认为true。 tabSize: integertab字符的宽度，默认为4 。 indentWithTabs: boolean在缩进时，是否需要把 n*tab宽度个空格替换成n个tab字符，默认为false 。 electricChars: boolean在输入可能改变当前的缩进时，是否重新缩进，默认为true （仅在mode支持缩进时有效）。 specialChars: RegExp需要被占位符(placeholder)替换的特殊字符的正则表达式。最常用的是非打印字符。默认为：/[\\u0000-\\u0019\\u00ad\\u200b-\\u200f\\u2028\\u2029\\ufeff]/。 specialCharPlaceholder: function(char) → Element这是一个接收由specialChars选项指定的字符作为参数的函数，此函数会产生一个用来显示指定字符的DOM节点。默认情况下，显示一个红点（•），这个红点有一个带有前面特殊字符编码的提示框。 rtlMoveVisually: booleanDetermines whether horizontal cursor movement through right-to-left (Arabic, Hebrew) text is visual (pressing the left arrow moves the cursor left) or logical (pressing the left arrow moves to the next lower index in the string, which is visually right in right-to-left text). The default is false on Windows, and true on other platforms.（这段完全不晓得搞啥子鬼） keyMap: string配置快捷键。默认值为default，即 codemorrir.js 内部定义。其它在key map目录下。 extraKeys: object给编辑器绑定与前面keyMap配置不同的快捷键。 lineWrapping: boolean在长行时文字是换行(wrap)还是滚动(scroll)，默认为滚动(scroll)。 lineNumbers: boolean是否在编辑器左侧显示行号。 firstLineNumber: integer行号从哪个数开始计数，默认为1 。 lineNumberFormatter: function(line: integer) → string使用一个函数设置行号。 gutters: array用来添加额外的gutter（在行号gutter前或代替行号gutter）。值应该是CSS名称数组，每一项定义了用于绘制gutter背景的宽度（还有可选的背景）。为了能明确设置行号gutter的位置（默认在所有其它gutter的右边），也可以包含CodeMirror-linenumbers类。类名是用于传给setGutterMarker的键名(keys)。 fixedGutter: boolean设置gutter跟随编辑器内容水平滚动（false）还是固定在左侧（true或默认）。 scrollbarStyle: string设置滚动条。默认为”native”，显示原生的滚动条。核心库还提供了”null”样式，此样式会完全隐藏滚动条。Addons可以设置更多的滚动条模式。 coverGutterNextToScrollbar: boolean当fixedGutter启用，并且存在水平滚动条时，在滚动条最左侧默认会显示gutter，当此项设置为true时，gutter会被带有CodeMirror-gutter-filler类的元素遮挡。inputStyle: string选择CodeMirror处理输入和焦点的方式。核心库定义了textarea和contenteditable输入模式。在移动浏览器上，默认是contenteditable，在桌面浏览器上，默认是textarea。在contenteditable模式下对IME和屏幕阅读器支持更好。 readOnly: boolean|string编辑器是否只读。如果设置为预设的值 “nocursor”，那么除了设置只读外，编辑区域还不能获得焦点。 showCursorWhenSelecting: boolean在选择时是否显示光标，默认为false。 lineWiseCopyCut: boolean启用时，如果在复制或剪切时没有选择文本，那么就会自动操作光标所在的整行。 undoDepth: integer最大撤消次数，默认为200（包括选中内容改变事件） 。 historyEventDelay: integer在输入或删除时引发历史事件前的毫秒数。 tabindex: integer编辑器的tabindex。 autofocus: boolean是否在初始化时自动获取焦点。默认情况是关闭的。但是，在使用textarea并且没有明确指定值的时候会被自动设置为true。 dragDrop: boolean是否允许拖放，默认为true。 allowDropFileTypes: array默认为null。当设置此项时，只接收包含在此数组内的文件类型拖入编辑器。文件类型为MIME名称。 cursorBlinkRate: number光标闪动的间隔，单位为毫秒。默认为530。当设置为0时，会禁用光标闪动。负数会隐藏光标。 cursorScrollMargin: number当光标靠近可视区域边界时，光标距离上方和下方的距离。默认为0 。 cursorHeight: number光标高度。默认为1，也就是撑满行高。对一些字体，设置0.85看起来会更好。 resetSelectionOnContextMenu: boolean设置在选择文本外点击打开上下文菜单时，是否将光标移动到点击处。默认为true。 workTime, workDelay: number通过一个假的后台线程高亮 workTime 时长，然后使用 timeout 休息 workDelay 时长。默认为200和300 。（完全不懂这个功能是在说啥） pollInterval: number指明CodeMirror向对应的textarea滚动（写数据）的速度（获得焦点时）。大多数的输入都是通过事件捕获，但是有的输入法（如IME）在某些浏览器上并不会生成事件，所以使用数据滚动。默认为100毫秒。 flattenSpans: boolean默认情况下，CodeMirror会将使用相同class的两个span合并成一个。通过设置此项为false禁用此功能。 addModeClass: boolean当启用时（默认禁用），会给每个标记添加额外的表示生成标记的mode的以cm-m开头的CSS样式类。例如，XML mode产生的标记，会添加cm-m-xml类。 maxHighlightLength: number当需要高亮很长的行时，为了保持响应性能，当到达某些位置时，编辑器会直接将其他行设置为纯文本(plain text)。默认为10000，可以设置为Infinity来关闭此功能。 viewportMargin: integer指定当前滚动到视图中内容上方和下方要渲染的行数。这会影响到滚动时要更新的行数。通常情况下应该使用默认值10。可以设置值为Infinity始终渲染整个文档。注意：这样设置在处理大文档时会影响性能。 如果你要设置代码框的大小该怎么做呢？ 1editor.setSize('800px', '950px'); //设置代码框的长宽 另外，如果你想给代码框赋值，该怎么办呢？ 12editor.setValue(\"\"); //给代码框赋值editor.getValue(); //获取代码框的值 如果你再想在其他地方设置新的属性，可以像下面这样写： 1editor.setOption(\"readOnly\", true); //类似这种 总结上面就大概讲了下 Code Mirror 怎么使用，那么我们来看看效果吧 我自我感觉还是可以的哈！ 里面所有涉及的代码在 GitHub 里可以下载：https://github.com/zhisheng17/CoderBlog/tree/master/CodeMirror 文章原创，转载务必请注明原创地址：http://www.54tianzhisheng.cn/2017/12/09/CodeMirror/ 最后fuck 无脑的推酷爬虫，竟然把我所有文章最后的原创链接都给去掉了，这是我现在想到的一种对策方法。任何其他形式的转载，也必须把我文章所有内容加上，不得做任何修改，否则请别转载了！","tags":[{"name":"前端","slug":"前端","permalink":"http://www.54tianzhisheng.cn/tags/前端/"}]},{"title":"Netty系列文章（一）：Netty 源码阅读之初始环境搭建","date":"2017-12-07T16:00:00.000Z","path":"2017/12/08/netty-01-env/","text":"Netty 简介Netty 是由 JBOSS 提供的一个开源的 java 网络编程框架，主要是对 java 的 nio 包进行了再次封装。Netty 比 java 原生的nio 包提供了更加强大、稳定的功能和易于使用的 api。 netty 的作者是 Trustin Lee，这是一个韩国人，他还开发了另外一个著名的网络编程框架，mina。二者在很多方面都十分相似，它们的线程模型也是基本一致 。不过 netty 社区的活跃程度要 mina 高得多。版本选择： 3.x 目前企业使用最多的版本，最为稳定。例如dubbo使用的就是3.x版本 4.x 引入了内存池等重大特性，可以有效的降低GC负载，rocketmq使用的就是4.x 5.x 已经被废弃了，具体可参见 https://github.com/netty/netty/issues/4466 所以这里我搭建的源码阅读环境是存在的 4.1 版本。 准备工具 IDEA 2017 环境搭建在 IDEA 中导入项目地址：https://github.com/netty/netty.git ，然后就会自动下载项目所有的依赖，但是请注意： 必须在 IDEA 中将 Profiles 中的所有都勾选上，否则会导致很多 jar 包拉不下来，如下图： 然后就是耐心等待了，一直到所有的 jar 包拉取下来。 中途你可能会遇到如下问题： 这里的是 1.5 版本，导致我们如果想用些高级的语法会完全报错。 如果你把这个版本设置为 8 的版本后， 下面会提示你，项目是从 maven 导过来的，如果 maven 配置改变重新 reimport 后，任何在这里的改变都会丢失。 同时你会看到项目的 Java Compile 版本是 1.5 的，如下图： 同样，你在这里修改，如果 maven 配置改变重新 reimport 后，任何在这里的改变也都会丢失。我估计碰到这种问题的不少。 总结起来原因就是 maven 中的编译版本就是 1.5 的，所以才会导致这里的问题发生，如果想完全修改好（一劳永逸）。请直接对 pom 文件动刀，就是干！ 只需把大项目（netty-parent）的那个 pom.xml 修改个属性，把版本信息提高到 1.8。 在等待它拉取 jar 包吧 搞完了之后发现还有两个模块（netty-bom、netty-dev-tools）不能设置到 版本，只能手动的和上面那种设置 language level 和 Java compile 为 1.8 了。 最后你会发现这里的完全没有报错了，开心不？ 代码行数统计额，看到项目这么多子模块，你都不知道该从哪里下手开始看，那么我就写了个简单的 Java 脚本去大概的统计每个子项目代码的行数。先看看统计结果： 整个项目差不多 23 万。（过滤了空行、各种注释和 @Override 之后的 Java 代码行数），靠这个数字很吓人！ 来看看我的脚本代码吧： 123456789101112131415public static void main(String[] args) throws Exception &#123; long count = Files.walk(Paths.get(\"C:\\\\JetBrains\\\\IDEAProject\\\\netty\\\\transport-udt\")) // 递归获得项目目录下的所有文件 .filter(file -&gt; !Files.isDirectory(file)) // 筛选出文件 .filter(file -&gt; file.toString().endsWith(\".java\")) // 筛选出 java 文件 .flatMap(Try.of(file -&gt; Files.lines(file), Stream.empty())) // 将会抛出受检异常的 Lambda 包装为 抛出非受检异常的 Lambda .filter(line -&gt; !line.trim().isEmpty()) // 过滤掉空行 .filter(line -&gt; !line.trim().startsWith(\"//\")) //过滤掉 //之类的注释 .filter(line -&gt; !(line.trim().startsWith(\"/*\") &amp;&amp; line.trim().endsWith(\"*/\"))) //过滤掉/* */之类的注释 .filter(line -&gt; !(line.trim().startsWith(\"/*\") &amp;&amp; !line.trim().endsWith(\"*/\"))) //过滤掉以 /* 开头的注释（去除空格后的开头） .filter(line -&gt; !(!line.trim().startsWith(\"/*\") &amp;&amp; line.trim().endsWith(\"*/\"))) //过滤掉已 */ 结尾的注释 .filter(line -&gt; !line.trim().startsWith(\"*\")) //过滤掉 javadoc 中的文字注释 .filter(line -&gt; !line.trim().startsWith(\"@Override\")) //过滤掉方法上含 @Override 的 .count(); System.out.println(\"代码行数：\" + count);&#125; 后面我会把我阅读源码的中文注释及解析之类的更新到我的 GitHub 去（欢迎关注、我是来骗 star 的），https://github.com/zhisheng17/netty ，如果你不想去自己设置上面所说的这些（偷懒），那就直接 fork 我的这份吧！ 最后环境搭建就写到这里了，转载请注明地址：http://www.54tianzhisheng.cn/2017/12/08/netty-01-env/","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Netty","slug":"Netty","permalink":"http://www.54tianzhisheng.cn/tags/Netty/"}]},{"title":"RestTemplate 详解","date":"2017-12-02T16:00:00.000Z","path":"2017/12/03/RestTemplate/","text":"背景这段时间自己做的项目中需要调用服务提供者的服务（接口），具体就是：我这边需要将页面所输入的 Groovy 脚本代码传给别人提供的服务接口，然后那边返回脚本编译的结果给我，我需要将编译结果展示在页面，用的就是 RestTemplate 了，那 RestTemplate 是什么呢？简单说就是：简化了发起 HTTP 请求以及处理响应的过程，并且支持 REST 。下文就稍微总结下。 如何使用先讲讲如何使用吧，我项目是 SpringBoot 项目，可以在启动类中加入： 1234@Beanpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; 然后在 Controller 层中引入： 12@Autowiredprivate RestTemplate restTemplate; 接下来就可以在 Controller 中各个方法中使用 restTemplate 了，但是 restTemplate 里面有什么方法呢？ RestTemplate 内部方法 从图中 RestTemplate 可以看到有很多方法，我们可以提取出主要的几种方法是： GET POST PUT DELETE HEAD OPTIONS EXCHANGE EXECUTE 图片中依然可以知道 RestTemplate 类中的方法主要是来自接口 RestOperations，下面我们具体看看这些方法里面的具体实现与该如何使用。 Get 方法在 RestTemplate 中，发送一个 GET 请求，我们可以通过如下两种方式： getForEntity getForEntity 方法的返回值是一个ResponseEntity&lt;T&gt;，ResponseEntity&lt;T&gt;是 Spring 对 HTTP 请求响应的封装，包括了几个重要的元素，如响应码、contentType、contentLength、响应消息体等。比如下面一个例子： 1234567891011121314@RequestMapping(\"/gethello\")public String getHello() &#123; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/hello\", String.class); String body = responseEntity.getBody(); HttpStatus statusCode = responseEntity.getStatusCode(); int statusCodeValue = responseEntity.getStatusCodeValue(); HttpHeaders headers = responseEntity.getHeaders(); StringBuffer result = new StringBuffer(); result.append(\"responseEntity.getBody()：\").append(body).append(\"&lt;hr&gt;\") .append(\"responseEntity.getStatusCode()：\").append(statusCode).append(\"&lt;hr&gt;\") .append(\"responseEntity.getStatusCodeValue()：\").append(statusCodeValue).append(\"&lt;hr&gt;\") .append(\"responseEntity.getHeaders()：\").append(headers).append(\"&lt;hr&gt;\"); return result.toString();&#125; 关于这段代码，说如下几点： getForEntity 的第一个参数为我要调用的服务的地址，这里我调用了服务提供者提供的 /hello 接口，注意这里是通过服务名调用而不是服务地址，如果写成服务地址就没法实现客户端负载均衡了。（备注：我项目中需要通过 ConsulClient 去获取服务名，然后在去获取服务的 IP 和 Port，并把它拼接起来组合成我的服务地址，所以就没法实现客户端的负载均衡了，如果要是实现负载均衡，可以在 SpringBoot 启动类的中加入注解 @LoadBalanced, 如下: 12345@Bean@LoadBalancedpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; ） getForEntity 第二个参数 String.class 表示我希望返回的 body 类型是 String 拿到返回结果之后，将返回结果遍历打印出来 有时候我在调用服务提供者提供的接口时，可能需要传递参数，有两种不同的方式: 123456789101112@RequestMapping(\"/sayhello\")public String sayHello() &#123; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/sayhello?name=&#123;1&#125;\", String.class, \"张三\"); return responseEntity.getBody();&#125;@RequestMapping(\"/sayhello2\")public String sayHello2() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", \"李四\"); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/sayhello?name=&#123;name&#125;\", String.class, map); return responseEntity.getBody();&#125; 可以用一个数字做占位符，最后是一个可变长度的参数，来一 一替换前面的占位符 也可以前面使用 name={name} 这种形式，最后一个参数是一个 map，map 的 key 即为前边占位符的名字，map的 value 为参数值 第一个调用地址也可以是一个URI而不是字符串，这个时候我们构建一个URI即可，参数神马的都包含在URI中了，如下： 1234567@RequestMapping(\"/sayhello3\")public String sayHello3() &#123; UriComponents uriComponents = UriComponentsBuilder.fromUriString(\"http://HELLO-SERVICE/sayhello?name=&#123;name&#125;\").build().expand(\"王五\").encode(); URI uri = uriComponents.toUri(); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(uri, String.class); return responseEntity.getBody();&#125; 通过Spring中提供的UriComponents来构建Uri即可。 当然，服务提供者不仅可以返回String，也可以返回一个自定义类型的对象，比如我的服务提供者中有如下方法： 1234@RequestMapping(value = \"/getbook1\", method = RequestMethod.GET)public Book book1() &#123; return new Book(\"三国演义\", 90, \"罗贯中\", \"花城出版社\");&#125; 对于该方法我可以在服务消费者中通过如下方式来调用： 12345@RequestMapping(\"/book1\")public Book book1() &#123; ResponseEntity&lt;Book&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/getbook1\", Book.class); return responseEntity.getBody();&#125; 运行结果如下： getForObject ​ getForObject 函数实际上是对 getForEntity 函数的进一步封装，如果你只关注返回的消息体的内容，对其他信息都不关注，此时可以使用 getForObject，举一个简单的例子，如下： 12345@RequestMapping(\"/book2\")public Book book2() &#123; Book book = restTemplate.getForObject(\"http://HELLO-SERVICE/getbook1\", Book.class); return book;&#125; POST 方法在 RestTemplate 中，POST 请求可以通过如下三个方法来发起： postForEntity 该方法和get请求中的getForEntity方法类似，如下例子： 1234567@RequestMapping(\"/book3\")public Book book3() &#123; Book book = new Book(); book.setName(\"红楼梦\"); ResponseEntity&lt;Book&gt; responseEntity = restTemplate.postForEntity(\"http://HELLO-SERVICE/getbook2\", book, Book.class); return responseEntity.getBody();&#125; 方法的第一参数表示要调用的服务的地址 方法的第二个参数表示上传的参数 方法的第三个参数表示返回的消息体的数据类型 我这里创建了一个Book对象，这个Book对象只有name属性有值，将之传递到服务提供者那里去，服务提供者代码如下： 12345678@RequestMapping(value = \"/getbook2\", method = RequestMethod.POST)public Book book2(@RequestBody Book book) &#123; System.out.println(book.getName()); book.setPrice(33); book.setAuthor(\"曹雪芹\"); book.setPublisher(\"人民文学出版社\"); return book;&#125; 服务提供者接收到服务消费者传来的参数book，给其他属性设置上值再返回，调用结果如下： postForObject 如果你只关注，返回的消息体，可以直接使用postForObject。用法和getForObject一致。 postForLocation postForLocation 也是提交新资源，提交成功之后，返回新资源的 URI，postForLocation 的参数和前面两种的参数基本一致，只不过该方法的返回值为 URI ，这个只需要服务提供者返回一个 URI 即可，该 URI 表示新资源的位置。 PUT 方法 在 RestTemplate 中，PUT 请求可以通过 put 方法调用，put 方法的参数和前面介绍的 postForEntity 方法的参数基本一致，只是 put 方法没有返回值而已。举一个简单的例子，如下： 123456@RequestMapping(\"/put\")public void put() &#123; Book book = new Book(); book.setName(\"红楼梦\"); restTemplate.put(\"http://HELLO-SERVICE/getbook3/&#123;1&#125;\", book, 99);&#125; book对象是我要提交的参数，最后的99用来替换前面的占位符{1} DELETE 方法 delete 请求我们可以通过 delete 方法调用来实现，如下例子： 1234@RequestMapping(\"/delete\")public void delete() &#123; restTemplate.delete(\"http://HELLO-SERVICE/getbook4/&#123;1&#125;\", 100);&#125; HEADER 方法 返回资源的所有 HTTP headers。 OPTIONS 问可以执行哪些方法。 EXCHANGE 与其它接口的不同： 允许调用者指定HTTP请求的方法（GET,POST,PUT等） 可以在请求中增加body以及头信息，其内容通过参数 HttpEntity&lt;?&gt;requestEntity 描述 exchange支持‘含参数的类型’（即泛型类）作为返回类型，该特性通过 ParameterizedTypeReferenceresponseType 描述 EXECUTE细心的你，不知道有没有发现上面所有的方法内部返回值都调用了同一个方法 —— execute 方法。 下面我们来看看： 可以看到，Excute方法只是将 String 格式的 URI 转成了 java.net.URI，之后调用了doExecute方法。整个调用过程关键起作用的是 doExecute 方法 doExecute 方法 这里需要了解两个类： RequestCallback 和 ResponseExtractor RestTemplate 类中可以看到他们两的实现类。 RequestCallback ：用于操作请求头和body，在请求发出前执行。 该接口有两个实现类： AcceptHeaderRequestCallback 只处理请求头，用于getXXX()方法。 HttpEntityRequestCallback 继承于AcceptHeaderRequestCallback可以处理请求头和body，用于putXXX()、postXXX()和exchange()方法。 ResponseExtractor：解析HTTP响应的数据，而且不需要担心异常和资源的关闭 上面图纸这个实现类 ResponseEntityResponseExtractor 的作用是：使用 HttpMessageConverterExtractor 提取 body（委托模式），然后将 body 和响应头、状态封装成 ResponseEntity 对象。 最后转载请注明地址：http://www.54tianzhisheng.cn/2017/12/03/RestTemplate/ 参考资料1、https://www.cnblogs.com/caolei1108/p/6169950.html 2、https://segmentfault.com/a/1190000011093597 如果想和我进一步交流请关注：","tags":[{"name":"Spring","slug":"Spring","permalink":"http://www.54tianzhisheng.cn/tags/Spring/"}]},{"title":"实习圈群里提问小记","date":"2017-12-01T16:00:00.000Z","path":"2017/12/02/wx-01/","text":"","tags":[{"name":"实习圈","slug":"实习圈","permalink":"http://www.54tianzhisheng.cn/tags/实习圈/"}]},{"title":"Docker系列文章（一）：基于 Harbor 搭建 Docker 私有镜像仓库","date":"2017-11-25T16:00:00.000Z","path":"2017/11/26/Docker-harbor/","text":"什么是 Harbor？第一次使用这个的时候是刚进公司处理的第一个任务的时候，发现 Harbor 就是一个用于存储和分发 Docker 镜像的企业级Registry 服务器。网上找到一个 Harbor 的架构图： Harbor 是 VMware 公司开源的企业级 DockerRegistry 项目，项目地址为 https://github.com/vmware/harbor。其目标是帮助用户迅速搭建一个企业级的 Docker registry 服务。它以 Docker 公司开源的 registry 为基础，提供了管理UI，基于角色的访问控制(Role Based Access Control)，AD/LDAP集成、以及审计日志(Auditlogging) 等企业用户需求的功能，同时还原生支持中文。Harbor 的每个组件都是以 Docker 容器的形式构建的，使用 Docker Compose 来对它进行部署。 环境准备1、自己在腾讯云买的服务器（CentOS7.3） 2、Docker 版本：17.05.0-ce 3、Docker-compose：1.17.1 4、Harbor：1.1.2 安装 Docker因为系统是 CentOS 7.3 ，内核啥的都已经是 3.10，所以不用担心内核升级的问题，一些操作啥的在 7.x 上操作也很方便。 1234567891011121314151617181920212223yum update //系统版本更新vim /etc/yum.repos.d/docker.repo //添加以下内容[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpg//下面安装 Docker 引擎yum install docker-engine -y//安装docker引擎，此步也可作为更新docker版本的操作：先#systemctl stop docker 停止docker服务，再#yum install docker-engine 更新docker版本systemctl enable docker.servicesystemctl start docker //启动docker守护进程docker info //查看docker运行情况docker -v //查看版本信息 修改 Docker 配置文件 /etc/default/docker 如下： 1DOCKER_OPTS=\"--registry-mirror=http://aad0405c.m.daocloud.io\" //换成国内的镜像加速源，不然拉取镜像简直龟速，不想在吐槽了 使用 service docker restart 重启 Docker 服务即可。 或者用官方提供的方式： 1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://ef017c13.m.daocloud.io 安装 Docker-compose如果是想直接命令安装也行， 1234567891011121314下载指定版本的docker-composesudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose对二进制文件赋可执行权限chmod +x /usr/local/bin/docker-compose测试下docker-compose是否安装成功docker-compose --version出现如下docker-compose version 1.17.1, build 6d101fb 但是，这种方法简直龟速，幸好还有种方法， 见这里：https://docs.docker.com/compose/install/#install-compose 这种需要通过 Python 的 pip 安装 安装 pip123456789wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gztar -vxf setuptools-1.4.2.tar.gzcd setuptools-1.4.2python2.7 setup.py install //因为服务器自带 Python 2.7easy_install-2.7 pip 安装 docker compose123pip install docker-composedocker-compose --version //测试安装是否成功 安装 Harbor12345wget https://github.com/vmware/harbor/releases/download/v1.1.2/harbor-offline-installer-v1.1.2.tgz离线安装包,也是龟速，把这个下载链接用迅雷下载，速度却贼快，嘿嘿，然后再传到服务器上去，整个过程快很多！tar -zxvf harbor-offline-installer-v1.1.2.tgz 解压缩之后，进入目录下会看到 harbor.cfg 文件，该文件就是 Harbor 的配置文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495## Configuration file of Harbor# hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhosthostname = 115.159.227.249 #这里我先配置我的服务器IP地址# 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置onui_url_protocol = http# mysql数据库root用户默认密码root123，实际使用时修改下db_password = root123#Maximum number of job workers in job servicemax_job_workers = 3#Determine whether or not to generate certificate for the registry&apos;s token.#If the value is on, the prepare script creates new root cert and private key#for generating token to access the registry. If the value is off the default key/cert will be used.#This flag also controls the creation of the notary signer&apos;s cert.customize_crt = on#The path of cert and key files for nginx, they are applied only the protocol is set to httpsssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.key#The path of secretkey storagesecretkey_path = /data#Admiral&apos;s url, comment this attribute, or set its value to NA when Harbor is standaloneadmiral_url = NA#NOTES: The properties between BEGIN INITIAL PROPERTIES and END INITIAL PROPERTIES#only take effect in the first boot, the subsequent changes of these properties#should be performed on web ui#************************BEGIN INITIAL PROPERTIES************************#Email account settings for sending out password resetting emails.#Email server uses the given username and password to authenticate on TLS connections to host and act as identity.#Identity left blank to act as username.email_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = false##The initial password of Harbor admin, only works for the first time when Harbor starts.#It has no effect after the first launch of Harbor.# 启动Harbor后，管理员UI登录的密码，默认是Harbor12345harbor_admin_password = Harbor12345# 认证方式，这里支持多种认证方式，如LADP、本次存储、数据库认证。默认是db_auth，mysql数据库认证auth_mode = db_auth#The url for an ldap endpoint.ldap_url = ldaps://ldap.mydomain.com#A user&apos;s DN who has the permission to search the LDAP/AD server.#If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd.#ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com#the password of the ldap_searchdn#ldap_search_pwd = password#The base DN from which to look up a user in LDAP/ADldap_basedn = ou=people,dc=mydomain,dc=com#Search filter for LDAP/AD, make sure the syntax of the filter is correct.#ldap_filter = (objectClass=person)# The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/AD ldap_uid = uid#the scope to search for users, 1-LDAP_SCOPE_BASE, 2-LDAP_SCOPE_ONELEVEL, 3-LDAP_SCOPE_SUBTREEldap_scope = 3#Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds.ldap_timeout = 5# 是否开启自注册self_registration = on# Token有效时间，默认30分钟token_expiration = 30# 用户创建项目权限控制，默认是everyone（所有人），也可以设置为adminonly（只能管理员）project_creation_restriction = everyone#Determine whether the job service should verify the ssl cert when it connects to a remote registry.#Set this flag to off when the remote registry uses a self-signed or untrusted certificate.verify_remote_cert = on#************************END INITIAL PROPERTIES************************ 启动 harbor，修改完配置文件后，在的当前目录执行./install.sh，Harbor服务就会根据当期目录下的docker-compose.yml开始下载依赖的镜像，检测并按照顺序依次启动各个服务。 启动完成后，我们访问刚设置的 hostname 即可，http://115.159.227.249/，默认是80端口，如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。 登录 Web Harbor , 输入用户名 admin，默认密码（或已修改密码）登录系统。 我们可以看到系统各个模块如下： 项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等 日志：仓库各个镜像create、push、pull等操作日志 系统管理 用户管理：新增/删除用户、设置管理员等 复制管理：新增/删除从库目标、新建/删除/启停复制规则等 配置管理：认证模式、复制、邮箱设置、系统设置等 其他设置 用户设置：修改用户名、邮箱、名称信息 修改密码：修改用户密码 注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。 我们要尝试下能不能把自己 Docker 里面的镜像 push 到 Harbor 的 library 里来（默认这个 library 项目是公开的，所有人都可以有读的权限，都不需要 docker login 进来，就可以拉取里面的镜像）。 注意： 为了后面留坑，我这里先 在自己的 docker.service 中添加仓库：（这是个坑，建议你先按照我说的做，不然下面可能会一直登录不上） 1234vim /usr/lib/systemd/system/docker.service里面的这行修改为：（其实就是添加 --insecure-registry 115.159.227.249 ）ExecStart=/usr/bin/dockerd --insecure-registry 115.159.227.249 添加完了后重新启动 docker： 1systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl start docker 启动 docker 服务: 1service docker start 登录：（为了测试下能否登录成功） 12345admin登录$ docker login 115.159.227.249Username: adminPassword:Login Succeeded 打 tag 并 push 12345678910docker tag ubuntu:15.10 115.159.227.249/library/ubuntu:15.10 //给我的镜像打个 tagdocker push 115.159.227.249/library/ubuntuThe push refers to a repository [115.159.227.249/library/ubuntu]98d59071f692: Pushedaf288f00b8a7: Pushed4b955941a4d0: Pushedf121afdbbd5d: Pushed15.10: digest: sha256:ec89c4a90f45f5e103860191890f48d8379e0504a2881ff706aef0768dc0321b size: 1150 上传完毕后，登录Web Harbor，选择项目 library，就可以看到我刚 push 的镜像了。 同理，你也可以测试下从 Harbor pull 镜像到你的 Docker 中去，这里就不继续演示了。 最后转载请注明地址为：http://www.54tianzhisheng.cn/2017/11/26/Docker-harbor/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.54tianzhisheng.cn/tags/Docker/"}]},{"title":"谈谈我的理财","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/Money-management/","text":"背景最开始接触理财是去年的时候，在我的一个群里（几个好朋友），有个朋友他女朋友是学金融的，当时还开玩笑地说叫她带带我们怎么买股票、基金、黄金这些东西呢。后来在群里偶尔聊下这方面的东西！ 黄金那时我第才开始接触黄金，也是自己一个人买了点支付宝里面的存金宝，（不多，就几百块），后来慢慢的加仓和减仓，刚开始的时候，因为是刚上手这些东西，比较对这每天的数字增长和降低很在意，一天打开蚂蚁聚宝的次数很多，老是看着每天的实时参考金价，反正就是心理各种不踏实。就是那种患得患失的感觉，哈哈，我也不知道怎么形容了。。。😳 那时也不懂，老是追涨低抛，没有打算长期持有。附图： 现在看看这图，想想自己以前真傻。可以发现现在已经没买黄金了，对，在今年的时候主要是关注些基金！ 基金通过买黄金，发现，黄金增长下跌确实不怎么那么尽人意，只赚了一点点。然后慢慢在关注着基金，发现有些基金的还是收入效果还是很好的。 什么是基金呢？ 好像有很多种，但是我的理解是，我们散户有点闲钱，打算投资，又没时间去买股票，一是缺乏经验，不知道买哪支，而是没空去整天盯着股票的走势。这时就出现了一个平台，我们散户把钱投给这个平台，平台有专业的人去进行买股票，如买股票有盈利，则大家一起赚钱，如果亏，则一起亏，我对基金就是这样的理解，也不知道对不对？ 再说说我现在主要买的基金吧，看看收益图： 嘿嘿，这几个是我观察很久了，并觉得长期看好的基金了，当然以前也买过一两个基金，有个亏得不少，有涨有跌，现在觉得又要平常心，如果是长期看好的，没必要纠结这一两天的涨跌，等过段时间再来看看效果咋样（心态一定要好），如果遇到被套的话，有时也需要装死心态，哈哈！另外，我还自选了一批基金，正在观察中，等有时间把觉得还行的给统计下！ 股票这个不太懂，不过目前觉得我自己公司的股票也还不错，打算看什么时候有机会买点，这东西都是靠自己慢慢研究出来的，然后就是看看高人指点，我倒是关注了点微信公众号讲这方面的知识，微博也关注了几个，在蚂蚁财富里面也关注些，觉得有些还是很靠谱的，还是一样，有时间继续做个统计，然后发在我的小密圈里面。 友金所投了这个是因为进 stormzhang 的小密圈，通过注册并投资点可以免费进他的小密圈，不然得花 199 元，这算很优惠了。因为是新用户，所以这个收益很高，一个月期限，12% 的收益！ 余额宝支付宝里的，平常钱也一般放这里面，因为平时用支付宝比较多，放这里，可以付款的时候选择直接余额宝支付，另外还有差不多 4% 的收益，可以随时存取，比较方便！ 最后有时间将上面所说的：自选基金列表、微信公众号、微博这几个列表发在我的小密圈里。 最重要的话还是得说三遍： 投资有风险，需谨慎！ 投资有风险，需谨慎！ 投资有风险，需谨慎！","tags":[{"name":"投资理财","slug":"投资理财","permalink":"http://www.54tianzhisheng.cn/tags/投资理财/"}]},{"title":"基于分布式环境下限流系统的设计","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/flow-control/","text":"前提业务背景就拿前些天的双十一的 “抢券活动” 来说，一般是设置整点开始抢的，你想想，淘宝的用户群体非常大，可以达到亿级别，而服务接口每秒能处理的量是有限的，那么这个时候问题就会出现，我们如何通过程序来控制用户抢券呢，于是就必须加上这个限流功能了。 生产环境1、服务接口所能提供的服务上限（limit）假如是 500次/s 2、用户请求接口的次数未知，QPS可能达到 800次/s，1000次/s，或者更高 3、当服务接口的访问频率超过 500次/s，超过的量将拒绝服务，多出的信息将会丢失 4、线上环境是多节点部署的，但是调用的是同一个服务接口 于是，为了保证服务的可用性，就要对服务接口调用的速率进行限制（接口限流）。 什么是限流？限流是对系统的出入流量进行控制，防止大流量出入，导致资源不足，系统不稳定。 限流系统是对资源访问的控制组件，控制主要的两个功能：限流策略和熔断策略，对于熔断策略，不同的系统有不同的熔断策略诉求，有的系统希望直接拒绝、有的系统希望排队等待、有的系统希望服务降级、有的系统会定制自己的熔断策略，这里只针对限流策略这个功能做详细的设计。 限流算法1、限制瞬时并发数Guava RateLimiter 提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。 2、限制某个接口的时间窗最大请求数即一个时间窗口内的请求数，如想限制某个接口/服务每秒/每分钟/每天的请求数/调用量。如一些基础服务会被很多其他系统调用，比如商品详情页服务会调用基础商品服务调用，但是怕因为更新量比较大将基础服务打挂，这时我们要对每秒/每分钟的调用量进行限速；一种实现方式如下所示： 12345678910111213141516171819LoadingCache&lt;Long, AtomicLong&gt; counter = CacheBuilder.newBuilder() .expireAfterWrite(2, TimeUnit.SECONDS) .build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long seconds) throws Exception &#123; return new AtomicLong(0); &#125; &#125;);long limit = 1000;while(true) &#123; //得到当前秒 long currentSeconds = System.currentTimeMillis() / 1000; if(counter.get(currentSeconds).incrementAndGet() &gt; limit) &#123; System.out.println(\"限流了:\" + currentSeconds); continue; &#125; //业务处理&#125; 使用Guava的Cache来存储计数器，过期时间设置为2秒（保证1秒内的计数器是有的），然后我们获取当前时间戳然后取秒数来作为KEY进行计数统计和限流，这种方式也是简单粗暴，刚才说的场景够用了。 3、令牌桶 算法描述： 假如用户配置的平均发送速率为r，则每隔1/r秒一个令牌被加入到桶中 假设桶中最多可以存放b个令牌。如果令牌到达时令牌桶已经满了，那么这个令牌会被丢弃 当流量以速率v进入，从桶中以速率v取令牌，拿到令牌的流量通过，拿不到令牌流量不通过，执行熔断逻辑 属性 长期来看，符合流量的速率是受到令牌添加速率的影响，被稳定为：r 因为令牌桶有一定的存储量，可以抵挡一定的流量突发情况 M是以字节/秒为单位的最大可能传输速率。 M&gt;r T max = b/(M-r) 承受最大传输速率的时间 B max = T max * M 承受最大传输速率的时间内传输的流量 优点：流量比较平滑，并且可以抵挡一定的流量突发情况 4、Google guava 提供的工具库中 RateLimiter 类（内部也是采用令牌桶算法实现）最快的方式是使用 RateLimit 类，但是这仅限制在单节点，如果是分布式系统，每个节点的 QPS 是一样的，请求量到服务接口那的话就是 QPS * 节点数 了。所以这种方案在分布式的情况下不适用！ 5、基于 Redis 实现，存储两个 key，一个用于计时，一个用于计数。请求每调用一次，计数器增加 1，若在计时器时间内计数器未超过阈值，则可以处理任务。这种能够很好地解决了分布式环境下多实例所导致的并发问题。因为使用redis设置的计时器和计数器均是全局唯一的，不管多少个节点，它们使用的都是同样的计时器和计数器，因此可以做到非常精准的流控。 代码就不公布了，毕竟涉及公司隐私了。 最后参考文章： 基于Redis的限流系统的设计 感兴趣的可以看看别人的代码是怎么写的：https://github.com/wukq/rate-limiter 转载请注明文章地址为：http://www.54tianzhisheng.cn/2017/11/18/flow-control/","tags":[{"name":"Redis","slug":"Redis","permalink":"http://www.54tianzhisheng.cn/tags/Redis/"},{"name":"流控","slug":"流控","permalink":"http://www.54tianzhisheng.cn/tags/流控/"}]},{"title":"Maven 中 dependencies 与 dependencyManagement 的区别","date":"2017-11-10T16:00:00.000Z","path":"2017/11/11/Maven-dependencies-dependencyManagement/","text":"前提这段时间项目中遇到过了一些 Jar 包冲突的问题，很多是由于我们项目模块很多的时候，用 Maven 管理不当导致的冲突问题，本文就这个问题参考网上的资料，于是总结下 Maven 中 dependencies 与 dependencyManagement 的区别。 假设项目结构如下： parent 为父模块，抽象出来管理子项目的公共依赖，为了项目的正确运行，必须让所有的子项目使用依赖项的统一版本，必须确保应用的各个项目的依赖项和版本一致，才能保证测试的和发布的是相同的结果。 dependencyManagement在项目的 parent 层，可以通过 dependencyManagement 元素来管理 jar 包的版本，让子项目中引用一个依赖而不用显示的列出版本号。 parent 中 pom.xml 123456789101112131415161718&lt;properties&gt; &lt;version.framework&gt;1.0-SNAPSHOT&lt;/version.framework&gt; &lt;javaee-api.version&gt;1.0-SNAPSHOT&lt;/javaee-api.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;framework-cache&lt;/artifactId&gt; &lt;version&gt;$&#123;version.framework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;$&#123;javaee-api.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; extendion 中的 pom.xml 123456789101112131415161718192021&lt;parent&gt; &lt;artifactId&gt;parent&lt;/artifactId&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../parent/pom.xml&lt;/relativePath&gt;&lt;/parent&gt;&lt;!--依赖关系--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;framework-cache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 这样做的好处：统一管理项目的版本号，确保应用的各个项目的依赖和版本一致，才能保证测试的和发布的是相同的成果，因此，在顶层 pom 中定义共同的依赖关系。同时可以避免在每个使用的子项目中都声明一个版本号，这样想升级或者切换到另一个版本时，只需要在父类容器里更新，不需要任何一个子项目的修改；如果某个子项目需要另外一个版本号时，只需要在 dependencies 中声明一个版本号即可。子类就会使用子类声明的版本号，不继承于父类版本号。 我们知道 Maven 的继承和 Java 的继承一样，是无法实现多重继承的，如果10个、20个甚至更多模块继承自同一个模块，那么按照我们之前的做法，这个父模块的 dependencyManagement 会包含大量的依赖。如果你想把这些依赖分类以更清晰的管理，那就不可能了，import scope 依赖能解决这个问题。你可以把 dependencyManagement 放到单独的专门用来管理依赖的 POM 中，然后在需要使用依赖的模块中通过 import scope 依赖，就可以引入dependencyManagement。例如可以写这样一个用于依赖管理的 POM： 12345678910111213141516171819202122&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng.sample&lt;/groupId&gt; &lt;artifactId&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactid&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.8.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactid&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 然后就可以通过非继承的方式来引入这段依赖管理配置： 1234567891011121314151617181920&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng.sample&lt;/groupId&gt; &lt;artifactid&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactid&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactid&gt;log4j&lt;/artifactId&gt; &lt;/dependency&gt; 这样，父模块的 POM 就会非常干净，由专门的 packaging 为 pom 的 POM 来管理依赖，也契合的面向对象设计中的单一职责原则。此外，我们还能够创建多个这样的依赖管理 POM，以更细化的方式管理依赖。这种做法与面向对象设计中使用组合而非继承也有点相似的味道。 dependencies相对于 dependencyManagement，所有声明在父项目中 dependencies 里的依赖都会被子项目自动引入，并默认被所有的子项目继承。 区别 dependencies 即使在子项目中不写该依赖项，那么子项目仍然会从父项目中继承该依赖项（全部继承） dependencyManagement 里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且 version 和 scope 都读取自父 pom; 另外如果子项目中指定了版本号，那么会使用子项目中指定的jar版本。 消除多模块插件配置重复与 dependencyManagement 类似的，我们也可以使用 pluginManagement 元素管理插件。一个常见的用法就是我们希望项目所有模块的使用 Maven Compiler Plugin 的时候，都使用 Java 1.8，以及指定 Java 源文件编码为 UTF-8，这时可以在父模块的 POM 中如下配置 pluginManagement： 12345678910111213141516&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 这段配置会被应用到所有子模块的 maven-compiler-plugin 中，由于 Maven 内置了 maven-compiler-plugin 与生命周期的绑定，因此子模块就不再需要任何 maven-compiler-plugin 的配置了。 与依赖配置不同的是，通常所有项目对于任意一个依赖的配置都应该是统一的，但插件却不是这样，例如你可以希望模块 A 运行所有单元测试，模块 B 要跳过一些测试，这时就需要配置 maven-surefire-plugin 来实现，那样两个模块的插件配置就不一致了。这也就是说，简单的把插件配置提取到父 POM 的 pluginManagement 中往往不适合所有情况，那我们在使用的时候就需要注意了，只有那些普适的插件配置才应该使用 pluginManagement 提取到父 POM 中。 关于插件 pluginManagement，Maven 并没有提供与 import scope 依赖类似的方式管理，那我们只能借助继承关系，不过好在一般来说插件配置的数量远没有依赖配置那么多，因此这也不是一个问题。 最后你看到的只是冰山一角，更多请看书籍《Maven 实战》 。 转载请注明地址：http://www.54tianzhisheng.cn/2017/11/11/Maven-dependencies-dependencyManagement/","tags":[{"name":"Maven","slug":"Maven","permalink":"http://www.54tianzhisheng.cn/tags/Maven/"}]},{"title":"送你一份双十一剁手书单【墙裂推荐】","date":"2017-11-10T16:00:00.000Z","path":"2017/11/11/recommended-books/","text":"昨晚和朋友聊天说双十一都买啥，结果 TMD 竟然都是书籍，不愧是标准的程序猿。所以这里想推荐基佬的一份书单，方便大家在双十一剁剁剁，建议先收藏本书单，认真啃完一本再买下一本，扎实走完每一步，希望书单能在你想要进一步打怪升级的路上，给予些许帮助！ 书籍列表《Effective Java 中文版》 豆瓣评分：9.1【1235 人评价】 推荐理由：本书介绍了在Java编程中78条极具实用价值的经验规则，这些经验规则涵盖了大多数开发人员每天所面临的问题的解决方案。 友情提示：同推荐《重构 : 改善既有代码的设计》、《代码整洁之道》、《代码大全》，有一定的内容重叠。 《Java性能权威指南》 豆瓣评分：8.2【44 人评价】 推荐理由：市面上介绍Java的书有很多，但专注于Java性能的并不多，能游刃有余地展示Java性能优化难点的更是凤毛麟角，本书即是其中之一。通过使用JVM和Java平台，以及Java语言和应用程序接口，本书详尽讲解了Java性能调优的相关知识，帮助读者深入理解Java平台性能的各个方面，最终使程序如虎添翼。 《Spring揭秘》 豆瓣评分：9.0 【162 人评价】 推荐理由：Spring 使用者不得不读！ 推荐博客：Spring4All社区 推荐公众号：Spring4All社区 《SpringBoot揭秘》 豆瓣评分：6.8 【44 人评价】 推荐理由：《Spring揭秘》相同作者。SpringBoot 入门书籍。 作者博客：扶墙老师说：一个架构士的思考与沉淀 作者公众号：扶墙老师说 付费教程：《Java 微服务实践 - Spring Boot 系列》 《MyBatis技术内幕》 豆瓣评分：暂无 推荐理由：以MyBatis 3.4为基础，针对MyBatis的架构设计和实现细节进行了详细分析，其中穿插介绍了MyBatis源码中涉及的基础知识、设计模式以及笔者自己在实践中的思考。 作者博客：祖大俊的博客 《有效的单元测试》 豆瓣评分：7.4 【18 人评价】 推荐理由：Java 单元测试入门。 《Java并发编程实战》 豆瓣评分：9.0 【651 人评价】 推荐理由：本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。 推荐博客：并发编程网 推荐公众号：并发编程网 《Netty实战》 豆瓣评分：7.5【24 人评价】 豆瓣评分：8.1【83 人评价】 《Netty in Action》英文版 推荐理由：Netty之父”Trustin Lee作序推荐。 推荐公众号：Netty之家 《深入剖析Tomcat》 豆瓣评分：8.3【118 人评价】 豆瓣评分：8.9【73 人评价】 《How Tomcat Works》英文版 推荐理由：本书深入剖析Tomcat 4和Tomcat 5中的每个组件，并揭示其内部工作原理。通过学习本书，你将可以自行开发Tomcat组件，或者扩展已有的组件。 《Nginx 中文官方文档》 豆瓣评分：暂无 推荐理由：暂时未找到大家评价不错的 Nginx 实战相关书籍，先推荐看中文翻译的官方文档。如果你有合适的推荐，烦请告诉我。 《深入理解Nginx》 豆瓣评分：8.5【138 人评价】 推荐理由：书中首先通过介绍官方Nginx的基本用法和配置规则，帮助读者了解一般Nginx模块的用法，然后重点介绍了如何开发HTTP模块(含HTTP过滤模块)来得到定制化的Nginx，其中包括开发—个功能复杂的模块所需要了解的各种知识，并对内存池的实现细节及TCP协议进行了详细介绍；接着，综合Nginx框架代码分析了Nginx架构的设计理念和技巧，此外，还新增了如何在模块中支持HTTP变量，以及与slab共享内存等相关的内容，相信通过完善，可进一步帮助读者更好地开发出功能丰富、性能—流的Nginx模块。 友情提示：相对适用于 Nginx 开发者。Nginx 使用者可以了解。 《深入理解Java虚拟机：JVM高级特性与最佳实践》 豆瓣评分：8.9 【657 人评价】 推荐理由：不去了解 JVM 的工程师，和咸鱼有什么区别？ 推荐公众号：你假笨 推荐博客：你假笨@JVM 推荐小程序：JVMPocket 《Java核心技术系列：Java虚拟机规范（Java SE 8版）》 豆瓣评分：暂无评价 豆瓣评分：8.3 【27 人评价】《Java虚拟机规范(Java SE 7版)》 推荐理由：基于Java SE 8,Oracle官方发布，Java虚拟机技术创建人撰写，国内Java技术专家翻译，是深度了解Java虚拟机和Java语言实现细节的必读之作 推荐博客：占小狼的简书 推荐公众号：占小狼的博客 《Go语言编程》 豆瓣评分：7.1 【444 人评价】 推荐理由：这本书从整体的写作风格来说，会以介绍 Go 语言特性为主，示例则尽量采用作者平常的实践，而不是一个没有太大实际意义的语法示范样例。 友情提示：本书作者背景极强，许式伟为原金山WPS首席架构师、曾是盛大创新院研究员，目前是国内Go语言实践圈子公认的Go语言专家。 《 Go语言学习笔记》 豆瓣评分：8.4 【57 人评价】 推荐理由：基于Go1.6， 解析语言规范，深入剖析Go运行时源码 友情提示：雨痕大大，教科书级人物。 《MySQL技术内幕——InnoDB存储引擎》 豆瓣评分：8.6 【104 人评价】 推荐理由：从源代码的角度深度解析了InnoDB的体系结构、实现原理、工作机制，并给出了大量最佳实践，能帮助你系统而深入地掌握InnoDB，更重要的是，它能为你设计管理高性能、高可用的数据库系统提供绝佳的指导。 推荐公众号：DBAplus社群 《高性能MySQL》 豆瓣评分：9.3 【245 人评价】 推荐理由：对于想要了解MySQL性能提升的人来说，这是一本不可多得的书。书中没有各种提升性能的秘籍，而是深入问题的核心，详细的解释了每种提升性能的原理，从而可以使你四两拨千斤。授之于鱼不如授之于渔，这本书做到了。 推荐公众号：老叶茶馆 《高可用MySQL》 豆瓣评分：8.0 【87 人评价】 推荐理由：《高性能MySQL》的姊妹篇。 《MongoDB权威指南》 豆瓣评分：8.0 【69 人评价】 推荐理由：算是普通的参考书了，没有特别有深度的讲解。其实就是一本正常的介绍mongoDB是怎么用的，也可以作为nosql学习的入门。作为指南书，还是很合格的符合期望。 推荐博客：MongoDB 中文社区 推荐公众号：MongoDB 中文社区 《Redis开发与运维》 豆瓣评分：8.8 【41 人评价】 推荐理由：从开发、运维两个角度总结了Redis实战经验，深入浅出地剖析底层实现，包含大规模集群开发与运维的实际案例、应用技巧。全面覆盖Redis 基本功能及应用，图示丰富，讲解细腻。 推荐博客：Redis 中国用户组 推荐公众号：CRUG 《Redis设计与实现》 豆瓣评分：8.5 【427 人评价】 推荐理由：系统而全面地描述了 Redis 内部运行机制。图示丰富，描述清晰，并给出大量参考信息，是NoSQL数据库开发人员案头必备。 《NoSQL精粹》 豆瓣评分：8.2 【226 人评价】 推荐理由：书中全方位比较了关系型数据库与NoSQL数据库的异同；分别以Riak、MongoDB、Cassandra和Neo4J为代表，详细讲解了键值数据库、文档数据库、列族数据库和图数据库这4大类NoSQL数据库的优劣势、用法和适用场合；深入探讨了实现NoSQL数据库系统的各种细节，以及与关系型数据库的混用。 《ElasticSearch 可扩展的开源弹性搜索解决方案》 豆瓣评分：7.3 【23 人评价】 推荐理由：基于ElasticSearch 的0.2 版本，覆盖了ElasticSearch 各种功能和命令的应用，全面、详细地介绍了开源、分布式、RESTful，具有全文检索功能的搜索引擎ElasticSearch。 友情提示：本书 ElasticSearch 比较旧，不忍推荐。仅适合入门，有其他合适的 ElasticSearch 书籍，烦请告诉我。《Elasticsearch权威指南》中文版，目前正在翻译中。 推荐博客：Elastic 中文社区 《ELK Stack权威指南》 豆瓣评分：7.0 【10 人评价】 推荐理由：ELK stack是以Elasticsearch、Logstash、Kibana三个开源软件为主的数据处理工具链，是目前开源界最流行的实时数据分析解决方案，成为实时日志处理领域开源界的第一选择。 《ZooKeeper：分布式过程协同技术详解》 豆瓣评分：7.6 【49 人评价】 推荐理由：Zookeeper 入门 友情提示：翻译可能略显尴尬。 《从Paxos到Zookeeper分布式一致性原理与实践》 豆瓣评分：8.1 【187 人评价】 推荐理由：从分布式一致性的理论出发，向读者简要介绍几种典型的分布式一致性协议，以及解决分布式一致性问题的思路，其中重点讲解了Paxos和ZAB协议。同时，本书深入介绍了分布式一致性问题的工业解决方案——ZooKeeper，并着重向读者展示这一分布式协调框架的使用方法、内部实现及运维技巧，旨在帮助读者全面了解ZooKeeper，并更好地使用和运维ZooKeeper。 《RabbitMQ实战：高效部署分布式消息队列》 豆瓣评分：6.9 【47 人评价】 推荐理由：本书对RabbitMQ做了全面、翔实的讲解，体现了两位专家的真知灼见。本书首先介绍了有关MQ的历史，然后从基本的消息通信原理讲起，带领读者一路探索RabbitMQ的消息通信世界。 友情提示：本书 RabbitMQ 版本较旧。消息队列中间件 RabbitMQ、ActiveMQ、RocketMQ、Kafka 可以选择了解一下。 《Apache Kafka源码剖析》 豆瓣评分：7.8 【30 人评价】 推荐理由：以Kafka 0.10.0版本源码为基础，针对Kafka的架构设计到实现细节进行详细阐述。 《作业调度系统 Quartz 中文文档》 豆瓣评分：暂无 推荐理由：暂时未找到大家评价不错的 Quartz 实战相关书籍，先推荐看中文翻译的官方文档。如果你有合适的推荐，烦请告诉我。 友情提示：国内开源项目 Elastic-Job，XXL-Job 都可以选择了解。 《微服务设计》 豆瓣评分：8.1 【273 人评价】 推荐理由：通过Netflix等多个业界案例，从微服务架构演进到原理剖析，全面讲解建模集成部署等微服务所涉及的各种主题，微服务架构与实践指南。 《Spring Cloud微服务实战》 豆瓣评分：7.9【20 人评价】 推荐理由：从时下流行的微服务架构概念出发，详细介绍了Spring Cloud针对微服务架构中几大核心要素的解决方案和基础组件。对于各个组件的介绍，主要以示例与源码结合的方式来帮助读者更好地理解这些组件的使用方法以及运行原理。同时，在介绍的过程中，还包含了作者在实践中所遇到的一些问题和解决思路，可供读者在实践中作为参考。 作者博客：http://blog.didispace.com/ 作者公众号：didispace 《亿级流量网站架构核心技术》 豆瓣评分：7.6【57 人评价】 推荐理由：总结并梳理了亿级流量网站高可用和高并发原则，通过实例详细介绍了如何落地这些原则。本书分为四部分：概述、高可用原则、高并发原则、案例实战。 作者博客：开涛的博客 作者公众号：开涛的博客 《架构即未来：现代企业可扩展的Web架构、流程和组织》 豆瓣评分：8.7【77 人评价】 推荐理由：任何一个持续成长的公司最终都需要解决系统、组织和流程的扩展性问题。本书汇聚了作者从eBay、VISA、Salesforce.com到Apple超过30年的丰富经验， 全面阐释了经过验证的信息技术扩展方法，对所需要掌握的产品和服务的平滑扩展做了详尽的论述，并在第1版的基础上更新了扩展的策略、技术和案例。 《Maven 实战》 豆瓣评分：8.1【563 人评价】 推荐理由：国内最权威的Maven专家的力作，唯一一本哦！ 《Jenkins权威指南》 豆瓣评分：暂无评分 推荐理由：Jenkins 唯一实体书。 友情提示：内容相对比较旧，大多是过时的案例。建议，快速过一遍。Jenkins 方面无特别好的选择推荐书籍。可以选择 Google 一些教程。 《鸟哥的Linux私房菜 （基础学习篇）》 豆瓣评分：9.1【2269 人评价】 推荐理由：本书是最具知名度的Linux入门书《鸟哥的Linux私房菜基础学习篇》的最新版，全面而详细地介绍了Linux操作系统。 友情提示：内容非常全面，建议挑选和自己实际工作相关度较高的，其他部分有需要再阅读。 《鸟哥的Linux私房菜 （服务器架设篇）》 豆瓣评分：8.8 【198 人评价】 推荐理由：您已有Linux基础，想要进一步学习服务器架设？还想了解如何维护与管理您的服务器？本书是您最佳的选择。 《Zabbix企业级分布式监控系统》 豆瓣评分：7.6 【39 人评价】 推荐理由：本书从运维（OPS）角度对Zabbix的各项功能进行了详细介绍，以自动化运维视角为出发点，对Zabbix的安装和配置、自动化功能、监控告警、性能调优、Zabbix API、Zabbix协议、RPM安装包定制，结合SaltStack实现自动化配置管理等内容进行了全方位的深入剖析。 《第一本Docker书》 豆瓣评分：8.8 【63 人评价】 推荐理由：本书由Docker公司前服务与支持副总裁James Turnbull编写，是Docker开发指南。本书专注于Docker 1.9及以上版本，指导读者完成Docker的安装、部署、管理和扩展，带领读者经历从测试到生产的整个开发生命周期，让读者了解Docker适用于什么场景。 推荐博客：DockerOne 推荐公众号：DockerOne 《Docker——容器与容器云》 豆瓣评分：8.5 【99 人评价】 推荐理由：本书根据Docker 1.10版和Kubernetes 1.2版对第1版进行了全面更新，从实践者的角度出发，以Docker和Kubernetes为重点，沿着“基本用法介绍”到“核心原理解读”到“高级实践技巧”的思路，一本书讲透当前主流的容器和容器云技术，有助于读者在实际场景中利用Docker容器和容器云解决问题并启发新的思考。 《Kubernetes权威指南》 豆瓣评分：7.7【15 人评价】 推荐理由：Kubernetes重磅开山之作，针对Kubernetes v1.6和本书第2版进行大篇幅内容更新，全方位完美覆盖，可借鉴性极强。 推荐博客：Kubernetes 中文社区 推荐公众号：K8S 技术社区 《用Mesos框架构建分布式应用》 豆瓣评分：暂无评分 推荐理由：超级薄的一本书，看完之后，你会对 Mesos 会非常了解，并且极大可能性学会如何基于 Mesos 框架构建分布式应用。 《数据结构与算法分析：Java语言描述》 豆瓣评分：8.3【183 人评价】 推荐理由：本书是国外数据结构与算法分析方面的经典教材，使用卓越的Java编程语言作为实现工具讨论了数据结构（组织大量数据的方法）和算法分析（对算法运行时间的估计）。 友情提示：算法方法还有其他很好的书籍，例如《算法导论》、《算法（第四版）》，也可以选择阅读。重要的是，保持耐心，享受这个痛并快乐的过程。 《Head First 设计模式》 豆瓣评分：9.2【2394 人评价】 推荐理由：《Head First设计模式》(中文版)共有14章，每章都介绍了几个设计模式，完整地涵盖了四人组版本全部23个设计模式。 《HTTP权威指南》 豆瓣评分：8.7 【1126 人评价】 推荐理由：本书尝试着将HTTP中一些互相关联且常被误解的规则梳理清楚，并编写了一系列基于各种主题的章节，对HTTP各方面的特性进行了介绍。纵观全书，对HTTP“为什么”这样做进行了详细的解释，而不仅仅停留在它是“怎么做”的。 《TCP/IP详解 系列》 豆瓣评分：9.3 【1883 人评价】 推荐理由：完整而详细的TCP/IP协议指南。针对任何希望理解TCP/IP协议是如何实现的读者设计。 《Linux内核设计与实现》 豆瓣评分：8.7【286 人评价】 详细描述了Linux内核的主要子系统和特点，包括Linux内核的设计、实现和接口。从理论到实践涵盖了Linux内核的方方面面，可以满足读者的各种兴趣和需求。 友情提示：Linux内核方面不乏好书。本书篇幅方面较为合适。 《剑指Offer：名企面试官精讲典型编程题》 豆瓣评分：8.5【508 人评价】 推荐理由：剖析了80个典型的编程面试题，系统整理基础知识、代码质量、解题思路、优化效率和综合能力这5个面试要点。 推荐网站：牛客网-专业IT笔试面试备考平台 《程序员代码面试指南：IT名企算法与数据结构题目最优解》 豆瓣评分：8.4【32 人评价】 推荐理由：程序员刷题宝典！编程能力提升秘笈！精选IT名企真实代码面试题，全面覆盖算法与数据结构题型！ 《领域驱动设计》 豆瓣评分：9.0【115 人评价】 推荐理由：是领域驱动设计方面的经典之作。全书围绕着设计和开发实践，结合若干真实的项目案例，向读者阐述如何在真实的软件开发中应用领域驱动设计。 友情提示：理论的书籍往往较为枯燥，勤修内功是必须走的路。 《火球:UML大战需求分析》 豆瓣评分：7.9【115 人评价】 推荐理由：融合UML、非UML、需求分析及需求管理等各方面的知识，帮助读者解决UML业界问题、需求分析及需求管理问题。 友情提示：可能不是最好的 UML 书籍，但从是否能够阅读理解完的角度来说，本书可能是相对合适的。有兴趣的同学也可以看看《UML和模式应用》、《大象：Thinking in UML》。 TODO List待推荐主题书籍 TODO 《大数据日知录 架构与算法》TODO 《大型网站系统与Java中间件实践》TODO 《HotSpot实战》TODO 《垃圾回收的算法与实现》TODO 《彩色UML建模》TODO 《七周七并发模型》TODO 《Go程序设计语言》 [x] Go [ ] Node [x] Linux 内核 [x] 领域 [x] UML [x] Tomcat [x] SpringCloud [x] Java 基础 [x] Netty [x] MyBatis [x] 数据库 [x] MongoDB [x] Maven [x] DevOps [x] Linux 运维 [x] 面试 [x] 消息队列 [x] 设计模式 [x] 算法与数据结构 [x] Zookeeper [x] SpringBoot [x] Nginx [x] 定时任务 [x] 搜索引擎 [x] 协议 [x] 单元测试 [x] 重构 [x] 日志 [x] Docker [x] 监控 最后原文出处 http://www.iocoder.cn/Architecture/books-recommended/ 「芋道源码」欢迎转载，保留摘要，谢谢！","tags":[{"name":"书籍","slug":"书籍","permalink":"http://www.54tianzhisheng.cn/tags/书籍/"}]},{"title":"小白谈数据脱敏","date":"2017-10-27T16:00:00.000Z","path":"2017/10/28/Data-Desensitization/","text":"什么是数据脱敏？百度百科是这样描述的： 数据脱敏是指对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。在涉及客户安全数据或者一些商业性敏感数据的情况下，在不违反系统规则条件下，对真实数据进行改造并提供测试使用，如身份证号、手机号、卡号、客户姓名、客户地址、等个人敏感信息都需要通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。这样就可以在开发、测试和其他非生产环境以及外包环境中可以安全的使用脱敏后的真实数据集。 生活中的常见例子1、火车票： 2、淘宝网页上的收获地址信息： 敏感数据梳理在进行数据脱敏之前我们应该要确定公司的哪些数据（哪些表、哪些字段）要作为脱敏的目标，下面从用户、公司、卖家方面分析： 1、用户：名字、手机号码、身份证号码、固定电话、收货地址、电子邮箱、银行卡号、密码等 2、卖家：名字、手机号码、身份证号码、固定电话等 3、公司：交易金额、优惠券码、充值码等 确定脱敏规则确定好了公司的哪些数据要作为脱敏目标后，我们就需要制定脱敏的规则（具体的实施方法）。 常见方法： 1、替换：如统一将女性用户名替换为F，这种方法更像“障眼法”，对内部人员可以完全保持信息完整性，但易破解。 2、重排：序号12345 重排为 54321，按照一定的顺序进行打乱，很像“替换”， 可以在需要时方便还原信息，但同样易破解。 3、加密：编号 12345 加密为 23456，安全程度取决于采用哪种加密算法，一般根据实际情况而定。 4、截断：13811001111 截断为 138，舍弃必要信息来保证数据的模糊性，是比较常用的脱敏方法，但往往对生产不够友好。（丢失字段的长度） 5、掩码: 123456 -&gt; 1xxxx6，保留了部分信息，并且保证了信息的长度不变性，对信息持有者更易辨别， 如火车票上得身份信息。（常用方法） 6、日期偏移取整：20130520 12:30:45 -&gt; 20130520 12:00:00，舍弃精度来保证原始数据的安全性，一般此种方法可以保护数据的时间分布密度。 目前我的脱敏规则想法是： 1、【中文姓名】只显示第一个汉字，其他隐藏为2个星号，比如：李** 2、【身份证号】显示最后四位，其他隐藏。共计18位或者15位，比如：*************1234 3、【固定电话】 显示后四位，其他隐藏，比如：*******3241 4、【手机号码】前三位，后四位，其他隐藏，比如：135****6810 5、【地址】只显示到地区，不显示详细地址，比如：上海徐汇区漕河泾开发区*** 6、【电子邮箱】 邮箱前缀仅显示第一个字母，前缀其他隐藏，用星号代替，@及后面的地址显示，比如：d**@126.com 7、【银行卡号】前六位，后四位，其他用星号隐藏每位1个星号，比如：6222600**********1234 8、【密码】密码的全部字符都用代替，比如：* 根据以上规则进行数据脱敏！ 具体思路目前是这样的： 从原数据源查询到的生产数据 ——&gt; 数据脱敏 ——&gt; 更新到目标数据源。 原数据源、目标数据源、需要脱敏的表、字段等都放在配置文件中，做到可扩展性！ 脱敏工具代码根据以上规则已经写好了一份简单的脱敏规则工具类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119/** * 数据脱敏工具类 * Created by zhisheng_tian on 2017/10/25. */public class DesensitizedUtils &#123; /** * 【中文姓名】只显示第一个汉字，其他隐藏为2个星号，比如：李** * * @param fullName * @return */ public static String chineseName(String fullName) &#123; if (StringUtils.isBlank(fullName)) &#123; return \"\"; &#125; String name = StringUtils.left(fullName, 1); return StringUtils.rightPad(name, StringUtils.length(fullName), \"*\"); &#125; /** * 【身份证号】显示最后四位，其他隐藏。共计18位或者15位，比如：*************1234 * * @param id * @return */ public static String idCardNum(String id) &#123; if (StringUtils.isBlank(id)) &#123; return \"\"; &#125; String num = StringUtils.right(id, 4); return StringUtils.leftPad(num, StringUtils.length(id), \"*\"); &#125; /** * 【固定电话】 显示后四位，其他隐藏，比如：*******3241 * * @param num * @return */ public static String fixedPhone(String num) &#123; if (StringUtils.isBlank(num)) &#123; return \"\"; &#125; return StringUtils.leftPad(StringUtils.right(num, 4), StringUtils.length(num), \"*\"); &#125; /** * 【手机号码】前三位，后四位，其他隐藏，比如：135****6810 * * @param num * @return */ public static String mobilePhone(String num) &#123; if (StringUtils.isBlank(num)) &#123; return \"\"; &#125; return StringUtils.left(num, 3).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(num, 4), StringUtils.length(num), \"*\"), \"***\")); &#125; /** * 【地址】只显示到地区，不显示详细地址，比如：上海徐汇区漕河泾开发区*** * * @param address * @param sensitiveSize 敏感信息长度 * @return */ public static String address(String address, int sensitiveSize) &#123; if (StringUtils.isBlank(address)) &#123; return \"\"; &#125; int length = StringUtils.length(address); return StringUtils.rightPad(StringUtils.left(address, length - sensitiveSize), length, \"*\"); &#125; /** * 【电子邮箱】 邮箱前缀仅显示第一个字母，前缀其他隐藏，用星号代替，@及后面的地址显示，比如：d**@126.com * * @param email * @return */ public static String email(String email) &#123; if (StringUtils.isBlank(email)) &#123; return \"\"; &#125; int index = StringUtils.indexOf(email, \"@\"); if (index &lt;= 1) return email; else return StringUtils.rightPad(StringUtils.left(email, 1), index, \"*\").concat(StringUtils.mid(email, index, StringUtils.length(email))); &#125; /** * 【银行卡号】前六位，后四位，其他用星号隐藏每位1个星号，比如：6222600**********1234 * * @param cardNum * @return */ public static String bankCard(String cardNum) &#123; if (StringUtils.isBlank(cardNum)) &#123; return \"\"; &#125; return StringUtils.left(cardNum, 6).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(cardNum, 4), StringUtils.length(cardNum), \"*\"), \"******\")); &#125; /** * 【密码】密码的全部字符都用*代替，比如：****** * * @param password * @return */ public static String password(String password) &#123; if (StringUtils.isBlank(password)) &#123; return \"\"; &#125; String pwd = StringUtils.left(password, 0); return StringUtils.rightPad(pwd, StringUtils.length(password), \"*\"); &#125;&#125; 最后转载请注明地址：http://www.54tianzhisheng.cn/2017/10/28/Data-Desensitization/","tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.54tianzhisheng.cn/tags/数据库/"}]},{"title":"HBase 集群监控","date":"2017-10-20T16:00:00.000Z","path":"2017/10/21/HBase-metrics/","text":"为什么需要监控？为了保证系统的稳定性，可靠性，可运维性。 掌控集群的核心性能指标，了解集群的性能表现。 集群出现问题时及时报警，便于运维同学及时修复问题。 集群重要指标值异常时进行预警，将问题扼杀在摇篮中，不用等集群真正不可用时才采取行动。 当集群出现问题时，监控系统可以帮助我们更快的定位问题和解决问题 如何构建 HBase 集群监控系统？公司有自己的监控系统，我们所要做的就是将 HBase 中我们关心的指标项发送到监控系统去，问题就转换为我们开发，采集并返回哪些 HBase 集群监控指标项。 HBase 集群监控指标采集的监控数据主要包括以下几个方面：某台机器 OS 层面上的数据，例如 CPU、内存、磁盘、网络、load、网络流量等；某台 regionserver（或master）机器 jvm 的状态，例如关于线程的信息，GC 的次数和时间，内存使用状况，以及 ERROR、WARN、Fatal 事件出现的次数；regionserver（或 master）进程中的统计信息。 可以通过以下地址获取 HBase 提供的 JMX 信息的 web 页面 1http://your_master:60010/jmx //所有的bean JMX web 页面的数据格式是json格式，信息很多！ OS 监控数据HBase 中对于 OS 的监控数据，主要是 OperatingSystem 的对象来进行的，如下就是我提取出来的 JSON 信息， 1234567891011121314151617181920&#123; \"name\" : \"java.lang:type=OperatingSystem\", \"modelerType\" : \"com.sun.management.UnixOperatingSystem\", \"MaxFileDescriptorCount\" : 1000000, \"OpenFileDescriptorCount\" : 413, \"CommittedVirtualMemorySize\" : 1892225024, \"FreePhysicalMemorySize\" : 284946432, \"FreeSwapSpaceSize\" : 535703552, \"ProcessCpuLoad\" : 0.0016732901066722444, \"ProcessCpuTime\" : 59306210000000, \"SystemCpuLoad\" : 0.018197029910060655, \"TotalPhysicalMemorySize\" : 16660848640, \"TotalSwapSpaceSize\" : 536862720, \"AvailableProcessors\" : 8, \"Arch\" : \"amd64\", \"SystemLoadAverage\" : 0.0, \"Name\" : \"Linux\", \"Version\" : \"2.6.32-431.11.7.el6.ucloud.x86_64\", \"ObjectName\" : \"java.lang:type=OperatingSystem\" &#125; 其中比较重要的指标有 OpenFileDescriptorCount , FreePhysicalMemorySize , ProcessCpuLoad , SystemCpuLoad , AvailableProcessors , SystemLoadAverage JVM 监控数据Hbase 中对于 JVM 的监控数据，主要是 JvmMetrics 的对象来进行的，如下就是我提取出来的 JSON 信息， 12345678910111213141516171819202122232425262728293031&#123; \"name\" : \"Hadoop:service=HBase,name=JvmMetrics\", \"modelerType\" : \"JvmMetrics\", \"tag.Context\" : \"jvm\", \"tag.ProcessName\" : \"Master\", \"tag.SessionId\" : \"\", \"tag.Hostname\" : \"uhadoop-qrljqo-master2\", \"MemNonHeapUsedM\" : 53.846107, \"MemNonHeapCommittedM\" : 85.84375, \"MemNonHeapMaxM\" : 130.0, \"MemHeapUsedM\" : 79.05823, \"MemHeapCommittedM\" : 240.125, \"MemHeapMaxM\" : 989.875, \"MemMaxM\" : 989.875, \"GcCountParNew\" : 15190, \"GcTimeMillisParNew\" : 72300, \"GcCountConcurrentMarkSweep\" : 2, \"GcTimeMillisConcurrentMarkSweep\" : 319, \"GcCount\" : 15192, \"GcTimeMillis\" : 72619, \"ThreadsNew\" : 0, \"ThreadsRunnable\" : 21, \"ThreadsBlocked\" : 0, \"ThreadsWaiting\" : 144, \"ThreadsTimedWaiting\" : 18, \"ThreadsTerminated\" : 0, \"LogFatal\" : 0, \"LogError\" : 0, \"LogWarn\" : 0, \"LogInfo\" : 0 &#125; JvmMetrics 主要统计的信息包括：内存的使用状态信息；GC的统计信息；线程的统计信息；以及事件的统计信息。 内存的统计信息主要是：JVM 当前已经使用的 NonHeapMemory 的大小、以及配置的 NonHeapMemory 的大小；JVM 当前已经使用的 HeapMemory 的大小、以及配置的 HeapMemory 的大小； JVM 运行时的可以使用的最大的内存的大小。 GC 的统计较为简单，仅统计了进程在固定间隔内 GC 的次数和花费的总时间。 线程的统计，主要是统计进程内当前线程的处于 NEW 、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED 这六种状态下的线程数量。 对于事件的统计，主要统计固定时间间隔内的 Fatal、Error、Warn 以及 Info 的数量。(这块好像不怎么重要) Region Servers 健康你也可以通过如下地址： 1http://your_master:60010/jmx?qry=Hadoop:service=HBase,name=Master,sub=Server 获得到 Region Servers 健康值： 123456789101112131415161718&#123; \"name\" : \"Hadoop:service=HBase,name=Master,sub=Server\", \"modelerType\" : \"Master,sub=Server\", \"tag.liveRegionServers\" : \"xxx\", \"tag.deadRegionServers\" : \"\", \"tag.zookeeperQuorum\" : \"xxx\", \"tag.serverName\" : \"xxx2,60000,1495683310213\", \"tag.clusterId\" : \"e5e044a3-ef9f-48f7-ba63-637376f5fa90\", \"tag.isActiveMaster\" : \"true\", \"tag.Context\" : \"master\", \"tag.Hostname\" : \"xxx\", \"masterActiveTime\" : 1495683312239, \"masterStartTime\" : 1495683310213, \"averageLoad\" : 143.66666666666666, \"numRegionServers\" : 3, \"numDeadRegionServers\" : 0, \"clusterRequests\" : 1297834323 &#125; MemoryPool从全部的 JSON 值中你会看到很多种 MemoryPool 值，比如 Par Eden Space 、CMS Perm Gen、Par Survivor Space、CMS Old Gen、Code Cache ，按需获取吧。 总结任何一个服务的监控系统都是一个不断迭代，不断优化的过程，不可能一开始就做到最好。监控总是比问题发生来的更早一些，而每一次出问题，又进一步加强相应方面的监控，我们需要让监控系统从出问题时才报警到可能出现问题时就预警逐渐过渡，最终让监控系统成为我们保证系统稳定性的一个有力工具。 最后监控指标有很多，但请按需获取 ! 转载文章请注明原出处，谢谢支持！ http://www.54tianzhisheng.cn/2017/10/21/HBase-metrics/ 参考资料1、hbase性能监控（一） 2、hbase性能监控（二） 3、hbase性能监控（三） 4、HBase 集群监控系统构建 5、hbase jmx常用监控指标 推荐相关文章1、ElasticSearch 单个节点监控 2、ElasticSearch 集群监控","tags":[{"name":"HBase","slug":"HBase","permalink":"http://www.54tianzhisheng.cn/tags/HBase/"}]},{"title":"Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控","date":"2017-10-17T16:00:00.000Z","path":"2017/10/18/ElasticSearch-nodes-metrics/","text":"集群健康监控是对集群信息进行高度的概括，节点统计值 API 提供了集群中每个节点的统计值。节点统计值很多，在监控的时候仍需要我们清楚哪些指标是最值得关注的。 集群健康监控可以参考这篇文章：ElasticSearch 集群监控 节点信息 Node Info :1curl -XGET &apos;http://localhost:9200/_nodes&apos; 执行上述命令可以获取所有 node 的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071_nodes: &#123; total: 2, successful: 2, failed: 0&#125;,cluster_name: \"elasticsearch\",nodes: &#123; MSQ_CZ7mTNyOSlYIfrvHag: &#123; name: \"node0\", transport_address: \"192.168.180.110:9300\", host: \"192.168.180.110\", ip: \"192.168.180.110\", version: \"5.5.0\", build_hash: \"260387d\", total_indexing_buffer: 103887667, roles:&#123;...&#125;, settings: &#123;...&#125;, os: &#123; refresh_interval_in_millis: 1000, name: \"Linux\", arch: \"amd64\", version: \"3.10.0-229.el7.x86_64\", available_processors: 4, allocated_processors: 4 &#125;, process: &#123; refresh_interval_in_millis: 1000, id: 3022, mlockall: false &#125;, jvm: &#123; pid: 3022, version: \"1.8.0_121\", vm_name: \"Java HotSpot(TM) 64-Bit Server VM\", vm_version: \"25.121-b13\", vm_vendor: \"Oracle Corporation\", start_time_in_millis: 1507515225302, mem: &#123; heap_init_in_bytes: 1073741824, heap_max_in_bytes: 1038876672, non_heap_init_in_bytes: 2555904, non_heap_max_in_bytes: 0, direct_max_in_bytes: 1038876672 &#125;, gc_collectors: [], memory_pools: [], using_compressed_ordinary_object_pointers: \"true\", input_arguments:&#123;&#125; &#125; thread_pool:&#123; force_merge: &#123;&#125;, fetch_shard_started: &#123;&#125;, listener: &#123;&#125;, index: &#123;&#125;, refresh: &#123;&#125;, generic: &#123;&#125;, warmer: &#123;&#125;, search: &#123;&#125;, flush: &#123;&#125;, fetch_shard_store: &#123;&#125;, management: &#123;&#125;, get: &#123;&#125;, bulk: &#123;&#125;, snapshot: &#123;&#125; &#125; transport: &#123;...&#125;, http: &#123;...&#125;, plugins: [], modules: [], ingest: &#123;...&#125; &#125; 上面是我已经简写了很多数据之后的返回值，但是指标还是很多，有些是一些常规的指标，对于监控来说，没必要拿取。从上面我们可以主要关注以下这些指标: 1os, process, jvm, thread_pool, transport, http, ingest and indices 节点统计 nodes-statistics节点统计值 API 可通过如下命令获取： 1GET /_nodes/stats 得到： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158_nodes: &#123; total: 2, successful: 2, failed: 0&#125;,cluster_name: \"elasticsearch\",nodes: &#123; MSQ_CZ7mTNyOSlYI0yvHag: &#123; timestamp: 1508312932354, name: \"node0\", transport_address: \"192.168.180.110:9300\", host: \"192.168.180.110\", ip: \"192.168.180.110:9300\", roles: [], indices: &#123; docs: &#123; count: 6163666, deleted: 0 &#125;, store: &#123; size_in_bytes: 2301398179, throttle_time_in_millis: 122850 &#125;, indexing: &#123;&#125;, get: &#123;&#125;, search: &#123;&#125;, merges: &#123;&#125;, refresh: &#123;&#125;, flush: &#123;&#125;, warmer: &#123;&#125;, query_cache: &#123;&#125;, fielddata: &#123;&#125;, completion: &#123;&#125;, segments: &#123;&#125;, translog: &#123;&#125;, request_cache: &#123;&#125;, recovery: &#123;&#125; &#125;, os: &#123; timestamp: 1508312932369, cpu: &#123; percent: 0, load_average: &#123; 1m: 0.09, 5m: 0.12, 15m: 0.08 &#125; &#125;, mem: &#123; total_in_bytes: 8358301696, free_in_bytes: 1381613568, used_in_bytes: 6976688128, free_percent: 17, used_percent: 83 &#125;, swap: &#123; total_in_bytes: 8455712768, free_in_bytes: 8455299072, used_in_bytes: 413696 &#125;, cgroup: &#123; cpuacct: &#123;&#125;, cpu: &#123; control_group: \"/user.slice\", cfs_period_micros: 100000, cfs_quota_micros: -1, stat: &#123;&#125; &#125; &#125;&#125;,process: &#123; timestamp: 1508312932369, open_file_descriptors: 228, max_file_descriptors: 65536, cpu: &#123; percent: 0, total_in_millis: 2495040 &#125;, mem: &#123; total_virtual_in_bytes: 5002465280 &#125;&#125;,jvm: &#123; timestamp: 1508312932369, uptime_in_millis: 797735804, mem: &#123; heap_used_in_bytes: 318233768, heap_used_percent: 30, heap_committed_in_bytes: 1038876672, heap_max_in_bytes: 1038876672, non_heap_used_in_bytes: 102379784, non_heap_committed_in_bytes: 108773376, pools: &#123; young: &#123; used_in_bytes: 62375176, max_in_bytes: 279183360, peak_used_in_bytes: 279183360, peak_max_in_bytes: 279183360 &#125;, survivor: &#123; used_in_bytes: 175384, max_in_bytes: 34865152, peak_used_in_bytes: 34865152, peak_max_in_bytes: 34865152 &#125;, old: &#123; used_in_bytes: 255683208, max_in_bytes: 724828160, peak_used_in_bytes: 255683208, peak_max_in_bytes: 724828160 &#125; &#125; &#125;, threads: &#123;&#125;, gc: &#123;&#125;, buffer_pools: &#123;&#125;, classes: &#123;&#125;&#125;, thread_pool: &#123; bulk: &#123;&#125;, fetch_shard_started: &#123;&#125;, fetch_shard_store: &#123;&#125;, flush: &#123;&#125;, force_merge: &#123;&#125;, generic: &#123;&#125;, get: &#123;&#125;, index: &#123; threads: 1, queue: 0, active: 0, rejected: 0, largest: 1, completed: 1 &#125; listener: &#123;&#125;, management: &#123;&#125;, refresh: &#123;&#125;, search: &#123;&#125;, snapshot: &#123;&#125;, warmer: &#123;&#125; &#125;, fs: &#123;&#125;, transport: &#123; server_open: 13, rx_count: 11696, rx_size_in_bytes: 1525774, tx_count: 10282, tx_size_in_bytes: 1440101928 &#125;, http: &#123; current_open: 4, total_opened: 23 &#125;, breakers: &#123;&#125;, script: &#123;&#125;, discovery: &#123;&#125;, ingest: &#123;&#125;&#125; 节点名是一个 UUID，上面列举了很多指标，下面讲解下： 索引部分 indices这部分列出了这个节点上所有索引的聚合过的统计值 ： docs 展示节点内存有多少文档，包括还没有从段里清除的已删除文档数量。 store 部分显示节点耗用了多少物理存储。这个指标包括主分片和副本分片在内。如果限流时间很大，那可能表明你的磁盘限流设置得过低。 indexing 显示已经索引了多少文档。这个值是一个累加计数器。在文档被删除的时候，数值不会下降。还要注意的是，在发生内部 索引操作的时候，这个值也会增加，比如说文档更新。 还列出了索引操作耗费的时间，正在索引的文档数量，以及删除操作的类似统计值。 get 显示通过 ID 获取文档的接口相关的统计值。包括对单个文档的 GET 和 HEAD 请求。 search 描述在活跃中的搜索（ open_contexts ）数量、查询的总数量、以及自节点启动以来在查询上消耗的总时间。用 query_time_in_millis / query_total 计算的比值，可以用来粗略的评价你的查询有多高效。比值越大，每个查询花费的时间越多，你应该要考虑调优了。 fetch 统计值展示了查询处理的后一半流程（query-then-fetch 里的 fetch ）。如果 fetch 耗时比 query 还多，说明磁盘较慢，或者获取了太多文档，或者可能搜索请求设置了太大的分页（比如， size: 10000 ）。 merges 包括了 Lucene 段合并相关的信息。它会告诉你目前在运行几个合并，合并涉及的文档数量，正在合并的段的总大小，以及在合并操作上消耗的总时间。 filter_cache 展示了已缓存的过滤器位集合所用的内存数量，以及过滤器被驱逐出内存的次数。过多的驱逐数 可能 说明你需要加大过滤器缓存的大小，或者你的过滤器不太适合缓存（比如它们因为高基数而在大量产生，就像是缓存一个 now 时间表达式）。 不过，驱逐数是一个很难评定的指标。过滤器是在每个段的基础上缓存的，而从一个小的段里驱逐过滤器，代价比从一个大的段里要廉价的多。有可能你有很大的驱逐数，但是它们都发生在小段上，也就意味着这些对查询性能只有很小的影响。 把驱逐数指标作为一个粗略的参考。如果你看到数字很大，检查一下你的过滤器，确保他们都是正常缓存的。不断驱逐着的过滤器，哪怕都发生在很小的段上，效果也比正确缓存住了的过滤器差很多。 field_data 显示 fielddata 使用的内存， 用以聚合、排序等等。这里也有一个驱逐计数。和 filter_cache 不同的是，这里的驱逐计数是很有用的：这个数应该或者至少是接近于 0。因为 fielddata 不是缓存，任何驱逐都消耗巨大，应该避免掉。如果你在这里看到驱逐数，你需要重新评估你的内存情况，fielddata 限制，请求语句，或者这三者。 segments 会展示这个节点目前正在服务中的 Lucene 段的数量。 这是一个重要的数字。大多数索引会有大概 50–150 个段，哪怕它们存有 TB 级别的数十亿条文档。段数量过大表明合并出现了问题（比如，合并速度跟不上段的创建）。注意这个统计值是节点上所有索引的汇聚总数。记住这点。 memory 统计值展示了 Lucene 段自己用掉的内存大小。 这里包括底层数据结构，比如倒排表，字典，和布隆过滤器等。太大的段数量会增加这些数据结构带来的开销，这个内存使用量就是一个方便用来衡量开销的度量值。 操作系统和进程部分OS 和 Process 部分基本是自描述的，不会在细节中展开讲解。它们列出来基础的资源统计值，比如 CPU 和负载。OS 部分描述了整个操作系统，而 Process 部分只显示 Elasticsearch 的 JVM 进程使用的资源情况。 这些都是非常有用的指标，不过通常在你的监控技术栈里已经都测量好了。统计值包括下面这些： CPU 负载 内存使用率 （mem.used_percent） Swap 使用率 打开的文件描述符 （open_file_descriptors） JVM 部分jvm 部分包括了运行 Elasticsearch 的 JVM 进程一些很关键的信息。 最重要的，它包括了垃圾回收的细节，这对你的 Elasticsearch 集群的稳定性有着重大影响。 123456789101112jvm: &#123; timestamp: 1508312932369, uptime_in_millis: 797735804, mem: &#123; heap_used_in_bytes: 318233768, heap_used_percent: 30, heap_committed_in_bytes: 1038876672, heap_max_in_bytes: 1038876672, non_heap_used_in_bytes: 102379784, non_heap_committed_in_bytes: 108773376, &#125;&#125; jvm 部分首先列出一些和 heap 内存使用有关的常见统计值。你可以看到有多少 heap 被使用了，多少被指派了（当前被分配给进程的），以及 heap 被允许分配的最大值。理想情况下，heap_committed_in_bytes 应该等于 heap_max_in_bytes 。如果指派的大小更小，JVM 最终会被迫调整 heap 大小——这是一个非常昂贵的操作。如果你的数字不相等，阅读 堆内存:大小和交换 学习如何正确的配置它。 heap_used_percent 指标是值得关注的一个数字。Elasticsearch 被配置为当 heap 达到 75% 的时候开始 GC。如果你的节点一直 &gt;= 75%，你的节点正处于 内存压力 状态。这是个危险信号，不远的未来可能就有慢 GC 要出现了。 如果 heap 使用率一直 &gt;=85%，你就麻烦了。Heap 在 90–95% 之间，则面临可怕的性能风险，此时最好的情况是长达 10–30s 的 GC，最差的情况就是内存溢出（OOM）异常。 线程池部分Elasticsearch 在内部维护了线程池。 这些线程池相互协作完成任务，有必要的话相互间还会传递任务。通常来说，你不需要配置或者调优线程池，不过查看它们的统计值有时候还是有用的，可以洞察你的集群表现如何。 每个线程池会列出已配置的线程数量（ threads ），当前在处理任务的线程数量（ active ），以及在队列中等待处理的任务单元数量（ queue ）。 如果队列中任务单元数达到了极限，新的任务单元会开始被拒绝，你会在 rejected 统计值上看到它反映出来。这通常是你的集群在某些资源上碰到瓶颈的信号。因为队列满意味着你的节点或集群在用最高速度运行，但依然跟不上工作的蜂拥而入。 这里的一系列的线程池，大多数你可以忽略，但是有一小部分还是值得关注的： indexing 普通的索引请求的线程池 bulk 批量请求，和单条的索引请求不同的线程池 get Get-by-ID 操作 search 所有的搜索和查询请求 merging 专用于管理 Lucene 合并的线程池 网络部分 transport 显示和 传输地址 相关的一些基础统计值。包括节点间的通信（通常是 9300 端口）以及任意传输客户端或者节点客户端的连接。如果看到这里有很多连接数不要担心；Elasticsearch 在节点之间维护了大量的连接。 http 显示 HTTP 端口（通常是 9200）的统计值。如果你看到 total_opened 数很大而且还在一直上涨，这是一个明确信号，说明你的 HTTP 客户端里有没启用 keep-alive 长连接的。持续的 keep-alive 长连接对性能很重要，因为连接、断开套接字是很昂贵的（而且浪费文件描述符）。请确认你的客户端都配置正确。 参考资料1、nodes-info 2、nodes-stats 3、ES监控指标 最后：转载请注明地址：http://www.54tianzhisheng.cn/2017/10/18/ElasticSearch-nodes-metrics/","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"Elasticsearch 系列文章（三）：ElasticSearch 集群监控","date":"2017-10-14T16:00:00.000Z","path":"2017/10/15/ElasticSearch-cluster-health-metrics/","text":"最近在做 ElasticSearch 的信息（集群和节点）监控，特此稍微整理下学到的东西。这篇文章主要介绍集群的监控。 要监控哪些 ElasticSearch metrics Elasticsearch 提供了大量的 Metric，可以帮助您检测到问题的迹象，在遇到节点不可用、out-of-memory、long garbage collection times 的时候采取相应措施。但是指标太多了，有时我们并不需要这么多，这就需要我们进行筛选。 集群健康一个 Elasticsearch 集群至少包括一个节点和一个索引。或者它 可能有一百个数据节点、三个单独的主节点，以及一小打客户端节点——这些共同操作一千个索引（以及上万个分片）。 不管集群扩展到多大规模，你都会想要一个快速获取集群状态的途径。Cluster Health API 充当的就是这个角色。你可以把它想象成是在一万英尺的高度鸟瞰集群。它可以告诉你安心吧一切都好，或者警告你集群某个地方有问题。 让我们执行一下 cluster-health API 然后看看响应体是什么样子的： 1GET _cluster/health 和 Elasticsearch 里其他 API 一样，cluster-health 会返回一个 JSON 响应。这对自动化和告警系统来说，非常便于解析。响应中包含了和你集群有关的一些关键信息： 123456789101112&#123; \"cluster_name\": \"elasticsearch_zach\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 1, \"number_of_data_nodes\": 1, \"active_primary_shards\": 10, \"active_shards\": 10, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0&#125; 响应信息中最重要的一块就是 status 字段。状态可能是下列三个值之一 : status 含义 green 所有的主分片和副本分片都已分配。你的集群是 100% 可用的。 yellow 所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。不过，你的高可用性在某种程度上被弱化。如果 更多的 分片消失，你就会丢数据了。把 yellow 想象成一个需要及时调查的警告。 red 至少一个主分片（以及它的全部副本）都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。 number_of_nodes 和 number_of_data_nodes 这个命名完全是自描述的。 active_primary_shards 指出你集群中的主分片数量。这是涵盖了所有索引的汇总值。 active_shards 是涵盖了所有索引的所有分片的汇总值，即包括副本分片。 relocating_shards 显示当前正在从一个节点迁往其他节点的分片的数量。通常来说应该是 0，不过在 Elasticsearch 发现集群不太均衡时，该值会上涨。比如说：添加了一个新节点，或者下线了一个节点。 initializing_shards 是刚刚创建的分片的个数。比如，当你刚创建第一个索引，分片都会短暂的处于 initializing 状态。这通常会是一个临时事件，分片不应该长期停留在 initializing状态。你还可能在节点刚重启的时候看到 initializing 分片：当分片从磁盘上加载后，它们会从initializing 状态开始。 unassigned_shards 是已经在集群状态中存在的分片，但是实际在集群里又找不着。通常未分配分片的来源是未分配的副本。比如，一个有 5 分片和 1 副本的索引，在单节点集群上，就会有 5 个未分配副本分片。如果你的集群是 red 状态，也会长期保有未分配分片（因为缺少主分片）。 集群统计集群统计信息包含 集群的分片数，文档数，存储空间，缓存信息，内存使用率，插件内容，文件系统内容，JVM 作用状况，系统 CPU，OS 信息，段信息。 查看全部统计信息命令： 1curl -XGET &apos;http://localhost:9200/_cluster/stats?human&amp;pretty&apos; 返回 JSON 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177&#123; \"timestamp\": 1459427693515, \"cluster_name\": \"elasticsearch\", \"status\": \"green\", \"indices\": &#123; \"count\": 2, \"shards\": &#123; \"total\": 10, \"primaries\": 10, \"replication\": 0, \"index\": &#123; \"shards\": &#123; \"min\": 5, \"max\": 5, \"avg\": 5 &#125;, \"primaries\": &#123; \"min\": 5, \"max\": 5, \"avg\": 5 &#125;, \"replication\": &#123; \"min\": 0, \"max\": 0, \"avg\": 0 &#125; &#125; &#125;, \"docs\": &#123; \"count\": 10, \"deleted\": 0 &#125;, \"store\": &#123; \"size\": \"16.2kb\", \"size_in_bytes\": 16684, \"throttle_time\": \"0s\", \"throttle_time_in_millis\": 0 &#125;, \"fielddata\": &#123; \"memory_size\": \"0b\", \"memory_size_in_bytes\": 0, \"evictions\": 0 &#125;, \"query_cache\": &#123; \"memory_size\": \"0b\", \"memory_size_in_bytes\": 0, \"total_count\": 0, \"hit_count\": 0, \"miss_count\": 0, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 &#125;, \"completion\": &#123; \"size\": \"0b\", \"size_in_bytes\": 0 &#125;, \"segments\": &#123; \"count\": 4, \"memory\": \"8.6kb\", \"memory_in_bytes\": 8898, \"terms_memory\": \"6.3kb\", \"terms_memory_in_bytes\": 6522, \"stored_fields_memory\": \"1.2kb\", \"stored_fields_memory_in_bytes\": 1248, \"term_vectors_memory\": \"0b\", \"term_vectors_memory_in_bytes\": 0, \"norms_memory\": \"384b\", \"norms_memory_in_bytes\": 384, \"doc_values_memory\": \"744b\", \"doc_values_memory_in_bytes\": 744, \"index_writer_memory\": \"0b\", \"index_writer_memory_in_bytes\": 0, \"version_map_memory\": \"0b\", \"version_map_memory_in_bytes\": 0, \"fixed_bit_set\": \"0b\", \"fixed_bit_set_memory_in_bytes\": 0, \"file_sizes\": &#123;&#125; &#125;, \"percolator\": &#123; \"num_queries\": 0 &#125; &#125;, \"nodes\": &#123; \"count\": &#123; \"total\": 1, \"data\": 1, \"coordinating_only\": 0, \"master\": 1, \"ingest\": 1 &#125;, \"versions\": [ \"5.6.3\" ], \"os\": &#123; \"available_processors\": 8, \"allocated_processors\": 8, \"names\": [ &#123; \"name\": \"Mac OS X\", \"count\": 1 &#125; ], \"mem\" : &#123; \"total\" : \"16gb\", \"total_in_bytes\" : 17179869184, \"free\" : \"78.1mb\", \"free_in_bytes\" : 81960960, \"used\" : \"15.9gb\", \"used_in_bytes\" : 17097908224, \"free_percent\" : 0, \"used_percent\" : 100 &#125; &#125;, \"process\": &#123; \"cpu\": &#123; \"percent\": 9 &#125;, \"open_file_descriptors\": &#123; \"min\": 268, \"max\": 268, \"avg\": 268 &#125; &#125;, \"jvm\": &#123; \"max_uptime\": \"13.7s\", \"max_uptime_in_millis\": 13737, \"versions\": [ &#123; \"version\": \"1.8.0_74\", \"vm_name\": \"Java HotSpot(TM) 64-Bit Server VM\", \"vm_version\": \"25.74-b02\", \"vm_vendor\": \"Oracle Corporation\", \"count\": 1 &#125; ], \"mem\": &#123; \"heap_used\": \"57.5mb\", \"heap_used_in_bytes\": 60312664, \"heap_max\": \"989.8mb\", \"heap_max_in_bytes\": 1037959168 &#125;, \"threads\": 90 &#125;, \"fs\": &#123; \"total\": \"200.6gb\", \"total_in_bytes\": 215429193728, \"free\": \"32.6gb\", \"free_in_bytes\": 35064553472, \"available\": \"32.4gb\", \"available_in_bytes\": 34802409472 &#125;, \"plugins\": [ &#123; \"name\": \"analysis-icu\", \"version\": \"5.6.3\", \"description\": \"The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.\", \"classname\": \"org.elasticsearch.plugin.analysis.icu.AnalysisICUPlugin\", \"has_native_controller\": false &#125;, &#123; \"name\": \"ingest-geoip\", \"version\": \"5.6.3\", \"description\": \"Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database\", \"classname\": \"org.elasticsearch.ingest.geoip.IngestGeoIpPlugin\", \"has_native_controller\": false &#125;, &#123; \"name\": \"ingest-user-agent\", \"version\": \"5.6.3\", \"description\": \"Ingest processor that extracts information from a user agent\", \"classname\": \"org.elasticsearch.ingest.useragent.IngestUserAgentPlugin\", \"has_native_controller\": false &#125; ] &#125;&#125; 内存使用和 GC 指标在运行 Elasticsearch 时，内存是您要密切监控的关键资源之一。 Elasticsearch 和 Lucene 以两种方式利用节点上的所有可用 RAM：JVM heap 和文件系统缓存。 Elasticsearch 运行在Java虚拟机（JVM）中，这意味着JVM垃圾回收的持续时间和频率将成为其他重要的监控领域。 上面返回的 JSON，监控的指标有我个人觉得有这些： nodes.successful nodes.failed nodes.total nodes.mem.used_percent nodes.process.cpu.percent nodes.jvm.mem.heap_used 可以看到 JSON 文件是很复杂的，如果从这复杂的 JSON 中获取到对应的指标（key）的值呢，这里请看文章 ：JsonPath —— JSON 解析神器 最后这里主要讲下 ES 集群的一些监控信息，有些监控指标是个人觉得需要监控的，但是具体情况还是得看需求了。下篇文章主要讲节点的监控信息。转载请注明地址：http://www.54tianzhisheng.cn/2017/10/15/ElasticSearch-cluster-health-metrics/ 参考资料1、How to monitor Elasticsearch performance 2、ElasticSearch 性能监控 3、cluster-health 4、cluster-stats 相关阅读1、Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、全文搜索引擎 Elasticsearch 集群搭建入门教程","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"JsonPath —— JSON 解析神器","date":"2017-10-13T16:00:00.000Z","path":"2017/10/14/JsonPath/","text":"真乃神器也，再复杂的 Json 都能给你解析出来，非常方便的获取 JSON 的内容，很强大！语法简介 JsonPath 描述 $ 根节点 @ 当前节点 .or[] 子节点 .. 选择所有符合条件的节点 * 所有节点 [] 迭代器标示，如数组下标 [,] 支持迭代器中做多选 [start:end:step] 数组切片运算符 ?() 支持过滤操作 () 支持表达式计算 JSON 值： 1234567891011121314151617181920&#123; \"store\": &#123; \"book\": [ &#123; \"category\": \"reference\", \"author\": \"Nigel Rees\", \"title\": \"Sayings of the Century\", \"price\": 8.95 &#125;, &#123; \"category\": \"fiction\", \"author\": \"Evelyn Waugh\", \"title\": \"Sword of Honour\", \"price\": 12.99, \"isbn\": \"0-553-21311-3\" &#125; ], \"bicycle\": &#123; \"color\": \"red\", \"price\": 19.95 &#125; &#125;&#125; 导包：import com.jayway.jsonpath.JsonPath 解析代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344//输出book[0]的author值String author = JsonPath.read(json, \"$.store.book[0].author\");System.out.println(\"author\\t\"+author);//输出全部author的值，使用Iterator迭代List&lt;String&gt; authors = JsonPath.read(json, \"$.store.book[*].author\");System.out.println(\"authors\\t\"+authors);//输出book[*]中category == 'reference'的bookList&lt;Object&gt; books = JsonPath.read(json, \"$.store.book[?(@.category == 'reference')]\");System.out.println(\"books\\t\"+books);//输出book[*]中category == 'reference'的book或者List&lt;Object&gt; books2 = JsonPath.read(json, \"$.store.book[?(@.category == 'reference' || @.price&gt;10)]\");System.out.println(\"books2\\t\"+books2);//输出book[*]中category == 'reference'的book的authorList&lt;Object&gt; books1 = JsonPath.read(json, \"$.store.book[?(@.category == 'reference')].author\");System.out.println(\"books1\\t\"+books1);//输出book[*]中price&gt;10的bookList&lt;Object&gt; b1 = JsonPath.read(json, \"$.store.book[?(@.price&gt;10)]\");System.out.println(\"b1\"+b1);//输出book[*]中含有isbn元素的bookList&lt;Object&gt; b2 = JsonPath.read(json, \"$.store.book[?(@.isbn)]\");System.out.println(\"b2\"+b2);//输出该json中所有price的值List&lt;Double&gt; prices = JsonPath.read(json, \"$..price\");System.out.println(\"prices\"+prices);//输出该json中所有title的值List&lt;Double&gt; title = JsonPath.read(json, \"$..title\");System.out.println(\"title\"+title);//输出该json中book 0,1的值List&lt;Double&gt; book01 = JsonPath.read(json, \"$..book[0,1]\");System.out.println(\"book01\"+book01);/* //输出该json中book 0,1的值List&lt;Double&gt; book012 = JsonPath.read(json, \"$..book[-2:]\");System.out.println(\"book012\"+book012);*///可以提前编辑一个路径，并多次使用它JsonPath path = JsonPath.compile(\"$.store.book[*]\");List&lt;Object&gt; b3 = path.read(json);System.out.println(\"path\\t\"+path+\"\\n\"+b3); 用法比较简单，多使用几次就会使用了！文章主要参考网上！原谅很多天不跟博的我现在竟然这样水了这么一篇文章，哈哈！实在是忙！","tags":[{"name":"JSON","slug":"JSON","permalink":"http://www.54tianzhisheng.cn/tags/JSON/"}]},{"title":"Centos7 搭建最新 Nexus3 Maven 私服","date":"2017-10-13T16:00:00.000Z","path":"2017/10/14/Nexus3-Maven/","text":"Maven 介绍Apache Maven 是一个创新的软件项目管理和综合工具。Maven 提供了一个基于项目对象模型（POM）文件的新概念来管理项目的构建，可以从一个中心资料片管理项目构建，报告和文件。Maven 最强大的功能就是能够自动下载项目依赖库。Maven 提供了开发人员构建一个完整的生命周期框架。开发团队可以自动完成项目的基础工具建设，Maven 使用标准的目录结构和默认构建生命周期。在多个开发团队环境时，Maven 可以设置按标准在非常短的时间里完成配置工作。由于大部分项目的设置都很简单，并且可重复使用，Maven 让开发人员的工作更轻松，同时创建报表，检查，构建和测试自动化设置。Maven 项目的结构和内容在一个 XML 文件中声明，pom.xml 项目对象模型（POM），这是整个 Maven 系统的基本单元。 Maven 提供了开发人员的方式来管理：1）Builds2）Documentation3）Reporting4）Dependencies5）SCMs6）Releases7）Distribution8）mailing list概括地说，Maven 简化和标准化项目建设过程。处理编译，分配，文档，团队协作和其他任务的无缝连接。Maven 增加可重用性并负责建立相关的任务。Maven 最初设计，是以简化 Jakarta Turbine 项目的建设。在几个项目，每个项目包含了不同的 Ant 构建文件。 JAR 检查到 CVS。Apache 组织开发 Maven 可以建立多个项目，发布项目信息，项目部署，在几个项目中 JAR 文件提供团队合作和帮助。 Maven 主要目标是提供给开发人员：1）项目是可重复使用，易维护，更容易理解的一个综合模型。2）插件或交互的工具，这种声明性的模式。 私服介绍私服是指私有服务器，是架设在局域网的一种特殊的远程仓库，目的是代理远程仓库及部署第三方构建。有了私服之后，当 Maven 需要下载构件时，直接请求私服，私服上存在则下载到本地仓库；否则，私服请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载。 Nexus 介绍Nexus 是一个强大的 Maven 仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。如果使用了公共的 Maven 仓库服务器，可以从 Maven 中央仓库下载所需要的构件（Artifact），但这通常不是一个好的做法。正常做法是在本地架设一个 Maven 仓库服务器，即利用 Nexus 私服可以只在一个地方就能够完全控制访问和部署在你所维护仓库中的每个 Artifact。Nexus 在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷, 节省外网带宽和时间，Nexus 私服就可以满足这样的需要。Nexus 是一套 “开箱即用” 的系统不需要数据库，它使用文件系统加 Lucene 来组织数据。Nexus 使用 ExtJS 来开发界面，利用 Restlet 来提供完整的 REST APIs，通过 m2eclipse 与 Eclipse 集成使用。Nexus 支持 WebDAV 与 LDAP 安全身份认证。Nexus 还提供了强大的仓库管理功能，构件搜索功能，它基于 REST，友好的 UI 是一个 extjs 的 REST 客户端，它占用较少的内存，基于简单文件系统而非数据库。 为什么要构建 Nexus 私服？如果没有 Nexus 私服，我们所需的所有构件都需要通过 maven 的中央仓库和第三方的 Maven 仓库下载到本地，而一个团队中的所有人都重复的从 maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。很多情况下项目的开发都是在内网进行的，连接不到 maven 仓库怎么办呢？开发的公共构件怎么让其它项目使用？这个时候我们不得不为自己的团队搭建属于自己的 maven 私服，这样既节省了网络带宽也会加速项目搭建的进程，当然前提条件就是你的私服中拥有项目所需的所有构件。 总之，在本地构建 nexus 私服的好处有：1）加速构建；2）节省带宽；3）节省中央 maven 仓库的带宽；4）稳定（应付一旦中央服务器出问题的情况）；5）控制和审计；6）能够部署第三方构件；7）可以建立本地内部仓库；8）可以建立公共仓库这些优点使得 Nexus 日趋成为最流行的 Maven 仓库管理器。 1. 安装 jdk1.8关于 jdk1.8 的安装, 在这里就不做赘述了 2. 安装 maven关于 maven 的安装, 本文在这里就不详细写了 3. 安装 nexus31. 下载 nexus-3.6.0-02-unix.tar.gz官网链接地址：https://www.sonatype.com/download-oss-sonatype 下载 linux 最新版本，直接下载速度可能很慢，建议用迅雷下载会快很多的。 2. 解压1tar -zxvf nexus-3.6.0-02-unix.tar.gz -C /usr/local/ 3. 启动 nexus312cd /usr/local/nexus-3.6.0-02/bin/./nexus run &amp; 稍等一会 (首次启动会比较慢), 当出现以下日志的时候表示启动成功! 12345-------------------------------------------------Started Sonatype Nexus OSS 3.6.0-02------------------------------------------------- 4. 开启远程访问端口关闭防火墙，并开启远程访问端口 8081 123vim /etc/sysconfig/iptables添加：-A INPUT -p tcp -m state --state NEW -m tcp --dport 8081 -j ACCEPT 5. 测试 123nexus3默认端口是:8081nexus3默认账号是:adminnexus3默认密码是:admin123 6. 设置开机自启动123ln -s /usr/local/nexus-3.6.0-02/bin/nexus /etc/init.d/nexus3chkconfig --add nexus3chkconfig nexus3 on 7. 修改 nexus3 的运行用户为 root1vim nexus.rc 12//设置run_as_user=&quot;root&quot; 8. 修改 nexus3 启动时要使用的 jdk 版本1vim nexus 第 14 行: 1INSTALL4J_JAVA_HOME_OVERRIDE=/usr/local/java/jdk1.8.0_144 9. 修改 nexus3 默认端口 (可选)12cd /usr/local/nexus-3.6.0-02/etc/vim nexus-default.properties 默认端口: 8081 1application-port=8081 10. 修改 nexus3 数据以及相关日志的存储位置 (可选)：12[root@MiWiFi-R3-srv bin]# cd /usr/local/nexus-3.6.0-02/bin/[root@MiWiFi-R3-srv bin]# vim nexus.vmoptions 123-XX:LogFile=./sonatype-work/nexus3/log/jvm.log-Dkaraf.data=./sonatype-work/nexus3-Djava.io.tmpdir=./sonatype-work/nexus3/tmp 出现上面 5 中的测试页面，说明配置 nexus 成功！ 点击右上角 “Log in”， 输入默认用户名 (admin) 和默认密码（admin123）登录 至此, nexus3_maven 的私服就搭建完成了!!! 可以点击上面的 “设置” 图标，在 “设置” 里可以添加用户、角色，对接 LDAP 等的设置，如下： 可以在 “管理” 里查看 nexus 的系统信息 注意：1.component name 的一些说明： 1）maven-central：maven 中央库，默认从 https://repo1.maven.org/maven2 / 拉取 jar 2）maven-releases：私库发行版 jar 3）maven-snapshots：私库快照（调试版本）jar 4）maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 基础配置 settings.xml 中使用。 2.Nexus 默认的仓库类型有以下四种： 1）group(仓库组类型)：又叫组仓库，用于方便开发人员自己设定的仓库； 2）hosted(宿主类型)：内部项目的发布仓库（内部开发人员，发布上去存放的仓库）； 3）proxy(代理类型)：从远程中央仓库中寻找数据的仓库（可以点击对应的仓库的 Configuration 页签下 Remote Storage Location 属性的值即被代理的远程仓库的路径）； 4）virtual(虚拟类型)：虚拟仓库（这个基本用不到，重点关注上面三个仓库的使用）； 3.Policy(策略): 表示该仓库为发布 (Release) 版本仓库还是快照 (Snapshot) 版本仓库； 4.Public Repositories 下的仓库 1）3rd party: 无法从公共仓库获得的第三方发布版本的构件仓库，即第三方依赖的仓库，这个数据通常是由内部人员自行下载之后发布上去； 2）Apache Snapshots: 用了代理 ApacheMaven 仓库快照版本的构件仓库 3）Central: 用来代理 maven 中央仓库中发布版本构件的仓库 4）Central M1 shadow: 用于提供中央仓库中 M1 格式的发布版本的构件镜像仓库 5）Codehaus Snapshots: 用来代理 CodehausMaven 仓库的快照版本构件的仓库 6）Releases: 内部的模块中 release 模块的发布仓库，用来部署管理内部的发布版本构件的宿主类型仓库；release 是发布版本； 7）Snapshots: 发布内部的 SNAPSHOT 模块的仓库，用来部署管理内部的快照版本构件的宿主类型仓库；snapshots 是快照版本，也就是不稳定版本所以自定义构建的仓库组代理仓库的顺序为：Releases，Snapshots，3rd party，Central。也可以使用 oschina 放到 Central 前面，下载包会更快。 5.Nexus 默认的端口是 8081，可以在 etc/nexus-default.properties 配置中修改。 6.Nexus 默认的用户名密码是 admin/admin123 当遇到奇怪问题时，重启 nexus，重启后 web 界面要 1 分钟左右后才能访问。 8.Nexus 的工作目录是 sonatype-work（路径一般在 nexus 同级目录下）12345678[root@master-node local]# pwd/usr/local[root@master-node local]# ls nexus/bin deploy etc lib LICENSE.txt NOTICE.txt public system[root@master-node local]# ls sonatype-work/nexus3[root@master-node local]# ls sonatype-work/nexus3/backup blobs cache db elasticsearch etc generated-bundles health-check instances keystores lock log orient port tmp Nexus 仓库分类的概念：1）Maven 可直接从宿主仓库下载构件, 也可以从代理仓库下载构件, 而代理仓库间接的从远程仓库下载并缓存构件2）为了方便, Maven 可以从仓库组下载构件, 而仓库组并没有时间的内容 (下图中用虚线表示, 它会转向包含的宿主仓库或者代理仓库获得实际构件的内容). Nexus 的 web 界面功能介绍1.Browse Server Content 1.1 Search这个就是类似 Maven 仓库上的搜索功能，就是从私服上查找是否有哪些包。注意：1）在 Search 这级是支持模糊搜索的，如图所示： 2）如果进入具体的目录，好像不支持模糊搜索，如图所示： 1.2 Browse 1）Assets这是能看到所有的资源，包含 Jar，已经对 Jar 的一些描述信息。2）Components这里只能看到 Jar 包。 2.Server Adminstration And configuration看到这个选项的前提是要进行登录的，如上面已经介绍登陆方法，右上角点击 “Sign In” 的登录按钮，输入 admin/admin123, 登录成功之后，即可看到此功能，如图所示： 2.1 Blob Stores文件存储的地方，创建一个目录的话，对应文件系统的一个目录，如图所示： 2.2 Repositories 1）Proxy这里就是代理的意思，代理中央 Maven 仓库，当 PC 访问中央库的时候，先通过 Proxy 下载到 Nexus 仓库，然后再从 Nexus 仓库下载到 PC 本地。这样的优势只要其中一个人从中央库下来了，以后大家都是从 Nexus 私服上进行下来，私服一般部署在内网，这样大大节约的宽带。创建 Proxy 的具体步骤1 点击 “Create Repositories” 按钮 2 选择要创建的类型 3 填写详细信息Name：就是为代理起个名字Remote Storage: 代理的地址，Maven 的地址为: https://repo1.maven.org/maven2/Blob Store: 选择代理下载包的存放路径 2）HostedHosted 是宿主机的意思，就是怎么把第三方的 Jar 放到私服上。Hosted 有三种方式，Releases、SNAPSHOT、MixedReleases: 一般是已经发布的 Jar 包Snapshot: 未发布的版本Mixed：混合的Hosted 的创建和 Proxy 是一致的，具体步骤和上面基本一致。如下： 注意事项：Deployment Pollcy: 需要把策略改成 “Allow redeploy”。 3）Group能把两个仓库合成一个仓库来使用，目前没使用过，所以没做详细的研究。 2.3 Security这里主要是用户、角色、权限的配置（上面已经提到了在这里添加用户和角色等） 2.4 Support包含日志及数据分析。 2.5 System主要是邮件服务器，调度的设置地方这部分主要讲怎么和 Maven 做集成, 集成的方式主要分以下种情况：代理中央仓库、Snapshot 包的管理、Release 包的管理、第三方 Jar 上传到 Nexus 上。 代理中央仓库只要在 PMO 文件中配置私服的地址（比如 http://192.168.1.14:8081）即可，配置如下： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven-central&lt;/id&gt; &lt;name&gt;maven-central&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-central/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt;&lt;/repositories&gt; Snapshot 包的管理1）修改 Maven 的 settings.xml 文件，加入认证机制 12345&lt;servers&gt; &lt;server&gt;&lt;id&gt;nexus&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 2）修改工程的 Pom 文件 123456789101112&lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;site&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Sites&lt;/name&gt; &lt;url&gt;dav:http://192.168.1.14:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/site&gt;&lt;/distributionManagement&gt; 注意事项: 上面修改的 Pom 文件如截图中的名字要跟 / usr/local/maven/conf/settings.xml 文件中的名字一定要对应上。 3）上传到 Nexus 上 1– 项目编译成的 jar 是 Snapshot(POM 文件的头部) 1234&lt;groupId&gt;com.zhisheng&lt;/groupId&gt;&lt;artifactId&gt;test-nexus&lt;/artifactId&gt;&lt;version&gt;1.0.0-SHAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 2– 使用 mvn deploy 命令运行即可（运行结果在此略过） 3– 因为 Snapshot 是快照版本，默认他每次会把 Jar 加一个时间戳，做为历史备份版本。 Releases 包的管理1）与 Snapshot 大同小异，只是上传到私服上的 Jar 包不会自动带时间戳2）与 Snapshot 配置不同的地方，就是工程的 PMO 文件，加入 repository 配置 1234567&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 3）打包的时候需要把 Snapshot 去掉 1234&lt;groupId&gt;com.zhisheng&lt;/groupId&gt;&lt;artifactId&gt;test-nexus&lt;/artifactId&gt;&lt;version&gt;1.0.0&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; | 第三方 Jar 上传到 Nexus[root@master-node src]# mvn deploy:deploy-file -DgroupId=org.jasig.cas.client -DartifactId=cas-client-core -Dversion=3.1.3 -Dpackag注意事项：-DrepositoryId=nexus 对应的就是 Maven 中 settings.xml 的认证配的名字。 最后搭建的时候是参考网上博客，写篇完整的博客再回馈给网上。转载请注明地址：http://www.54tianzhisheng.cn/2017/10/14/Nexus3-Maven/","tags":[{"name":"Maven","slug":"Maven","permalink":"http://www.54tianzhisheng.cn/tags/Maven/"}]},{"title":"Google Guava 缓存实现接口的限流","date":"2017-09-22T16:00:00.000Z","path":"2017/09/23/Guava-limit/","text":"项目背景最近项目中需要进行接口保护，防止高并发的情况把系统搞崩，因此需要对一个查询接口进行限流，主要的目的就是限制单位时间内请求此查询的次数，例如 1000 次，来保护接口。参考了 开涛的博客聊聊高并发系统限流特技 ，学习了其中利用 Google Guava 缓存实现限流的技巧，在网上也查到了很多关于 Google Guava 缓存的博客，学到了好多，推荐一个博客文章：http://ifeve.com/google-guava-cachesexplained/, 关于 Google Guava 缓存的更多细节或者技术，这篇文章讲的很详细；这里我们并不是用缓存来优化查询，而是利用缓存，存储一个计数器，然后用这个计数器来实现限流。 效果实验1234567static LoadingCache&lt;Long, AtomicLong&gt; count = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long o) throws Exception &#123; //System.out.println(\"Load call!\"); return new AtomicLong(0L); &#125; &#125;); 上面，我们通过 CacheBuilder 来新建一个 LoadingCache 缓存对象 count，然后设置其有效时间为 1 秒，即每 1 秒钟刷新一次；缓存中，key 为一个 long 型的时间戳类型，value 是一个计数器，使用原子性的 AtomicLong 保证自增和自减操作的原子性， 每次查询缓存时如果不能命中，即查询的时间戳不在缓存中，则重新加载缓存，执行 load 将当前的时间戳的计数值初始化为 0。这样对于每一秒的时间戳，能计算这一秒内执行的次数，从而达到限流的目的；这是要执行的一个 getCounter 方法： 123456public class Counter &#123; static int counter = 0; public static int getCounter() throws Exception&#123; return counter++; &#125;&#125; 现在我们创建多个线程来执行这个方法： 12345678910111213141516171819202122public class Test &#123; public static void main(String args[]) throws Exception &#123; for(int i = 0;i&lt;100;i++) &#123; new Thread()&#123; @Override public void run() &#123; try &#123; System.out.println(Counter.getCounter()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); &#125; &#125;&#125; 这样执行的话，执行结果很简单，就是很快地执行这个 for 循环，迅速打印 0 到 99 折 100 个数，不再贴出。这里的 for 循环执行 100 个进程时间是很快的，那么现在我们要限制每秒只能有 10 个线程来执行 getCounter() 方法，该怎么办呢，上面讲的限流方法就派上用场了： 12345678910111213141516171819202122public class Counter &#123; static LoadingCache&lt;Long, AtomicLong&gt; count = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long o) throws Exception &#123; System.out.println(\"Load call!\"); return new AtomicLong(0L); &#125; &#125;); static long limits = 10; static int counter = 0; public static synchronized int getCounter() throws Exception&#123; while (true) &#123; //获取当前的时间戳作为key Long currentSeconds = System.currentTimeMillis() / 1000; if (count.get(currentSeconds).getAndIncrement() &gt; limits) &#123; continue; &#125; return counter++; &#125; &#125;&#125; 这样一来，就可以限制每秒的执行数了。对于每个线程，获取当前时间戳，如果当前时间 (当前这 1 秒) 内有超过 10 个线程正在执行，那么这个进程一直在这里循环，直到下一秒，或者更靠后的时间，重新加载，执行 load，将新的时间戳的计数值重新为 0。执行结果：每秒执行 11 个（因为从 0 开始），每一秒之后，load 方法会执行一次； 12345678910111213141516171819202122232425为了更加直观，我们可以让每个for循环sleep一段时间：public class Test &#123; public static void main(String args[]) throws Exception &#123; for(int i = 0;i&lt;100;i++) &#123; new Thread()&#123; @Override public void run() &#123; try &#123; System.out.println(Counter.getCounter()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); Thread.sleep(100); &#125; &#125;&#125; 在上述这样的情况下，一个线程如果遇到当前时间正在执行的线程超过 limit 值就会一直在 while 循环，这样会浪费大量的资源，我们在做限流的时候，如果出现这种情况，可以不进行 while 循环，而是直接抛出异常或者返回，来拒绝这次执行（查询），这样便可以节省资源。 最后本篇文章地址： http://www.54tianzhisheng.cn/2017/09/23/Guava-limit/","tags":[{"name":"Guava","slug":"Guava","permalink":"http://www.54tianzhisheng.cn/tags/Guava/"}]},{"title":"面试过阿里等互联网大公司，我知道了这些套路","date":"2017-09-16T16:00:00.000Z","path":"2017/09/17/Interview-summary/","text":"前面感谢一波因为看到掘金在做秋招求职征文大赛，赞助商也有牛客网，自己前段时间也稍微写了篇博客总结我的大学生活，那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 博客中稍微简单的介绍了下自己的求职，重点是推荐了下我自己看过的那些书籍，对我帮助真的很大。 如今借这么个机会，回馈掘金和牛客网，想想自己这一年在掘金也写过不少文章，从 0 个粉丝到如今被 11047 人（截止写此篇文章时）关注，有点小激动，竟然这么多粉，也不知道真正活跃的用户有多少。不管怎样，这一年在掘金还是收获很多的，不仅可以阅读到很多大神的文章，学习新的知识，而且还遇到了好几个不错的哥们，如今平常也有和他们交流，比如 ：芋道源码 老哥人就很不错，在上海还和老哥见过面，吃过饭，平常对我帮助也很大，会推荐一些很有用的书籍给我看。欢迎大家关注他的博客：芋道源码的博客 ，里面有好几系列的源码分析博客文章呢。至于牛客网，我就更是老用户了，印象中好像是大一的时候注册的，那时有空的话就会去上面刷几道基础题，写写题解，坚持了好久了，如今早已是红名了。（其实是水出来的，哈哈）在牛客网遇到的大神也是超多，好多朋友几乎都是通过牛客网认识的，那时早的时候一起在一群讨论问题，别提那场面了，震惊，我等弱渣瑟瑟发抖。感谢叶神，左神，牛妹！ 说着说着，好像偏题了。 正式进入话题吧！ 正文开始本篇秋招求职征文主要分享如下几方面：招聘职位需求套路 、招聘面试的套路、简历撰写套路、简历投递套路 、找工作经历 、自己面试面经 、实习感悟、书籍推荐 、优秀网站推荐 、优秀博客推荐 、求职资料放送。 招聘职位需求套路摘举下几个公司的招聘需求：（from lagou） 1、Java开发校招生( 有赞 ) 职位诱惑：福利好待遇佳，技术氛围浓，有大牛带成长快职位描述： 有赞2018校招官方网申地址（请在官网投递，勿直接在Lagou上投递）：https://job.youzan.com/campus岗位职责 我们拥有世界级的 SaaS 电商解决方案，每天处理几百万订单、几亿条消息，并且量级不断攀升； 我们开放了有赞云，连接了数十万开发者，大大提升了 SaaS 对商家产生的价值； 我们正在新零售的潮流中激流勇进、开疆拓土，用产品技术撬动巨大的市场； 而你的工作，就是参与这些大流量系统的研发，哪怕提升1%的性能和稳定性都将是激动人心的时刻。 岗位要求 2018届本科及以上学历应届毕业生，计算机或者软件工程相关专业； 具备扎实的计算机基础知识，至少熟练使用一门主流开发语言； 积极参与开发实践，如果拥有引以为豪的项目经历则加分； 热衷数据结构与算法，如果一不小心在 ACM 赛场摘过金，夺过银则加分； 能在 Linux 上写任何脚本，比王者荣耀上手还快则加分； 快速学习新鲜事物，自我驱动追求卓越，积极应对问题和变化。 2、京东居家生活事业部-汽车用品招聘实习生（2018届） 职位诱惑：京东商城 职位描述：京东商城-汽车用品部门招聘实习生 我们需要这样的你： 2018届毕业生（本科或硕士均可） 学习能力强 担当、抗压、接受变化 能长期实习（优秀者有转正机会） 需要一个大的平台来展示和发挥自己的能力 你将收获： 重新认识快速成长的自己 一份世界500强的实习经历 一群优秀的伙伴 3、爱奇艺 Java 实习生 - 游戏事业部 要求：至少 6 个月以上每周三天以上实习。 本科以上学历，计算机、软件工程相关专业； 基础扎实，熟悉 Java 编程，熟悉 Spring、MyBatis 等框架优先； 熟悉 SQL 语句，熟练使用 MySQL 数据库； 良好的沟通、表达、协调能力，富有激情，学习能力强； 有 GitHub 账号或者技术博客优先； 热爱游戏行业优先。 这里随便找了三个，从招聘需求里看，好多公司目前招聘的话在招聘需求中并不怎么会写的很清楚，有的也不会说明要求的技术栈，这其实有时会对我们这种新人来说，有点不好的，这样的话我们就没有明确的目标去复习，还有就是一些加分项，其实也是有点帮助的。就比如有些招聘上面的说有优秀博客和 GitHub 者优先，这两点的话我们其实可以在大学慢慢积累出来的，对面试确实有帮助，我好些面试机会都是靠这两个的。还有套路就是，别光信他这招聘需求，进去面试可能就不问你这些方面的问题了，那些公司几乎都是这么个套路：面试造火箭，入职拧螺丝 ！ 进去公司之前可能需要你懂很多东西，但是进去的话还只是专门做一方面的东西。不管怎样，如果你有机会进去大公司的话（而且适合去），还是去大公司吧，出来大厂光环不少。 认真耐心地拧螺丝钉，说不定有机会去造大火箭——正规大公司的节奏。 短时间把螺丝拧出花，说不定有机会造小火箭——上升中创业公司的节奏。 招聘面试的套路参考：https://mp.weixin.qq.com/s/qRwDowetBkJqpeMeAZsIpA 一个在掘金上认识的老哥，在京东工作，写的不错，干脆分享下。大家可以去看他的博客，http://mindwind.me/ 当时我求职的时候通过作者博客也学到不少东西。 一次集中的扩招需求，有点像每年一度的晋升评审，都需要对大量的候选人进行定级评审，因为每一个新招聘的人员都会对其有一个定级的过程。 维度： 通用能力：考察其沟通表达、学习成长等 专业知识：考察其知识的掌握、深度、广度等 专业能力：考察其技能应用的能力和结果 工作业绩：考察其工作成果、产出、创新点等 价值观：考察其认知、理解、行为等 整个面试过程会包括下面几个部分： 自我介绍一开始的简短自我介绍，考察点在于对自我的总结、归纳和认知能力。观察其表达的逻辑性和清晰性，有个整体印象。 项目经历一般我不会专门问一些比较死的专业技术点之类的知识，都是套在候选人的项目经历和过往经验中穿插。通过其描述，来判断其掌握知识点的范围和深度，以及在实际的案例中如何运用这些知识与技能解决真正的问题的。 所以，不会有所谓的题库。每一个我决定面试的候选人，都是提前细读其简历，提炼场景和发掘需要问的问题，相当于面试前有个二三十分钟的备课过程，组织好面试时的交互过程与场景，以顺利达到我想要了解的点。 团队合作通常还会问候选人其所在团队中的角色，他们的工作模式、协作方式，并给出一些真实的场景化案例观察其应对的反应。评价一下关于他周围的同事、下属或领导，了解他在团队中的自我定位。这里的考察点是沟通协作方面的通用能力。 学习成长这个维度考察的关键点包括：成长潜力、职业生涯规划的清晰度。人与人之间成长速度的关键差距，我自己观察得出的结论在于：自驱力。而路径的清晰性，也是产生自驱的一个源动力，否则可能会感觉迷茫，而陷于困顿。 文化匹配这算是价值观的一部分吧。其实，这是最难考核的，我没有什么好方法，基本靠感觉。曾经有过好几次碰到经历和技能都不错的人，但总是感觉哪里不对，但又着急要人，就放进来了。但最终感觉是对的，合作很快就结束了，人也走了。 综合评价总结点评候选人的优势、劣势并进行技术定级，定级也没有绝对标准，而是相对的。我一般就是和周围觉得差不多级别的人的平均水准比较下，大概就会有一个技术级别的判断。 套路 招聘面试，其实是一个对人的筛选，而筛选的本质是匹配 —— 匹配人与职位。第一，你得非常清楚地理解，这个职位需要什么样属性的人。第二，确定你的候选人是否拥有这个职位要求的必须属性。那么，首先回答第一个问题，一般的职位需要什么样的属性？ 属性，又可以进一步拆解为三个层次。第一层次是「技能（Skills）」，技能是你习得的一种工具，就像程序员会用某种语言和框架来编写某类应用程序。第二层次是「能力（Abilities）」，能力是你运用工具的思考和行为方式，用同样的语言和框架编写同样程序的程序员能力可以差别很大。而第三层次是「价值观（Values）」，价值观是一个人根深蒂固的信念以及驱动行为的原因与动力所在。 简历撰写套路参考：https://mp.weixin.qq.com/s/3f8hGAQ-auLdkxkQ8XG3CQ 简历，是如此重要，它是获得一份满意工作的敲门砖，但不同的简历敲门的声响可不同。 但很多时候简历给人的感觉也似乎微不足道，因为没有人会真正细致的去读一份简历。而仅仅是快速的浏览一遍，就几乎同时对一个候选人形成了一种要么强烈，要么无感的印象。现实中的真实情况是，你的简历只有十几二十秒的时间窗口机会会被浏览到，然后就决定了能否进入下一步。 要让面试官看了你的简历后：知道你做过什么？看看技能、经历与岗位需求的匹配度，然后再问问你是谁？你通过简历散发出来的味道是什么感觉，我愿意和这样的人一起共事么？ 一份简历的最少必要内容包括： 个人信息 姓名 年龄 手机 邮箱 教育经历 博士（硕士、本科） 有多个全部写出来，最高学历写在上面 工作经历（最匹配职位需求的，挑选出来的 TOP3 的项目） 项目1 项目背景上下文（场景、问题） 你在其中的角色（职责、发挥的作用、结果度量） 与此项经历有关的知识与技能（技术栈） 项目2 项目3 附加信息 博客：持续有内容，不碎碎念 开源：GitHub 持续 commit 社区：有一定专业影响力的 书籍：用心写的 演讲：行业大会级别的 专利：凑数的就算了 论文：学术界比较有影响力的 爱好：真正的兴趣点 对于我们学生，缺乏工作经历，那就写写独特的学习或实习经历。同学们大家都共有的经历就不要随便写上去凑数了。对于学生，看重的是通用能力，学习能力，适应能力以及对工作的态度和热情。如果没有区分度高的经历，那么有作品也是很好的。比如将你的做的网站部署出来，把地址写在简历上。 关于技术栈部分的技术术语，很多程序员不太注意。比如，把 Java 写成 java 或 JAVA，Java 已是一个专有品牌名词，大小写要完全符合，这一点和 iOS 类似（i 小写，OS 大写）。另外，像 HTML，CSS 则全部大写，因为这是多个单词的缩写。一些小小的细节就能读出你的专业性和散发出来的味道。最后，技术术语不是罗列得多就好，不是真正熟练的技能，不要轻易写进简历。因为这将给你自己挖坑。你可以将你自己擅长的或者很熟的知识点写进去，有时想着重就加粗或者打个括号，这样可以挖坑给面试官，让他去问你熟悉的（前提要确保你真的能讲清楚，我试过这个方法很有效的）。 然后就是简历格式了，最好是 PDF 了，Word 在不同的电脑上的打开效果可能不一样，格式可能会变，况且有些人的电脑不一定装了 Word，不过我喜欢用 Markdown 写简历，简洁，适合程序员，然后把 Markdown 转换成 PDF 出来。 简历投递套路内推 有内推通道尽量走内推通道，不知道方便多少，而且成功几率也很大！找熟人，找学长学姐吧！牛客网讨论区很多内推帖子，可以去找找。不过今年的好多公司的内推通道都不咋管用了，套路越来越多了。记得去年好多公司内推都是免笔试，直接进入面试阶段，今年直接变成内推免简历筛选，进入笔试。因为现在的内推越来越不靠谱，直接面试的话，会增加公司的面试成本，干脆笔试再筛选一部分人。 拉勾网 拉勾上还是算不错的。 Boss 直聘 虽说前段时间出现了程序员找工作进入传销最后导致死亡的惨事发生，但是里面总比智联招聘和前程无忧靠谱点。因为智联招聘和前程无忧几乎被广告党和培训机构给占领了。 脉脉 里面招应届生和实习生比较少，但是也有，可以试试。 总之，简历投递给公司之前，请确认下这家公司到底咋样，先去百度了解下，别被坑了，每个平台都有一些居心不良的广告党等着你上钩，千万别上当！！！ 找工作经历这段经历，算是自己很难忘记的经历吧。既辛酸既充实的日子！也很感谢自己在这段时间的系统复习，感觉把自己的基础知识再次聚集在一起了，自己的能力在这一段时间提升的也很快。后面有机会的话我也想写一系列的相关文章，为后来准备工作（面试）的同学提供一些自己的帮助。自己在找工作的这段时间面过的公司也有几家大厂，但是结果都不是很好，对我自己有很大的压力，当时心里真的感觉 ：“自己真的有这么差”，为什么一直被拒，当时很怀疑自己的能力，自己也有总结原因。一是面试的时候自己准备的还不够充分，虽说自己脑子里对这些基础有点印象，但是面试的时候自己稍紧张下就描述不怎么清楚了，导致面试官觉得你可能广度够了，深度还不够（这是阿里面试官电话面试说的）；二是自己的表达能力还是有所欠缺，不能够将自己所要表达的东西说出来，这可能我要在后面加强的地方；三是我的学校问题。在面了几家公司失败后，终于面了家公司要我了，我也确定在这家公司了。很幸运，刚出来，就有一个很好（很负责）的架构师带我，这周就给了我一个很牛逼的项目给我看，里面新东西很多，说吃透了这个项目，以后绝对可以拿出去吹逼（一脸正经.jpg）。找工作期间，自己也经常去收集一些博客，并把它保存下来，这样能够让自己下次更好的系统复习，还在牛客网整理了很多面经，每天看几篇面经，知道面试一般问什么问题，都有啥套路，其实你看多了面经就会发现，面试考的题目几乎都差不多，区别不是很大。目前我的找工作经历就简短的介绍到这里了，如果感兴趣的话，可以加群：528776268 期待志同道合的你。 自己面试面经亚信地址：http://www.54tianzhisheng.cn/2017/08/04/yaxin/ 1）自我介绍（说到一个亮点：长期坚持写博客，面试官觉得这个习惯很好，算加分项吧） 2）看到简历项目中用到 Solr，详细的问了下 Solr（自己介绍了下 Solr 的使用场景和建立索引等东西） 3）项目里面写了一个 “ 敏感词和 JS 标签过滤防 XSS 攻击”，面试官让我讲了下这个 XSS 攻击，并且是怎样实现的 4）项目里写了支持 Markdown，问是不是自己写的解析代码，（回答不是，自己引用的是 GitHub上的一个开源项目解析的） 5）想问我前端的知识，我回复到：自己偏后端开发，前端只是了解，然后面试官就不问了 6）问我考不考研？ 7）觉得杭州怎么样？是打算就呆在杭州还是把杭州作为一个跳板？ 8）有啥小目标？以后是打算继续技术方向，还是先技术后管理（还开玩笑的说：是不是赚他几个亿，当时我笑了笑） 9）有啥兴趣爱好？ 总结：面试问的问题不算多，主要是通过简历上项目所涉及的东西提问的，如果自己不太会的切记不要写上去。面试主要考察你回答问题来判断你的逻辑是否很清楚。 爱奇艺地址：http://www.54tianzhisheng.cn/2017/08/04/iqiyi/ 笔试（半个小时）题目：（记得一些） 1、重载重写的区别？ 2、转发和重定向的区别？ 3、画下 HashMap 的结构图？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 4、statement 和 preparedstatement 区别？ 5、JSP 中一个 中取值与直接取值的区别？会有什么安全问题？ 6、实现一个线程安全的单例模式 7、一个写 sql 语句的题目 8、自己实现一个 List，（主要实现 add等常用方法） 9、Spring 中 IOC 和 AOP 的理解？ 10、两个对象的 hashcode 相同，是否对象相同？equal() 相同呢？ 11、@RequestBody 和 @ResponseBody 区别？ 12、JVM 一个错误，什么情况下会发生？ 13、常用的 Linux 命令？ 第一轮面试（80 分钟）1、自我介绍 2、介绍你最熟悉的一个项目 3、讲下这个 XSS 攻击 4、HashMap 的结构？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 5、HashMap 中怎么解决冲突的？（要我详细讲下） 6、ConcurrentHashMap 和 HashTable 中线程安全的区别？为啥建议用 ConcurrentHashMap ？能把 ConcurrentHashMap 里面的实现详细的讲下吗？ 7、Session 和 Cookie 的区别？ 8、你项目中登录是怎样做的，用的 Cookie 和 Session？ 9、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 10、问了好几个注解的作用？ 11、statement 和 preparedstatement 区别？ 12、$ 和 # 的区别？以及这两个在哪些地方用？ 13、前面项目介绍了数据是爬虫爬取过来的，那你讲讲你的爬虫是多线程的吧？ 14、讲讲 Python 中的多线程和 Java 中的多线程区别？ 15、自己刚好前几天在看线程池，立马就把面试官带到我熟悉的线程池，和面试官讲了下 JDK 自带的四种线程池、ThreadPoolExecutor 类中的最重要的构造器里面的七个参数，然后再讲了下线程任务进入线程池和核心线程数、缓冲队列、最大线程数量比较。 16、线程同步，你了解哪几种方式？ 17、讲下 Synchronized？ 18、讲下 RecentLock 可重入锁？ 什么是可重入锁？为什么要设计可重入锁？ 19、讲下 Volatile 吧？他是怎样做到同步的？ 20、Volatile 为什么不支持原子性？举个例子 21、Atomic 怎么设计的？（没看过源码，当时回答错了，后来才发现里面全部用 final 修饰的属性和方法） 22、问几个前端的标签吧？（问了一个不会，直接说明我偏后端，前端只是了解，后面就不问了） 23、SpringBoot 的了解？ 24、Linux 常用命令？ 25、JVM 里的几个问题？ 26、事务的特性？ 27、隔离级别？ 28、网络状态码？以 2、3、4、5 开头的代表什么意思。 29、并发和并行的区别？ 30、你有什么问题想问我的？ 一面面完后面试官和说这份试卷是用来考 1~3 年开发工作经验的，让我准备一下，接下来的二面。 第二轮面试（半个小时）1、一上来就问怎么简历名字都没有，我指了简历第一行的我的名字，还特意大写了，然后就问学校是不是在上海，我回答在南昌（感觉被鄙视了一波，后面我在回答问题的时候面试官就一直在玩手机，估计后面对我的印象就不是很好了） 2、自我介绍 3、说一说数据库建表吧（从范式讲） 4、讲讲多态？（这个我答出来了，可是面试官竟然说不是这样吧，可能面试官没听请，后面还说我是不是平时写多态比较少，感觉这个也让面试官对我印象减分） 5、将两个数转换（不借助第三个参数） 6、手写个插入排序吧（写完了和面试官讲了下执行流程） 7、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 8、问了几个常用的 Linux 命令？ 9、也问到多线程？和一面一样把自己最近看的线程池也讲了一遍 10、学 Java 多久了？ 11、你有什么想问的？ 总结：面试题目大概就是这么多了，有些问题自己也忘记了，面试题目顺序不一定是按照上面所写的。再次感谢爱奇艺的第一面面试官了，要不是他帮忙内推的，我可能还没有机会收到面试机会。自己接到爱奇艺面试邀请电话是星期一晚上快7点中的，之后加了面试官微信约好了星期四面试的（时间准备较短，之前没系统的复习过）。星期四一大早（5点就起床了），然后就收拾了下，去等公交车，转了两次车，然后再做地铁去爱奇艺公司的，总共路上花费时间四个多小时。总的来说，这次面试准备的时间不是很充裕，所以准备的个人觉得不是很好，通过这次的面试，发现面试还是比较注重基础和深度的，我也知道了自己的一些弱处，还需要在哪里加强，面试技巧上也要掌握些。为后面的其他公司继续做好充足的准备。加油！！！ 阿里地址：http://www.54tianzhisheng.cn/2017/08/04/alibaba/ （菜鸟网络部门）（49 分钟） 2017.08.02 晚上9点21打电话过来，预约明天什么时候有空面试，约好第二天下午两点。 2017.08.03 下午两点10分打过来了。 说看了我的博客和 GitHub，觉得我学的还行，知识广度都还不错，但是还是要问问具体情况，为什么没看到你春招的记录，什么原因没投阿里？非得说一个原因，那就是：我自己太菜了，不敢投。 1、先自我介绍 2、什么是多态？哪里体现了多态的概念？ 3、HashMap 源码分析，把里面的东西问了个遍？最后问是不是线程安全？引出 ConcurrentHashMap 4、ConcurrentHashMap 源码分析 5、类加载，双亲委托机制 6、Java内存模型（一开始说的不是他想要的，主要想问我堆和栈的细节） 7、垃圾回收算法 8、线程池，自己之前看过，所以说的比较多，最后面试官说了句：看你对线程池了解还是很深了 9、事务的四种特性 10、什么是死锁？ 11、乐观锁和悲观锁的策略 12、高可用网站的设计（有什么技术实现） 13、低耦合高内聚 14、设计模式了解不？你用过哪几种，为什么用，单例模式帮我们做什么东西？有什么好处？ 15、你参与什么项目中成长比较快？学到了什么东西，以前是没有学过的？ 16、项目中遇到的最大困难是怎样的？是怎么解决的？ 17、智力题（两根不均匀的香，点一头烧完要一个小时，怎么确定15分钟） 18、你有什么问题想要问我的？ 19、问了菜鸟网络他们部门主要做什么？ 20、对我这次面试做个评价：看了你博客和 GitHub，知道你对学习的热情还是很高的，花了不少功夫，后面有通知！ 总结：面试总的来说，第一次电话面试，感觉好紧张，好多问题自己会点，但是其中的细节没弄清楚，自己准备的也不够充分。面试官很友好，看到我紧张，也安慰我说不要紧，不管以后出去面试啥的，不需要紧张，公司问的问题可能很广，你只需要把你知道的说出来就行，不会的直接说不会就行。之前一直不敢投阿里，因为自己准备的完全不够充分，但是在朋友磊哥的帮助下，还是试了下，不管结果怎么样，经历过总比没有的好。 后面说有通知，结果并没有，只看到官网的投递按钮变灰了。在掘金上一个朋友（我隔壁学校的），当时看我挂了说要不要让他租一起的隔壁邻居再内推下淘宝，我想想还是算了，自己目前能力真的是有限，达不到进阿里的要求！不过还是要感谢那个哥们，人真的超级好，虽然我们未曾谋面，但是有机会的话，我一定会请你吃饭的。 哔哩哔哩首先直接根据简历项目开问，自我介绍都没有。 1、登录从前端到后端整个过程描述一遍？越详细越好，说到密码加密，网络传输，后台验证用户名和密码，Cookie 设置等。具体问我密码加密是前台还是后台加密，说了在后台加密？面试官说，那你做这个项目有什么意思？密码传输都是明文的，默认 HTTP 传递是明文传输，当时被面试官带进前台加密还是后台加密的沟里去了，没想到用 HTTPS ，后来后来的路上查了些资料才知道的，面试过程中他很想我说前台加密，但是前台加密算法那代码就摆在那里，很容易就给破解了吧，也没给点提示说 HTTPS，我只好投降 2、写一个查询的 sql 语句 3、线程同步的方法？Synchronized、Volatile、（面试官好像觉得 Volatile 不可以做到同步，我和他说了半天的 Volatile 原理 ，他竟然不认同，我开始怀疑他的实力了）、ThreadLocal、Atomic。 说到这些了，我当时竟然没把他带进我我给他挖的坑里去（线程池，之前好好研究过呢，可惜了） 4、Spring IOC 和 AOP 的理解？叫我写 AOP 的代码，我没写 5、JDK 动态代理和 Cglib 代理区别？ 5、你觉得项目里面你觉得哪些技术比较好？我指了两个，然后他也没有问下去。 6、解释下 XSS 攻击 7、Spring 和 SpringBoot 的区别？ 8、JVM 垃圾回收算法？分代中为什么要分三层？ 9、OOM 是什么？什么情况会发生？ 10、你觉得你有啥优点？ 然后就叫我等一会，一会有人事来通知我，结果过了一会人事叫我可以回去等通知了。 总结：到公司的时候已经一点多钟了，面试直接在一个很多人的地方（吃饭的地方）直接面的，周围还有人再吃饭，场景有点尴尬，面试过程感觉很随意，想到什么问题就问什么，完全没有衔接，问到的有些地方感觉面试官自己都不清楚，还怀疑我所说的，另外就是问题比较刁钻，总体技术也就那样吧！ 目前所在公司当时是我现在的老大（架构师）面的，先是电话面试过一次，问的问题也比较难，不过最后还是觉得我基础还是不错的。最后叫我去公司面试下，来到公司面试问的问题那就更难了，几乎好多都回答不出来，但是简单的说了下思路，最后再叫主任面试了下，问的问题就很简单了，最后就是 HR 面了，主要说了下工资问题和什么时候能报道！这几次面试的问题当时由于时间比较紧，也没去整理，现在也记不清楚了！目前自己已经工作了快一个月了，给的项目也完全是新东西，对我的挑战也很大，有时自己也确实不怎么知道，不过我老大很耐心的教我，对我也很不错，这也是我打算留在这里的原因，碰到个好老大不易！必须好好珍惜！ 实习感悟进公司是架构运维组中的 Java 实习开发，目前实习已经快一个月了，说实话，实习后才发现一天真的很忙，写下这篇征文也是在周末整理大晚上写的。刚进公司就给了一个 Consul 的服务发现与注册和健康检查的项目，里面涉及的东西有 Consul、Docker、Nginx、Lua、ElasticSearch 还有几个很轻量级的框架，对我来说几乎都是新东西，确实需要时间去了解，再优化和改里面的 bug 的过程中，幸好我老大和我理了几次思路，才让我对整个项目有所进展，后续继续是在优化这项目（可能以后这个项目的所有东西都是我来做）。在上海，住的地方离公司有一定的距离，上班几乎要一个小时，每天花在上班路上的时间很多，这也导致我每天感觉很忙。公司上班时间比较弹性，无打卡，虽说公司不加班，但是每天自己都不怎么会按点下班，自己也想在实习阶段多学点东西！这段时间也是最关键的时间，碰到个问题，要花好久时间才能解决，也有可能未必解决得了，有时觉得自己啥都不会，这么点东西都做不好，有点否定自己。这也确实是自己的技术知识栈缺乏，和自己学的 SSM、Spring Boot 这些都不相关，也不怎么写业务逻辑代码。所以感觉很痛苦，不像自己以前写的代码那样顺畅，当然可能是自己以前自己写的项目太 low 了。 看到掘金-凯伦征文中写到： 公司其实并不期望刚刚进来的你，能够创造多少价值。新人是要成长的，在成长期难免会遇到各种各样的小问题，这可能是大多数人的必经之路，因为你所看到的同事，他们都比你在工作领域待的时间更久，有更多的经验，可以把他们作为目标，但不要把他们作为现在自己的标准，那样会压力太大。 感觉这段话对我现在很受用！ 加油，好好挺过这个阶段，别轻易说放弃！ 书籍推荐大学，我不怎么喜欢玩游戏，自己也还算不怎么堕落吧，看了以下的一些书籍，算是对我后面写博客、找工作也有很大的帮助。如果你是大神，请忽略，如果你还是还在大学，和我一样不想把时间浪费在游戏上，可以看看我推荐的一些书籍，有想讨论的请在评论下留下你的评论或者加上面给的群号。 Java1、《Java 核心技术》卷一 、卷二 两本书，算是入门比较好的书籍了 2、《疯狂 Java 讲义》 很厚的一本书，里面的内容也是很注重基础了 3、《Java 并发编程的艺术》—— 方腾飞 、魏鹏、程晓明著 方腾飞 是并发编程网的创始人，里面的文章确实还不错，可以多看看里面的文章，收获绝对很大。 4、《 Java多线程编程核心技术》—— 高洪岩著 这本书也算是入门多线程编程的不错书籍，我之前还写了一篇读书笔记呢，《Java 多线程编程核心技术》学习笔记及总结 , 大家如果不想看书的可以去看我的笔记。 5、《Java 并发编程实战》 这本书讲的有点难懂啊，不过确实也是一本很好的书，以上三本书籍如果都弄懂了，我觉得你并发编程这块可能大概就 OK 了，然后再去看看线程池的源码，了解下线程池，我觉得那就更棒了。不想看的话，请看我的博客：Java 线程池艺术探索 我个人觉得还是写的很不错，那些大厂面试也几乎都会问线程池的东西，然后大概内容也就是我这博客写的 6、《Effective Java》中文版 第二版 算是 Java 的进阶书籍了，面试好多问题也是从这出来的 7、《深入理解 Java 虚拟机——JVM高级特性与最佳实践》第二版 这算是国内讲 JVM 最清楚的书了吧，目前还是只看了一遍，后面继续啃，大厂面试几乎也是都会考 JVM 的，阿里面 JVM 特别多，想进阿里的同学请一定要买这本书去看。 8、《深入分析Java Web技术内幕 修订版》许令波著 里面知识很广，每一章都是一个不同的知识，可见作者的优秀，不愧是阿里大神。 9、《大型网站系统与 Java 中间件实践》—— 曽宪杰 著 作者是前淘宝技术总监，见证了淘宝网的发展，里面的讲的内容也是很好，看完能让自己也站在高处去思考问题。 10、《大型网站技术架构 —— 核心原理与案例分析》 —— 李智慧 著 最好和上面那本书籍一起看，效果更好，两本看完了，提升思想的高度！ 11、《疯狂Java.突破程序员基本功的16课》 李刚 著 书中很注重 Java 的一些细节，讲的很深入，但是书中的错别字特多，可以看看我的读书笔记：《疯狂 Java 突破程序员基本功的 16 课》读书笔记 12、《Spring 实战》 Spring 入门书籍 13、《Spring 揭秘》—— 王福强 著 这本书别提多牛了，出版时期为 2009 年，豆瓣评分为 9.0 分，写的是真棒！把 Spring 的 IOC 和 AOP 特性写的很清楚，把 Spring 的来龙去脉讲的很全。墙裂推荐这本书籍，如果你想看 Spring，作者很牛，资深架构师，很有幸和作者有过一次交流，当时因为自己的一篇博客 Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子，竟然找到我想叫我去实习，可惜了，当时差点就跟着他混了。作者还有一本书 《Spring Boot 揭秘》。 14、《Spring 技术内幕》—— 深入解析 Spring 架构与设计原理 讲解 Spring 源码，深入了内部机制，个人觉得还是不错的。 15、Spring 官方的英文文档 这个别提了，很好，能看英文尽量看英文 16、《跟开涛学 Spring 3》 《跟开涛学 Spring MVC》 京东大神，膜 17、《看透springMvc源代码分析与实践》 算是把 Spring MVC 源码讲的很好的了 见我的笔记： 1、通过源码详解 Servlet 2 、看透 Spring MVC 源代码分析与实践 —— 网站基础知识 3 、看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 4 、看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 18、《Spring Boot 实战》 19、Spring Boot 官方 Reference Guide 网上好多写 SpringBoot 的博客，几乎和这个差不多。 20、《JavaEE开发的颠覆者: Spring Boot实战》 21、MyBatis 当然是官方的文档最好了，而且还是中文的。 自己也写过几篇文章，帮助过很多人入门，传送门： 1、通过项目逐步深入了解Mybatis（一）/) 2、通过项目逐步深入了解Mybatis（二）/) 3、通过项目逐步深入了解Mybatis（三）/) 4、通过项目逐步深入了解Mybatis（四）/) 22、《深入理解 Java 内存模型》—— 程晓明 著 我觉得每个 Java 程序员都应该了解下 Java 的内存模型，该书籍我看的是电子版的，不多，但是讲的却很清楚，把重排序、顺序一致性、Volatile、锁、final等写的很清楚。 Linux《鸟哥的Linux私房菜 基础学习篇(第三版) 》 鸟哥的Linux私房菜：服务器架设篇(第3版) 鸟哥的书 计算机网络《计算机网络第六版——谢希仁 编》 《计算机网络自顶向下方法》 计算机系统《代码揭秘：从C／C.的角度探秘计算机系统 —— 左飞》 《深入理解计算机系统》 《计算机科学导论_佛罗赞》 数据库《高性能MySQL》 《Mysql技术内幕InnoDB存储引擎》 Python这门语言语法很简单，上手快，不过我目前好久没用了，都忘得差不多了。当时是看的廖雪峰的 Python 博客 自己也用 Python 做爬虫写过几篇博客，不过有些是在前人的基础上写的。感谢那些栽树的人！ 工具Git ： 廖雪峰的 Git 教程 IDEA：IntelliJ IDEA 简体中文专题教程 Maven：《Maven实战》 其他《如何高效学习-斯科特杨》 教你怎样高效学习的 《软技能：代码之外的生存指南》 程序员除了写代码，还得懂点其他的软技能。 《提问的智慧“中文版”》 《How-To-Ask-Questions-The-Smart-Way》 作为程序员的你，一定要学会咋提问，不然别人都不想鸟你。 优秀网站推荐1、GitHub 别和我说不知道 2、InfoQ 文章很不错 3、CSDN 经常看博客专家的博客，里面大牛很多，传送门：zhisheng 4、知乎 多关注些大牛，看他们吹逼 5、掘金 自己也在上面写专栏，粉丝已经超过一万了，传送门 ：zhisheng 6、并发编程网 前面已经介绍 7、developerworks 上面的博客也很好 8、博客园 里面应该大牛也很多，不过自己没在上面写过博客 9、微信公众号 关注了很多人，有些人的文章确实很好，平时也经常看。 10、牛客网 刷笔试题不错的地方，里面大牛超多，怀念叶神和左神讲课的时候，还有很有爱的牛妹。 优秀博客推荐廖雪峰 Git 和 Python 入门文章就是从他博客看的 阮一峰的网络日志 酷壳-陈皓 RednaxelaFX R大，牛逼的不得了 江南白衣 老司机 stormzhang 人称帅逼张，微信公众号写的不错 你假笨 阿里搞 JVM 的，很厉害 占小狼 泥瓦匠BYSocket 崔庆才 写了好多 Python 爬虫相关的文章 纯洁的微笑 SpringBoot 系列不错，其他的文章自己看了感觉是自己喜欢的那种文笔 程序猿DD 周立 芋道源码的博客 好多系列的源码分析 zhisheng 这个是我不要脸，竟然把自己博客地址的写上去了 求职资料放送自己在准备找工作那段时间，系统的复习了下大学所学的知识，期间在网上参考了很多不错的博客，并收集下来了，个人觉得还是不错的，因为这是包含了自己的心血，所以一直没怎么送出来，只给过我的几个同学，还有就是一些学习视频和实战项目视频。借着这次征文的机会，我想送给那些有缘人，希望你或许是那种在求职道路上正在艰难走着的人；或许是大一大二的学弟学妹们却想好好学习，有个奋斗的目标，不堪在大学堕落的；或许是工作一两年后感觉基础还比较薄弱的。要资料的时候期望你能简单的介绍下自己，期望你！联系方式请看文章最下面。 最后送一句话，越努力，越幸运，祝早日成为大神！ 这些地方可以找到我： blog: http://www.54tianzhisheng.cn/ GitHub: https://github.com/zhisheng17 QQ 群：528776268","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"Linux 下 lua 开发环境安装及安装 luafilesystem","date":"2017-09-14T16:00:00.000Z","path":"2017/09/15/linux-lua-lfs-install/","text":"火云邪神语录：天下武功，无坚不破，唯快不破！Nginx 的看家本领就是速度，Lua 的拿手好戏亦是速度，这两者的结合在速度上无疑有基因上的优势。 最近一直再折腾这个，干脆就稍微整理下。以防后面继续跳坑！ 安装： 1.先安装 lua 的相关依赖安装 C 开发环境由于 gcc 包需要依赖 binutils 和 cpp 包，另外 make 包也是在编译中常用的，所以一共需要 9 个包来完成安装，因此我们只需要执行 9 条指令即可： 12345678910gcc：命令未找到（解决方法）yum install cppyum install binutilsyum install glibcyum install glibc-kernheadersyum install glibc-commonyum install glibc-develyum install gccyum install makeyum install readline-devel 2.安装 lua5.1.5下载地址：http://www.lua.org/ftp/ 1234567891011121314151617181920tar -zxvf lua-5.1.5.tar.gzcd lua-5.1.5vi Makefile设置 INSTALL_TOP= /usr/local/luamake linuxmake testmake installrm -rf /usr/bin/lualn -s /usr/local/lua/bin/lua /usr/bin/lualn -s /usr/local/lua/share/lua /usr/share/lua设置环境变量：vim /etc/profile添加：export LUA_HOME=/usr/local/luaexport PATH=$PATH:$LUA_HOME/bin环境变量生效：source /etc/profile 3、安装 luarocks是一个 Lua 包管理器，基于 Lua 语言开发，提供一个命令行的方式来管理 Lua 包依赖、安装第三方 Lua 包等。 地址： https://github.com/luarocks/luarocks 12345678910111213141516使用 luarocks-2.2.1 版本在我机器上没有问题，但是使用 luarocks-2.4.2 出现问题wget http://luarocks.org/releases/luarocks-2.2.1.tar.gztar -zxvf luarocks-2.2.1.tar.gzcd luarocks-2.2.1./configure --with-lua=/usr/local --with-lua-include=/usr/local/lua/include设置环境变量：export LUA_LUAROCKS_PATH=/usr/local/luarocks-2.2.1export PATH=$PATH:$LUA_LUAROCKS_PATHmake &amp; make install 4、安装 luafilesystem是一个用于 lua 进行文件访问的库，可以支持 lua 5.1 和 lua5.2，且是跨平台的，在为 lua 安装 lfs 之前需要先安装luarocks。因为自己的需求刚好需要这模块。 地址：https://github.com/keplerproject/luafilesystem 文档： http://keplerproject.github.io/luafilesystem/index.html 1luarocks install luafilesystem 5、测试测试 lua 是否安装成功 lua -v 结果： 1Lua 5.1.5 Copyright (C) 1994-2012 Lua.org, PUC-Rio 测试 luafilesystem 是否安装成功 a.lua 123456789local lfs = require&quot;lfs&quot;function Rreturn(filePath) local time = os.date(&quot;%a, %d %b %Y %X GMT&quot;, lfs.attributes(filePath).modification) --打印文件的修改时间 print(time)endRreturn(&quot;/opt/lua/a.txt&quot;) a.txt 123abc 运行： 1lua a.lua 结果： 1Tue, 12 Sep 2017 18:43:13 GMT 出现打印出时间的结果就意味着已经安装好了。 当然以上这是在 Linux 安装的， Windows 上的其实比这还简单了，但是安装 luafilesystem 的话需要自己去下载个 lfs.dll ，然后把这个放到 lua 的安装路径去。很简单的，这里就不细说了。 出现过的错误：123456789101112[root@n1 lua-5.1.5]# make linux testcd src &amp;&amp; make linuxmake[1]: Entering directory `/opt/lua-5.1.5/src&apos;make all MYCFLAGS=-DLUA_USE_LINUX MYLIBS=&quot;-Wl,-E -ldl -lreadline -lhistory -lncurses&quot;make[2]: Entering directory `/opt/lua-5.1.5/src&apos;gcc -O2 -Wall -DLUA_USE_LINUX -c -o lapi.o lapi.cmake[2]: gcc：命令未找到make[2]: *** [lapi.o] 错误 127make[2]: Leaving directory `/opt/lua-5.1.5/src&apos;make[1]: *** [linux] 错误 2make[1]: Leaving directory `/opt/lua-5.1.5/src&apos;make: *** [linux] 错误 2 原因：最开始的那些依赖没安装","tags":[{"name":"lua","slug":"lua","permalink":"http://www.54tianzhisheng.cn/tags/lua/"}]},{"title":"Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程","date":"2017-09-08T16:00:00.000Z","path":"2017/09/09/Elasticsearch-install/","text":"介绍ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。基百科、Stack Overflow、Github 都采用它。 本文从零开始，讲解如何使用 Elasticsearch 搭建自己的全文搜索引擎。每一步都有详细的说明，大家跟着做就能学会。 环境1、VMware 2、Centos 6.6 3、Elasticsearch 5.5.2 4、JDK 1.8 VMware 安装以及在 VMware 中安装 Centos 这个就不说了，环境配置直接默认就好，不过分配给机器的内存最好设置大点（建议 2G）， 使用 dhclient 命令来自动获取 IP 地址，查看获取的 IP 地址则使用命令 ip addr 或者 ifconfig ，则会看到网卡信息和 lo 卡信息。 给虚拟机额中的 linux 设置固定的 ip（因为后面发现每次机器重启后又要重新使用 dhclient 命令来自动获取 IP 地址） 1vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改： 12onboot=yesbootproto=static 增加：（下面可设置可不设置） 123IPADDR=192.168.1.113 网卡IP地址GATEWAY=192.168.1.1NETMASK=255.255.255.0 设置好之后，把网络服务重启一下， service network restart 修改 ip 地址参考： http://jingyan.baidu.com/article/e4d08ffdd417660fd3f60d70.html 大环境都准备好了，下面开始安装步骤： 安装 JDK 1.8先卸载自带的 openjdk，查找 openjdk 1rpm -qa | grep java 卸载 openjdk 12yum -y remove java-1.7.0-openjdk-1.7.0.65-2.5.1.2.el65.x8664yum -y remove java-1.6.0-openjdk-1.6.0.0-11.1.13.4.el6.x86_64 解压 JDK 安装包： 附上jdk1.8的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 解压完成后配置一下环境变量就 ok 1、在/usr/local/下创建Java文件夹 12cd /usr/local/ 进入目录mkdir java 新建java目录 2、文件夹创建完毕，把安装包拷贝到 Java 目录中，然后解压 jdk 到当前目录 12cp /usr/jdk-8u144-linux-x64.tar.gz /usr/local/java/ **注意匹配你自己的文件名** 拷贝到java目录tar -zxvf jdk-8u144-linux-x64.tar.gz 解压到当前目录（Java目录） 3、解压完之后，Java目录中会出现一个jdk1.8.0_144的目录，这就解压完成了。之后配置一下环境变量。编辑/etc/下的profile文件，配置环境变量 12345678vi /etc/profile 进入profile文件的编辑模式在最后边追加一下内容(**配置的时候一定要根据自己的目录情况而定哦！**) JAVA_HOME=/usr/local/java/jdk1.8.0_144 CLASSPATH=$JAVA_HOME/lib/ PATH=$PATH:$JAVA_HOME/bin export PATH JAVA_HOME CLASSPATH 之后保存并退出文件之后。 让文件生效：source /etc/profile 在控制台输入Java 和 Java -version 看有没有信息输出，如下： java -version 123java version &quot;1.8.0_144&quot; Java(TM) SE Runtime Environment (build 1.8.0_60-b27) Java HotSpot(TM) Client VM (build 25.60-b23, mixed mode) 能显示以上信息，就说明 JDK 安装成功啦 安装 Maven因为后面可能会用到 maven ，先装上这个。 1、下载 maven 1wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.tar.gz 2、解压至 /usr/local 目录 1tar -zxvf apache-maven-3.2.5-bin.tar.gz 3、配置公司给的配置 替换成公司给的 setting.xml 文件，修改关于本地仓库的位置, 默认位置: ${user.home}/.m2/repository 4、配置环境变量etc/profile 最后添加以下两行 12export MAVEN_HOME=/usr/local/apache-maven-3.2.5export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin 5、测试 123[root@localhost ~]# mvn -vApache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T09:29:23-08:00)Maven home: /usr/local/apache-maven-3.2.5 VMware 虚拟机里面的三台机器 IP 分别是： 123192.168.153.133192.168.153.134192.168.153.132 配置 hosts在 /etc/hosts下面编写：ip node 节点的名字（域名解析） 1vim /etc/hosts 新增： 123192.168.153.133 es1192.168.153.134 es2192.168.153.132 es3 设置 SSH 免密码登录安装expect命令 ： yum -y install expect 将 ssh_p2p.jar 随便解压到任何目录下： (这个 jar 包可以去网上下载) 1unzip ssh_p2p.zip 修改 resource 的 ip 值 1vim /ssh_p2p/deploy_data/resource （各个节点和账户名，密码，free代表相互都可以无密码登陆） 123456#设置为你每台虚拟机的ip地址，用户名，密码address=(&quot;192.168.153.133,root,123456,free&quot;&quot;192.168.153,134,root,123456,free&quot;&quot;192.168.153.132,root,123456,free&quot;) 修改 start.sh 的运行权限 1chmod u+x start.sh 运行 1./start.sh 测试： ssh ip地址 （测试是否可以登录） 安装 ElasticSearch下载地址： https://www.elastic.co/downloads/elasticsearch 123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gzcd /usr/localtar -zxvf elasticsearch-5.5.2.tar.gz su tzs 切换到 tzs 用户下 ( 默认不支持 root 用户) sh /usr/local/elasticsearch/bin/elasticsearch -d 其中 -d 表示后台启动 在 vmware 上测试是否成功：curl http://localhost:9200/ 出现如上图这样的效果，就代表已经装好了。 elasticsearch 默认 restful-api 的端口是 9200 不支持 IP 地址，也就是说无法从主机访问虚拟机中的服务，只能在本机用 http://localhost:9200 来访问。如果需要改变，需要修改配置文件 /usr/local/elasticsearch/config/elasticsearch.yml 文件，加入以下两行： 12network.bind_host: 0.0.0.0network.publish_host: _nonloopback:ipv4 或去除 network.host 和 http.port 之前的注释，并将 network.host 的 IP 地址修改为本机外网 IP。然后重启，Elasticsearch 关闭方法（输入命令：ps -ef | grep elasticsearch ，找到进程，然后 kill 掉就行了。 如果外网还是不能访问，则有可能是防火墙设置导致的 ( 关闭防火墙：service iptables stop ) 修改配置文件：vim config/elasticsearch.yml cluster.name : my-app (集群的名字，名字相同的就是一个集群) node.name : es1 （节点的名字, 和前面配置的 hosts 中的 name 要一致） path.data: /data/elasticsearch/data （数据的路径。没有要创建（mkdir -p /data/elasticsearch/{data,logs}），并且给执行用户权限 chown tzs /data/elasticsearch/{data,logs} -R ）path.logs: /data/elasticsearch/logs （数据 log 信息的路径，同上）network.host: 0.0.0.0 //允许外网访问，也可以是自己的ip地址http.port: 9200 //访问的端口discovery.zen.ping.unicast.hosts: [“192.168.153.133”, “192.168.153.134”, “192.168.153.132”] //各个节点的ip地址 记得需要添加上：（这个是安装 head 插件要用的， 目前不需要）http.cors.enabled: truehttp.cors.allow-origin: “*” 最后在外部浏览器的效果如下图： 安装 IK 中文分词可以自己下载源码使用 maven 编译，当然如果怕麻烦可以直接下载编译好的 https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v5.5.2 注意下载对应的版本放在 plugins 目录下 解压 unzip elasticsearch-analysis-ik-5.5.2.zip 在 es 的 plugins 下新建 ik 目录 mkdir ik 将刚才解压的复制到ik目录下 cp -r elasticsearch/* ik 删除刚才解压后的 12rm -rf elasticsearchrm -rf elasticsearch-analysis-ik-5.5.2.zip IK 带有两个分词器ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语 ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有 安装完 IK 中文分词器后（当然不止这种中文分词器，还有其他的，可以参考我的文章 Elasticsearch 默认分词器和中分分词器之间的比较及使用方法），测试区别如下： ik_max_wordcurl -XGET ‘http://192.168.153.134:9200/_analyze?pretty&amp;analyzer=ik_max_word‘ -d ‘联想是全球最大的笔记本厂商’ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;笔记&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;本厂&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 8 &#125; ]&#125; ik_smartcurl -XGET ‘http://localhost:9200/_analyze?pretty&amp;analyzer=ik_smart‘ -d ‘联想是全球最大的笔记本厂商’ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125; ]&#125; 安装 head 插件elasticsearch-head 是一个 elasticsearch 的集群管理工具，它是完全由 html5 编写的独立网页程序，你可以通过插件把它集成到 es。 效果如下图：（图片来自网络） 安装 git123yum remove gityum install gitgit clone git://github.com/mobz/elasticsearch-head.git 拉取 head 插件到本地，或者直接在 GitHub 下载 压缩包下来 安装nodejs先去官网下载 node-v8.4.0-linux-x64.tar.xz 12tar -Jxv -f node-v8.4.0-linux-x64.tar.xzmv node-v8.4.0-linux-x64 node 环境变量设置： 1vim /etc/profile 新增： 123export NODE_HOME=/opt/nodeexport PATH=$PATH:$NODE_HOME/binexport NODE_PATH=$NODE_HOME/lib/node_modules 使配置文件生效（这步很重要，自己要多注意这步） 1source /etc/profile 测试是否全局可用了： 1node -v 然后 12345mv elasticsearch-head headcd head/npm install -g grunt-clinpm installgrunt server 再 es 的配置文件中加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 在浏览器打开 http://192.168.153.133:9100/ 就可以看到效果了， 遇到问题把坑都走了一遍，防止以后再次入坑，特此记录下来 1、ERROR Could not register mbeans java.security.AccessControlException: access denied (“javax.management.MBeanTrustPermission” “register”) 改变 elasticsearch 文件夹所有者到当前用户 sudo chown -R noroot:noroot elasticsearch 这是因为 elasticsearch 需要读写配置文件，我们需要给予 config 文件夹权限，上面新建了 elsearch 用户，elsearch 用户不具备读写权限，因此还是会报错，解决方法是切换到管理员账户，赋予权限即可： sudo -i chmod -R 775 config 2、[WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root 原因是elasticsearch默认是不支持用root用户来启动的。 解决方案一：Des.insecure.allow.root=true 修改/usr/local/elasticsearch-2.4.0/bin/elasticsearch， 添加 ES_JAVA_OPTS=”-Des.insecure.allow.root=true” 或执行时添加： sh /usr/local/elasticsearch-2.4.0/bin/elasticsearch -d -Des.insecure.allow.root=true 注意：正式环境用root运行可能会有安全风险，不建议用root来跑。 解决方案二：添加专门的用户 1234useradd elasticchown -R elastic:elastic elasticsearch-2.4.0su elasticsh /usr/local/elasticsearch-2.4.0/bin/elasticsearch -d 3、UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in 只是警告，使用新的linux版本，就不会出现此类问题了。 4、ERROR: [4] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 原因：无法创建本地文件问题,用户最大可创建文件数太小 解决方案：切换到 root 用户，编辑 limits.conf 配置文件， 添加类似如下内容： vim /etc/security/limits.conf 添加如下内容: 1234* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 [2]: max number of threads [1024] for user [tzs] is too low, increase to at least [2048] 原因：无法创建本地线程问题,用户最大可创建线程数太小 解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件。 vim /etc/security/limits.d/90-nproc.conf 找到如下内容： soft nproc 1024 修改为 soft nproc 2048 [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 原因：最大虚拟内存太小 root用户执行命令： sysctl -w vm.max_map_count=262144 或者修改 /etc/sysctl.conf 文件，添加 “vm.max_map_count”设置设置后，可以使用$ sysctl -p [4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk 原因：Centos6不支持SecComp，而ES5.4.1默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。详见 ：https://github.com/elastic/elasticsearch/issues/22899 解决方法：在elasticsearch.yml中新增配置bootstrap.system_call_filter，设为false，注意要在Memory下面:bootstrap.memory_lock: falsebootstrap.system_call_filter: false 5、 java.lang.IllegalArgumentException: property [elasticsearch.version] is missing for plugin [head] 再 es 的配置文件中加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 最后整个搭建的过程全程自己手动安装，不易，如果安装很多台机器，是否可以写个脚本之类的自动搭建呢？可以去想想的。首发于：http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/ ，转载请注明出处，谢谢配合！","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法","date":"2017-09-07T16:00:00.000Z","path":"2017/09/08/Elasticsearch-analyzers/","text":"介绍：ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 Elasticsearch中，内置了很多分词器（analyzers）。下面来进行比较下系统默认分词器和常用的中文分词器之间的区别。 系统默认分词器：1、standard 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_standardanalyzer.html 英文的处理能力同于StopAnalyzer.支持中文采用的方法为单字切分。他会将词汇单元转换成小写形式，并去除停用词和标点符号。 12345/**StandardAnalyzer分析器*/public void standardAnalyzer(String msg)&#123; StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg);&#125; 2、simple 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simple-analyzer.html 如何使用: http://www.yiibai.com/lucene/lucene_simpleanalyzer.html 功能强于WhitespaceAnalyzer, 首先会通过非字母字符来分割文本信息，然后将词汇单元统一为小写形式。该分析器会去掉数字类型的字符。 12345/**SimpleAnalyzer分析器*/ public void simpleAnalyzer(String msg)&#123; SimpleAnalyzer analyzer = new SimpleAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 3、Whitespace 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_whitespaceanalyzer.html 仅仅是去除空格，对字符没有lowcase化,不支持中文；并且不对生成的词汇单元进行其他的规范化处理。 12345/**WhitespaceAnalyzer分析器*/ public void whitespaceAnalyzer(String msg)&#123; WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 4、Stop 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_stopanalyzer.html StopAnalyzer的功能超越了SimpleAnalyzer，在SimpleAnalyzer的基础上增加了去除英文中的常用单词（如the，a等），也可以更加自己的需要设置常用单词；不支持中文 12345/**StopAnalyzer分析器*/ public void stopAnalyzer(String msg)&#123; StopAnalyzer analyzer = new StopAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 5、keyword 分词器KeywordAnalyzer把整个输入作为一个单独词汇单元，方便特殊类型的文本进行索引和检索。针对邮政编码，地址等文本信息使用关键词分词器进行索引项建立非常方便。 6、pattern 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html 一个pattern类型的analyzer可以通过正则表达式将文本分成”terms”(经过token Filter 后得到的东西 )。接受如下设置: 一个 pattern analyzer 可以做如下的属性设置: lowercase terms是否是小写. 默认为 true 小写. pattern 正则表达式的pattern, 默认是 \\W+. flags 正则表达式的flags stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是空的列表 7、language 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html 一个用于解析特殊语言文本的analyzer集合。（ arabic,armenian, basque, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, finnish, french,galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian,persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai.）可惜没有中文。不予考虑 8、snowball 分词器一个snowball类型的analyzer是由standard tokenizer和standard filter、lowercase filter、stop filter、snowball filter这四个filter构成的。 snowball analyzer 在Lucene中通常是不推荐使用的。 9、Custom 分词器是自定义的analyzer。允许多个零到多个tokenizer，零到多个 Char Filters. custom analyzer 的名字不能以 “_”开头. The following are settings that can be set for a custom analyzer type: Setting Description tokenizer 通用的或者注册的tokenizer. filter 通用的或者注册的token filters char_filter 通用的或者注册的 character filters position_increment_gap 距离查询时，最大允许查询的距离，默认是100 自定义的模板： 1234567891011121314151617181920212223242526index : analysis : analyzer : myAnalyzer2 : type : custom tokenizer : myTokenizer1 filter : [myTokenFilter1, myTokenFilter2] char_filter : [my_html] position_increment_gap: 256 tokenizer : myTokenizer1 : type : standard max_token_length : 900 filter : myTokenFilter1 : type : stop stopwords : [stop1, stop2, stop3, stop4] myTokenFilter2 : type : length min : 0 max : 2000 char_filter : my_html : type : html_strip escaped_tags : [xxx, yyy] read_ahead : 1024 10、fingerprint 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-fingerprint-analyzer.html 中文分词器：1、ik-analyzerhttps://github.com/wks/ik-analyzer IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和最大词长两种切分模式；具有83万字/秒（1600KB/S）的高速处理能力。 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符 优化的词典存储，更小的内存占用。支持用户词典扩展定义 针对Lucene全文检索优化的查询分析器IKQueryParser(作者吐血推荐)；引入简单搜索表达式，采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。 Maven用法： 12345&lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt;&lt;/dependency&gt; 在IK Analyzer加入Maven Central Repository之前，你需要手动安装，安装到本地的repository，或者上传到自己的Maven repository服务器上。 要安装到本地Maven repository，使用如下命令，将自动编译，打包并安装：mvn install -Dmaven.test.skip=true Elasticsearch添加中文分词安装IK分词插件https://github.com/medcl/elasticsearch-analysis-ik 进入elasticsearch-analysis-ik-master 更多安装请参考博客： 1、为elastic添加中文分词 ： http://blog.csdn.net/dingzfang/article/details/42776693 2、如何在Elasticsearch中安装中文分词器(IK+pinyin) ：http://www.cnblogs.com/xing901022/p/5910139.html 3、Elasticsearch 中文分词器 IK 配置和使用 ： http://blog.csdn.net/jam00/article/details/52983056 ik 带有两个分词器ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语 ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有 区别： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133# ik_max_wordcurl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_max_word&apos; -d &apos;联想是全球最大的笔记本厂商&apos;#返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;笔记&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;本厂&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 8 &#125; ]&#125;# ik_smartcurl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_smart&apos; -d &apos;联想是全球最大的笔记本厂商&apos;# 返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125; ]&#125; 下面我们来创建一个索引，使用 ik创建一个名叫 iktest 的索引，设置它的分析器用 ik ，分词器用 ik_max_word，并创建一个 article 的类型，里面有一个 subject 的字段，指定其使用 ik_max_word 分词器 12345678910111213141516171819202122curl -XPUT &apos;http://localhost:9200/iktest?pretty&apos; -d &apos;&#123; &quot;settings&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;ik&quot; : &#123; &quot;tokenizer&quot; : &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;article&quot; : &#123; &quot;dynamic&quot; : true, &quot;properties&quot; : &#123; &quot;subject&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot; : &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125;&apos; 批量添加几条数据，这里我指定元数据 _id 方便查看，subject 内容为我随便找的几条新闻的标题 123456789101112curl -XPOST http://localhost:9200/iktest/article/_bulk?pretty -d &apos;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;＂闺蜜＂崔顺实被韩检方传唤 韩总统府促彻查真相&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;韩举行＂护国训练＂ 青瓦台:决不许国家安全出问题&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;媒体称FBI已经取得搜查令 检视希拉里电邮&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;4&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;村上春树获安徒生奖 演讲中谈及欧洲排外问题&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;5&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;希拉里团队炮轰FBI 参院民主党领袖批其“违法”&quot; &#125;&apos; 查询 “希拉里和韩国” 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071curl -XPOST http://localhost:9200/iktest/article/_search?pretty -d&apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;subject&quot; : &quot;希拉里和韩国&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;font color=&apos;red&apos;&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/font&gt;&quot;], &quot;fields&quot; : &#123; &quot;subject&quot; : &#123;&#125; &#125; &#125;&#125;&apos;#返回&#123; &quot;took&quot; : 113, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 4, &quot;max_score&quot; : 0.034062363, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.034062363, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;韩举行＂护国训练＂ 青瓦台:决不许国家安全出问题&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;&lt;font color=red&gt;韩&lt;/font&gt;举行＂护&lt;font color=red&gt;国&lt;/font&gt;训练＂ 青瓦台:决不许国家安全出问题&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.0076681254, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;媒体称FBI已经取得搜查令 检视希拉里电邮&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;媒体称FBI已经取得搜查令 检视&lt;font color=red&gt;希拉里&lt;/font&gt;电邮&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 0.006709609, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;希拉里团队炮轰FBI 参院民主党领袖批其“违法”&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;&lt;font color=red&gt;希拉里&lt;/font&gt;团队炮轰FBI 参院民主党领袖批其“违法”&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.0021509775, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;＂闺蜜＂崔顺实被韩检方传唤 韩总统府促彻查真相&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;＂闺蜜＂崔顺实被&lt;font color=red&gt;韩&lt;/font&gt;检方传唤 &lt;font color=red&gt;韩&lt;/font&gt;总统府促彻查真相&quot; ] &#125; &#125; ] &#125;&#125; 这里用了高亮属性 highlight，直接显示到 html 中，被匹配到的字或词将以红色突出显示。若要用过滤搜索，直接将 match 改为 term 即可 热词更新配置网络词语日新月异，如何让新出的网络热词（或特定的词语）实时的更新到我们的搜索当中呢 先用 ik 测试一下 12345678910111213141516171819202122232425262728293031323334353637curl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_max_word&apos; -d &apos;成龙原名陈港生&apos;#返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;成龙&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;原名&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;陈&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;港&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;生&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125; ]&#125; ik 的主词典中没有”陈港生” 这个词，所以被拆分了。现在我们来配置一下 修改 IK 的配置文件 ：ES 目录/plugins/ik/config/ik/IKAnalyzer.cfg.xml 修改如下： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;custom/mydict.dic;custom/single_word_low_freq.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/ext_stopword.dic&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;http://192.168.1.136/hotWords.php&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;&lt;/properties&gt; 这里我是用的是远程扩展字典，因为可以使用其他程序调用更新，且不用重启 ES，很方便；当然使用自定义的 mydict.dic 字典也是很方便的，一行一个词，自己加就可以了 既然是远程词典，那么就要是一个可访问的链接，可以是一个页面，也可以是一个txt的文档，但要保证输出的内容是 utf-8 的格式 hotWords.php 的内容 12345678$s = &lt;&lt;&lt;'EOF'陈港生元楼蓝瘦EOF;header('Last-Modified: '.gmdate('D, d M Y H:i:s', time()).' GMT', true, 200);header('ETag: \"5816f349-19\"');echo $s; ik 接收两个返回的头部属性 Last-Modified 和 ETag，只要其中一个有变化，就会触发更新，ik 会每分钟获取一次重启 Elasticsearch ，查看启动记录，看到了三个词已被加载进来 再次执行上面的请求，返回, 就可以看到 ik 分词器已经匹配到了 “陈港生” 这个词，同理一些关于我们公司的专有名字（例如：永辉、永辉超市、永辉云创、云创 …. ）也可以自己手动添加到字典中去。 2、结巴中文分词特点：1、支持三种分词模式： 精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 2、支持繁体分词 3、支持自定义词典 3、THULACTHULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点： 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。 中文分词工具thulac4j发布 1、规范化分词词典，并去掉一些无用词； 2、重写DAT（双数组Trie树）的构造算法，生成的DAT size减少了8%左右，从而节省了内存； 3、优化分词算法，提高了分词速率。 12345&lt;dependency&gt; &lt;groupId&gt;io.github.yizhiru&lt;/groupId&gt; &lt;artifactId&gt;thulac4j&lt;/artifactId&gt; &lt;version&gt;$&#123;thulac4j.version&#125;&lt;/version&gt;&lt;/dependency&gt; http://www.cnblogs.com/en-heng/p/6526598.html thulac4j支持两种分词模式： SegOnly模式，只分词没有词性标注； SegPos模式，分词兼有词性标注。 12345678910// SegOnly modeString sentence = \"滔滔的流水，向着波士顿湾无声逝去\";SegOnly seg = new SegOnly(\"models/seg_only.bin\");System.out.println(seg.segment(sentence));// [滔滔, 的, 流水, ，, 向着, 波士顿湾, 无声, 逝去]// SegPos modeSegPos pos = new SegPos(\"models/seg_pos.bin\");System.out.println(pos.segment(sentence));//[滔滔/a, 的/u, 流水/n, ，/w, 向着/p, 波士顿湾/ns, 无声/v, 逝去/v] 4、NLPIR中科院计算所 NLPIR：http://ictclas.nlpir.org/nlpir/ (可直接在线分析中文) 下载地址：https://github.com/NLPIR-team/NLPIR 中科院分词系统(NLPIR)JAVA简易教程: http://www.cnblogs.com/wukongjiuwo/p/4092480.html 5、ansj分词器https://github.com/NLPchina/ansj_seg 这是一个基于n-Gram+CRF+HMM的中文分词的java实现. 分词速度达到每秒钟大约200万字左右（mac air下测试），准确率能达到96%以上 目前实现了.中文分词. 中文姓名识别 . 用户自定义词典,关键字提取，自动摘要，关键字标记等功能可以应用到自然语言处理等方面,适用于对分词效果要求高的各种项目. maven 引入： 12345&lt;dependency&gt; &lt;groupId&gt;org.ansj&lt;/groupId&gt; &lt;artifactId&gt;ansj_seg&lt;/artifactId&gt; &lt;version&gt;5.1.1&lt;/version&gt;&lt;/dependency&gt; 调用demo 1234String str = \"欢迎使用ansj_seg,(ansj中文分词)在这里如果你遇到什么问题都可以联系我.我一定尽我所能.帮助大家.ansj_seg更快,更准,更自由!\" ; System.out.println(ToAnalysis.parse(str)); 欢迎/v,使用/v,ansj/en,_,seg/en,,,(,ansj/en,中文/nz,分词/n,),在/p,这里/r,如果/c,你/r,遇到/v,什么/r,问题/n,都/d,可以/v,联系/v,我/r,./m,我/r,一定/d,尽我所能/l,./m,帮助/v,大家/r,./m,ansj/en,_,seg/en,更快/d,,,更/d,准/a,,,更/d,自由/a,! 6、哈工大的LTPhttps://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ltp LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。 关于LTP的使用，请参考: http://ltp.readthedocs.io/zh_CN/latest/ 7、庖丁解牛下载地址：http://pan.baidu.com/s/1eQ88SZS 使用分为如下几步： 配置dic文件：修改paoding-analysis.jar中的paoding-dic-home.properties文件，将“#paoding.dic.home=dic”的注释去掉，并配置成自己dic文件的本地存放路径。eg：/home/hadoop/work/paoding-analysis-2.0.4-beta/dic 把Jar包导入到项目中：将paoding-analysis.jar、commons-logging.jar、lucene-analyzers-2.2.0.jar和lucene-core-2.2.0.jar四个包导入到项目中，这时就可以在代码片段中使用庖丁解牛工具提供的中文分词技术，例如： 123456789101112Analyzer analyzer = new PaodingAnalyzer(); //定义一个解析器String text = \"庖丁系统是个完全基于lucene的中文分词系统，它就是重新建了一个analyzer，叫做PaodingAnalyzer，这个analyer的核心任务就是生成一个可以切词TokenStream。\"; &lt;span style=\"font-family: Arial, Helvetica, sans-serif;\"&gt;//待分词的内容&lt;/span&gt;TokenStream tokenStream = analyzer.tokenStream(text, new StringReader(text)); //得到token序列的输出流try &#123; Token t; while ((t = tokenStream.next()) != null) &#123; System.out.println(t); //输出每个token &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 8、sogo在线分词sogo在线分词采用了基于汉字标注的分词方法，主要使用了线性链链CRF（Linear-chain CRF）模型。词性标注模块主要基于结构化线性模型（Structured Linear Model） 在线使用地址为：http://www.sogou.com/labs/webservice/ 9、word分词地址： https://github.com/ysc/word word分词是一个Java实现的分布式的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。能通过自定义配置文件来改变组件行为，能自定义用户词库、自动检测词库变化、支持大规模分布式环境，能灵活指定多种分词算法，能使用refine功能灵活控制分词结果，还能使用词频统计、词性标注、同义标注、反义标注、拼音标注等功能。提供了10种分词算法，还提供了10种文本相似度算法，同时还无缝和Lucene、Solr、ElasticSearch、Luke集成。注意：word1.3需要JDK1.8 maven 中引入依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apdplat&lt;/groupId&gt; &lt;artifactId&gt;word&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; ElasticSearch插件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344451、打开命令行并切换到elasticsearch的bin目录cd elasticsearch-2.1.1/bin2、运行plugin脚本安装word分词插件：./plugin install http://apdplat.org/word/archive/v1.4.zip安装的时候注意： 如果提示： ERROR: failed to download 或者 Failed to install word, reason: failed to download 或者 ERROR: incorrect hash (SHA1) 则重新再次运行命令，如果还是不行，多试两次如果是elasticsearch1.x系列版本，则使用如下命令：./plugin -u http://apdplat.org/word/archive/v1.3.1.zip -i word3、修改文件elasticsearch-2.1.1/config/elasticsearch.yml，新增如下配置：index.analysis.analyzer.default.type : &quot;word&quot;index.analysis.tokenizer.default.type : &quot;word&quot;4、启动ElasticSearch测试效果，在Chrome浏览器中访问：http://localhost:9200/_analyze?analyzer=word&amp;text=杨尚川是APDPlat应用级产品开发平台的作者5、自定义配置修改配置文件elasticsearch-2.1.1/plugins/word/word.local.conf6、指定分词算法修改文件elasticsearch-2.1.1/config/elasticsearch.yml，新增如下配置：index.analysis.analyzer.default.segAlgorithm : &quot;ReverseMinimumMatching&quot;index.analysis.tokenizer.default.segAlgorithm : &quot;ReverseMinimumMatching&quot;这里segAlgorithm可指定的值有：正向最大匹配算法：MaximumMatching逆向最大匹配算法：ReverseMaximumMatching正向最小匹配算法：MinimumMatching逆向最小匹配算法：ReverseMinimumMatching双向最大匹配算法：BidirectionalMaximumMatching双向最小匹配算法：BidirectionalMinimumMatching双向最大最小匹配算法：BidirectionalMaximumMinimumMatching全切分算法：FullSegmentation最少词数算法：MinimalWordCount最大Ngram分值算法：MaxNgramScore如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching 10、jcseg分词器https://code.google.com/archive/p/jcseg/ 11、stanford分词器Stanford大学的一个开源分词工具，目前已支持汉语。 首先，去【1】下载Download Stanford Word Segmenter version 3.5.2，取得里面的 data 文件夹，放在maven project的 src/main/resources 里。 然后，maven依赖添加： 123456789101112131415161718192021222324&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;corenlp.version&gt;3.6.0&lt;/corenlp.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;classifier&gt;models&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;classifier&gt;models-chinese&lt;/classifier&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 测试： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.Properties;import edu.stanford.nlp.ie.crf.CRFClassifier;public class CoreNLPSegment &#123; private static CoreNLPSegment instance; private CRFClassifier classifier; private CoreNLPSegment()&#123; Properties props = new Properties(); props.setProperty(\"sighanCorporaDict\", \"data\"); props.setProperty(\"serDictionary\", \"data/dict-chris6.ser.gz\"); props.setProperty(\"inputEncoding\", \"UTF-8\"); props.setProperty(\"sighanPostProcessing\", \"true\"); classifier = new CRFClassifier(props); classifier.loadClassifierNoExceptions(\"data/ctb.gz\", props); classifier.flags.setProperties(props); &#125; public static CoreNLPSegment getInstance() &#123; if (instance == null) &#123; instance = new CoreNLPSegment(); &#125; return instance; &#125; public String[] doSegment(String data) &#123; return (String[]) classifier.segmentString(data).toArray(); &#125; public static void main(String[] args) &#123; String sentence = \"他和我在学校里常打桌球。\"; String ret[] = CoreNLPSegment.getInstance().doSegment(sentence); for (String str : ret) &#123; System.out.println(str); &#125; &#125;&#125; 博客： https://blog.sectong.com/blog/corenlp_segment.html http://blog.csdn.net/lightty/article/details/51766602 12、SmartcnSmartcn为Apache2.0协议的开源中文分词系统，Java语言编写，修改的中科院计算所ICTCLAS分词系统。很早以前看到Lucene上多了一个中文分词的contribution，当时只是简单的扫了一下.class文件的文件名，通过文件名可以看得出又是一个改的ICTCLAS的分词系统。 http://lucene.apache.org/core/5_1_0/analyzers-smartcn/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.html 13、pinyin 分词器pinyin分词器可以让用户输入拼音，就能查找到相关的关键词。比如在某个商城搜索中，输入 yonghui，就能匹配到 永辉。这样的体验还是非常好的。 pinyin分词器的安装与IK是一样的。下载地址：https://github.com/medcl/elasticsearch-analysis-pinyin 一些参数请参考 GitHub 的 readme 文档。 这个分词器在1.8版本中，提供了两种分词规则： pinyin,就是普通的把汉字转换成拼音； pinyin_first_letter，提取汉字的拼音首字母 使用： 1.Create a index with custom pinyin analyzer 1234567891011121314151617181920212223curl -XPUT http://localhost:9200/medcl/ -d&apos;&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : true, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;remove_duplicated_term&quot; : true &#125; &#125; &#125; &#125;&#125;&apos; 2.Test Analyzer, analyzing a chinese name, such as 刘德华 1http://localhost:9200/medcl/_analyze?text=%e5%88%98%e5%be%b7%e5%8d%8e&amp;analyzer=pinyin_analyzer 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;liu&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;de&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;hua&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;刘德华&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; 3.Create mapping 12345678910111213141516171819curl -XPOST http://localhost:9200/medcl/folks/_mapping -d&apos;&#123; &quot;folks&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: &quot;no&quot;, &quot;term_vector&quot;: &quot;with_offsets&quot;, &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 4.Indexing 1curl -XPOST http://localhost:9200/medcl/folks/andy -d&apos;&#123;&quot;name&quot;:&quot;刘德华&quot;&#125;&apos; 5.Let’s search 12345http://localhost:9200/medcl/folks/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8Ecurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:%e5%88%98%e5%be%b7curl http://localhost:9200/medcl/folks/_search?q=name.pinyin:liucurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:ldhcurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:de+hua 6.Using Pinyin-TokenFilter 123456789101112131415161718192021222324252627curl -XPUT http://localhost:9200/medcl1/ -d&apos;&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;user_name_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;whitespace&quot;, &quot;filter&quot; : &quot;pinyin_first_letter_and_full_pinyin_filter&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;pinyin_first_letter_and_full_pinyin_filter&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_none_chinese&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;trim_whitespace&quot; : true, &quot;keep_none_chinese_in_first_letter&quot; : true &#125; &#125; &#125; &#125;&#125;&apos; Token Test:刘德华 张学友 郭富城 黎明 四大天王 1curl -XGET http://localhost:9200/medcl1/_analyze?text=%e5%88%98%e5%be%b7%e5%8d%8e+%e5%bc%a0%e5%ad%a6%e5%8f%8b+%e9%83%ad%e5%af%8c%e5%9f%8e+%e9%bb%8e%e6%98%8e+%e5%9b%9b%e5%a4%a7%e5%a4%a9%e7%8e%8b&amp;analyzer=user_name_analyzer 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;zxy&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;gfc&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;lm&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;sdtw&quot;, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 19, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; 7.Used in phrase query (1)、 123456789101112131415161718192021222324252627282930PUT /medcl/ &#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:false, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘德华&quot; &#125;&#125; &#125; (2)、 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647PUT /medcl/ &#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:false, &quot;keep_separate_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125; &#125; POST /medcl/folks/andy &#123;&quot;name&quot;:&quot;刘德华&quot;&#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘德h&quot; &#125;&#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘dh&quot; &#125;&#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;dh&quot; &#125;&#125; &#125; 14、Mmseg 分词器也支持 Elasticsearch 下载地址：https://github.com/medcl/elasticsearch-analysis-mmseg/releases 根据对应的版本进行下载 如何使用： 1、创建索引： 1curl -XPUT http://localhost:9200/index 2、创建 mapping 123456789101112curl -XPOST http://localhost:9200/index/fulltext/_mapping -d&apos;&#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets&quot;, &quot;analyzer&quot;: &quot;mmseg_maxword&quot;, &quot;search_analyzer&quot;: &quot;mmseg_maxword&quot; &#125; &#125;&#125;&apos; 3.Indexing some docs 123456789101112131415curl -XPOST http://localhost:9200/index/fulltext/1 -d&apos;&#123;&quot;content&quot;:&quot;美国留给伊拉克的是个烂摊子吗&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/2 -d&apos;&#123;&quot;content&quot;:&quot;公安部：各地校车将享最高路权&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/3 -d&apos;&#123;&quot;content&quot;:&quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/4 -d&apos;&#123;&quot;content&quot;:&quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot;&#125;&apos; 4.Query with highlighting(查询高亮) 123456789101112curl -XPOST http://localhost:9200/index/fulltext/_search -d&apos;&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;content&quot; : &quot;中国&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : &#123; &quot;content&quot; : &#123;&#125; &#125; &#125;&#125;&apos; 5、结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 14, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 2, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;index&quot;, &quot;_type&quot;: &quot;fulltext&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;&lt;tag1&gt;中国&lt;/tag1&gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首 &quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;index&quot;, &quot;_type&quot;: &quot;fulltext&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;均每天扣1艘&lt;tag1&gt;中国&lt;/tag1&gt;渔船 &quot; ] &#125; &#125; ] &#125;&#125; 参考博客： 为elastic添加中文分词: http://blog.csdn.net/dingzfang/article/details/42776693 15、bosonnlp （玻森数据中文分析器）下载地址：https://github.com/bosondata/elasticsearch-analysis-bosonnlp 如何使用： 运行 ElasticSearch 之前需要在 config 文件夹中修改 elasticsearch.yml 来定义使用玻森中文分析器，并填写玻森 API_TOKEN 以及玻森分词 API 的地址，即在该文件结尾处添加： 12345678910111213141516171819202122index: analysis: analyzer: bosonnlp: type: bosonnlp API_URL: http://api.bosonnlp.com/tag/analysis # You MUST give the API_TOKEN value, otherwise it doesn&apos;t work API_TOKEN: *PUT YOUR API TOKEN HERE* # Please uncomment if you want to specify ANY ONE of the following # areguments, otherwise the DEFAULT value will be used, i.e., # space_mode is 0, # oov_level is 3, # t2s is 0, # special_char_conv is 0. # More detials can be found in bosonnlp docs: # http://docs.bosonnlp.com/tag.html # # # space_mode: put your value here(range from 0-3) # oov_level: put your value here(range from 0-4) # t2s: put your value here(range from 0-1) # special_char_conv: put your value here(range from 0-1) 需要注意的是 必须在 API_URL 填写给定的分词地址以及在API_TOKEN：PUT YOUR API TOKEN HERE 中填写给定的玻森数据API_TOKEN，否则无法使用玻森中文分析器。该 API_TOKEN 是注册玻森数据账号所获得。 如果配置文件中已经有配置过其他的 analyzer，请直接在 analyzer 下如上添加 bosonnlp analyzer。 如果有多个 node 并且都需要 BosonNLP 的分词插件，则每个 node 下的 yaml 文件都需要如上安装和设置。 另外，玻森中文分词还提供了4个参数（space_mode，oov_level，t2s，special_char_conv）可满足不同的分词需求。如果取默认值，则无需任何修改；否则，可取消对应参数的注释并赋值。 测试： 建立 index 1curl -XPUT &apos;localhost:9200/test&apos; 测试分析器是否配置成功 1curl -XGET &apos;localhost:9200/test/_analyze?analyzer=bosonnlp&amp;pretty&apos; -d &apos;这是玻森数据分词的测试&apos; 结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;这&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;玻森&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;数据&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;分词&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;测试&quot;, &quot;start_offset&quot; : 9, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 6 &#125; ]&#125; 配置 Token Filter 现有的 BosonNLP 分析器没有内置 token filter，如果有过滤 Token 的需求，可以利用 BosonNLP Tokenizer 和 ES 提供的 token filter 搭建定制分析器。 步骤 配置定制的 analyzer 有以下三个步骤： 添加 BosonNLP tokenizer在 elasticsearch.yml 文件中 analysis 下添加 tokenizer， 并在 tokenizer 中添加 BosonNLP tokenizer 的配置： 123456789101112131415161718192021222324index: analysis: analyzer: ... tokenizer: bosonnlp: type: bosonnlp API_URL: http://api.bosonnlp.com/tag/analysis # You MUST give the API_TOKEN value, otherwise it doesn&apos;t work API_TOKEN: *PUT YOUR API TOKEN HERE* # Please uncomment if you want to specify ANY ONE of the following # areguments, otherwise the DEFAULT value will be used, i.e., # space_mode is 0, # oov_level is 3, # t2s is 0, # special_char_conv is 0. # More detials can be found in bosonnlp docs: # http://docs.bosonnlp.com/tag.html # # # space_mode: put your value here(range from 0-3) # oov_level: put your value here(range from 0-4) # t2s: put your value here(range from 0-1) # special_char_conv: put your value here(range from 0-1) 添加 token filter 在 elasticsearch.yml 文件中 analysis 下添加 filter， 并在 filter 中添加所需 filter 的配置（下面例子中，我们以 lowercase filter 为例）： 123456789index: analysis: analyzer: ... tokenizer: ... filter: lowercase: type: lowercase 添加定制的 analyzer 在 elasticsearch.yml 文件中 analysis 下添加 analyzer， 并在 analyzer 中添加定制的 analyzer 的配置（下面例子中，我们把定制的 analyzer 命名为 filter_bosonnlp）： 12345678index: analysis: analyzer: ... filter_bosonnlp: type: custom tokenizer: bosonnlp filter: [lowercase] 自定义分词器虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。 字符过滤器： 字符过滤器 用来 整理 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 &lt;p&gt; 或者 &lt;div&gt; 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 html清除 字符过滤器 来移除掉所有的HTML标签，并且像把 &amp;Aacute; 转换为相对应的Unicode字符 Á 这样，转换HTML实体。 一个分析器可能有0个或者多个字符过滤器。 分词器: 一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 标准 分析器里使用的 标准 分词器 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。 词单元过滤器: 经过分词，作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。 词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 lowercase 和 stop 词过滤器 ，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 词干过滤器 把单词 遏制 为 词干。 ascii_folding 过滤器移除变音符，把一个像 “très” 这样的词转换为 “tres” 。 ngram 和 edge_ngram 词单元过滤器 可以产生 适合用于部分匹配或者自动补全的词单元。 创建一个自定义分析器我们可以在 analysis 下的相应位置设置字符过滤器、分词器和词单元过滤器: 1234567891011PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; ... custom character filters ... &#125;, &quot;tokenizer&quot;: &#123; ... custom tokenizers ... &#125;, &quot;filter&quot;: &#123; ... custom token filters ... &#125;, &quot;analyzer&quot;: &#123; ... custom analyzers ... &#125; &#125; &#125;&#125; 这个分析器可以做到下面的这些事: 1、使用 html清除 字符过滤器移除HTML部分。 2、使用一个自定义的 映射 字符过滤器把 &amp; 替换为 “和” ： 123456&quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125; 3、使用 标准 分词器分词。 4、小写词条，使用 小写 词过滤器处理。 5、使用自定义 停止 词过滤器移除自定义的停止词列表中包含的词： 123456&quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125; 我们的分析器定义用我们之前已经设置好的自定义过滤器组合了已经定义好的分词器和过滤器： 12345678&quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125; 汇总起来，完整的 创建索引 请求 看起来应该像这样： 1234567891011121314151617181920212223PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125;, &quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125;&#125;&#125;&#125; 索引被创建以后，使用 analyze API 来 测试这个新的分析器： 12GET /my_index/_analyze?analyzer=my_analyzerThe quick &amp; brown fox 下面的缩略结果展示出我们的分析器正在正确地运行： 12345678&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;quick&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;and&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;brown&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;fox&quot;, &quot;position&quot; : 5 &#125; ]&#125; 这个分析器现在是没有多大用处的，除非我们告诉 Elasticsearch在哪里用上它。我们可以像下面这样把这个分析器应用在一个 string 字段上： 123456789PUT /my_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; &#125; &#125;&#125; 最后整理参考网上资料，如有不正确的地方还请多多指教！","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！","date":"2017-08-27T16:00:00.000Z","path":"2017/08/28/recommend-books/","text":"开头2017.08.21 正式开启我入职的里程，现在已是工作了一个星期了，这个星期算是我入职的过渡期，算是知道了学校生活和工作的差距了，总之，尽快习惯这种生活吧。下面讲下自己的找工作经历和大学阅读的书籍，算是一种书籍推荐，为还在迷茫的你指引方向，同时为我三年的大学生活致敬！也激励我大四在公司实习能更上一层楼！ 找工作经历这段经历，算是自己很难忘记的经历吧。既辛酸既充实的日子！也很感谢自己在这段时间的系统复习，感觉把自己的基础知识再次聚集在一起了，自己的能力在这一段时间提升的也很快。后面有机会的话我也想写一系列的相关文章，为后来准备工作（面试）的同学提供一些自己的帮助。自己在找工作的这段时间面过的公司也有几家大厂，但是结果都不是很好，对我自己有很大的压力，当时心里真的感觉 ：“自己真的有这么差”，为什么一直被拒，当时很怀疑自己的能力，自己也有总结原因。一是面试的时候自己准备的还不够充分，虽说自己脑子里对这些基础有点印象，但是面试的时候自己稍紧张下就描述不怎么清楚了，导致面试官觉得你可能广度够了，深度还不够（这是阿里面试官电话面试说的）；二是自己的表达能力还是有所欠缺，不能够将自己所要表达的东西说出来，这可能我要在后面加强的地方；三是我的学校问题。在面了几家公司失败后，终于面了家公司要我了，我也确定在这家公司了。很幸运，刚出来，就有一个很好（很负责）的架构师带我，这周就给了我一个很牛逼的项目给我看（虽然自己目前还没有思路改里面的代码），里面新东西很多，说吃透了这个项目，以后绝对可以拿出去吹逼（一脸正经.jpg）。目前我的找工作经历就简短的介绍到这里了，如果感兴趣的话，可以加群：528776268 进来和我讨论交流。 书籍推荐大学，我不怎么喜欢玩游戏，自己也还算不怎么堕落吧，看了以下的一些书籍，算是对我后面写博客、找工作也有很大的帮助。如果你是大神，请忽略，如果你还是还在大学，和我一样不想把时间浪费在游戏上，可以看看我推荐的一些书籍，有想讨论的请在评论下留下你的评论或者加上面给的群号。 Java1、《Java 核心技术》卷一 、卷二 两本书，算是入门比较好的书籍了 2、《疯狂 Java 讲义》 很厚的一本书，里面的内容也是很注重基础了 3、《Java 并发编程的艺术》—— 方腾飞 、魏鹏、程晓明著 方腾飞 是并发编程网的创始人，里面的文章确实还不错，可以多看看里面的文章，收获绝对很大。 4、《 Java多线程编程核心技术》—— 高洪岩著 这本书也算是入门多线程编程的不错书籍，我之前还写了一篇读书笔记呢，《Java 多线程编程核心技术》学习笔记及总结 , 大家如果不想看书的可以去看我的笔记。 5、《Java 并发编程实战》 这本书讲的有点难懂啊，不过确实也是一本很好的书，以上三本书籍如果都弄懂了，我觉得你并发编程这块可能大概就 OK 了，然后再去看看线程池的源码，了解下线程池，我觉得那就更棒了。不想看的话，请看我的博客：Java 线程池艺术探索 我个人觉得还是写的很不错，那些大厂面试也几乎都会问线程池的东西，然后大概内容也就是我这博客写的 6、《Effective Java》中文版 第二版 算是 Java 的进阶书籍了，面试好多问题也是从这出来的 7、《深入理解 Java 虚拟机——JVM高级特性与最佳实践》第二版 这算是国内讲 JVM 最清楚的书了吧，目前还是只看了一遍，后面继续啃，大厂面试几乎也是都会考 JVM 的，阿里面 JVM 特别多，想进阿里的同学请一定要买这本书去看。 8、《深入分析Java Web技术内幕 修订版》许令波著 里面知识很广，每一章都是一个不同的知识，可见作者的优秀，不愧是阿里大神。 9、《大型网站系统与 Java 中间件实践》—— 曽宪杰 著 作者是前淘宝技术总监，见证了淘宝网的发展，里面的讲的内容也是很好，看完能让自己也站在高处去思考问题。 10、《大型网站技术架构 —— 核心原理与案例分析》 —— 李智慧 著 最好和上面那本书籍一起看，效果更好，两本看完了，提升思想的高度！ 11、《疯狂Java.突破程序员基本功的16课》 李刚 著 书中很注重 Java 的一些细节，讲的很深入，但是书中的错别字特多，可以看看我的读书笔记：《疯狂 Java 突破程序员基本功的 16 课》读书笔记 12、《Spring 实战》 Spring 入门书籍 13、《Spring 揭秘》—— 王福强 著 这本书别提多牛了，出版时期为 2009 年，豆瓣评分为 9.0 分，写的是真棒！把 Spring 的 IOC 和 AOP 特性写的很清楚，把 Spring 的来龙去脉讲的很全。墙裂推荐这本书籍，如果你想看 Spring，作者很牛，资深架构师，很有幸和作者有过一次交流，当时因为自己的一篇博客 Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子，竟然找到我想叫我去实习，可惜了，当时差点就跟着他混了。作者还有一本书 《Spring Boot 揭秘》。 14、《Spring 技术内幕》—— 深入解析 Spring 架构与设计原理 讲解 Spring 源码，深入了内部机制，个人觉得还是不错的。 15、Spring 官方的英文文档 这个别提了，很好，能看英文尽量看英文 16、《跟开涛学 Spring 3》 《跟开涛学 Spring MVC》 京东大神，膜 17、《看透springMvc源代码分析与实践》 算是把 Spring MVC 源码讲的很好的了 见我的笔记： 1、通过源码详解 Servlet 2 、看透 Spring MVC 源代码分析与实践 —— 网站基础知识 3 、看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 4 、看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 18、《Spring Boot 实战》 19、Spring Boot 官方 Reference Guide 网上好多写 SpringBoot 的博客，几乎和这个差不多。 20、《JavaEE开发的颠覆者: Spring Boot实战》 21、MyBatis 当然是官方的文档最好了，而且还是中文的。 自己也写过几篇文章，帮助过很多人入门，传送门： 1、通过项目逐步深入了解Mybatis（一）/) 2、通过项目逐步深入了解Mybatis（二）/) 3、通过项目逐步深入了解Mybatis（三）/) 4、通过项目逐步深入了解Mybatis（四）/) 22、《深入理解 Java 内存模型》—— 程晓明 著 我觉得每个 Java 程序员都应该了解下 Java 的内存模型，该书籍我看的是电子版的，不多，但是讲的却很清楚，把重排序、顺序一致性、Volatile、锁、final等写的很清楚。 Linux《鸟哥的Linux私房菜 基础学习篇(第三版) 》 鸟哥的Linux私房菜：服务器架设篇(第3版) 鸟哥的书 计算机网络《计算机网络第六版——谢希仁 编》 《计算机网络自顶向下方法》 计算机系统《代码揭秘：从C／C.的角度探秘计算机系统 —— 左飞》 《深入理解计算机系统》 《计算机科学导论_佛罗赞》 数据库《高性能MySQL》 《Mysql技术内幕InnoDB存储引擎》 Python这门语言语法很简单，上手快，不过我目前好久没用了，都忘得差不多了。当时是看的廖雪峰的 Python 博客 自己也用 Python 做爬虫写过几篇博客，不过有些是在前人的基础上写的。感谢那些栽树的人！ 工具Git ： 廖雪峰的 Git 教程 IDEA：IntelliJ IDEA 简体中文专题教程 Maven：《Maven实战》 其他《如何高效学习-斯科特杨》 教你怎样高效学习的 《软技能：代码之外的生存指南》 程序员除了写代码，还得懂点其他的软技能。 《提问的智慧“中文版”》 《How-To-Ask-Questions-The-Smart-Way》 作为程序员的你，一定要学会咋提问，不然别人都不想鸟你。 优秀网站推荐1、GitHub 别和我说不知道 2、InfoQ 文章很不错 3、CSDN 经常看博客专家的博客，里面大牛很多，传送门：zhisheng 4、知乎 多关注些大牛，看他们吹逼 5、掘金 自己也在上面写专栏，粉丝已经超过一万了，传送门 ：zhisheng 6、并发编程网 前面已经介绍 7、developerworks 上面的博客也很好 8、博客园 里面应该大牛也很多，不过自己没在上面写过博客 9、微信公众号 关注了很多人，有些人的文章确实很好。 10、牛客网 刷笔试题不错的地方，里面大牛超多，怀念叶神和左神讲课的时候，还有很有爱的牛妹。 11、优秀博主的博客地址了 优秀博客推荐廖雪峰 Git 和 Python 入门文章就是从他博客看的 阮一峰的网络日志 酷壳-陈皓 RednaxelaFX R大，牛逼的不得了 江南白衣 老司机 stormzhang 人称帅逼张，微信公众号写的不错 你假笨 阿里搞 JVM 的，很厉害 占小狼 泥瓦匠BYSocket 崔庆才 写了好多 Python 爬虫相关的文章 纯洁的微笑 SpringBoot 系列不错，其他的文章自己看了感觉是自己喜欢的那种文笔 程序猿DD 周立 芋艿V的博客 好多系列的源码分析 zhisheng 这个是我不要脸，竟然把自己博客地址的写上去了 最后送一句话，越努力，越幸运，祝早日成为大神！ 这些地方可以找到我： blog: http://www.54tianzhisheng.cn/ GitHub: https://github.com/zhisheng17 QQ 群：528776268","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Ubuntu16.10 安装 Nginx","date":"2017-08-17T16:00:00.000Z","path":"2017/08/18/Ubuntu-install-Nginx/","text":"安装 Nginx 依赖库安装 gcc g++ 的依赖库Ubuntu 平台使用： 12apt-get install build-essentialapt-get install libtool CentOS 平台使用： 12345centos平台编译环境使用如下指令安装make：yum -y install gcc automake autoconf libtool make安装g++:yum install gcc gcc-c++ 安装 pcre 依赖库12sudo apt-get updatesudo apt-get install libpcre3 libpcre3-dev 安装 zlib 依赖库1apt-get install zlib1g-dev 安装 ssl 依赖库1apt-get install openssl 安装 Nginx在网上下载了 nginx-1.8.1.tar.gz 版本。 1234567891011121314151617#解压：tar -zxvf nginx-1.8.1.tar.gz#进入解压目录：cd nginx-1.8.1#配置：./configure --prefix=/usr/local/nginx#编辑nginx：make注意：这里可能会报错，提示“pcre.h No such file or directory”,具体详见：http://stackoverflow.com/questions/22555561/error-building-fatal-error-pcre-h-no-such-file-or-directory需要安装 libpcre3-dev,命令为：sudo apt-get install libpcre3-dev#安装nginx：sudo make install#启动nginx：sudo /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf注意：-c 指定配置文件的路径，不加的话，nginx会自动加载默认路径的配置文件，可以通过 -h查看帮助命令。#查看nginx进程：ps -ef|grep nginx Nginx 常用命令启动 Nginx切换到 /usr/local/nginx/sbin/ 目录下，执行命令 1./nginx 查看效果： 停止 Nginx12./nginx -s stop./nginx -s quit -s 都是采用向 Nginx 发送信号的方式。 Nginx 重新加载配置文件1./nginx -s reload 指定配置文件1./nginx -c /usr/local/nginx/conf/nginx.conf -c 表示 configuration，指定配置文件 查看 Nginx 版本12./nginx -v //查看 Nginx 版本信息的参数./nginx -V //查看 Nginx 详细的版本信息 检查配置文件是否正确1./nginx -t 如果出现测试失败，表示没有访问错误日志文件和进程，可以 sudo 一下。配置正确的话会有相关的提示。 显示帮助信息123./nginx -h或者./nginx -? Nginx 的特点和应用场合见文章：Nginx 基本知识快速入门 最后文章首发地址：zhisheng的博客 ，转载请注明地址 http://www.54tianzhisheng.cn/2017/08/18/Ubuntu-install-Nginx/","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://www.54tianzhisheng.cn/tags/Nginx/"}]},{"title":"马云热血励志演讲《最伟大的成功》","date":"2017-08-10T16:00:00.000Z","path":"2017/08/11/most-success/","text":"当你乐观的时候，总是会有机会的。那么你可能要问了“机会在哪呢？”你可能没有特别想实现的事，没有迫切要成功的欲望，没有勇攀高峰的决心，没有水滴石穿的毅力，也没有一颗所向披靡的强大心脏。那么，让来自马云的这段演讲来告诉你，你有什么。你有年轻的身体，你有奇妙的想法，你有乐观的心态，你有无限的可能性。 我想，马云的这段励志演讲为我们提供了一面镜子可以用于自照。上面“你有什么”说完了，接下来让我讨论一下“你可能没有什么”。 你总爱抱怨，机会却总是躲在人们抱怨的地方。你没有仔细想想怎么能把事情做得不一样。你没有行动力，你缺少坚持下去的长劲儿。你抗压能力差，你动不动就玻璃心。你不相信自己也不相信别人，你怕犯错。现在是不是觉得这碗鸡汤有点难以下咽了？如果认识到差距，不如从今天开始改变。明天的你只要比今天的你多迈出0.1步，也是进步。 为自己而工作。停止抱怨，用抱怨的时间多做事。把那些夜里冒出来的好点子在白天付诸行动，既然有了设想，那就行动起来。行动是你迈出的第一步，后面可能会更难，历经无数次动摇，面临无数次诱惑，感受无数次失败的苦味和难以为继的辛酸。顶住这一切，比常人更勇敢地去面对，并且坚持下去。排除万难，别被来自世人的非议和质疑影响。相信你自己，相信你的团队。服务好你的客户，之后再想怎么回馈社会。犯足够多的错，年轻时走过的弯路是最棒的收获。 马云还曾经说过“今天很艰难，明天比今天更难，后天可能是美好的，但更多的人死在了明天”。是不是感到膝盖中箭了？不妨干了这碗“毒鸡汤”。成功的法则本就并非千篇一律，你会有你自己向上的学问。那不如从明天开始，去摸索，去践行，哪怕只比昨天的你多迈出0.01步。 送给正在找实习工作的自己！加油！！！","tags":[{"name":"励志","slug":"励志","permalink":"http://www.54tianzhisheng.cn/tags/励志/"}]},{"title":"源码大招：不服来战！撸这些完整项目，你不牛逼都难！","date":"2017-08-07T16:00:00.000Z","path":"2017/08/08/android-projects/","text":"经常有人问我有没有什么项目代码，我回复说去 Github 找，但是还是好多人不知道如何找到那些比较好的项目。 今天花了点时间找了些安卓的项目，觉得还是不错的，几乎就是自己生活常用的一些 app ，如果你是一个 Android 开发者，我觉得撸完这些项目，你想不牛逼都难。 关注我 菜鸟新闻菜鸟新闻 客户端是一个仿照36Kr官方,实 时抓取36Kr官网数据的资讯类新闻客户端。 包括首页新闻,详情,发现,活动,实时数据抓取,侧滑效果,第三方登录以及分享,消息推送等相关功能客户端。 课程地址： http://www.cniao5.com/clazz/view/10076.html视频下载链接： http://pan.baidu.com/s/1eQLyQxc 密码：3ts1 项目源码下载地址：https://github.com/yxs666/cniao5-news 运行截图: .gif) KuaiChuan仿茄子快传的一款文件传输应用， 涉及到Socket通信，包括TCP，UDP通信 项目源码：https://github.com/mayubao/KuaiChuan 运行截图： CoolShopping一个仿拉手团购的购物App，采用Bmob后台实现短信验证码注册、登录、收藏、订单管理、自动更新等功能，数据抓取自拉手团购 项目地址：https://github.com/myxh/CoolShopping 运行截图： RNPolymerPoRNPolymerPo 是一个基于 React Native 的生活类聚合实战项目，目前由于没有 MAC 设备，所以没有适配 iOS，感兴趣的可以自行适配 app 目录下相关 JS 代码即可。 项目地址：https://github.com/yanbober/RNPolymerPo 运行截图： bilibili仿 bilibili 的客户端 项目地址：https://github.com/HotBitmapGG/bilibili-android-client 运行截图： StockChart采用主流rxjava+retrofit+dagger2框架，StockChart看股票的分时图，k线图。 项目地址：https://github.com/AndroidJiang/StockChart Android精准计步器项目地址：https://github.com/linglongxin24/DylanStepCount 运行截图： 菜鸟微博菜鸟微博《通过对新浪微博开发案例的详细解析，讲解了一个完整的 Android 实际项目的开发过程。 有新浪微博的主要功能，有Toolbar,RecyclerView等最新控件的用法；各种快速开发框架的使用，比如 Glide,PhotoView ，EventBus ，OKHttp，pullToRefresh等。 学习视频+源码 视频中还会讲到MVP设计模式以及一些架构师的入门知识。 课程地址： http://www.cniao5.com/clazz/view/10075.html视频下载链接： http://pan.baidu.com/s/1gexq3VP 密码：f0t9 项目地址：https://github.com/yxs666/cniao5-weibo 运行截图： 在线云打印平台一个在线云打印平台（android部分）含订单管理、百度地图、二维码等等 项目地址：https://github.com/LehmanHe/A4print 运行截图： 铜板街项目地址：https://github.com/robotlife/TongBanJie 运行截图： 礼物说项目地址：https://github.com/Orangelittle/Liwusuo IotXmpp本项目是基于XMPP的物联网客户端软件的实现，其实现的主要功能是一款能和物联网节点交互的即时通讯软件。目前支持九类传感器节点交互，主要有：温湿度、风扇、直流电机、LED灯、步进电机、门磁、光电接近、烟雾和光照。本软件不仅能和这些传感器节点交互，还实现了类似微信的订阅和取消订阅功能。当订阅一个节点后节点就会按照设定好的周期向客户端汇报数据，客户端也能设置周期、设置报警上下限等。这些功能的实现极大的方便了我们和物联网节点的交互。 项目地址：https://github.com/tiandawu/IotXmpp 项目截图： Lives生活娱乐结合的APP, 现有主要功能： 图书 翻译 音乐 视频 项目地址：https://github.com/Allyns/Lives 项目截图： CoCoin一款多视图记账APP 项目地址：https://github.com/Nightonke/CoCoin 运行截图： AppLockAppLock应用锁，保护你的隐私 项目地址：https://github.com/lizixian18/AppLock 运行截图： jianshi 简诗一款优雅的中国风Android App，包括Android端和Server端，支持登录注册，数据云端同步，离线数据存储和截屏分享等功能。 项目地址： 运行截图： storage-chooser一款文件管理器app 项目地址：https://github.com/codekidX/storage-chooser 运行截图： LQRWeChat仿最新版微信6.5.7（除图片选择器外）。本项目基于融云SDK，使用目前较火的 Rxjava+Retrofit+MVP+Glide 技术开发。相比上个版本，加入发送位置消息，红包消息等功能。 项目地址：https://github.com/GitLqr/LQRWeChat 运行截图： PonyExpress 小马快递小马快递，您的好帮手。查询并跟踪快递，快递信息及时掌握。支持全国100多家快递公司，支持扫码查询，智能识别快递公司。附带生成二维码小工具，方便实用。体积小巧，无广告，无多余权限。 项目地址：https://github.com/wangchenyan/PonyExpress 运行截图： CloudReader 云阅一款基于网易云音乐UI，使用Gank.Io及豆瓣api开发的符合Google Material Design的Android客户端。项目采取的是MVVM-DataBinding架构开发，现主要包括：干货区、电影区和书籍区三个子模块。DIY网易云音乐原来是如此Cool 项目地址：https://github.com/youlookwhat/CloudReader 运行截图： 硅谷商城是一款按照企业级标准研发的项目。本套代码是目前国内市场第一套详细讲解商城类项目的免费代码。该代码中的内容包括但不仅限于，框架的搭建 、主页模块、分类模块、发现模块、购物车模块和个人中心模块。项目中讲解的主流技术包括且不限于RadioGroup + Fragment、OKHttp、FastJson、RecyclerView、 ScrollViewContainer、Banner、倒计时秒杀、自定义购物车、支付宝等技术。该项目中讲解的技术可应用在电商、新闻、旅游、医疗、在线教育等领域。 项目地址：https://github.com/atguigu01/Shopping 运行截图： 觉得棒的，欢迎点赞和转发分享，谢谢大家！转载的话注明来源地址为 www.54tianzhisheng.cn/2017/05/13/android-projects/ 即可。","tags":[{"name":"Android","slug":"Android","permalink":"http://www.54tianzhisheng.cn/tags/Android/"}]},{"title":"Nginx 基本知识快速入门","date":"2017-08-04T16:00:00.000Z","path":"2017/08/05/Nginx/","text":"什么是 Nginx？Nginx 是一个高性能的 HTTP 和反向代理服务器，以高稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。 Nginx 特点 处理静态文件，索引文件以及自动索引；打开文件描述符缓冲． 无缓存的反向代理加速，简单的负载均衡和容错． FastCGI，简单的负载均衡和容错． 模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等 filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个 SSI，则这项处理可以并行运行，而不需要相互等待。 支持 SSL 和 TLSSNI． 主要应用场合1、静态 HTTP 服务器首先，Nginx是一个 HTTP 服务器，可以将服务器上的静态文件（如 HTML、图片）通过 HTTP 协议展现给客户端。 配置： 123456server &#123; listen 80; # 端口号 location / &#123; root /usr/share/nginx/html; # 静态文件路径 &#125;&#125; 2、反向代理服务器什么是反向代理？ 客户端本来可以直接通过 HTTP 协议访问某网站应用服务器，如果网站管理员在中间加上一个 Nginx，客户端请求Nginx，Nginx 请求应用服务器，然后将结果返回给客户端，此时 Nginx 就是反向代理服务器。 配置： 123456server &#123; listen 80; location / &#123; proxy_pass http://192.168.20.1:8080; # 应用服务器HTTP地址 &#125;&#125; 既然服务器可以直接 HTTP 访问，为什么要在中间加上一个反向代理，不是多此一举吗？反向代理有什么作用？继续往下看，下面的负载均衡、虚拟主机，都基于反向代理实现，当然反向代理的功能也不仅仅是这些。 3、负载均衡当网站访问量非常大，网站站长开心赚钱的同时，也摊上事儿了。因为网站越来越慢，一台服务器已经不够用了。于是将相同的应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。同时带来的好处是，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。 当我们网站进行大的升级更新时，我们不可能直接将所有的服务器都关掉，然后再升级的。通常我们都是批量的关掉一些服务器，去升级网站，当有用户的请求时则分配给其他还在运作的机器处理。当之前关掉的机器更新完成后，再次开启，然后又批量关掉部分机器，如上循环，直到最后全部机器都更新完成。这样就不会影响用户使用。 Nginx 可以通过反向代理来实现负载均衡。 配置： 12345678910upstream myapp &#123; server 192.168.20.1:8080; # 应用服务器1 server 192.168.20.2:8080; # 应用服务器2&#125;server &#123; listen 80; location / &#123; proxy_pass http://myapp; &#125;&#125; 4、虚拟主机网站访问量大，需要负载均衡。然而并不是所有网站都如此出色，有的网站，由于访问量太小，需要节省成本，将多个网站部署在同一台服务器上。 例如将 www.aaa.com 和 www.bbb.com 两个网站部署在同一台服务器上，两个域名解析到同一个 IP 地址，但是用户通过两个域名却可以打开两个完全不同的网站，互相不影响，就像访问两个服务器一样，所以叫两个虚拟主机。 配置： 12345678910111213141516171819server &#123; listen 80 default_server; server_name _; return 444; # 过滤其他域名的请求，返回444状态码&#125;server &#123; listen 80; server_name www.aaa.com; # www.aaa.com域名 location / &#123; proxy_pass http://localhost:8080; # 对应端口号8080 &#125;&#125;server &#123; listen 80; server_name www.bbb.com; # www.bbb.com域名 location / &#123; proxy_pass http://localhost:8081; # 对应端口号8081 &#125;&#125; 在服务器 8080 和 8081 两个端口分别开了一个应用，客户端通过不同的域名访问，根据 server_name 可以反向代理到对应的应用服务器。 虚拟主机的原理是通过 HTTP 请求头中的 Host 是否匹配 server_name 来实现的，有兴趣的同学可以研究一下 HTTP 协议。 另外，server_name 配置还可以过滤有人恶意将某些域名指向你的主机服务器。 5、FastCGINginx 本身不支持 PHP 等语言，但是它可以通过 FastCGI 来将请求扔给某些语言或框架处理（例如 PHP、Python、Perl）。 123456789server &#123; listen 80; location ~ \\.php$ &#123; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /PHP文件路径$fastcgi_script_name; # PHP文件路径 fastcgi_pass 127.0.0.1:9000; # PHP-FPM地址和端口号 # 另一种方式：fastcgi_pass unix:/var/run/php5-fpm.sock; &#125;&#125; 配置中将 .php 结尾的请求通过 FashCGI 交给 PHP-FPM 处理，PHP-FPM 是 PHP 的一个 FastCGI 管理器。有关FashCGI 可以查阅其他资料，本文不再介绍。 fastcgi_pass 和 proxy_pass 有什么区别？下面一张图带你看明白： 参考资料1、Nginx基本功能极速入门 2、Nginx 学习笔记","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://www.54tianzhisheng.cn/tags/Nginx/"}]},{"title":"秋招第三站 —— 内推阿里（一面）","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/alibaba/","text":"3、阿里巴巴（菜鸟网络部门）（一面 49 分钟）2017.08.02 晚上9点21打电话过来，预约明天什么时候有空面试，约好第二天下午两点。 2017.08.03 下午两点10分打过来了。 说看了我的博客和 GitHub，觉得我学的还行，知识广度都还不错，但是还是要问问具体情况，为什么没看到你春招的记录，什么原因没投阿里？非得说一个原因，那就是：我自己太菜了，不敢投。1、先自我介绍 2、什么是多态？哪里体现了多态的概念？ 3、HashMap 源码分析，把里面的东西问了个遍？最后问是不是线程安全？引出 ConcurrentHashMap 4、ConcurrentHashMap 源码分析 5、类加载，双亲委托机制 6、Java内存模型（一开始说的不是他想要的，主要想问我堆和栈的细节） 7、垃圾回收算法 8、线程池，自己之前看过，所以说的比较多，最后面试官说了句：看你对线程池了解还是很深了 9、事务的四种特性 10、什么是死锁？ 11、乐观锁和悲观锁的策略 12、高可用网站的设计（有什么技术实现） 13、低耦合高内聚 14、设计模式了解不？你用过哪几种，为什么用，单例模式帮我们做什么东西？有什么好处？ 15、你参与什么项目中成长比较快？学到了什么东西，以前是没有学过的？ 16、项目中遇到的最大困难是怎样的？是怎么解决的？ 17、智力题（两根不均匀的香，点一头烧完要一个小时，怎么确定15分钟） 18、你有什么问题想要问我的？ 19、问了菜鸟网络他们部门主要做什么？ 20、对我这次面试做个评价：看了你博客和 GitHub，知道你对学习的热情还是很高的，花了不少功夫，但是有些东西还是需要加强深度，阿里需要那种对技术有深度，有自己独到见解的人才。意思就是 GG 了。 总结：面试总的来说，第一次电话面试，感觉好紧张，好多问题自己会点，但是其中的细节没弄清楚，自己准备的也不够充分。面试官很友好，看到我紧张，也安慰我说不要紧，不管以后出去面试啥的，不需要紧张，公司问的问题可能很广，你只需要把你知道的说出来就行，不会的直接说不会就行。之前一直不敢投阿里，因为自己准备的完全不够充分，但是在朋友磊哥的帮助下，还是试了下，不管结果怎么样，经历过总比没有的好。","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"秋招第二站 —— 内推爱奇艺（一面二面）","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/iqiyi/","text":"第 2 站 、爱奇艺 后端 Java 开发实习生笔试（半个小时）题目：（记得一些） 1、重载重写的区别？ 2、转发和重定向的区别？3、画下 HashMap 的结构图？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 4、statement 和 preparedstatement 区别？ 5、JSP 中一个 中取值与直接取值的区别？会有什么安全问题？ 6、实现一个线程安全的单例模式 7、一个写 sql 语句的题目 8、自己实现一个 List，（主要实现 add等常用方法） 9、Spring 中 IOC 和 AOP 的理解？ 10、两个对象的 hashcode 相同，是否对象相同？equal() 相同呢？ 11、@RequestBody 和 @ResponseBody 区别？ 12、JVM 一个错误，什么情况下会发生？ 13、常用的 Linux 命令？ 第一轮面试（80 分钟）1、自我介绍 2、介绍你最熟悉的一个项目 3、讲下这个 XSS 攻击 4、HashMap 的结构？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 5、HashMap 中怎么解决冲突的？（要我详细讲下） 6、ConcurrentHashMap 和 HashTable 中线程安全的区别？为啥建议用 ConcurrentHashMap ？能把 ConcurrentHashMap 里面的实现详细的讲下吗？ 7、Session 和 Cookie 的区别？ 8、你项目中登录是怎样做的，用的 Cookie 和 Session？ 9、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 10、问了好几个注解的作用？ 11、statement 和 preparedstatement 区别？ 12、$ 和 # 的区别？以及这两个在哪些地方用？ 13、前面项目介绍了数据是爬虫爬取过来的，那你讲讲你的爬虫是多线程的吧？ 14、讲讲 Python 中的多线程和 Java 中的多线程区别？ 15、自己刚好前几天在看线程池，立马就把面试官带到我熟悉的线程池，和面试官讲了下 JDK 自带的四种线程池、ThreadPoolExecutor 类中的最重要的构造器里面的七个参数，然后再讲了下线程任务进入线程池和核心线程数、缓冲队列、最大线程数量比较。 16、线程同步，你了解哪几种方式？ 17、讲下 Synchronized？ 18、讲下 RecentLock 可重入锁？ 什么是可重入锁？为什么要设计可重入锁？ 19、讲下 Volatile 吧？他是怎样做到同步的？ 20、Volatile 为什么不支持原子性？举个例子 21、Atomic 怎么设计的？（没看过源码，当时回答错了，后来才发现里面全部用 final 修饰的属性和方法） 22、问几个前端的标签吧？（问了一个不会，直接说明我偏后端，前端只是了解，后面就不问了） 23、SpringBoot 的了解？ 24、Linux 常用命令？ 25、JVM 里的几个问题？ 26、事务的特性？ 27、隔离级别？ 28、网络状态码？以 2、3、4、5 开头的代表什么意思。 29、并发和并行的区别？ 30、你有什么问题想问我的？ 一面面完后面试官和说这份试卷是用来考 1~3 年开发工作经验的，让我准备一下，接下来的二面。 第二轮面试（半个小时）1、一上来就问怎么简历名字都没有，我指了简历第一行的我的名字，还特意大写了，然后就问学校是不是在上海，我回答在南昌（感觉被鄙视了一波，后面我在回答问题的时候面试官就一直在玩手机，估计后面对我的印象就不是很好了） 2、自我介绍 3、说一说数据库建表吧（从范式讲） 4、讲讲多态？（这个我答出来了，可是面试官竟然说不是这样吧，可能面试官没听请，后面还说我是不是平时写多态比较少，感觉这个也让面试官对我印象减分） 5、将两个数转换（不借助第三个参数） 6、手写个插入排序吧（写完了和面试官讲了下执行流程） 7、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 8、问了几个常用的 Linux 命令？ 9、也问到多线程？和一面一样把自己最近看的线程池也讲了一遍 10、学 Java 多久了？ 11、你有什么想问的？ 总结：面试题目大概就是这么多了，有些问题自己也忘记了，面试题目顺序不一定是按照上面所写的。再次感谢爱奇艺的第一面面试官了，要不是他帮忙内推的，我可能还没有机会收到面试机会。自己接到爱奇艺面试邀请电话是星期一晚上快7点中的，之后加了面试官微信约好了星期四面试的（时间准备较短，之前没系统的复习过）。星期四一大早（5点就起床了），然后就收拾了下，去等公交车，转了两次车，然后再做地铁去爱奇艺公司的，总共路上花费时间四个多小时。总的来说，这次面试准备的时间不是很充裕，所以准备的个人觉得不是很好，通过这次的面试，发现面试还是比较注重基础和深度的，我也知道了自己的一些弱处，还需要在哪里加强，面试技巧上也要掌握些。为后面的其他公司继续做好充足的准备。加油！！！","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"秋招第一站 —— 亚信科技","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/yaxin/","text":"第 1 站、亚信科技 Java 开发1）自我介绍（说到一个亮点：长期坚持写博客，面试官觉得这个习惯很好，算加分项吧） 2）看到简历项目中用到 Solr，详细的问了下 Solr（自己介绍了下 Solr 的使用场景和建立索引等东西）3）项目里面写了一个 “ 敏感词和 JS 标签过滤防 XSS 攻击”，面试官让我讲了下这个 XSS 攻击，并且是怎样实现的 4）项目里写了支持 Markdown，问是不是自己写的解析代码，（回答不是，自己引用的是 GitHub上的一个开源项目解析的） 5）想问我前端的知识，我回复到：自己偏后端开发，前端只是了解，然后面试官就不问了 6）问我考不考研？ 7）觉得杭州怎么样？是打算就呆在杭州还是把杭州作为一个跳板？ 8）有啥小目标？以后是打算继续技术方向，还是先技术后管理（还开玩笑的说：是不是赚他几个亿，当时我笑了笑） 9）有啥兴趣爱好？ 大概就记得这么多了，目前已经拿到 Offer 了。 总结：面试问的问题不算多，主要是通过简历上项目所涉及的东西提问的，如果自己不太会的切记不要写上去。面试主要考察你回答问题来判断你的逻辑是否很清楚。","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"Java 线程池艺术探索","date":"2017-07-28T16:00:00.000Z","path":"2017/07/29/ThreadPool/","text":"线程池Wiki 上是这样解释的：Thread Pool 作用：利用线程池可以大大减少在创建和销毁线程上所花的时间以及系统资源的开销！ 下面主要讲下线程池中最重要的一个类 ThreadPoolExecutor 。 ThreadPoolExecutor ThreadPoolExecutor 构造器： 有四个构造器的，挑了参数最长的一个进行讲解。 七个参数： corePoolSize：核心池的大小，在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止； unit：参数keepAliveTime的时间单位（DAYS、HOURS、MINUTES、SECONDS 等）； workQueue：阻塞队列，用来存储等待执行的任务； ArrayBlockingQueue （有界队列） LinkedBlockingQueue （无界队列） SynchronousQueue threadFactory：线程工厂，主要用来创建线程 handler：拒绝处理任务的策略 AbortPolicy：丢弃任务并抛出 RejectedExecutionException 异常。（默认这种） DiscardPolicy：也是丢弃任务，但是不抛出异常 DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） CallerRunsPolicy：由调用线程处理该任务 重要方法： execute()：通过这个方法可以向线程池提交一个任务，交由线程池去执行； shutdown()：关闭线程池； execute() 方法： 注：JDK 1.7 和 1.8 这个方法有点区别，下面代码是 1.8 中的。 1234567891011121314151617181920212223public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); //1、如果当前的线程数小于核心线程池的大小，根据现有的线程作为第一个 Worker 运行的线程，新建一个 Worker，addWorker 自动的检查当前线程池的状态和 Worker 的数量，防止线程池在不能添加线程的状态下添加线程 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //2、如果线程入队成功，然后还是要进行 double-check 的，因为线程在入队之后状态是可能会发生变化的 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // recheck 防止线程池状态的突变，如果突变，那么将 reject 线程，防止 workQueue 中增加新线程 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0)//上下两个操作都有 addWorker 的操作，但是如果在workQueue.offer 的时候 Worker 变为 0，那么将没有 Worker 执行新的 task，所以增加一个 Worker. addWorker(null, false); &#125; //3、如果 task 不能入队(队列满了)，这时候尝试增加一个新线程，如果增加失败那么当前的线程池状态变化了或者线程池已经满了然后拒绝task else if (!addWorker(command, false)) reject(command); &#125; 其中调用了 addWorker() 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768private boolean addWorker(Runnable firstTask, boolean core) &#123;// firstTask: 新增一个线程并执行这个任务，可空，增加的线程从队列获取任务；core：是否使用 corePoolSize 作为上限，否则使用 maxmunPoolSize retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. /** * rs!=Shutdown || fistTask！=null || workQueue.isEmpty * 如果当前的线程池的状态 &gt; SHUTDOWN 那么拒绝 Worker 的 add 如果 =SHUTDOWN * 那么此时不能新加入不为 null 的 Task，如果在 workQueue 为 empty 的时候不能加入任何类型的 Worker， * 如果不为 empty 可以加入 task 为 null 的 Worker, 增加消费的 Worker */ if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp;! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //如果当前的数量超过了 CAPACITY，或者超过了 corePoolSize 和 maximumPoolSize（试 core 而定） if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //CAS 尝试增加线程数，如果失败，证明有竞争，那么重新到 retry。 if (compareAndIncrementWorkerCount(c))// AtomicInteger 的 CAS 操作; break retry; c = ctl.get(); // Re-read ctl //判断当前线程池的运行状态,状态发生改变，重试 retry; if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask);// Worker 为内部类，封装了线程和任务，通过 ThreadFactory 创建线程，可能失败抛异常或者返回 null final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable // SHUTDOWN 以后的状态和 SHUTDOWN 状态下 firstTask 为 null，不可新增线程 throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s;//记录最大线程数 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w);//失败回退,从 wokers 移除 w, 线程数减一，尝试结束线程池(调用tryTerminate 方法) &#125; return workerStarted; &#125; 示意图： 执行流程： 1、当有任务进入时，线程池创建线程去执行任务，直到核心线程数满为止 2、核心线程数量满了之后，任务就会进入一个缓冲的任务队列中 当任务队列为无界队列时，任务就会一直放入缓冲的任务队列中，不会和最大线程数量进行比较 当任务队列为有界队列时，任务先放入缓冲的任务队列中，当任务队列满了之后，才会将任务放入线程池，此时会与线程池中最大的线程数量进行比较，如果超出了，则默认会抛出异常。然后线程池才会执行任务，当任务执行完，又会将缓冲队列中的任务放入线程池中，然后重复此操作。 shutdown() 方法： 1234567891011121314151617public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //判断是否可以操作目标线程 checkShutdownAccess(); //设置线程池状态为 SHUTDOWN, 此处之后，线程池中不会增加新 Task advanceRunState(SHUTDOWN); //中断所有的空闲线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; //转到 Terminate tryTerminate(); &#125; 参考资料：深入理解java线程池—ThreadPoolExecutor JDK 自带四种线程池分析与比较 1、newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 2、newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 3、newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 4、newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 四种线程池其实内部方法都是调用的 ThreadPoolExecutor 类，只不过利用了其不同的构造器方法而已（传入自己需要传入的参数），那么利用这个特性，我们自己也是可以实现自己定义的线程池的。 自定义线程池1、创建任务类 12345678910111213141516171819202122232425262728293031323334353637package com.zhisheng.thread.threadpool.demo;/** * Created by 10412 on 2017/7/24. * 任务 */public class MyTask implements Runnable&#123; private int taskId; //任务 id private String taskName; //任务名字 public int getTaskId() &#123; return taskId; &#125; public void setTaskId(int taskId) &#123; this.taskId = taskId; &#125; public String getTaskName() &#123; return taskName; &#125; public void setTaskName(String taskName) &#123; this.taskName = taskName; &#125; public MyTask(int taskId, String taskName) &#123; this.taskId = taskId; this.taskName = taskName; &#125; @Override public void run() &#123; System.out.println(\"当前正在执行 ****** 线程Id--&gt;\" + taskId + \",任务名称--&gt;\" + taskName); try &#123; Thread.currentThread().sleep(5 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程Id--&gt;\" + taskId + \",任务名称--&gt;\" + taskName + \" ----------- 执行完毕！\"); &#125;&#125; 2、自定义拒绝策略，实现 RejectedExecutionHandler 接口，重写 rejectedExecution 方法 12345678910111213141516package com.zhisheng.thread.threadpool.demo;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;/** * Created by 10412 on 2017/7/24. * 自定义拒绝策略，实现 RejectedExecutionHandler 接口 */public class RejectedThreadPoolHandler implements RejectedExecutionHandler&#123; public RejectedThreadPoolHandler() &#123; &#125; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.out.println(\"WARNING 自定义拒绝策略: Task \" + r.toString() + \" rejected from \" + executor.toString()); &#125;&#125; 3、创建线程池 1234567891011121314151617181920212223242526272829303132package com.zhisheng.thread.threadpool.demo;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * Created by 10412 on 2017/7/24. */public class ThreadPool&#123; public static void main(String[] args) &#123; //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s，有界队列长度为 3, //ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3)); //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s， 无界队列, //ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60L, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;()); //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s，有界队列长度为 3, 使用自定义拒绝策略 ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3), new RejectedThreadPoolHandler()); for (int i = 1; i &lt;= 10; i++) &#123; //创建 10 个任务 MyTask task = new MyTask(i, \"任务\" + i); //运行 pool.execute(task); System.out.println(\"活跃的线程数：\"+pool.getActiveCount() + \",核心线程数：\" + pool.getCorePoolSize() + \",线程池大小：\" + pool.getPoolSize() + \",队列的大小\" + pool.getQueue().size()); &#125; //关闭线程池 pool.shutdown(); &#125;&#125; 这里运行结果就不截图了，我在本地测试了代码是没问题的，感兴趣的建议还是自己跑一下，然后分析下结果是不是和前面分析的一样，如有问题，请在我博客下面评论！ 总结本文一开始讲了线程池的介绍和好处，然后分析了线程池中最核心的 ThreadPoolExecutor 类中构造器的七个参数的作用、类中两个重要的方法，然后在对比研究了下 JDK 中自带的四种线程池的用法和内部代码细节，最后写了一个自定义的线程池。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.54tianzhisheng.cn/tags/多线程/"},{"name":"线程池","slug":"线程池","permalink":"http://www.54tianzhisheng.cn/tags/线程池/"}]},{"title":"Java 性能调优需要格外注意的细节","date":"2017-07-24T16:00:00.000Z","path":"2017/07/25/Java-performance-tuning/","text":"昨天写了篇文章 《MySQL 处理海量数据时的一些优化查询速度方法》 ，其实开发中不止数据库要优化，还有我们本身的开发代码也需要优化，这样我们开发的产品才能够得到极致的体验。也许有些人认为这些细小的地方没有啥好修改的，改与不改对运行效率没啥大的影响？首先在我们本地一个人测试下效率是不怎么明显，但是如果到发布上线后，你的用户有几百万，甚至上千万，这些用户同时访问你的网站，那么你的网站是否经得住考验呢，效率那高不高呢，如果效率不高，那么需要多出很多买服务器的经费呢，所以想想还是很有必要注意这些小的细节。今天就讲讲一些 Java 性能调优需要格外注意的一些细节。 代码优化的目标： 减少代码的体积 提高代码的运行效率 代码优化细节1、尽量指定类、方法的final修饰符 带有final修饰符的类是不可派生的。在Java核心API中，有许多应用final的例子，例如java.lang.String，整个类都是final的。为类指定final修饰符可以让类不可以被继承，为方法指定final修饰符可以让方法不可以被重写。如果指定了一个类为final。Java编译器会寻找机会内联所有的final方法，内联对于提升Java运行效率作用重大，具体参见Java运行期优化。 此举能够使性能平均提高50% 。 2、尽量重用对象 特别是String对象的使用，出现字符串连接时应该使用StringBuilder/StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 3、尽可能使用局部变量 调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 4、及时关闭流 Java编程过程中，进行数据库连接、I/O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。 5、尽量减少对变量的重复计算 明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： 12for (int i = 0; i &lt; list.size(); i++)&#123;...&#125; 建议替换为： 12for (int i = 0, int length = list.size(); i &lt; length; i++)&#123;...&#125; 这样，在list.size()很大的时候，就减少了很多的消耗 6、尽量采用懒加载的策略，即在需要的时候才创建 例如： 1234String str = \"aaa\";if (i == 1)&#123; list.add(str);&#125; 建议替换为： 12345if (i == 1)&#123; String str = \"aaa\"; list.add(str);&#125; 7、慎用异常 异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为 fillInStackTrace() 的本地同步方法，fillInStackTrace() 方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 8、不要在循环中使用 try…catch…，应该把其放在最外层 除非不得已。如果毫无理由地这么写了，只要你的领导资深一点、有强迫症一点，八成就要骂你为什么写出这种垃圾代码来了。 9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度 比如 ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet 等等，以 StringBuilder 为例： （1）StringBuilder() // 默认分配16个字符的空间 （2）StringBuilder(int size) // 默认分配size个字符的空间 （3）StringBuilder(String str) // 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的 StringBuilder ）的来设定它的初始化容量，这样可以明显地提升性能。比如 StringBuilder 吧，length 表示当前的 StringBuilder 能保持的字符数量。因为当 StringBuilder 达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要 StringBuilder 达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中—-这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： （1）在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间； （2）把原来的4096个字符拷贝到新的的字符数组中去。 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成 new HashMap(128)、new HashMap(256) 都可以。 10、当复制大量数据时，使用 System.arraycopy() 命令 11、乘法和除法使用移位操作 例如： 12345for (val = 0; val &lt; 100000; val += 5)&#123; a = val * 8; b = val / 2;&#125; 用移位操作可以极大地提高性能，因为在计算机底层，对位的操作是最方便、最快的，因此建议修改为： 12345for (val = 0; val &lt; 100000; val += 5)&#123; a = val &lt;&lt; 3; b = val &gt;&gt; 1;&#125; 移位操作虽然快，但是可能会使代码不太好理解，因此最好加上相应的注释。 12、循环内不要不断创建对象引用 例如： 1234for (int i = 1; i &lt;= count; i++)&#123; Object obj = new Object();&#125; 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： 12345Object obj = null;for (int i = 0; i &lt;= count; i++) &#123; obj = new Object(); &#125; 这样的话，内存中只有一份 Object 对象引用，每次 new Object() 的时候，Object 对象引用指向不同的Object罢了，但是内存中只有一份，这样就大大节省了内存空间了。 13、基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用 ArrayList 14、尽量使用 HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用 Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销 15、不要将数组声明为 public static final 因为这毫无意义，这样只是定义了引用为 static final，数组的内容还是可以随意改变的，将数组声明为 public 更是一个安全漏洞，这意味着这个数组可以被外部类所改变。 16、尽量在合适的场合使用单例 使用单例可以减轻加载的负担、缩短加载的时间、提高加载的效率，但并不是所有地方都适用于单例，简单来说，单例主要适用于以下三个方面： （1）控制资源的使用，通过线程同步来控制资源的并发访问 （2）控制实例的产生，以达到节约资源的目的 （3）控制数据的共享，在不建立直接关联的条件下，让多个不相关的进程或线程之间实现通信 17、尽量避免随意使用静态变量 要知道，当某个对象被定义为static的变量所引用，那么gc通常是不会回收这个对象所占有的堆内存的，如： 1234public class A&#123; private static B b = new B();&#125; 此时静态变量b的生命周期与A类相同，如果A类不被卸载，那么引用B指向的B对象会常驻内存，直到程序终止 18、及时清除不再需要的会话 为了清除不再活动的会话，许多应用服务器都有默认的会话超时时间，一般为30分钟。当应用服务器需要保存更多的会话时，如果内存不足，那么操作系统会把部分数据转移到磁盘，应用服务器也可能根据MRU（最近最频繁使用）算法把部分不活跃的会话转储到磁盘，甚至可能抛出内存不足的异常。如果会话要被转储到磁盘，那么必须要先被序列化，在大规模集群中，对对象进行序列化的代价是很昂贵的。因此，当会话不再需要时，应当及时调用 HttpSession 的 invalidate() 方法清除会话。 19、实现 RandomAccess 接口的集合比如 ArrayList，应当使用最普通的 for 循环而不是 foreach 循环来遍历 这是JDK推荐给用户的。JDK API对于 RandomAccess 接口的解释是：实现 RandomAccess 接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现 RandomAccess 接口的类实例，假如是随机访问的，使用普通 for 循环效率将高于使用 foreach 循环；反过来，如果是顺序访问的，则使用 Iterator 会效率更高。可以使用类似如下的代码作判断： 12345678910if (list instanceof RandomAccess)&#123; for (int i = 0; i &lt; list.size(); i++)&#123;&#125;&#125;else&#123; Iterator&lt;?&gt; iterator = list.iterable(); while (iterator.hasNext()) &#123; iterator.next(); &#125;&#125; foreach 循环的底层实现原理就是迭代器 Iterator，参见Java语法糖1：可变长度参数以及 foreach 循环原理。所以后半句”反过来，如果是顺序访问的，则使用 Iterator 会效率更高”的意思就是顺序访问的那些类实例，使用 foreach 循环去遍历。 20、使用同步代码块替代同步方法 这点在多线程模块中的 synchronized 锁方法块一文中已经讲得很清楚了，除非能确定一整个方法都是需要进行同步的，否则尽量使用同步代码块，避免对那些不需要进行同步的代码也进行了同步，影响了代码执行效率。 21、将常量声明为 static final，并以大写命名 这样在编译期间就可以把这些内容放入常量池中，避免运行期间计算生成常量的值。另外，将常量的名字以大写命名也可以方便区分出常量与变量 22、不要创建一些不使用的对象，不要导入一些不使用的类 这毫无意义，如果代码中出现”The value of the local variable i is not used”、”The import java.util is never used”，那么请删除这些无用的内容 23、程序运行过程中避免使用反射 关于，请参见反射。反射是Java提供给用户一个很强大的功能，功能强大往往意味着效率不高。不建议在程序运行过程中使用尤其是频繁使用反射机制，特别是Method的invoke方法，如果确实有必要，一种建议性的做法是将那些需要通过反射加载的类在项目启动的时候通过反射实例化出一个对象并放入内存—-用户只关心和对端交互的时候获取最快的响应速度，并不关心对端的项目启动花多久时间。 24、使用数据库连接池和线程池 这两个池都是用于重用对象的，前者可以避免频繁地打开和关闭连接，后者可以避免频繁地创建和销毁线程 25、使用带缓冲的输入输出流进行IO操作 带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率 26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList这个，理解ArrayList和LinkedList的原理就知道了 27、不要让public方法中有太多的形参 public方法即对外提供的方法，如果给这些方法太多形参的话主要有两点坏处： 1、违反了面向对象的编程思想，Java讲求一切都是对象，太多的形参，和面向对象的编程思想并不契合 2、参数太多势必导致方法调用的出错概率增加 至于这个”太多”指的是多少个，3、4个吧。比如我们用JDBC写一个insertStudentInfo方法，有10个学生信息字段要插如Student表中，可以把这10个参数封装在一个实体类中，作为insert方法的形参。 28、字符串变量和字符串常量equals的时候将字符串常量写在前面 这是一个比较常见的小技巧了，如果有以下代码： 12String str = \"123\";if (str.equals(\"123\")) &#123;...&#125; 建议修改为： 12345String str = \"123\";if (\"123\".equals(str))&#123;...&#125; 这么做主要是可以避免空指针异常 29、请知道，在java中if (i == 1)和if (1 == i)是没有区别的，但从阅读习惯上讲，建议使用前者 平时有人问，”if (i == 1)”和”if (1== i)”有没有区别，这就要从C/C++讲起。 在C/C++中，”if (i == 1)”判断条件成立，是以0与非0为基准的，0表示false，非0表示true，如果有这么一段代码： 123456int i = 2;if (i == 1)&#123;...&#125;else&#123;...&#125; C/C++判断”i==1″不成立，所以以0表示，即false。但是如果： 1234567int i = 2;if (i = 1)&#123; ... &#125;else&#123;... &#125; 万一程序员一个不小心，把”if (i == 1)”写成”if (i = 1)”，这样就有问题了。在if之内将i赋值为1，if判断里面的内容非0，返回的就是true了，但是明明i为2，比较的值是1，应该返回的false。这种情况在C/C++的开发中是很可能发生的并且会导致一些难以理解的错误产生，所以，为了避免开发者在if语句中不正确的赋值操作，建议将if语句写为： 123456int i = 2;if (1 == i) &#123;... &#125;else&#123;... &#125; 这样，即使开发者不小心写成了”1 = i”，C/C++编译器也可以第一时间检查出来，因为我们可以对一个变量赋值i为1，但是不能对一个常量赋值1为i。 但是，在Java中，C/C++这种”if (i = 1)”的语法是不可能出现的，因为一旦写了这种语法，Java就会编译报错”Type mismatch: cannot convert from int to boolean”。但是，尽管Java的”if (i == 1)”和”if (1 == i)”在语义上没有任何区别，但是从阅读习惯上讲，建议使用前者会更好些。 30、不要对数组使用toString()方法 看一下对数组使用toString()打印出来的是什么： 12345public static void main(String[] args)&#123; int[] is = new int[]&#123;1, 2, 3&#125;; System.out.println(is.toString());&#125; 结果是： 1[I@18a992f 本意是想打印出数组内容，却有可能因为数组引用is为空而导致空指针异常。不过虽然对数组toString()没有意义，但是对集合toString()是可以打印出集合里面的内容的，因为集合的父类AbstractCollections重写了Object的toString()方法。 31、不要对超出范围的基本数据类型做向下强制转型 这绝不会得到想要的结果： 12345public static void main(String[] args)&#123; long l = 12345678901234L;int i = (int)l; System.out.println(i);&#125; 我们可能期望得到其中的某几位，但是结果却是： 1942892530 解释一下。Java中long是8个字节64位的，所以12345678901234在计算机中的表示应该是： 0000 0000 0000 0000 0000 1011 0011 1010 0111 0011 1100 1110 0010 1111 1111 0010 一个int型数据是4个字节32位的，从低位取出上面这串二进制数据的前32位是： 0111 0011 1100 1110 0010 1111 1111 0010 这串二进制表示为十进制1942892530，所以就是我们上面的控制台上输出的内容。从这个例子上还能顺便得到两个结论： 1、整型默认的数据类型是int，long l = 12345678901234L，这个数字已经超出了int的范围了，所以最后有一个L，表示这是一个long型数。顺便，浮点型的默认类型是double，所以定义float的时候要写成””float f = 3.5f” 2、接下来再写一句”int ii = l + i;”会报错，因为long + int是一个long，不能赋值给int 32、公用的集合类中不使用的数据一定要及时remove掉 如果一个集合类是公用的（也就是说不是方法里面的属性），那么这个集合里面的元素是不会自动释放的，因为始终有引用指向它们。所以，如果公用集合里面的某些数据不使用而不去remove掉它们，那么将会造成这个公用集合不断增大，使得系统有内存泄露的隐患。 33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+””最慢 把一个基本数据类型转为一般有三种方式，我有一个Integer型数据i，可以使用i.toString()、String.valueOf(i)、i+””三种方式，三种方式的效率如何，看一个测试： 1234567891011121314151617181920212223public static void main(String[] args)&#123; int loopTime = 50000; Integer i = 0; long startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = String.valueOf(i); &#125;System.out.println(\"String.valueOf()：\" + (System.currentTimeMillis() - startTime) + \"ms\"); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = i.toString(); &#125;System.out.println(\"Integer.toString()：\" + (System.currentTimeMillis() - startTime) + \"ms\");startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++)&#123; String str = i + \"\";&#125;System.out.println(\"i + \\\"\\\"：\" + (System.currentTimeMillis() - startTime) + \"ms\");&#125; 运行结果为： 1String.valueOf()：11ms Integer.toString()：5ms i + &quot;&quot;：25ms 所以以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： 1、String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 2、Integer.toString()方法就不说了，直接调用了 3、i + “”底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 三者对比下来，明显是2最快、1次之、3最慢 34、使用最有效率的方式去遍历Map 遍历Map的方式有很多，通常场景下我们需要的是遍历Map中的Key和Value，那么推荐使用的、效率最高的方式是： 123456789101112public static void main(String[] args)&#123;HashMap&lt;String, String&gt; hm = new HashMap&lt;String, String&gt;();hm.put(\"111\", \"222\");Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = hm.entrySet();Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iter = entrySet.iterator(); while (iter.hasNext())&#123;Map.Entry&lt;String, String&gt; entry = iter.next();System.out.println(entry.getKey() + \"\\t\" + entry.getValue());&#125;&#125; 如果你只是想遍历一下这个Map的key值，那用”Set keySet = hm.keySet();”会比较合适一些 35、对资源的close()建议分开操作 意思是，比如我有这么一段代码： 12345try&#123;XXX.close();YYY.close();&#125;catch (Exception e)&#123;...&#125; 建议修改为： 123456789try&#123; XXX.close(); &#125;catch (Exception e) &#123; ... &#125;try&#123; YYY.close(); &#125;catch (Exception e) &#123; ... &#125; 虽然有些麻烦，却能避免资源泄露。我想，如果没有修改过的代码，万一XXX.close()抛异常了，那么就进入了cath块中了，YYY.close()不会执行，YYY这块资源就不会回收了，一直占用着，这样的代码一多，是可能引起资源句柄泄露的。而改为上面的写法之后，就保证了无论如何XXX和YYY都会被close掉。 以上就是 Java 开发编程注意细节的全部内容了，感谢大家的阅读！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"Spring MVC系列文章（五）：看透 Spring MVC 源代码分析与实践 ——  Spring MVC 组件分析","date":"2017-07-20T16:00:00.000Z","path":"2017/07/21/Spring-MVC03/","text":"SpringBoot 系列文章 由于星期一接到面试通知，和面试官约好了星期四面试，所以这几天没更新完这系列的文章，面完试后立马就把这个解决掉。通过这次面试，也让我懂得了很多，知道了自己的一些不足之处，后面还要继续下功夫好好的深入复习下去。这几篇文章写的我觉得还是不够仔细，感兴趣的还是建议自己去看看源码。 第 11 章 —— 组件概览HandlerMapping 根据 request 找到对应的处理器 Handler 和 Interceptors。内部只有一个方法 1HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; HandlerAdapter Handler 适配器，内部方法如下： 123boolean supports(Object handler);//判断是否可以使用某个 HandlerModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; //具体使用long getLastModified(HttpServletRequest request, Object handler);//获取资源上一次修改的时间 HandlerExceptionResolver 根据异常设置 ModelAndView ，再交给 render 方法进行渲染。 12ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex) ViewResolver 用来将 String 类型的视图名和 Locale 解析为 View 类型的视图。 1View resolveViewName(String viewName, Locale locale) throws Exception; 它的一个实现类 BeanNameViewResolver，它重写 resolveViewName 方法如下: 1234567891011121314151617181920212223public View resolveViewName(String viewName, Locale locale) throws BeansException &#123; ApplicationContext context = getApplicationContext(); //如果应用上下文没有找到视图，返回 null if (!context.containsBean(viewName)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"No matching bean found for view name '\" + viewName + \"'\"); &#125; // Allow for ViewResolver chaining... return null; &#125; //如果找到的视图类型不匹配，也返回 null if (!context.isTypeMatch(viewName, View.class)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Found matching bean for view name '\" + viewName + \"' - to be ignored since it does not implement View\"); &#125; // Since we're looking into the general ApplicationContext here, // let's accept this as a non-match and allow for chaining as well... return null; &#125; //根据视图名称从 Spring 容器中查找 Bean，返回找到的 bean return context.getBean(viewName, View.class); &#125; RequestToViewNameTranslator 获取 request 中的视图名。接口里面也是只有一个方法： 1String getViewName(HttpServletRequest request) throws Exception; //根据 request 查找视图名 LocaleResolver 用于从 request 解析出 Locale。 123456public interface LocaleResolver &#123; //从 request 解析出 Locale Locale resolveLocale(HttpServletRequest request); //根据 request 设置 locale void setLocale(HttpServletRequest request, HttpServletResponse response, @Nullable Locale locale);&#125; ThemeResolver 解析主题 123456public interface ThemeResolver &#123; //通过给定的 request 查找主题名 String resolveThemeName(HttpServletRequest request); //根据给定的 request 设置主题名 void setThemeName(HttpServletRequest request, HttpServletResponse response, String themeName);&#125; 在 RequestContext.java 文件中可以获取主题： 1234567891011121314151617public String getThemeMessage(String code, String defaultMessage) &#123; //获取主题的信息 return getTheme().getMessageSource().getMessage(code, null, defaultMessage, this.locale); &#125;public Theme getTheme() &#123; //判断主题是否为空 if (this.theme == null) &#123; // 通过 RequestContextUtils 获取 request 中的主题名 this.theme = RequestContextUtils.getTheme(this.request); if (this.theme == null) &#123; //如果还是为空的话 //那就是没有有效的主题解析器和主题 this.theme = getFallbackTheme(); &#125; &#125; return this.theme; &#125; RequestContextUtils.getTheme() 方法： 1234567891011public static Theme getTheme(HttpServletRequest request) &#123; ThemeResolver themeResolver = getThemeResolver(request); ThemeSource themeSource = getThemeSource(request); if (themeResolver != null &amp;&amp; themeSource != null) &#123; String themeName = themeResolver.resolveThemeName(request); return themeSource.getTheme(themeName); &#125; else &#123; return null; &#125; &#125; MultipartResolver 用于处理上传请求，处理方法：将普通的 request 包装成 MultipartHttpServletRequest 12345678public interface MultipartResolver &#123; //根据 request 判断是否是上传请求 boolean isMultipart(HttpServletRequest request); //将 request 包装成 MultipartHttpServletRequest MultipartHttpServletRequest resolveMultipart(HttpServletRequest request) throws MultipartException; //清理上传过程中产生的临时资源 void cleanupMultipart(MultipartHttpServletRequest request);&#125; FlashMapManager FlashMap 主要在 redirect 中传递参数，FlashMapManager 用来管理 FlashMap 的。 1234567public interface FlashMapManager &#123; //恢复参数，并将恢复过的和超时的参数从保存介质中删除 @Nullable FlashMap retrieveAndUpdate(HttpServletRequest request, HttpServletResponse response); //将参数保存起来 void saveOutputFlashMap(FlashMap flashMap, HttpServletRequest request, HttpServletResponse response);&#125; 小结介绍 Spring MVC 中九大组件的接口、作用、内部方法实现及作用进行了简单的介绍，详细的还需大家自己去看源码。 总结Spring MVC 原理总结本质是一个 Servlet，这个 Servlet 继承自 HttpServlet。Spring MVC 中提供了三个层次的 Servlet：HttpServletBean、FrameworkServlet 和 DispatcherServlet。他们相互继承， HttpServletBean 直接继承自 Java 的 HttpServlet。HttpServletBean 用于将 Servlet 中的 Servlet 中配置的参数设置到相应的属性中，FrameworkServlet 初始化了 Spring MVC 中所使用的 WebApplicationContext，具体处理请求的 9 大组件是在 DispatcherServlet 中初始化的，整个继承图如下： 最后文章可转发，但请注明原创地址，谢谢支持。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"Spring MVC系列文章（三）：看透 Spring MVC 源代码分析与实践 ——  网站基础知识","date":"2017-07-13T16:00:00.000Z","path":"2017/07/14/Spring-MVC01/","text":"网站架构及其演变过程基础结构网络传输分解方式： 标准的 OSI 参考模型 TCP/IP 参考模型 SpringBoot 系列文章 海量数据的解决方案 缓存和页面静态化 缓存 通过程序直接保存在内存中 使用缓存框架 （Encache、Redis、Memcache） 页面静态化 使用模板技术生成（Velocity、FreeMaker等） 数据库优化 表结构优化 SQL 语句优化 分区 分表 索引优化 使用存储过程代替直接操作过程 分离活跃数据 批量读取和延迟修改 读写分离 分布式数据库 NoSQL 和 Hadoop 高并发的解决方案 应用和静态资源的分离：静态文件（图片、视频、JS、CSS等）放在专门的服务器上 页面缓存（Nginx 服务器、Squid 服务器） 集群与分布式 反向代理 CDN 底层优化：网络传输协议 常见协议和标准TCP/IP 协议IP：查找地址，对应着国际互联网 TCP：规范传输规则，对应着传输层 TCP 在传输之前会进行三次沟通，称 “三次握手”，传完数据断开的时候要进行四次沟通，称 “四次挥手”。 TCP 两个序号，三个标志位含义： seq：表示所传数据的序号。TCP 传输时每一个字节都有一个序号，发送数据的时候会将数据的第一个序号发送给对方，接收方会按序号检查是否接收完整了，如果没接收完整就需要重新传送，这样就可以保证数据的完整性。 ack：表示确认号。接收端用它来给发送端反馈已经成功接收到的数据信息，它的值为希望接收的下一个数据包起始序号。 ACK：确认位，只有 ACK = 1 的时候 ack 才起作用。正常通信时 ACK 为 1，第一次发起请求时因为没有需要确认接收的数据所以 ACK 为 0。 SYN：同步位，用于在建立连接时同步序号。刚开始建立连接时并没有历史接收的数据，所以 ack 也就没有办法设置，这是按照正常的机制就无法运行了，SYN 的作用就是解决这个问题的，当接收端接收到 SYN = 1 的报文时就会直接将 ack 设置为接收到的 seq + 1 的值，注意这里的值并不是检验后设置的，而是根据 SYN 直接设置的，这样正常的机制就可以运行了，所以 SYN 叫同步位。SYN 会在前两次握手时都为 1，这是因为通信的双方的 ack 都需要设置一个初始值。 FIN：终止位，用来在数据传输完毕后释放连接。 DNS 的设置DNS 解析参考域名设置，如下是我在腾讯云域名的设置 记录类型： A记录： 将域名指向一个IPv4地址（例如：8.8.8.8）CNAME：将域名指向另一个域名（例如 www.54tianzhisheng.cn）MX： 将域名指向邮件服务器地址TXT： 可任意填写，长度限制255，通常做SPF记录（反垃圾邮件）NS： 域名服务器记录，将子域名指定其他DNS服务器解析AAAA：将域名指向一个iPv6地址（例如：ff06:0:0:0:0:0:0:c3）SRV：记录提供特定服务的服务器（例如_xmpp-server._tcp）显性URL：将域名301重定向到另一个地址隐性URL：类似显性URL，但是会隐藏真实目标地址 主机记录： 要解析 www.54tianzhisheng.cn，请填写 www。主机记录就是域名前缀，常见用法有： www: 解析后的域名为 www.54tianzhisheng.cn。@: 直接解析主域名 54tianzhisheng.cn。*: 泛解析，匹配其他所有域名 .54tianzhisheng.cn。mail: 将域名解析为 mail.54tianzhisheng.cn，通常用于解析邮箱服务器。二级域名: 如：abc.54tianzhisheng.cn，填写abc。*手机网站: 如：m.54tianzhisheng.cn，填写m。 Java 中 Socket 的用法普通 Soket 的用法Socket 分为 ServerSocket 和 Socket 两大类。 ServerSocket 用于服务器端，可以通过 accept 方法监听请求，监听到请求后返回 Socket； Socket 用户具体完成数据传输，客户端直接使用 Socket 发送请求并传输数据。 随便写了个单方面发送消息的 demo： 客户端： 12345678910111213141516171819202122232425import java.io.IOException;import java.io.OutputStream;import java.net.Socket;/** * Created by 10412 on 2017/5/2. * TCP客户端： ①：建立tcp的socket服务，最好明确具体的地址和端口。这个对象在创建时，就已经可以对指定ip和端口进行连接(三次握手)。 ②：如果连接成功，就意味着通道建立了，socket流就已经产生了。只要获取到socket流中的读取流和写入流即可，只要通过getInputStream和getOutputStream就可以获取两个流对象。 ③：关闭资源。 *///单方面的输入！public class TcpClient&#123; public static void main(String[] args) &#123; try &#123; Socket s = new Socket(\"127.0.0.1\", 9999); OutputStream o = s.getOutputStream(); o.write(\"tcp sssss\".getBytes()); s.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 服务器端： 1234567891011121314151617181920212223242526272829303132import java.io.IOException;import java.io.InputStream;import java.net.ServerSocket;import java.net.Socket;/** * Created by 10412 on 2017/5/2. */public class TcpServer&#123; public static void main(String[] args) &#123; try &#123; ServerSocket ss = new ServerSocket(9999);//建立服务端的socket服务 Socket s = ss.accept();//获取客户端对象 String ip = s.getInetAddress().getHostAddress(); int port = s.getPort(); System.out.println(ip + \" : \" + port + \" connected\"); // 可以通过获取到的socket对象中的socket流和具体的客户端进行通讯。 InputStream ins = s.getInputStream();//读取客户端的数据，使用客户端对象的socket读取流 byte[] bytes = new byte[1024]; int len = ins.read(bytes); String text = new String(bytes, 0, len); System.out.println(text); //关闭资源 s.close(); ss.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; NioSocket 的用法见以前的一篇文章：Java NIO 系列教程 书中第五章简单的讲了下实现 HTTP 协议。第六章主要讲 Servlet，写了 Servlet 接口和其实现类。第七章把 Tomcat 分析的很不错，如果有读者感兴趣的话，可以去看看。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"Spring MVC系列文章（四）：看透 Spring MVC 源代码分析与实践 ——  俯视 Spring MVC","date":"2017-07-13T16:00:00.000Z","path":"2017/07/14/Spring-MVC02/","text":"Spring MVC SpringBoot 系列文章 Spring MVC 之初体验环境搭建在 IDEA 中新建一个 web 项目，用 Maven 管理项目的话，在 pom.xml 中加入 Spring MVC 和 Servlet 依赖即可。 12345678910111213&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-webmvc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.9.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/javax.servlet/javax.servlet-api --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Spring MVC 简单配置 在 web.xml 中配置 Servlet 创建 Spring MVC 的 xml 配置文件 创建 Controller 和 View 1、web.xml 12345678910111213141516171819202122232425262728293031323334&lt;!-- Spring MVC配置 --&gt;&lt;!-- ====================================== --&gt;&lt;servlet&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 可以自定义servlet.xml配置文件的位置和名称，默认为WEB-INF目录下，名称为[&lt;servlet-name&gt;]-servlet.xml，如spring-servlet.xml &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring-servlet.xml&lt;/param-value&gt;&amp;nbsp; 默认 &lt;/init-param&gt; --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- Spring配置 --&gt;&lt;!-- ====================================== --&gt;&lt;listener&gt; &lt;listenerclass&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;!-- 指定Spring Bean的配置文件所在目录。默认配置在WEB-INF目录下 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:config/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt; 2、spring-servlet.xml 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/context &lt;a href=\"http://www.springframework.org/schema/context/spring-context-3.0.xsd\"&gt;http://www.springframework.org/schema/context/spring-context-3.0.xsd&lt;/a&gt;\"&gt; &lt;!-- 启用spring mvc 注解 --&gt; &lt;context:annotation-config /&gt; &lt;!-- 设置使用注解的类所在的jar包 --&gt; &lt;context:component-scan base-package=\"controller\"&gt;&lt;/context:component-scan&gt; &lt;!-- 完成请求和注解POJO的映射 --&gt; &lt;bean class=\"org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter\" /&gt; &lt;!-- 对转向页面的路径解析。prefix：前缀， suffix：后缀 --&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" p:prefix=\"/jsp/\" p:suffix=\".jsp\" /&gt;&lt;/beans&gt; 3、Controller 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package controller;import javax.servlet.http.HttpServletRequest;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import entity.User;@Controller //类似Struts的Actionpublic class TestController &#123; @RequestMapping(\"/test/login.do\") // 请求url地址映射，类似Struts的action-mapping public String testLogin(@RequestParam(value=\"username\")String username, String password, HttpServletRequest request) &#123; // @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) // @RequestParam可简写为：@RequestParam(\"username\") if (!\"admin\".equals(username) || !\"admin\".equals(password)) &#123; return \"loginError\"; // 跳转页面路径（默认为转发），该路径不需要包含spring-servlet配置文件中配置的前缀和后缀 &#125; return \"loginSuccess\"; &#125; @RequestMapping(\"/test/login2.do\") public ModelAndView testLogin2(String username, String password, int age)&#123; // request和response不必非要出现在方法中，如果用不上的话可以去掉 // 参数的名称是与页面控件的name相匹配，参数类型会自动被转换 if (!\"admin\".equals(username) || !\"admin\".equals(password) || age &lt; 5) &#123; return new ModelAndView(\"loginError\"); // 手动实例化ModelAndView完成跳转页面（转发），效果等同于上面的方法返回字符串 &#125; return new ModelAndView(new RedirectView(\"../index.jsp\")); // 采用重定向方式跳转页面 // 重定向还有一种简单写法 // return new ModelAndView(\"redirect:../index.jsp\"); &#125; @RequestMapping(\"/test/login3.do\") public ModelAndView testLogin3(User user) &#123; // 同样支持参数为表单对象，类似于Struts的ActionForm，User不需要任何配置，直接写即可 String username = user.getUsername(); String password = user.getPassword(); int age = user.getAge(); if (!\"admin\".equals(username) || !\"admin\".equals(password) || age &lt; 5) &#123; return new ModelAndView(\"loginError\"); &#125; return new ModelAndView(\"loginSuccess\"); &#125; @Resource(name = \"loginService\") // 获取applicationContext.xml中bean的id为loginService的，并注入 private LoginService loginService; //等价于spring传统注入方式写get和set方法，这样的好处是简洁工整，省去了不必要得代码 @RequestMapping(\"/test/login4.do\") public String testLogin4(User user) &#123; if (loginService.login(user) == false) &#123; return \"loginError\"; &#125; return \"loginSuccess\"; &#125;&#125; @RequestMapping 可以写在方法上，也可以写在类上，上面代码方法上的 RequestMapping 都含有 /test ， 那么我们就可以将其抽出直接写在类上，那么方法里面就不需要写 /test 了。 如下即可： 12345678910111213141516@Controller@RequestMapping(\"/test\")public class TestController &#123; @RequestMapping(\"/login.do\") // 请求url地址映射，类似Struts的action-mapping public String testLogin(@RequestParam(value=\"username\")String username, String password, HttpServletRequest request) &#123; // @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) // @RequestParam可简写为：@RequestParam(\"username\") if (!\"admin\".equals(username) || !\"admin\".equals(password)) &#123; return \"loginError\"; // 跳转页面路径（默认为转发），该路径不需要包含spring-servlet配置文件中配置的前缀和后缀 &#125; return \"loginSuccess\"; &#125; //省略其他的&#125; 上面的代码方法的参数中可以看到有一个 @RequestParam 注解，其实还有 @PathVariable 。这两个的区别是啥呢？ @PathVariable 标记在方法的参数上，利用它标记的参数可以利用请求路径传值。 @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) 看如下例子： 123456789101112@RequestMapping(\"/user/&#123;userId&#125;\") // 请求url地址映射public String userinfo(Model model, @PathVariable(\"userId\") int userId, HttpSession session) &#123; System.out.println(\"进入 userinfo 页面\"); //判断是否有用户登录 User user1 = (User) session.getAttribute(\"user\"); if (user1 == null) &#123; return \"login\"; &#125; User user = userService.selectUserById(userId); model.addAttribute(\"user\", user); return \"userinfo\"; &#125; 上面例子中如果浏览器请求的是 /user/1 的时候，就表示此时的用户 id 为 1，此时就会先从 session 中查找是否有 “user” 属性，如果有的话，就代表用户此时处于登录的状态，如果没有的话，就会让用户返回到登录页面，这种机制在各种网站经常会使用的，然后根据这个 id = 1 ，去查找用户的信息，然后把查找的 “user” 放在 model 中，然后返回用户详情页面，最后在页面中用 $!{user.name} 获取用户的名字，同样的方式可以获取用户的其他信息，把所有的用户详情信息展示出来。 创建 Spring MVC 之器Spring MVC 核心 Servlet 架构图如下： Java 中常用的 Servlet 我在另外一篇文章写的很清楚了，有兴趣的请看：通过源码详解 Servlet ，这里我就不再解释了。 这里主要讲 Spring 中的 HttpServletBean、FrameworkServlet、DispatcherServlet 这三个类的创建过程。 通过上面的图，可以看到这三个类直接实现三个接口：EnvironmentCapable、EnvironmentAware、ApplicationContextAware。下面我们直接看下这三个接口的内部是怎样写的。 EnvironmentCapable.java 12345public interface EnvironmentCapable &#123; //返回组件的环境，可能返回 null 或者默认环境 @Nullable Environment getEnvironment();&#125; EnvironmentAware.java 1234public interface EnvironmentAware extends Aware &#123; //设置组件的运行环境 void setEnvironment(Environment environment);&#125; ApplicationContextAware.java 12345public interface ApplicationContextAware extends Aware &#123; //设置运行对象的应用上下文 //当类实现这个接口后，这个类可以获取ApplicationContext中所有的bean，也就是说这个类可以直接获取Spring配置文件中所有有引用到的bean对象 void setApplicationContext(ApplicationContext applicationContext) throws BeansException;&#125; 怎么使用这个这个接口呢？ 参考文章：org.springframework.context.ApplicationContextAware使用理解 HttpServletBean 这里就直接看其中最重要的 init() 方法的代码了： 12345678910111213141516171819202122232425262728293031323334/** * 将配置参数映射到此servlet的bean属性，并调用子类初始化。 * 如果 bean 配置不合法（或者需要的参数丢失）或者子类初始化发生错误，那么就会抛出 ServletException 异常 */@Overridepublic final void init() throws ServletException &#123; //日志代码删除了 // 从init参数设置bean属性。 //获得web.xml中的contextConfigLocation配置属性，就是spring MVC的配置文件 PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); //获取服务器的各种信息 ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); //模板方法，可以在子类中调用，做一些初始化工作，bw代表DispatcherServelt initBeanWrapper(bw); //将配置的初始化值设置到DispatcherServlet中 bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; //日志代码 throw ex; &#125; &#125; // Let subclasses do whatever initialization they like. //模板方法，子类初始化的入口方法 initServletBean(); //日志代码删除了&#125; FrameworkServlet 其中重要方法如下：里面也就两句关键代码，日志代码我直接删掉了 12345678910111213141516171819202122protected final void initServletBean() throws ServletException &#123; //日志代码删除了 long startTime = System.currentTimeMillis(); //就是 try 语句里面有两句关键代码 try &#123; //初始化 webApplicationContext this.webApplicationContext = initWebApplicationContext(); //模板方法， initFrameworkServlet(); &#125; catch (ServletException ex) &#123; this.logger.error(\"Context initialization failed\", ex); throw ex; &#125; catch (RuntimeException ex) &#123; this.logger.error(\"Context initialization failed\", ex); throw ex; &#125; //日志代码删除了 &#125; 再来看看上面代码中调用的 initWebApplicationContext() 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344protected WebApplicationContext initWebApplicationContext() &#123; //获取 rootContext WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // 上下文实例在构造时注入 - &gt;使用它 wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; // 如果上下文尚未刷新 -&gt; 提供诸如设置父上下文，设置应用程序上下文ID等服务 if (cwac.getParent() == null) &#123; // 上下文实例被注入没有显式的父类 -&gt; 将根应用程序上下文（如果有的话可能为null）设置为父级 cwac.setParent(rootContext); &#125; configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; // 当 WebApplicationContext 已经存在 ServletContext 中时，通过配置在 servlet 中的 ContextAttribute 参数获取 wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // 如果 WebApplicationContext 还没有创建，则创建一个 wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; // 当 ContextRefreshedEvent 事件没有触发时调用此方法，模板方法，可以在子类重写 onRefresh(wac); &#125; if (this.publishContext) &#123; // 将 ApplicationContext 保存到 ServletContext 中去 String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); if (this.logger.isDebugEnabled()) &#123; this.logger.debug(\"Published WebApplicationContext of servlet '\" + getServletName() + \"' as ServletContext attribute with name [\" + attrName + \"]\"); &#125; &#125; return wac; &#125; initWebApplicationContext 方法做了三件事： 获取 Spring 的根容器 rootContext 设置 webApplicationContext 并根据情况调用 onRefresh 方法 将 webApplicationContext 设置到 ServletContext 中 这里在讲讲上面代码中的 wac == null 的几种情况： 1）、当 WebApplicationContext 已经存在 ServletContext 中时，通过配置在 servlet 中的 ContextAttribute 参数获取，调用的是 findWebApplicationContext() 方法 123456789101112protected WebApplicationContext findWebApplicationContext() &#123; String attrName = getContextAttribute(); if (attrName == null) &#123; return null; &#125; WebApplicationContext wac = WebApplicationContextUtils.getWebApplicationContext(getServletContext(), attrName); if (wac == null) &#123; throw new IllegalStateException(\"No WebApplicationContext found: initializer not registered?\"); &#125; return wac; &#125; 2)、如果 WebApplicationContext 还没有创建，调用的是 createWebApplicationContext 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected WebApplicationContext createWebApplicationContext(@Nullable ApplicationContext parent) &#123; //获取创建类型 Class&lt;?&gt; contextClass = getContextClass(); //删除了打印日志代码 //检查创建类型 if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException( \"Fatal initialization error in servlet with name '\" + getServletName() + \"': custom WebApplicationContext class [\" + contextClass.getName() + \"] is not of type ConfigurableWebApplicationContext\"); &#125; //具体创建 ConfigurableWebApplicationContext wac = (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); wac.setEnvironment(getEnvironment()); wac.setParent(parent); //并设置的 contextConfigLocation 参数传给 wac，默认是 WEB-INFO/[ServletName]-Servlet.xml wac.setConfigLocation(getContextConfigLocation()); //调用的是下面的方法 configureAndRefreshWebApplicationContext(wac); return wac; &#125;protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information if (this.contextId != null) &#123; wac.setId(this.contextId); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(getServletContext().getContextPath()) + '/' + getServletName()); &#125; &#125; wac.setServletContext(getServletContext()); wac.setServletConfig(getServletConfig()); wac.setNamespace(getNamespace()); wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); // The wac environment's #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(getServletContext(), getServletConfig()); &#125; postProcessWebApplicationContext(wac); applyInitializers(wac); wac.refresh(); &#125; 里面还有 doXXX() 方法，大家感兴趣的可以去看看。 DispatcherServlet DispatcherServlet 继承自 FrameworkServlet，onRefresh 方法是 DispatcherServlet 的入口方法，在 initStrategies 方法中调用了 9 个初始化的方法。 这里分析其中一个初始化方法：initLocaleResolver() 方法 123456789101112private void initLocaleResolver(ApplicationContext context) &#123; try &#123; //在 context 中获取 this.localeResolver = context.getBean(LOCALE_RESOLVER_BEAN_NAME, LocaleResolver.class); //删除了打印日志的代码 &#125; catch (NoSuchBeanDefinitionException ex) &#123; //使用默认的策略 this.localeResolver = getDefaultStrategy(context, LocaleResolver.class); //删除了打印日志的代码 &#125; &#125; 查看默认策略代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647protected &lt;T&gt; T getDefaultStrategy(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; //调用 getDefaultStrategies 方法 List&lt;T&gt; strategies = getDefaultStrategies(context, strategyInterface); if (strategies.size() != 1) &#123; throw new BeanInitializationException( \"DispatcherServlet needs exactly 1 strategy for interface [\" + strategyInterface.getName() + \"]\"); &#125; return strategies.get(0); &#125; /** * Create a List of default strategy objects for the given strategy interface. * &lt;p&gt;The default implementation uses the \"DispatcherServlet.properties\" file (in the same * package as the DispatcherServlet class) to determine the class names. It instantiates * the strategy objects through the context's BeanFactory. */ @SuppressWarnings(\"unchecked\") protected &lt;T&gt; List&lt;T&gt; getDefaultStrategies(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; String key = strategyInterface.getName(); //根据策略接口的名字从 defaultStrategies 获取所需策略的类型 String value = defaultStrategies.getProperty(key); if (value != null) &#123; //如果有多个默认值的话，就以逗号分隔为数组 String[] classNames = StringUtils.commaDelimitedListToStringArray(value); List&lt;T&gt; strategies = new ArrayList&lt;&gt;(classNames.length); //按获取到的类型初始化策略 for (String className : classNames) &#123; try &#123; Class&lt;?&gt; clazz = ClassUtils.forName(className, DispatcherServlet.class.getClassLoader()); Object strategy = createDefaultStrategy(context, clazz); strategies.add((T) strategy); &#125; catch (ClassNotFoundException ex) &#123; throw new BeanInitializationException( \"Could not find DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]\", ex); &#125; catch (LinkageError err) &#123; throw new BeanInitializationException( \"Error loading DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]: problem with class file or dependent class\", err); &#125; &#125; return strategies; &#125; else &#123; return new LinkedList&lt;&gt;(); &#125; &#125; 其他几个方法大概也类似，我就不再写了。 小结主要讲了 Spring MVC 自身创建过程，分析了 Spring MVC 中 Servlet 的三个层次：HttpServletBean、FrameworkServlet 和 DispatcherServlet。HttpServletBean 继承自 Java 的 HttpServlet，其作用是将配置的参数设置到相应的属性上；FrameworkServlet 初始化了 WebApplicationContext；DispatcherServlet 初始化了自身的 9 个组件。 Spring MVC 之用分析 Spring MVC 是怎么处理请求的。首先分析 HttpServletBean、FrameworkServlet 和 DispatcherServlet 这三个 Servlet 的处理过程，最后分析 doDispatcher 的结构。 HttpServletBean 参与了创建工作，并没有涉及请求的处理。 FrameworkServlet 在类中的 service() 、doGet()、doPost()、doPut()、doDelete()、doOptions()、doTrace() 这些方法中可以看到都调用了一个共同的方法 processRequest() ，它是类在处理请求中最核心的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; long startTime = System.currentTimeMillis(); Throwable failureCause = null; //获取 LocaleContextHolder 中原来保存的 LocaleContext LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); //获取当前请求的 LocaleContext LocaleContext localeContext = buildLocaleContext(request); //获取 RequestContextHolder 中原来保存的 RequestAttributes RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); //获取当前请求的 ServletRequestAttributes ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor());//将当前请求的 LocaleContext 和 ServletRequestAttributes 设置到 LocaleContextHolder 和 RequestContextHolder initContextHolders(request, localeContext, requestAttributes); try &#123; //实际处理请求的入口，这是一个模板方法，在 Dispatcher 类中才有具体实现 doService(request, response); &#125;catch (ServletException ex) &#123; failureCause = ex; throw ex; &#125;catch (IOException ex) &#123; failureCause = ex; throw ex; &#125;catch (Throwable ex) &#123; failureCause = ex; throw new NestedServletException(\"Request processing failed\", ex); &#125;finally &#123; //将 previousLocaleContext，previousAttributes 恢复到 LocaleContextHolder 和 RequestContextHolder 中 resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) &#123; requestAttributes.requestCompleted(); &#125; //删除了日志打印代码 //发布了一个 ServletRequestHandledEvent 类型的消息 publishRequestHandledEvent(request, response, startTime, failureCause); &#125; &#125; DispatcherServlet 上一章中其实还没把该类讲清楚，在这个类中，里面的智行处理的入口方法应该是 doService 方法，方法里面调用了 doDispatch 进行具体的处理，在调用 doDispatch 方法之前 doService 做了一些事情：首先判断是不是 include 请求，如果是则对 request 的 Attribute 做个快照备份，等 doDispatcher 处理完之后（如果不是异步调用且未完成）进行还原 ，在做完快照后又对 request 设置了一些属性。 123456789101112131415161718192021222324252627282930313233343536373839protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX))&#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try &#123; //调用 doDispatch 方法 doDispatch(request, response); &#125;finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125; &#125; doDispatch() 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; //检查是不是上传请求 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. 根据 request 找到 Handler mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request.根据 Handler 找到对应的 HandlerAdapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. //处理 GET 、 HEAD 请求的 LastModified String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug(\"Last-Modified value for [\" + getRequestUri(request) + \"] is: \" + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; //执行相应的 Interceptor 的 preHandle if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. HandlerAdapter 使用 Handler 处理请求 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); //如果需要异步处理，直接返回 if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; //当 view 为空时，根据 request 设置默认 view applyDefaultViewName(processedRequest, mv); //执行相应 Interceptor 的 postHandler mappedHandler.applyPostHandle(processedRequest, response, mv); &#125;catch (Exception ex) &#123; dispatchException = ex; &#125;catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); &#125; //调用 processDispatchResult 方法处理上面处理之后的结果（包括处理异常，渲染页面，发出完成通知触发 Interceptor 的 afterCompletion） processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125;catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125;catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); &#125;finally &#123; //判断是否执行异步请求 if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125;else &#123; // Clean up any resources used by a multipart request. 删除上传请求的资源 if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125; &#125; Handler，HandlerMapping，HandlerAdapter 三个区别： Handler：处理器，对应 MVC 的 C层，也就是 Controller 层，具体表现形式有很多种，可以是类，方法，它的类型是 Object，只要可以处理实际请求就可以是 Handler。 HandlerMapping：用来查找 Handler 的。 HandlerAdapter ：Handler 适配器， 另外 View 和 ViewResolver 的原理与 Handler 和 HandlerMapping 的原理类似。 小结本章分析了 Spring MVC 的请求处理的过程。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"通过源码详解 Servlet","date":"2017-07-08T16:00:00.000Z","path":"2017/07/09/servlet/","text":"Servlet 结构 1、ServletServlet 该接口定义了5个方法。 init()，初始化 servlet 对象，完成一些初始化工作。它是由 servlet 容器控制的，该方法只能被调用一次 service()，接受客户端请求对象，执行业务操作，利用响应对象响应客户端请求。 destroy()，当容器监测到一个servlet从服务中被移除时，容器调用该方法，释放资源，该方法只能被调用一次。 getServletConfig()，ServletConfig 是容器向 servlet 传递参数的载体。 getServletInfo()，获取 servlet 相关信息。 Servlet 的生命周期： 1，初始化阶段 调用 init() 方法 2，响应客户请求阶段 调用 service() 方法 3，终止阶段 调用 destroy() 方法 在 Servlet 接口中的五个方法中涉及的接口有三个：ServletConfig 、 ServletRequest、 ServletResponse 这里先讲讲 ServletRequest 和 ServletResponse。 1）ServletRequest 由 Servlet 容器来管理，当客户请求到来时，容器创建一个 ServletRequest 对象，封装请求数据，同时创建一个 ServletResponse 对象，封装响应数据。这两个对象将被容器作为 service（）方法的参数传递给 Servlet，Serlvet 利用 ServletRequest 对象获取客户端发来的请求数据，利用 ServletResponse 对象发送响应数据。 下面是 ServletRequest 中所有的方法，根据方法名大概就可以猜到这些方法到底是干啥用的。 2）ServletResponse 发送响应数据 2、ServletConfigServletConfig 是容器向 servlet 传递参数的载体。 ServletConfig的4个常用方法： 1）public String getInitParameter（String name）：返回指定名称的初始化参数值； 2）public Enumeration getInitParameterNames（）：返回一个包含所有初始化参数名的 Enumeration 对象； 3）public String getServletName()：返回在 DD 文件中&lt;servlet-name&gt;元素指定的 Servlet 名称； 4）public ServletContext getServletContext（）：返回该 Servlet 所在的上下文对象； 这里详细讲下 ServletContext ： Servlet 上下文对象（ServletContext）：每个Web应用程序在被启动时都会创建一个唯一的上下文对象，Servlet 可通过其获得 Web 应用程序的初始化参数或 Servlet 容器的版本等信息，也可被 Servlet 用来与其他 Servlet 共享数据。 1、获得 ServletContext 应用： （1）、直接调用 getServletContext（）方法 ServletContext context = getServletContext（）; （2）、使用 ServletConfig 应用，再调用它的 getServletContext（）方法 ServletContext context = getServletConfig.getServletContext(); 2、获得应用程序的初始化参数： （1）、public String getInitParameter（String name）：返回指定参数名的字符串参数值，没有则返回 null； （2）、public Enumeration getInitParameterNames()：返回一个包含多有初始化参数名的 Enumeration 对象； 3、通过 ServletContext 对象获得资源 （1）、public URl getResource（String path）:返回由给定路径的资源的 URL 对象，以 “/” 开头，为相对路径，相对于Web 应用程序的文档根目录； （2）、public InputStream getResourceAsStream（String path）：从资源上获得一个 InputStream 对象，等价于getResource（path）.oprenStream(); （3）、public String getRealPath(String path)：返回给定的虚拟路径的真实路径； 4、登陆日志：使用 log（）方法可以将指定的消息写到服务器的日志文件中 （1）、public void log（String msg）：参数 msg 为写入日志文件消息 （2）、public void log（String msg，Throwable throwable）：将 msg 指定的消息和异常的栈跟踪信息写入日志文件 5、使用 RequestDispatcher 实现请求转发 （1）、RequestDispatcher getRequestDiapatcher(String path)：必须以 “/“ 开头相对于应用程序根目录，而ServletRequest 可以传递一个相对路径 （2）、RequestDipatcher getNamedDiapatcher（String name）：参数 name 为一个命名的 Servlet 对象 6、使用 ServletContext 对象存储数据 （1）、public void serAttribute（String name，Object object）：将给定名称的属性值对象绑定到上下文对象上； （2）、public Object getAttribute（String name）：返回绑定到上下文对象的给定名称的属性值； （3）、public Enumeration getAttributeNames()：返回绑定到上下文对象上的所有属性名的 Enumeration 对象； （4）、public void removeAttribute（String name）：删除绑定到上下文对象指定名称的属性； ServletRequest 共享的对象仅在请求的生存周期中可以被访问； HttpSession 共享的对象仅在会话的生存周期中可以被访问； ServletContext 共享的对象在整个 Web 应用程序启动的生存周期中可以被访问； 7、检索 Servlet 容器的信息 （1）、public String getServletInfo()：返回 Servlet 所运行容器的名称和版本； （2）、public int getMajorVersion（）：返回容器所支持的 Servlet API 的主版本号； （3）、public int getMinorVersion（）：返回容器所支持的 Servlet API 的次版本号； （4）、public String getServletContext（）：返回 ServletContext 对应的 web 应用程序名称 &lt;display-name&gt;元素定义的名称； 3、GenericServlet 抽象类GenericServlet 定义了一个通用的，不依赖具体协议的 Servlet，它实现了 Servlet 接口和 ServletConfig 接口，它的方法在文章的第一张图就给出了。 4、HttpServlet 抽象类4.1、HTTP 请求方式 GET : 获取由请求 URL 标识的资源 POST : 向 Web 服务器发送无限制长度的数据 PUT : 存储一个资源到请求的 URL DELETE : 删除由 URL 标识的资源 HEAD : 返回 URL 标识的头信息 OPTIONS : 返回服务器支持的 HTTP 方法 TRACE : 返回 TRACE 请求附带的头字段 4.2、对应的服务方法： doGet() : 调用服务器的资源, 并将其作为响应返回给客户端. doGet() 调用在 URL 里显示正在传送给 Servlet 的数据,这在系统的安全方面可能带来一些问题, 比如说, 用户登录时, 表单里的用户名和密码需要发送到服务器端, doGet() 调用会在浏览器的 URL 里显示用户名和密码. doPost() : 它用于把客户端的数据传给服务端, 使用它可以以隐藏方式给服务器端发送数据. Post 适合发送大量数据. doPut() : 调用和 doPost() 相似, 并且它允许客户端把真正的文件存放在服务器上, 而不仅仅是传送数据. doDelete() : 它允许客户端删除服务器端的文件或者 Web 页面．它的使用非常少． doHead() : 它用于处理客户端的 Head 调用,并且返回一个 response. 当客户端只需要响应的 Header 时,它就发出一个Header 请求.这种情况下客户端往往关心响应的长度和响应的 MIME 类型. doOptions(): 它用于处理客户端的 Options 调用,通过这个调用, 客户端可以获得此 Servlet 支持的方法.如果 Servlet 覆盖了 doPost() 方法, 那么将返回: Allow: POST, TRACE, OPTIONS, HEAD doTrace：处理 TRACE 请求 4.3、Servlet Service 方法详解12345678910111213141516public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; HttpServletRequest request; HttpServletResponse response; // 如果传入的 HTTP 请求和 HTTP 响应不是 HTTP 的领域模型，则抛出 Servlet 异常，这个异常会被 Servlet 容器所处理 if (!(req instanceof HttpServletRequest &amp;&amp; res instanceof HttpServletResponse)) &#123; throw new ServletException(\"non-HTTP request or response\"); &#125; // 既然是 HTTP 协议绑定的 Serlvet, 强制转换到 HTTP 的领域模型 request = (HttpServletRequest) req; response = (HttpServletResponse) res; // 如果传入的请求和响应是预期的 HTTP 请求和 HTTP 响应，则调用 HttpServlet 的 service() 方法。 service(request, response); &#125; 4.4、HttpServlet service 方法详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 从 HTTP 请求中取得这次请求所使用的 HTTT 方法 String method = req.getMethod(); // 如果这次请求使用 GET 方法 if (method.equals(METHOD_GET)) &#123; // 取得这个 Servlet 的最后修改的时间 long lastModified = getLastModified(req); if (lastModified == -1) &#123; // servlet doesn't support if-modified-since, no reason // to go through further expensive logic //-1 代表这个 Servlet 不支持最后修改操作，直接调用 doGet() 进行处理 HTTP GET 请求 doGet(req, resp); &#125; else &#123; // 如果这个 Servlet 支持最后修改操作，取得请求头中包含的请求的最后修改时间 long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); if (ifModifiedSince &lt; lastModified) &#123; // If the servlet mod time is later, call doGet() // Round down to the nearest second for a proper compare // A ifModifiedSince of -1 will always be less // 如果请求头中包含的修改时间早于这个 Servlet 的最后修改时间，说明这个 Servlet 自从客户上一次 HTTP 请求已经被修改了 , 设置最新修改时间到响应头中 maybeSetLastModified(resp, lastModified); // 调用 doGet 进行进行处理 HTTP GET 请求 doGet(req, resp); &#125; else &#123; // 如果请求头中包含修改时间晚于这个 Servlet 的最后修改时间，说明这个 Servlet 自从请求的最后修改时间后没有更改过，这种情况下，仅仅返回一个 HTTP 响应状态 SC_NOT_MODIFIED resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); &#125; &#125; &#125; else if (method.equals(METHOD_HEAD)) &#123; // 如果这次请求使用 HEAD 方法 // 如果这个 Servlet 支持最后修改操作，则设置这个 Servlet 的最后修改时间到响应头中 long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); // 和对 HTTP GET 方法处理不同的是，无论请求头中的修改时间是不是早于这个 Sevlet 的最后修改时间，都会发 HEAD 响应给客户，因为 HTTP HEAD 响应是用来查询 Servlet 头信息的操作 doHead(req, resp); &#125; else if (method.equals(METHOD_POST)) &#123; // 如果这次请求使用 POST 方法 doPost(req, resp); &#125; else if (method.equals(METHOD_PUT)) &#123; // 如果这次请求使用 PUT 方法 doPut(req, resp); &#125; else if (method.equals(METHOD_DELETE)) &#123; // 如果这次请求使用 DELETE 方法 doDelete(req, resp); &#125; else if (method.equals(METHOD_OPTIONS)) &#123; // 如果这次请求使用 OPTIONS 方法 doOptions(req,resp); &#125; else if (method.equals(METHOD_TRACE)) &#123; // 如果这次请求使用 TRACE 方法 doTrace(req,resp); &#125; else &#123; // Note that this means NO servlet supports whatever // method was requested, anywhere on this server. // 如果这次请求是其他未知方法，返回错误代码 SC_NOT_IMPLEMENTED 给 HTTP 响应，并且显示一个错误消息，说明这个操作是没有实现的 String errMsg = lStrings.getString(\"http.method_not_implemented\"); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); &#125; &#125; 5、Servlet 的多线程问题1、当涉及到 Servlet 需要共享资源是，需保证 Servlet 是线程安全的 2、注意事项： （1）、用方法的局部变量保持请求中的专有数据； （2）、只用 Servlet 的成员变量来存放那些不会改变的数据； （3）、对可能被请求修改的成员变量同步（用 Synchronized 关键字修饰）； （4）、如果 Servlet 访问外部资源，那么需要同步访问这些资源； 3、实现 SingleThreadModel 接口的 Servlet 在被多个客户请求时一个时刻只能有一个线程运行，不推荐使用。 4、如果必须在 servlet 使用同步代码，应尽量在最小的范围上（代码块）进行同步，同步代码越少，Servlet 执行才能越好，避免对 doGet() 或 doPost() 方法同步。 总结全文首先通过一张 Servlet 中的核心 Servlet 类图关系，了解了几种 Servlet 之间的关系及其内部方法。然后在分别介绍这几种 Servlet，通过分析部分重要方法的源码来了解，还介绍了 Servlet 中多线程的问题的解决方法。 注：文章原创，首发于：zhisheng 的博客，文章可转载但请注明地址为：http://www.54tianzhisheng.cn/2017/07/09/servlet/","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Servlet","slug":"Servlet","permalink":"http://www.54tianzhisheng.cn/tags/Servlet/"}]},{"title":"Velocity 循环指令一种好的解决方法","date":"2017-06-27T16:00:00.000Z","path":"2017/06/28/Velocity-foreach/","text":"前提前台的数据经常是由需要通过 foreach 循环获取。 好的解决方案：（拿我最近做的一个项目做例子）购物商城左边的导航栏，商品大分类和小分类（Category） 1、在 model 包下创建一个 ViewObject 类 1234567891011public class ViewObject&#123; private Map&lt;String, Object&gt; objs = new HashMap&lt;&gt;(); public void set(String key, Object value) &#123; objs.put(key, value); &#125; public Object get(String key) &#123; return objs.get(key); &#125;&#125; 2、在 controller 包下创建个 BaseController 类 12345678910111213141516171819202122232425262728293031323334/** * 在每个页面显示图书大分类，抽离出来 * @return */ public List&lt;ViewObject&gt; selectAllCategory() &#123; List&lt;Category&gt; categories = categoryService.selectAllCategory(); List&lt;ViewObject&gt; vos = new ArrayList&lt;&gt;(); for (Category category : categories) &#123; ViewObject vo = new ViewObject(); vo.set(\"category\", category); vo.set(\"id\", category.getId()); //System.out.println(\"category 中的 id 是 \"+category.getId()); vos.add(vo); &#125; return vos; &#125; /** * 获取图书的小分类，在这里将小分类中的大分类id查找出来，保存在 cds.id 中， * 然后在模板引擎中通过将 vos.id 和 cds.id 相比较。然后如果相同的话，就取出来放在对应的大分类下 * @return */ public List&lt;ViewObject&gt; selectAllCategoryDetail() &#123; List&lt;CategoryDetail&gt; categoryDetails = categoryDetailService.selectAllCategoryDetail(); List&lt;ViewObject&gt; cds = new ArrayList&lt;&gt;(); for (CategoryDetail categoryDetail : categoryDetails) &#123; ViewObject vo = new ViewObject(); vo.set(\"categoryDetail\", categoryDetail); //System.out.println(\"categoryDetail 中的 categoryDetail id =\" + categoryDetail.getId() + \"category id = \" + categoryDetail.getCategory_id() + \" name = \" + categoryDetail.getName()); vo.set(\"id\", categoryDetail.getCategory_id()); cds.add(vo); &#125; return cds; &#125; 3、在 IndexController 类下，需要继承 BaseController.java 类 12345678910111213/** * 返回首页 * @param model * @return */ @RequestMapping(path = &#123;\"/\", \"/index\"&#125;) public String index(Model model) &#123; //模板引擎设置图书分类左边导航栏 model.addAttribute(\"vos\", selectAllCategory()); model.addAttribute(\"cds\", selectAllCategoryDetail()); //返回主页 return \"index\"; &#125; 4、抽离导航部分的代码 left.html 1234567891011121314151617&lt;!--左边图书分类导航栏--&gt;&lt;div class=\"c3_b1_left\"&gt; &lt;dl&gt; #foreach($vo in $vos) &lt;dd&gt; &lt;h1&gt;$!&#123;vo.category.name&#125;&lt;/h1&gt; &lt;p&gt; #foreach($cd in $cds) #if($vo.id == $cd.id) &lt;a href=\"/list\"&gt;$!&#123;cd.categoryDetail.name&#125;&lt;/a&gt; #end #end &lt;/p&gt; &lt;/dd&gt; #end &lt;/dl&gt;&lt;/div&gt; 5、首页中相应的位置引入 left.html 1#parse(\"left.html\") 这样就可以解决问题了，可是有时候我们需要控制循环的个数，因为我们网页端可能只需要特定的数据量 那么就需要中断 foreach，可以使用 #break 指令终止循环 123456#foreach( $vo in $vos ) #if( $foreach.count &gt; 5 ) #break #end $!&#123;vo.customer.Name&#125;#end 参考Velocity入门指南——第七章 循环指令","tags":[{"name":"Velocity","slug":"Velocity","permalink":"http://www.54tianzhisheng.cn/tags/Velocity/"}]},{"title":"AJAX 学习","date":"2017-06-22T16:00:00.000Z","path":"2017/06/23/AJAX/","text":"背景最近的项目中大量地方需要使用 AJAX，无奈，谁叫我既要写前台又要写后台呢，只好学习下这个技术点，主要参考 W3school 文档，下面记录下这些知识点，便于日后自己查阅，下面的一些测试代码建议在 W3school 中测试。 AJAX 基础： AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。 AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 在很多网站可以见到使用这种技术。 AJAX - XMLHttpRequest 创建 XMLHttpRequest 对象 XMLHttpRequest 是 AJAX 的基础。XMLHttpRequest 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 创建 XMLHttpRequest 对象的语法： 1variable = new XMLHttpRequest(); 但是对于老版本的 Internet Explorer （IE5 和 IE6）却是使用 ActiveX 对象，所以在开发中为了适应大多数的浏览器，常使用如下： 123456789var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp = new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp = new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125; 向服务器发送请求 使用 XMLHttpRequest 对象的 open() 和 send() 方法： 12xmlhttp.open(\"GET\",\"test1.txt\",true);xmlhttp.send(); method description open(method, url, async) 规定请求的类型、URL 以及是否异步处理请求。method：请求的类型；GET 或 POST url：文件在服务器上的位置 async：true（异步）或 false（同步） send(string) 将请求发送到服务器。string：仅用于 POST 请求 GET 还是 POST？ 与 POST 相比，GET 更简单也更快，并且在大部分情况下都能用。 然而，在以下情况中，请使用 POST 请求： 无法使用缓存文件（更新服务器上的文件或数据库） 向服务器发送大量数据（POST 没有数据量限制） 发送包含未知字符的用户输入时，POST 比 GET 更稳定也更可靠 示例：GET 请求 1、简单的 GET 请求 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/demo_get.asp?t=\" + Math.random(),true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2、通过 GET 方法发送信息 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/demo_get2.asp?fname=Bill&amp;lname=Gates\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 示例：POST 请求 1、简单 POST 请求 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"POST\",\"/ajax/demo_post.asp\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2、像 HTML 表单那样 POST 数据，请使用 setRequestHeader() 来添加 HTTP 头。然后在 send() 方法中规定您希望发送的数据 123456789101112131415161718192021222324252627282930313233343536&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"POST\",\"/ajax/demo_post2.asp\",true);xmlhttp.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\");xmlhttp.send(\"fname=Bill&amp;lname=Gates\");&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 注意：setRequestHeader(header, value) 向请求添加 HTTP 头，header: 规定头的名称, value: 规定头的值。 url - 服务器上的文件 open() 方法的 url 参数是服务器上文件的地址： 1xmlhttp.open(\"GET\",\"ajax_test.asp\",true); 该文件可以是任何类型的文件，比如 .txt 和 .xml，或者服务器脚本文件，比如 .asp 和 .php （在传回响应之前，能够在服务器上执行任务）。 异步 - True or False ？ XMLHttpRequest 对象如果要用于 AJAX 的话，其 open() 方法的 async 参数必须设置为 true，对于 web 开发人员来说，发送异步请求是一个巨大的进步。很多在服务器执行的任务都相当费时。AJAX 出现之前，这可能会引起应用程序挂起或停止。 通过 AJAX，JavaScript 无需等待服务器的响应，而是： 在等待服务器响应时执行其他脚本 当响应就绪后对响应进行处理 Async = true 当使用 async = true 时，请规定在响应处于 onreadystatechange 事件中的就绪状态时执行的函数 123456789101112131415161718192021222324252627282930313233&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/test1.txt\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"myDiv\"&gt;&lt;h2&gt;Let AJAX change this text&lt;/h2&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;通过 AJAX 改变内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; Async = false 如需使用 async = false，请将 open() 方法中的第三个参数改为 false 不推荐使用 async = false，但是对于一些小型的请求，也是可以的。 请记住，JavaScript 会等到服务器响应就绪才继续执行。如果服务器繁忙或缓慢，应用程序会挂起或停止。 注释：当您使用 async=false 时，请不要编写 onreadystatechange 函数 - 把代码放到 send() 语句后面即可： 123456789101112131415161718192021222324252627&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.open(\"GET\",\"/ajax/test1.txt\",false);xmlhttp.send();document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText;&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"myDiv\"&gt;&lt;h2&gt;Let AJAX change this text&lt;/h2&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;通过 AJAX 改变内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; 服务器响应 使用 XMLHttpRequest 对象的 responseText 或 responseXML 属性。 responseText 获得字符串形式的响应数据。 responseXML 获得 XML 形式的响应数据。 1、responseText 属性 如果来自服务器的响应并非 XML，请使用 responseText 属性。 responseText 属性返回字符串形式的响应，因此您可以这样使用： 1document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; 2、responseXML 属性 如果来自服务器的响应是 XML，而且需要作为 XML 对象进行解析，请使用 responseXML 属性： 请求 books.xml 文件，并解析响应： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;var txt,x,i;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; xmlDoc=xmlhttp.responseXML; txt=\"\"; x=xmlDoc.getElementsByTagName(\"title\"); for (i=0;i&lt;x.length;i++) &#123; txt=txt + x[i].childNodes[0].nodeValue + \"&lt;br /&gt;\"; &#125; document.getElementById(\"myDiv\").innerHTML=txt; &#125; &#125;xmlhttp.open(\"GET\",\"/example/xmle/books.xml\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;My Book Collection:&lt;/h2&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;获得我的图书收藏列表&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; onreadystatechange 事件 当请求被发送到服务器时，我们需要执行一些基于响应的任务。每当 readyState 改变时，就会触发 onreadystatechange 事件。readyState 属性存有 XMLHttpRequest 的状态信息。 下面是 XMLHttpRequest 对象的三个重要的属性： onreadystatechange 存储函数（或函数名），每当 readyState 属性改变时，就会调用该函数 readyState 存有 XMLHttpRequest 的状态。从 0 到 4 发生变化。 0: 请求未初始化 1: 服务器连接已建立 2: 请求已接收 3: 请求处理中 4: 请求已完成，且响应已就绪 status 200: “OK” 404: 未找到页面 在 onreadystatechange 事件中，我们规定当服务器响应已做好被处理的准备时所执行的任务。 当 readyState 等于 4 且状态为 200 时，表示响应已就绪： 1234567xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125; 使用 Callback 函数 callback 函数是一种以参数形式传递给另一个函数的函数。 如果您的网站上存在多个 AJAX 任务，那么您应该为创建 XMLHttpRequest 对象编写一个标准 的函数，并为每个 AJAX 任务调用该函数。 该函数调用应该包含 URL 以及发生 onreadystatechange 事件时执行的任务（每次调用可能不尽相同）： 12345678910function myFunction()&#123;loadXMLDoc(\"ajax_info.txt\",function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;);&#125; AJAX - 高级ASP/PHP 请求实例 - AJAX 用于创造动态性更强的应用程序。 AJAX 可用来与数据库进行动态通信。 AJAX 可用来与 XML 文件进行交互式通信。 AJAX 实例使用 XMLHttpRequest 对象的实例","tags":[{"name":"AJAX","slug":"AJAX","permalink":"http://www.54tianzhisheng.cn/tags/AJAX/"}]},{"title":"Java IO流学习超详细总结（图文并茂）","date":"2017-06-22T16:00:00.000Z","path":"2017/06/23/java-io/","text":"Java流操作有关的类或接口：Java流类图结构： 流的概念和作用流是一组有顺序的，有起点和终点的字节集合，是对数据传输的总称或抽象。即数据在两设备间的传输称为流，流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。 IO流的分类 根据处理数据类型的不同分为：字符流和字节流 根据数据流向不同分为：输入流和输出流 字符流和字节流字符流的由来： 因为数据编码的不同，而有了对字符进行高效操作的流对象。本质其实就是基于字节流读取时，去查了指定的码表。 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。 输入流和输出流对输入流只能进行读操作，对输出流只能进行写操作，程序中需要根据待传输数据的不同特性而使用不同的流。 Java IO流对象1.输入字节流InputStreamIO 中输入字节流的继承图可见上图，可以看出： InputStream 是所有的输入字节流的父类，它是一个抽象类。 ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。PipedInputStream 是从与其它线程共用的管道中读取数据，与Piped 相关的知识后续单独介绍。 ObjectInputStream 和所有FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 2.输出字节流OutputStreamIO 中输出字节流的继承图可见上图，可以看出： OutputStream 是所有的输出字节流的父类，它是一个抽象类。 ByteArrayOutputStream、FileOutputStream 是两种基本的介质流，它们分别向Byte 数组、和本地文件中写入数据。PipedOutputStream 是向与其它线程共用的管道中写入数据， ObjectOutputStream 和所有FilterOutputStream 的子类都是装饰流。 3.字节流的输入与输出的对应 图中蓝色的为主要的对应部分，红色的部分就是不对应部分。紫色的虚线部分代表这些流一般要搭配使用。从上面的图中可以看出Java IO 中的字节流是极其对称的。“存在及合理”我们看看这些字节流中不太对称的几个类吧！ LineNumberInputStream 主要完成从流中读取数据时，会得到相应的行号，至于什么时候分行、在哪里分行是由改类主动确定的，并不是在原始中有这样一个行号。在输出部分没有对应的部分，我们完全可以自己建立一个LineNumberOutputStream，在最初写入时会有一个基准的行号，以后每次遇到换行时会在下一行添加一个行号，看起来也是可以的。好像更不入流了。 PushbackInputStream 的功能是查看最后一个字节，不满意就放入缓冲区。主要用在编译器的语法、词法分析部分。输出部分的BufferedOutputStream 几乎实现相近的功能。 StringBufferInputStream 已经被Deprecated，本身就不应该出现在InputStream 部分，主要因为String 应该属于字符流的范围。已经被废弃了，当然输出部分也没有必要需要它了！还允许它存在只是为了保持版本的向下兼容而已。 SequenceInputStream 可以认为是一个工具类，将两个或者多个输入流当成一个输入流依次读取。完全可以从IO 包中去除，还完全不影响IO 包的结构，却让其更“纯洁”――纯洁的Decorator 模式。 PrintStream 也可以认为是一个辅助工具。主要可以向其他输出流，或者FileInputStream 写入数据，本身内部实现还是带缓冲的。本质上是对其它流的综合运用的一个工具而已。一样可以踢出IO 包！System.out 和System.out 就是PrintStream 的实例！ 4.字符输入流Reader在上面的继承关系图中可以看出： Reader 是所有的输入字符流的父类，它是一个抽象类。 CharReader、StringReader 是两种基本的介质流，它们分别将Char 数组、String中读取数据。PipedReader 是从与其它线程共用的管道中读取数据。 BufferedReader 很明显就是一个装饰器，它和其子类负责装饰其它Reader 对象。 FilterReader 是所有自定义具体装饰流的父类，其子类PushbackReader 对Reader 对象进行装饰，会增加一个行号。 InputStreamReader 是一个连接字节流和字符流的桥梁，它将字节流转变为字符流。FileReader 可以说是一个达到此功能、常用的工具类，在其源代码中明显使用了将FileInputStream 转变为Reader 的方法。我们可以从这个类中得到一定的技巧。Reader 中各个类的用途和使用方法基本和InputStream 中的类使用一致。后面会有Reader 与InputStream 的对应关系。 5.字符输出流Writer在上面的关系图中可以看出： Writer 是所有的输出字符流的父类，它是一个抽象类。 CharArrayWriter、StringWriter 是两种基本的介质流，它们分别向Char 数组、String 中写入数据。PipedWriter 是向与其它线程共用的管道中写入数据， BufferedWriter 是一个装饰器为Writer 提供缓冲功能。 PrintWriter 和PrintStream 极其类似，功能和使用也非常相似。 OutputStreamWriter 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类（具体可以研究一SourceCode）。功能和使用和OutputStream 极其类似，后面会有它们的对应图。 6.字符流的输入与输出的对应 7.字符流与字节流转换转换流的特点： 其是字符流和字节流之间的桥梁 可对读取到的字节数据经过指定编码转换成字符 可对读取到的字符数据经过指定编码转换成字节 何时使用转换流？ 当字节和字符之间有转换动作时； 流操作的数据需要编码或解码时。 具体的对象体现： InputStreamReader:字节到字符的桥梁 OutputStreamWriter:字符到字节的桥梁 这两个流对象是字符体系中的成员，它们有转换作用，本身又是字符流，所以在构造的时候需要传入字节流对象进来。 8.File类File类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 9.RandomAccessFile类该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点： 该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。 该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw) 注意：该对象在实例化时，如果要操作的文件不存在，会自动创建；如果文件存在，写数据未指定位置，会从头开始写，即覆盖原有的内容。 可以用于多线程下载或多个线程同时写数据到文件。 更多精彩文章请见我的个人博客地址：http://www.54tianzhisheng.cn","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://www.54tianzhisheng.cn/tags/IO/"}]},{"title":"java.sql.SQLException Field 'id' doesn't have a default value","date":"2017-06-19T16:00:00.000Z","path":"2017/06/20/Java-error1/","text":"1、错误描述 在做一个电商网站项目时，使用 Mybatis + MySQL 时出现问题 Caused by: java.sql.SQLException: Field &#39;id&#39; doesn&#39;t have a default value ，网上很多人说是 MyBatis 插入数据行 ID 没生成自增。但是我尝试好久，没解决该问题。 2、错误原因 后来才发现是因为创建数据库时的建表语句中的 id 是主键的，但是在插入的过程中，没有给予数值，并且没有让 id 自增。 3、解决办法 修改数据库表中的id，让其自增（在插入的过程中，不插入id数据时）。 （我是直接将整个数据库都导出来，然后在每个表的 id 后面加上一个 auto_increment）, 如下 ：12345678910CREATE TABLE `d_user` ( `id` int(11) NOT NULL auto_increment, `name` varchar(45) DEFAULT NULL, `password` varchar(45) DEFAULT NULL, `zip` varchar(45) DEFAULT NULL, `address` varchar(45) DEFAULT NULL, `phone` varchar(45) DEFAULT NULL, `email` varchar(45) DEFAULT NULL, PRIMARY KEY (`id`))","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"中缀表达式转换成前缀和后缀表达式这类题目的超实用解题技巧","date":"2017-06-18T16:00:00.000Z","path":"2017/06/19/中缀表达式转换成前缀和后缀表达式这类题目的超实用解题技巧/","text":"看到了标题如果还不了解的这几个概念的请先看看博客：详解前缀、中缀、后缀表达式 先给几个中缀表达式转换成后缀表达式题目做做吧，最后我们在总结超实用的技巧！！ 1. 表达式“X=A+B*（C–D）/E”的后缀表示形式可以为 A. XAB+CDE/-*= B. XA+BC-DE/*= C. XABCD-*E/+= D. XABCDE+*/= 先把答案说出来吧，不过你可以自己先好好的想想可以怎么做才能更快的把答案选出来呢？ 答案：C 先看看下面图片中的这种方法如何？ 2. 已知-算术表达式的中缀表达式为a-(b+c/d)*e,其后缀形式为() A. -a+b*c/d B. -a+b*cd/e C. -+*abc/de D. abcd/+e*- 答案：D 3. 算术表达式a+b*(c+d/e)转为后缀表达式后为() A. ab+cde/* B. abcde/+*+ C. abcde/*++ D. abcde*/++ 答案：B 4. 表达式a*(b+c)-d的后缀表达式是() A. abcd*+- B. abc+*d- C. abc*+d- D. -+*abcd 答案：B 好，题目我们也看了这么多了，那我们该如何解决这一类的题目呢？如果你看了文章首部的那篇 博客的话，那你肯定会觉得那个解法很复杂，如果真的是在笔试中出现这样的题目，那得耗费不 少的时间啊。有人要问了，说了那么一堆，那究竟有没有什么快速的方法呢或者说有没有什么简 单的方法可以直接口算的把答案写出来呢，答案是：有的！而且还真的是特别的简单！！！ 解题重点： 这里我给出一个中缀表达式~ a+b*c-(d+e) 第一步：按照运算符的优先级对所有的运算单位加括号 则式子变成拉：((a+(b*c))-(d+e)) 第二步：转换前缀与后缀表达式 1. 前缀表达式：把运算符号移动到对应的括号前面 则变成拉：-( +(a *(bc)) +(de)) 把括号去掉：-+a*bc+de 前缀表达式出现 2. 后缀表达式：把运算符号移动到对应的括号后面 则变成拉：((a(bc)* )+ (de)+ )- 把括号去掉：abc*+de+- 后缀表达式出现 发现没有，前缀表达式，后缀表达式是不需要用括号来进行优先级的确定的。 如果你习惯拉他的运算方法。计算的时候也就是从两个操作数的前面 或者后面找运算符。而不是中间找，那么也就直接可以口算啦！ 你说这种方法是不是很简单啊！！！ 现在你再去看看刚才的那四道题目，是不是很简单的答案就口算出来了啊！！！ 6不6？","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"表达式","slug":"表达式","permalink":"http://www.54tianzhisheng.cn/tags/表达式/"}]},{"title":"循环队列的相关条件和公式","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/循环队列的相关条件和公式/","text":"循环队列的相关条件和公式：队尾指针是rear,队头是front，其中QueueSize为循环队列的最大长度 队空条件：rear==front 队满条件：(rear+1) %QueueSIze==front 计算队列长度：（rear-front+QueueSize）%QueueSize 入队：（rear+1）%QueueSize 出队：（front+1）%QueueSize 队列中元素的个数： (rear-front+QueueSize)%QueueSize","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://www.54tianzhisheng.cn/tags/算法/"},{"name":"循环队列","slug":"循环队列","permalink":"http://www.54tianzhisheng.cn/tags/循环队列/"}]},{"title":"Bootstrap入门需掌握的知识点（二）","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/Bootstrap入门需掌握的知识点（二）/","text":"相关阅读：Bootstrap入门需掌握的知识点（一）表格基本实例为任意 &lt;table&gt; 标签添加 .table 类可以为其赋予基本的样式 — 少量的内补（padding）和水平方向的分隔线。 123&lt;table class=&quot;table&quot;&gt; ...&lt;/table&gt; 条纹状表格通过 .table-striped 类可以给 &lt;tbody&gt; 之内的每一行增加斑马条纹样式。 123&lt;table class=\"table table-striped\"&gt; ...&lt;/table&gt; 带边框的表格添加 .table-bordered 类为表格和其中的每个单元格增加边框。 123&lt;table class=\"table table-bordered\"&gt; ...&lt;/table&gt; 鼠标悬停通过添加 .table-hover 类可以让 中的每一行对鼠标悬停状态作出响应。 123&lt;table class=\"table table-hover\"&gt; ...&lt;/table&gt; 状态类通过这些状态类可以为行或单元格设置颜色。 Class 描述 .active 鼠标悬停在行或单元格上时所设置的颜色 .success 标识成功或积极的动作 .info 标识普通的提示信息或动作 .warning 标识警告或需要用户注意 .danger 标识危险或潜在的带来负面影响的动作 123456789101112131415&lt;!-- On rows --&gt;&lt;tr class=\"active\"&gt;...&lt;/tr&gt;&lt;tr class=\"success\"&gt;...&lt;/tr&gt;&lt;tr class=\"warning\"&gt;...&lt;/tr&gt;&lt;tr class=\"danger\"&gt;...&lt;/tr&gt;&lt;tr class=\"info\"&gt;...&lt;/tr&gt;&lt;!-- On cells (`td` or `th`) --&gt;&lt;tr&gt; &lt;td class=\"active\"&gt;...&lt;/td&gt; &lt;td class=\"success\"&gt;...&lt;/td&gt; &lt;td class=\"warning\"&gt;...&lt;/td&gt; &lt;td class=\"danger\"&gt;...&lt;/td&gt; &lt;td class=\"info\"&gt;...&lt;/td&gt;&lt;/tr&gt; 响应式表格将任何 .table 元素包裹在 .table-responsive 元素内，即可创建响应式表格，其会在小屏幕设备上（小于768px）水平滚动。当屏幕大于 768px 宽度时，水平滚动条消失。 12345&lt;div class=\"table-responsive\"&gt; &lt;table class=\"table\"&gt; ... &lt;/table&gt;&lt;/div&gt; 表单基本实例单独的表单控件会被自动赋予一些全局样式。所有设置了 .form-control 类的 &lt;input&gt;、&lt;textarea&gt; 和 &lt;select&gt; 元素都将被默认设置宽度属性为 width: 100%;。 将 label 元素和前面提到的控件包裹在 .form-group 中可以获得最好的排列。 123456789101112131415161718192021&lt;form role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputEmail1\"&gt;Email address&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" id=\"exampleInputEmail1\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputPassword1\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"exampleInputPassword1\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputFile\"&gt;File input&lt;/label&gt; &lt;input type=\"file\" id=\"exampleInputFile\"&gt; &lt;p class=\"help-block\"&gt;Example block-level help text here.&lt;/p&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Check me out &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Submit&lt;/button&gt;&lt;/form&gt; 注意：不要将表单组直接和输入框组混合使用，建议将输入框组嵌套到表单组中使用。 内联表单为 &lt;form&gt; 元素添加 .form-inline 类可使其内容左对齐并且表现为 inline-block 级别的控件。只适用于视口（viewport）至少在 768px 宽度时（视口宽度再小的话就会使表单折叠）。 注意： 需要手动设置宽度在 Bootstrap 中，输入框和单选/多选框控件默认被设置为 width: 100%; 宽度。在内联表单，我们将这些元素的宽度设置为 width: auto;，因此，多个控件可以排列在同一行。根据你的布局需求，可能需要一些额外的定制化组件。 一定要添加 label 标签如果你没有为每个输入控件设置 label 标签，屏幕阅读器将无法正确识别。对于这些内联表单，你可以通过为label 设置 .sr-only 类将其隐藏。 12345678910111213141516171819202122&lt;form class=\"form-inline\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\" for=\"exampleInputEmail2\"&gt;Email address&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" id=\"exampleInputEmail2\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"input-group\"&gt; &lt;div class=\"input-group-addon\"&gt;@&lt;/div&gt; &lt;input class=\"form-control\" type=\"email\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\" for=\"exampleInputPassword2\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"exampleInputPassword2\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Sign in&lt;/button&gt;&lt;/form&gt; 水平排列的表单通过为表单添加 .form-horizontal 类，并联合使用 Bootstrap 预置的栅格类，可以将 label 标签和控件组水平并排布局。这样做将改变 .form-group 的行为，使其表现为栅格系统中的行（row），因此就无需再额外添加 .row 了。 12345678910111213141516171819202122232425262728&lt;form class=\"form-horizontal\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputEmail3\" class=\"col-sm-2 control-label\"&gt;Email&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"email\" class=\"form-control\" id=\"inputEmail3\" placeholder=\"Email\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword3\" class=\"col-sm-2 control-label\"&gt;Password&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword3\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Sign in&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 被支持的控件表单布局实例中展示了其所支持的标准表单控件。 1、输入框包括大部分表单控件、文本输入域控件，还支持所有 HTML5 类型的输入控件：text、password、datetime、datetime-local、date、month、time、week、number、email、url、search、tel 和 color。 1&lt;input type=\"text\" class=\"form-control\" placeholder=\"Text input\"&gt; 如需在文本输入域 &lt;input&gt; 前面或后面添加文本内容或按钮控件，请参考输入控件组。 2、文本域支持多行文本的表单控件。可根据需要改变 rows 属性。 1&lt;textarea class=\"form-control\" rows=\"3\"&gt;&lt;/textarea&gt; 3、多选和单选框多选框（checkbox）用于选择列表中的一个或多个选项，而单选框（radio）用于从多个选项中只选择一个。 设置了 disabled 属性的单选或多选框都能被赋予合适的样式。对于和多选或单选框联合使用的 &lt;label&gt; 标签，如果也希望将悬停于上方的鼠标设置为“禁止点击”的样式，请将 .disabled 类赋予 .radio、.radio-inline、.checkbox 、.checkbox-inline 或 &lt;fieldset&gt;。 12345678910111213141516171819202122232425262728293031&lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"\"&gt; Option one is this and that&amp;mdash;be sure to include why it's great &lt;/label&gt;&lt;/div&gt;&lt;div class=\"checkbox disabled\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"\" disabled&gt; Option two is disabled &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios1\" value=\"option1\" checked&gt; Option one is this and that&amp;mdash;be sure to include why it's great &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios2\" value=\"option2\"&gt; Option two can be something else and selecting it will deselect option one &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio disabled\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios3\" value=\"option3\" disabled&gt; Option three is disabled &lt;/label&gt;&lt;/div&gt; 内联单选和多选框通过将 .checkbox-inline 或 .radio-inline 类应用到一系列的多选框（checkbox）或单选框（radio）控件上，可以使这些控件排列在一行。 12345678910111213141516171819&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox1\" value=\"option1\"&gt; 1&lt;/label&gt;&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox2\" value=\"option2\"&gt; 2&lt;/label&gt;&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox3\" value=\"option3\"&gt; 3&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio1\" value=\"option1\"&gt; 1&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio2\" value=\"option2\"&gt; 2&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio3\" value=\"option3\"&gt; 3&lt;/label&gt; 复选框 12345678910&lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"blankCheckbox\" value=\"option1\"&gt; &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"blankRadio\" id=\"blankRadio1\" value=\"option1\"&gt; &lt;/label&gt;&lt;/div&gt; 下拉列表（select）使用默认选项或添加 multiple 属性可以同时显示多个选项。 123456789101112131415&lt;select class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt;&lt;/select&gt;&lt;select multiple class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt;&lt;/select&gt; 静态控件如果需要在表单中将一行纯文本和 label 元素放置于同一行，为 &lt;p&gt; 元素添加 .form-control-static 类即可。 1234567891011121314&lt;form class=\"form-horizontal\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"col-sm-2 control-label\"&gt;Email&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;p class=\"form-control-static\"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword\" class=\"col-sm-2 control-label\"&gt;Password&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 1234567891011&lt;form class=\"form-inline\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\"&gt;Email&lt;/label&gt; &lt;p class=\"form-control-static\"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword2\" class=\"sr-only\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword2\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Confirm identity&lt;/button&gt;&lt;/form&gt; 输入框焦点我们将某些表单控件的默认 outline 样式移除，然后对 :focus 状态赋予 box-shadow 属性。 被禁用的输入框为输入框设置 disabled 属性可以防止用户输入，并能对外观做一些修改，使其更直观。 1&lt;input class=\"form-control\" id=\"disabledInput\" type=\"text\" placeholder=\"Disabled input here...\" disabled&gt; 1234567891011121314151617181920&lt;form role=\"form\"&gt; &lt;fieldset disabled&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"disabledTextInput\"&gt;Disabled input&lt;/label&gt; &lt;input type=\"text\" id=\"disabledTextInput\" class=\"form-control\" placeholder=\"Disabled input\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"disabledSelect\"&gt;Disabled select menu&lt;/label&gt; &lt;select id=\"disabledSelect\" class=\"form-control\"&gt; &lt;option&gt;Disabled select&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Can't check this &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Submit&lt;/button&gt; &lt;/fieldset&gt;&lt;/form&gt; 只读输入框为输入框设置 readonly 属性可以禁止用户输入，并且输入框的样式也是禁用状态。 1&lt;input class=\"form-control\" type=\"text\" placeholder=\"Readonly input here…\" readonly&gt; 校验状态Bootstrap 对表单控件的校验状态，如 error、warning 和 success 状态，都定义了样式。使用时，添加 .has-warning、.has-error 或 .has-success 类到这些控件的父元素即可。任何包含在此元素之内的 .control-label、.form-control 和 .help-block 元素都将接受这些校验状态的样式。 123456789101112131415161718192021222324252627282930313233343536&lt;div class=\"form-group has-success\"&gt; &lt;label class=\"control-label\" for=\"inputSuccess1\"&gt;Input with success&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputSuccess1\"&gt;&lt;/div&gt;&lt;div class=\"form-group has-warning\"&gt; &lt;label class=\"control-label\" for=\"inputWarning1\"&gt;Input with warning&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputWarning1\"&gt;&lt;/div&gt;&lt;div class=\"form-group has-error\"&gt; &lt;label class=\"control-label\" for=\"inputError1\"&gt;Input with error&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputError1\"&gt;&lt;/div&gt;&lt;div class=\"has-success\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxSuccess\" value=\"option1\"&gt; Checkbox with success &lt;/label&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=\"has-warning\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxWarning\" value=\"option1\"&gt; Checkbox with warning &lt;/label&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=\"has-error\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxError\" value=\"option1\"&gt; Checkbox with error &lt;/label&gt; &lt;/div&gt;&lt;/div&gt; 添加额外的图标你还可以针对校验状态为输入框添加额外的图标。只需设置相应的 .has-feedback 类并添加正确的图标即可。 Feedback icons only work with textual &lt;input class=&quot;form-control&quot;&gt; elements. 图标、label 和输入控件组对于不带有 label 标签的输入框以及右侧带有附加组件的输入框组，需要手动为其图标定位。为了让所有用户都能访问你的网站，我们强烈建议为所有输入框添加 label 标签。如果你不希望将 label 标签展示出来，可以通过添加 sr-only 类来实现。如果的确不能添加 label 标签，请调整图标的 top 值。对于输入框组，请根据你的实际情况调整 right 值。 123456789101112131415&lt;div class=\"form-group has-success has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputSuccess2\"&gt;Input with success&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputSuccess2\"&gt; &lt;span class=\"glyphicon glyphicon-ok form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=\"form-group has-warning has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputWarning2\"&gt;Input with warning&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputWarning2\"&gt; &lt;span class=\"glyphicon glyphicon-warning-sign form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=\"form-group has-error has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputError2\"&gt;Input with error&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputError2\"&gt; &lt;span class=\"glyphicon glyphicon-remove form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt; 控件尺寸通过 .input-lg 类似的类可以为控件设置高度，通过 .col-lg-* 类似的类可以为控件设置宽度。 高度尺寸创建大一些或小一些的表单控件以匹配按钮尺寸。 1234567&lt;input class=\"form-control input-lg\" type=\"text\" placeholder=\".input-lg\"&gt;&lt;input class=\"form-control\" type=\"text\" placeholder=\"Default input\"&gt;&lt;input class=\"form-control input-sm\" type=\"text\" placeholder=\".input-sm\"&gt;&lt;select class=\"form-control input-lg\"&gt;...&lt;/select&gt;&lt;select class=\"form-control\"&gt;...&lt;/select&gt;&lt;select class=\"form-control input-sm\"&gt;...&lt;/select&gt; 按钮预定义样式使用下面列出的类可以快速创建一个带有预定义样式的按钮。 1234567891011121314151617181920&lt;!-- Standard button --&gt;&lt;button type=\"button\" class=\"btn btn-default\"&gt;Default&lt;/button&gt;&lt;!-- Provides extra visual weight and identifies the primary action in a set of buttons --&gt;&lt;button type=\"button\" class=\"btn btn-primary\"&gt;Primary&lt;/button&gt;&lt;!-- Indicates a successful or positive action --&gt;&lt;button type=\"button\" class=\"btn btn-success\"&gt;Success&lt;/button&gt;&lt;!-- Contextual button for informational alert messages --&gt;&lt;button type=\"button\" class=\"btn btn-info\"&gt;Info&lt;/button&gt;&lt;!-- Indicates caution should be taken with this action --&gt;&lt;button type=\"button\" class=\"btn btn-warning\"&gt;Warning&lt;/button&gt;&lt;!-- Indicates a dangerous or potentially negative action --&gt;&lt;button type=\"button\" class=\"btn btn-danger\"&gt;Danger&lt;/button&gt;&lt;!-- Deemphasize a button by making it look like a link while maintaining button behavior --&gt;&lt;button type=\"button\" class=\"btn btn-link\"&gt;Link&lt;/button&gt; 尺寸需要让按钮具有不同尺寸吗？使用 .btn-lg、.btn-sm 或 .btn-xs 可以获得不同尺寸的按钮。 12345678910111213141516&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-lg\"&gt;Large button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-lg\"&gt;Large button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary\"&gt;Default button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default\"&gt;Default button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-sm\"&gt;Small button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-sm\"&gt;Small button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-xs\"&gt;Extra small button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-xs\"&gt;Extra small button&lt;/button&gt;&lt;/p&gt; 通过给按钮添加 .btn-block 类可以将其拉伸至父元素100%的宽度，而且按钮也变为了块级（block）元素。 12&lt;button type=\"button\" class=\"btn btn-primary btn-lg btn-block\"&gt;Block level button&lt;/button&gt;&lt;button type=\"button\" class=\"btn btn-default btn-lg btn-block\"&gt;Block level button&lt;/button&gt; 激活状态当按钮处于激活状态时，其表现为被按压下去（底色更深、边框夜色更深、向内投射阴影）。对于 &lt;button&gt; 元素，是通过 :active 状态实现的。对于 &lt;a&gt;元素，是通过 .active 类实现的。然而，你还可以将 .active 应用到 &lt;button&gt;上，并通过编程的方式使其处于激活状态。 button 元素由于 :active 是伪状态，因此无需额外添加，但是在需要让其表现出同样外观的时候可以添加 .active 类。 12&lt;button type=\"button\" class=\"btn btn-primary btn-lg active\"&gt;Primary button&lt;/button&gt;&lt;button type=\"button\" class=\"btn btn-default btn-lg active\"&gt;Button&lt;/button&gt; 链接（&lt;a&gt;）元素可以为基于 &lt;a&gt;元素创建的按钮添加 .active 类。 12&lt;a href=\"#\" class=\"btn btn-primary btn-lg active\" role=\"button\"&gt;Primary link&lt;/a&gt;&lt;a href=\"#\" class=\"btn btn-default btn-lg active\" role=\"button\"&gt;Link&lt;/a&gt; 按钮类为 &lt;a&gt;、&lt;button&gt; 或 &lt;input&gt; 元素应用按钮类 1234&lt;a class=\"btn btn-default\" href=\"#\" role=\"button\"&gt;Link&lt;/a&gt;&lt;button class=\"btn btn-default\" type=\"submit\"&gt;Button&lt;/button&gt;&lt;input class=\"btn btn-default\" type=\"button\" value=\"Input\"&gt;&lt;input class=\"btn btn-default\" type=\"submit\" value=\"Submit\"&gt; 图片响应式图片在 Bootstrap 版本 3 中，通过为图片添加 .img-responsive 类可以让图片支持响应式布局。其实质是为图片设置了 max-width: 100%; 和 height: auto; 属性，从而让图片在其父元素中更好的缩放。 1&lt;img src=\"...\" class=\"img-responsive\" alt=\"Responsive image\"&gt; 图片形状通过为 &lt;img&gt; 元素添加以下相应的类，可以让图片呈现不同的形状。 123&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-rounded&quot;&gt;&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-circle&quot;&gt;&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-thumbnail&quot;&gt;","tags":[{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"前端","slug":"前端","permalink":"http://www.54tianzhisheng.cn/tags/前端/"}]},{"title":"Bootstrap入门需掌握的知识点（一）","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/Bootstrap入门需掌握的知识点（一）/","text":"BootstrapBootstrap中文网：http://www.bootcss.com/ 1.什么是 Bootstrap？ 官方介绍：简洁、直观、强悍的前端开发框架，让web开发更迅速、简单。 Bootstrap 下载 Bootstrap3下载地址：http://v3.bootcss.com/getting-started/#download Bootstrap 文件目录结构 1234567891011121314151617dist -css -bootstrap.css -bootstrap.css.map -bootstrap.min.css（常用） -bootstrap-theme.css -bootstrap-theme.css.map -bootstrap-theme.min.css -fonts -glyphicons-halflings-regular.eot -glyphicons-halflings-regular.svg -glyphicons-halflings-regular.ttf -glyphicons-halflings-regular.woff -js -bootstrap.js -bootstrap.min.js（常用） -npm.js Bootstrap 依赖 要想使用 Bootstrap ，那么必须先引入 jQuery（jquery.min.js）文件。 5.使用 Bootstrap 压缩版本适于实际应用，未压缩版本适于开发调试过程 直接引用官网下载下来的文件。 使用 Bootstrap 中文网提供的免费 CDN 服务。 1234567891011&lt;!-- 新 Bootstrap 核心 CSS 文件 --&gt;&lt;link rel=\"stylesheet\" href=\"http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap.min.css\"&gt;&lt;!-- 可选的Bootstrap主题文件（一般不用引入） --&gt;&lt;link rel=\"stylesheet\" href=\"http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap-theme.min.css\"&gt;&lt;!-- jQuery文件。务必在bootstrap.min.js 之前引入 --&gt;&lt;script src=\"http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt;&lt;!-- 最新的 Bootstrap 核心 JavaScript 文件 --&gt;&lt;script src=\"http://cdn.bootcss.com/bootstrap/3.3.0/js/bootstrap.min.js\"&gt;&lt;/script&gt; Bootstrap 基本模板 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-cn\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; &lt;title&gt;Bootstrap 基本模板&lt;/title&gt; &lt;!-- Bootstrap --&gt; &lt;link href=\"css/bootstrap.min.css\" rel=\"stylesheet\"&gt; &lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt; &lt;!-- WARNING: Respond.js doesn't work if you view the page via file:// --&gt; &lt;!--[if lt IE 9]&gt; &lt;script src=\"http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js\"&gt;&lt;/script&gt; &lt;script src=\"http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js\"&gt;&lt;/script&gt; &lt;![endif]--&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;你好，世界！&lt;/h1&gt; &lt;!-- jQuery (necessary for Bootstrap's JavaScript plugins) --&gt; &lt;script src=\"http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt; &lt;!-- Include all compiled plugins (below), or include individual files as needed --&gt; &lt;script src=\"js/bootstrap.min.js\"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; Bootstrap 实例精选：http://v3.bootcss.com/getting-started/#examples 全局 CSS 样式HTML5 文档类型Bootstrap 使用到的某些 HTML 元素和 CSS 属性需要将页面设置为 HTML5 文档类型。 1234&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-CN\"&gt; ...&lt;/html&gt; 移动设备优先在 bootstrap3 中移动设备优先考虑的。为了保证适当的绘制和触屏缩放，需要在&lt;head&gt;之中添加 viewport 元数据标签。 1&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; 在移动设备浏览器上，可以通过视口 viewport 设置meta属性为user-scalable=no可以禁用其缩放（zooming）功能，这样后用户只能滚动屏幕。（看情况而定） 1&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scala=1, user-scalable=no\"&gt; 排版与链接Bootstrap 排版、链接样式设置了基本的全局样式。分别是： 为 body 元素设置 background-color: #fff; 使用 @font-family-base、@font-size-base 和 @line-height-base a变量作为排版的基本参数 为所有链接设置了基本颜色 @link-color ，并且当链接处于 :hover 状态时才添加下划线 这些样式都能在 scaffolding.less 文件中找到对应的源码。 Normalize.css为了增强跨浏览器表现的一致性，bootstrap使用了 Normalize.css，这是由 Nicolas Gallagher 和 Jonathan Neal 维护的一个CSS 重置样式库。 布局容器Bootstrap 需要为页面内容和栅格系统包裹一个 .container 容器。Bootstrap提供了两个作此用处的类。注意，由于 padding等属性的原因，这两种容器类不能互相嵌套。 .container 类用于固定宽度并支持响应式布局的容器。 123&lt;div class=\"container\"&gt; ...&lt;/div&gt; .container-fluid 类用于 100% 宽度，占据全部视口（viewport）的容器。 123&lt;div class=\"container-fluid\"&gt; ...&lt;/div&gt; 栅格系统Bootstrap 提供了一套响应式、移动设备优先的流式栅格系统，随着屏幕或视口（viewport）尺寸的增加，系统会自动分为最多12列。它包含了易于使用的预定义类，还有强大的mixin 用于生成更具语义的布局。 简介栅格系统用于通过一系列的行（row）与列（column）的组合来创建页面布局，你的内容就可以放入这些创建好的布局中。下面就介绍一下 Bootstrap 栅格系统的工作原理： “行（row）”必须包含在 .container （固定宽度）或 .container-fluid （100% 宽度）中，以便为其赋予合适的排列（aligment）和内补（padding）。 通过“行（row）”在水平方向创建一组“列（column）”。 你的内容应当放置于“列（column）”内，并且，只有“列（column）”可以作为行（row）”的直接子元素。 类似 .row 和 .col-xs-4 这种预定义的类，可以用来快速创建栅格布局。Bootstrap 源码中定义的 mixin 也可以用来创建语义化的布局。 通过为“列（column）”设置 padding 属性，从而创建列与列之间的间隔（gutter）。通过为 .row 元素设置负值margin 从而抵消掉为 .container 元素设置的 padding，也就间接为“行（row）”所包含的“列（column）”抵消掉了padding。 The negative margin is why the examples below are outdented. It’s so that content within grid columns is lined up with non-grid content. Grid columns are created by specifying the number of twelve available columns you wish to span. For example, three equal columns would use three .col-xs-4. 如果一“行（row）”中包含了的“列（column）”大于 12，多余的“列（column）”所在的元素将被作为一个整体另起一行排列。 Grid classes apply to devices with screen widths greater than or equal to the breakpoint sizes, and override grid classes targeted at smaller devices. Therefore, applying any .col-md- class to an element will not only affect its styling on medium devices but also on large devices if a .col-lg- class is not present. 通过研究后面的实例，可以将这些原理应用到你的代码中。 媒体查询在栅格系统中，我们在 Less 文件中使用以下媒体查询（media query）来创建关键的分界点阈值。 1234567891011/* 超小屏幕（手机，小于 768px） *//* 没有任何媒体查询相关的代码，因为这在 Bootstrap 中是默认的（还记得 Bootstrap 是移动设备优先的吗？） *//* 小屏幕（平板，大于等于 768px） */@media (min-width: @screen-sm-min) &#123; ... &#125;/* 中等屏幕（桌面显示器，大于等于 992px） */@media (min-width: @screen-md-min) &#123; ... &#125;/* 大屏幕（大桌面显示器，大于等于 1200px） */@media (min-width: @screen-lg-min) &#123; ... &#125; 偶尔也会在媒体查询代码中包含 max-width 从而将 CSS 的影响限制在更小范围的屏幕大小之内 1234@media (max-width: @screen-xs-max) &#123; ... &#125;@media (min-width: @screen-sm-min) and (max-width: @screen-sm-max) &#123; ... &#125;@media (min-width: @screen-md-min) and (max-width: @screen-md-max) &#123; ... &#125;@media (min-width: @screen-lg-min) &#123; ... &#125; 栅格参数通过下表可以详细查看 Bootstrap 的栅格系统是如何在多种屏幕设备上工作的。 超小屏幕 手机 (&lt;768px) 小屏幕 平板 (≥768px) 中等屏幕 桌面显示器 (≥992px) 大屏幕 大桌面显示器 (≥1200px) 栅格系统行为 总是水平排列 开始是堆叠在一起的，当大于这些阈值时将变为水平排列C 同左 同左 .container 最大宽度 None （自动） 750px 970px 1170px 类前缀 .col-xs- .col-sm- .col-md- .col-lg- 列（column）数 12 12 12 12 最大列（column）宽 自动 ~62px ~81px ~97px 槽（gutter）宽 30px （每列左右均有 15px） 同左 同左 同左 可嵌套 是 是 是 是 偏移（Offsets） 是 是 是 是 列排序 是 是 是 是 实例：从堆叠到水平排列使用单一的一组 .col-md-* 栅格类，就可以创建一个基本的栅格系统，在手机和平板设备上一开始是堆叠在一起的（超小屏幕到小屏幕这一范围），在桌面（中等）屏幕设备上变为水平排列。所有“列（column）必须放在 ” .row 内。 123456789101112131415161718192021222324252627&lt;div class=\"row\"&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-8\"&gt;.col-md-8&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-6\"&gt;.col-md-6&lt;/div&gt; &lt;div class=\"col-md-6\"&gt;.col-md-6&lt;/div&gt;&lt;/div&gt; 实例：移动设备和桌面屏幕是否不希望在小屏幕设备上所有列都堆叠在一起？那就使用针对超小屏幕和中等屏幕设备所定义的类吧，即 .col-xs-*和 .col-md-*。请看下面的实例，研究一下这些是如何工作的。 123456789101112131415161718&lt;!-- Stack the columns on mobile by making one full-width and the other half-width --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-12 col-md-8\"&gt;.col-xs-12 .col-md-8&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt;&lt;/div&gt;&lt;!-- Columns start at 50% wide on mobile and bump up to 33.3% wide on desktop --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt;&lt;/div&gt;&lt;!-- Columns are always 50% wide, on mobile and desktop --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-6\"&gt;.col-xs-6&lt;/div&gt; &lt;div class=\"col-xs-6\"&gt;.col-xs-6&lt;/div&gt;&lt;/div&gt; 排版标题HTML 中的所有标题标签，到 均可使用。另外，还提供了 .h1 到 .h6 类，为的是给内联（inline）属性的文本赋予标题的样式 123456&lt;h1&gt;h1. Bootstrap heading&lt;/h1&gt;&lt;h2&gt;h2. Bootstrap heading&lt;/h2&gt;&lt;h3&gt;h3. Bootstrap heading&lt;/h3&gt;&lt;h4&gt;h4. Bootstrap heading&lt;/h4&gt;&lt;h5&gt;h5. Bootstrap heading&lt;/h5&gt;&lt;h6&gt;h6. Bootstrap heading&lt;/h6&gt; 在标题内还可以包含 &lt;small&gt; 标签或赋予 .small 类的元素，可以用来标记副标题。 123456&lt;h1&gt;h1. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h1&gt;&lt;h2&gt;h2. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h2&gt;&lt;h3&gt;h3. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h3&gt;&lt;h4&gt;h4. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h4&gt;&lt;h5&gt;h5. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h5&gt;&lt;h6&gt;h6. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h6&gt; 页面主体Bootstrap 将全局 font-size 设置为 14px，line-height 设置为 1.428。这些属性直接赋予 元素和所有段落元素。另外， （段落）元素还被设置了等于 1/2 行高（即 10px）的底部外边距（margin）。 中心内容通过添加 .lead 类可以让段落突出显示。 1&lt;p class=\"lead\"&gt;...&lt;/p&gt; 使用 Less 工具构建variables.less 文件中定义的两个 Less 变量决定了排版尺寸：@font-size-base 和 @line-height-base。第一个变量定义了全局 font-size 基准，第二个变量是 line-height 基准。我们使用这些变量和一些简单的公式计算出其它所有页面元素的 margin、 padding 和 line-height。自定义这些变量即可改变 Bootstrap 的默认样式 内联文本元素标记文本为了高亮文本，可以使用 &lt;mark&gt; 标签 1You can use the mark tag to &lt;mark&gt;highlight&lt;/mark&gt; text. 被删除的文本对于被删除的文本，可以使用 &lt;del&gt; 标签。 1&lt;del&gt;This line of text is meant to be treated as deleted text.&lt;/del&gt; 无用文本对于无用文本可以使用 &lt;s&gt; 标签。 1&lt;s&gt;This line of text is meant to be treated as no longer accurate.&lt;/s&gt; 插入文本而外插入文本使用 &lt;ins&gt; 标签 1&lt;ins&gt;This line of text is meant to be treated as an addition to the document.&lt;/ins&gt; 带下划线的文本为文本添加下划线，使用 &lt;u&gt; 标签。 1&lt;u&gt;This line of text will render as underlined&lt;/u&gt; 小号文本使用标签 &lt;small&gt; 着重强调使用标签 &lt;strong&gt; 标签 斜体使用 &lt;em&gt; 标签 文本对齐 12345&lt;p class=\"text-left\"&gt;Left aligned text.&lt;/p&gt;&lt;p class=\"text-center\"&gt;Center aligned text.&lt;/p&gt;&lt;p class=\"text-right\"&gt;Right aligned text.&lt;/p&gt;&lt;p class=\"text-justify\"&gt;Justified text.&lt;/p&gt;&lt;p class=\"text-nowrap\"&gt;No wrap text.&lt;/p&gt; 改变大小写 123&lt;p class=\"text-lowercase\"&gt;Lowercased text.&lt;/p&gt;&lt;p class=\"text-uppercase\"&gt;Uppercased text.&lt;/p&gt;&lt;p class=\"text-capitalize\"&gt;Capitalized text.&lt;/p&gt; 引用在你的文档中引用其他的来源，可以使用 &lt;blockquote&gt; 来表示引用样式。对于直接引用，建议使用 &lt;p&gt; 标签。 123&lt;blockquote&gt; &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante.&lt;/p&gt;&lt;/blockquote&gt; 列表无序列表排列顺序无关紧要的一列元素。 123&lt;ul&gt; &lt;li&gt;...&lt;/li&gt;&lt;/ul&gt; 有序列表顺序至关重要的一组元素 123&lt;ol&gt; &lt;li&gt;...&lt;/li&gt;&lt;/ol&gt; 代码内联代码1For example, &lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt; should be wrapped as inline. 用户输入通过 kbd 标签标记用户通过键盘输入的内容。 12To switch directories, type &lt;kbd&gt;cd&lt;/kbd&gt; followed by the name of the directory.&lt;br&gt;To edit settings, press &lt;kbd&gt;&lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;,&lt;/kbd&gt;&lt;/kbd&gt; 代码块多行代码可以使用 &lt;pre&gt; 标签。为了正确的展示代码，注意将尖括号做转义处理。 变量通过 &lt;var&gt; 标签标记变量 程序输出通过 &lt;samp&gt; 标签来标记程序输出的内容 期待后面的文章！","tags":[{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"前端","slug":"前端","permalink":"http://www.54tianzhisheng.cn/tags/前端/"}]},{"title":"搭建一个博客项目后的碎碎念","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/blog-talk/","text":"前言 以前大二的时候就想一个人独立做一个由 Java 开发的个人博客, 可耐当时还很弱鸡，一个人难以独挡一片，因为要会的东西太多，后来自己看到很多都是由 WordPress 搭建的博客，很多模板很漂亮，可是自己要稍微对 “拍黄片” 了解一点，并且里面的各种插件特特别的多。去年的时候就开始用上了 GitHub Page 搭建静态的博客，因为自己一直习惯用 Markdown 写作，写完后，软件可以直接生成 PDF 和 HTML 文件，这样就很方便了，直接将自己的 HTML、PDF 和 MD 文件一起 push 到 GitHub 上，然后自己在通过域名加上文章链接就可以直接访问我的博客了，这样就省了很多事了。还提供了 PDF 和 MD 版本，对有不同需求的人都可满足了。可是后来觉得这样的逼格还是不够高，就又开始折腾 Hexo 了，发现用 Hexo 也是很非常简单的（其实是看到 Hexo 的 yilia 主题非常漂亮）。于是就换上了 Hexo 了，自己在这上面写博客也很方便。每次用软件写完后，在 Git Bash 下敲一行命令 hexo d -g 就行了，很方便！前段时间看到了一款开源的博客（由 Java 搭建而成）—— Tale，主题比较简洁，符合程序员的范。也刚好符合自己最初的想法，但是我是没打算放弃现在的博客，就是有一个想法，自己也跟着在那个基础山修改下。（因为 Tale 使用的是轻量级 mvc 框架 Blade 开发，我好像不太了解这个框架呢），想着就 SpringBoot 开发比较快，上手也简单。当时就有这个想法，可怜没时间，不过前些天发现有人就是基于那个 Tale 博客重新修改了，用的就是 SpringBoot ，哇，果然是英雄所见略同。当时就和作者邮件联系了，于是蹭这些天的时间赶紧去看看，结果不只是看看，完全自己就全部敲了一遍，终于在今天搞定了，为了庆祝，才写下这篇文章，好好记录这些美好的时刻（博客可以完全发挥，不限题材）。通过自己深入这个项目，才能够很了解内部的实现方式，这点收获很大，这十天时间花的值，再此感谢两位原作者 ZHENFENG13 、otale 。 博客介绍Tale 使用了轻量级 mvc 框架 Blade 开发，默认主题使用了漂亮的 pinghsu 。 My-Blog 使用的是 Docker + SpringBoot + Mybatis + thymeleaf 打造的一个个人博客模板。 Blog 是自己花了十天的时间把整个项目的代码都敲了一遍，熟悉了整个项目，做了优化，去除了 Docker， 其中修改了原来的一些 bug，并在原作者的项目中提出了 issue ， 原作者已修复。 : 喜欢该项目的话，可以给项目点个 star，如果你想在这基础上修改，那么建议你 fork 该项目，然后再修改哦。 博客首页： 归档： 友链： 关于： 搜索： 后台管理 管理登录： 管理首页： 发布文章： 文章管理： 页面管理： 分类标签： 文件管理： 友链管理： 系统设置： 最后我什么我这么喜欢折腾博客呢，熟悉我的朋友都知道，我再很多平台都写过博客，有些是他们平台的运营人员邀请过去的。可是在这些平台上写博客终究是没有感觉，如今自己在自己的博客网站写文章，比较轻松，而且也符合我的写作风格。在其他的平台都有些大大小小的不适（对程序员来说应该是 bug），虽然目前还是会在这些平台继续发布我新写的文章，但是我保证最新的文章，首发肯定是我自己的博客网站，有些是不会在其他平台发的，有觉得不错的可以 RSS 订阅我的博客，或者是直接收藏网址下来。自从写博客下来遇到很多志同道合的人，这点正是让我觉得有写下去的必要了。自己将会坚持下去，时刻警醒自己：勿忘初心！最后的最后，还是想说一句：如果你想和我一样折腾博客，那么我建议你先在一家平台坚持写下去，等博客数量上来了，在自己折腾自己的博客网站。还有就是你想提高自己的话，还是需要很在意你的基础，然后就是要多练手几个项目，我自己在练手这个项目的时候就收获很多。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"详解 Filter 过滤器","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/详解 Filter 过滤器/","text":"1、简介 Filter也称之为过滤器，它是Servlet技术中最实用的技术，WEB开发人员通过Filter技术，对web服务器管理的所有web资源：例如Jsp, Servlet, 静态图片文件或静态 html 文件等进行拦截，从而实现一些特殊的功能。例如实现URL级别的权限访问控制、过滤敏感词汇、压缩响应信息等一些高级功能。 它主要用于对用户请求进行预处理，也可以对HttpServletResponse 进行后处理。使用Filter 的完整流程：Filter 对用户请求进行预处理，接着将请求交给Servlet 进行处理并生成响应，最后Filter 再对服务器响应进行后处理。 Filter功能： 在HttpServletRequest 到达 Servlet 之前，拦截客户的 HttpServletRequest 。 根据需要检查 HttpServletRequest ，也可以修改HttpServletRequest 头和数据。 在HttpServletResponse 到达客户端之前，拦截HttpServletResponse 。 根据需要检查 HttpServletResponse ，也可以修改HttpServletResponse头和数据。 2、如何实现拦截 Filter接口中有一个doFilter方法，当开发人员编写好Filter，并配置对哪个web资源进行拦截后，WEB服务器每次在调用web资源的service方法之前，都会先调用一下filter的doFilter方法，因此，在该方法内编写代码可达到如下目的： 调用目标资源之前，让一段代码执行。 是否调用目标资源（即是否让用户访问web资源）。 web服务器在调用doFilter方法时，会传递一个filterChain对象进来，filterChain对象是filter接口中最重要的一个对象，它也提供了一个doFilter方法，开发人员可以根据需求决定是否调用此方法，调用该方法，则web服务器就会调用web资源的service方法，即web资源就会被访问，否则web资源不会被访问。 3、Filter开发两步走 编写java类实现Filter接口，并实现其doFilter方法。 在 web.xml 文件中使用和元素对编写的filter类进行注册，并设置它所能拦截的资源。 web.xml配置各节点介绍： 12345678910111213141516&lt;filter-name&gt;用于为过滤器指定一个名字，该元素的内容不能为空。&lt;filter-class&gt;元素用于指定过滤器的完整的限定类名。&lt;init-param&gt;元素用于为过滤器指定初始化参数，它的子元素&lt;param-name&gt;指定参数的名字，&lt;param-value&gt;指定参数的值。在过滤器中，可以使用FilterConfig接口对象来访问初始化参数。&lt;filter-mapping&gt;元素用于设置一个 Filter 所负责拦截的资源。一个Filter拦截的资源可通过两种方式来指定：Servlet 名称和资源访问的请求路径&lt;filter-name&gt;子元素用于设置filter的注册名称。该值必须是在&lt;filter&gt;元素中声明过的过滤器的名字&lt;url-pattern&gt;设置 filter 所拦截的请求路径(过滤器关联的URL样式)&lt;servlet-name&gt;指定过滤器所拦截的Servlet名称。&lt;dispatcher&gt;指定过滤器所拦截的资源被 Servlet 容器调用的方式，可以是REQUEST,INCLUDE,FORWARD和ERROR之一，默认REQUEST。用户可以设置多个&lt;dispatcher&gt; 子元素用来指定 Filter 对资源的多种调用方式进行拦截。&lt;dispatcher&gt; 子元素可以设置的值及其意义：REQUEST：当用户直接访问页面时，Web容器将会调用过滤器。如果目标资源是通过RequestDispatcher的include()或forward()方法访问时，那么该过滤器就不会被调用。INCLUDE：如果目标资源是通过RequestDispatcher的include()方法访问时，那么该过滤器将被调用。除此之外，该过滤器不会被调用。FORWARD：如果目标资源是通过RequestDispatcher的forward()方法访问时，那么该过滤器将被调用，除此之外，该过滤器不会被调用。ERROR：如果目标资源是通过声明式异常处理机制调用时，那么该过滤器将被调用。除此之外，过滤器不会被调用。 4、Filter链 在一个web应用中，可以开发编写多个Filter，这些Filter组合起来称之为一个Filter链。 web服务器根据Filter在web.xml文件中的注册顺序，决定先调用哪个Filter，当第一个Filter的doFilter方法被调用时，web服务器会创建一个代表Filter链的FilterChain对象传递给该方法。在doFilter方法中，开发人员如果调用了FilterChain对象的doFilter方法，则web服务器会检查FilterChain对象中是否还有filter，如果有，则调用第2个filter，如果没有，则调用目标资源。 多个过滤器执行顺序 一个目标资源可以指定多个过滤器，过滤器的执行顺序是在web.xml文件中的部署顺序： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;myFilter1&lt;/filter-name&gt; &lt;filter-class&gt;cn.cloud.filter.MyFilter1&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myFilter1&lt;/filter-name&gt; &lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;myFilter2&lt;/filter-name&gt; &lt;filter-class&gt;cn. cloud.filter.MyFilter2&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myFilter2&lt;/filter-name&gt; &lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; MyFilter1 12345678public class MyFilter1 extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"filter1 start...\"); chain.doFilter(request, response);//放行，执行MyFilter2的doFilter()方法 System.out.println(\"filter1 end...\"); &#125;&#125; MyFilter2 12345678public class MyFilter2 extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"filter2 start...\"); chain.doFilter(request, response);//放行，执行目标资源 System.out.println(\"filter2 end...\"); &#125;&#125; 12345&lt;body&gt; This is my JSP page. &lt;br&gt; &lt;h1&gt;index.jsp&lt;/h1&gt; &lt;%System.out.println(\"index.jsp\"); %&gt; &lt;/body&gt; 当有用户访问index.jsp页面时，输出结果如下： 12345filter1 start...filter2 start...index.jspfilter2 end...filter1 end... 5、Filter的生命周期 1public void init(FilterConfig filterConfig) throws ServletException;//初始化 和我们编写的Servlet程序一样，Filter的创建和销毁由WEB服务器负责。 web 应用程序启动时，web 服务器将创建Filter 的实例对象，并调用其init方法，读取web.xml配置，完成对象的初始化功能，从而为后续的用户请求作好拦截的准备工作（filter对象只会创建一次，init方法也只会执行一次）。开发人员通过init方法的参数，可获得代表当前filter配置信息的FilterConfig对象。 1public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException;//拦截请求 这个方法完成实际的过滤操作。当客户请求访问与过滤器关联的URL（目标资源）的时候，Servlet过滤器将先执行doFilter方法。FilterChain参数用于访问后续过滤器。 1public void destroy();//销毁 服务器在创建Filter对象之后，把Filter放到缓存中一直使用（会驻留在内存），通常不会销毁它，当web应用移除或服务器停止时才销毁Filter对象。在Web容器卸载 Filter 对象之前被调用。该方法在Filter的生命周期中仅执行一次。在这个方法中，可以释放过滤器使用的资源。 6、FilterConfig接口 用户在配置filter时，可以使用为filter配置一些初始化参数，当web容器实例化Filter对象，调用其init方法时，会把封装了filter初始化参数的filterConfig对象传递进来。因此开发人员在编写filter时，通过filterConfig对象的方法，就可获得以下内容： 1234String getFilterName();//得到filter的名称；与&lt;filter-name&gt;元素对应。String getInitParameter(String name);//返回在部署描述中指定名称的初始化参数的值。如果不存在返回null，与&lt;init-param&gt;元素对应.Enumeration getInitParameterNames();//返回过滤器的所有初始化参数的名字的枚举集合。public ServletContext getServletContext();//返回Servlet上下文对象的引用。 7、FilterChain doFilter()方法的参数中有一个类型为FilterChain的参数，它只有一个方法：doFilter(ServletRequest,ServletResponse) doFilter() 方法的放行，让请求流访问目标资源！其实调用该方法的意思是，当前 Filter 放行了，但不代表其他过滤器也放行。一个目标资源上，可能部署了多个过滤器，所以调用 FilterChain 类的 doFilter() 方法表示的是执行下一个过滤器的 doFilter() 方法，或者是执行目标资源！ 如果当前过滤器是最后一个过滤器，那么调用 chain.doFilter() 方法表示执行目标资源，而不是最后一个过滤器，那么 chain.doFilter() 表示执行下一个过滤器的 doFilter() 方法。 8、过滤器的应用场景 执行目标资源之前做预处理工作，例如设置编码，这种试通常都会放行，只是在目标资源执行之前做一些准备工作； 通过条件判断是否放行，例如校验当前用户是否已经登录，或者用户IP是否已经被禁用； 在目标资源执行后，做一些后续的特殊处理工作，例如把目标资源输出的数据进行处理 设置目标资源 在web.xml文件中部署Filter时，可以通过“*”来执行目标资源： 1234&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 特性与Servlet完全相同！通过这一特性，可以在用户访问敏感资源时，执行过滤器，例如：/admin/*，可以把所有管理员才能访问的资源放到/admin路径下，这时可以通过过滤器来校验用户身份。 还可以为指定目标资源为某个Servlet，例如： 12345678910111213141516&lt;servlet&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.cloud.servlet.MyServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/abc&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;filter-class&gt;cn.cloud.filter.MyFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;/filter-mapping&gt; 当用户访问http://localhost:8080/filtertest/abc时，会执行名字为myservlet的Servlet，这时会执行过滤器。 9、四种拦截方式 写一个过滤器，指定过滤的资源为b.jsp，然后在浏览器中直接访问b.jsp，会发现过滤器执行了.但是，当在a.jsp中request.getRequestDispathcer(“/b.jsp”).forward(request,response)时，就不会再执行过滤器了！也就是说，默认情况下，只能直接访问目标资源才会执行过滤器，而forward执行目标资源，不会执行过滤器！ 12345678public class MyFilter extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"myfilter...\"); chain.doFilter(request, response); &#125;&#125; 12345678&lt;filter&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;filter-class&gt;cn.itcast.filter.MyFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 123&lt;body&gt; &lt;h1&gt;b.jsp&lt;/h1&gt; &lt;/body&gt; 12345&lt;h1&gt;a.jsp&lt;/h1&gt; &lt;% request.getRequestDispatcher(\"/b.jsp\").forward(request, response); %&gt; &lt;/body&gt; 在浏览器输入： http://localhost:8080/filtertest/b.jsp 直接访问b.jsp时，会执行过滤器内容； http://localhost:8080/filtertest/a.jsp 访问a.jsp，但a.jsp会forward到b.jsp，这时就不会执行过滤器！ 过滤器有四种拦截方式！分别是：REQUEST、FORWARD、INCLUDE、ERROR。 REQUEST：直接访问目标资源时执行过滤器。包括：在地址栏中直接访问、表单提交、超链接、重定向，只要在地址栏中可以看到目标资源的路径，就是REQUEST FORWARD：转发访问执行过滤器。包括RequestDispatcher#forward()方法、标签都是转发访问 INCLUDE：包含访问执行过滤器。包括RequestDispatcher#include()方法、标签都是包含访问 ERROR：当目标资源在web.xml中配置为中时，并且真的出现了异常，转发到目标资源时，会执行过滤器。 可以在中添加0~n个子元素，来说明当前访问的拦截方式。 如： 123456&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt; &lt;/filter-mapping&gt; 最为常用的就是REQUEST和FORWARD两种拦截方式，而INCLUDE和ERROR都比较少用！其中INCLUDE比较好理解，ERROR方式不易理解，下面给出ERROR拦截方式的例子： 123456789&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt; &lt;/filter-mapping&gt; &lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/b.jsp&lt;/location&gt; &lt;/error-page&gt; 1234567&lt;body&gt; &lt;h1&gt;a.jsp&lt;/h1&gt; &lt;% if(true) throw new RuntimeException(\"嘻嘻~\"); %&gt; &lt;/body&gt; 10、Filter使用案例 1、使用Filter验证用户登录安全控制 前段时间参与维护一个项目，用户退出系统后，再去地址栏访问历史，根据url，仍然能够进入系统响应页面。我去检查一下发现对请求未进行过滤验证用户登录。添加一个filter搞定问题！ 先在web.xml配置 123456789101112131415161718192021222324&lt;filter&gt; &lt;filter-name&gt;SessionFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.action.login.SessionFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;logonStrings&lt;/param-name&gt;&lt;!-- 对登录页面不进行过滤 --&gt; &lt;param-value&gt;/project/index.jsp;login.do&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;includeStrings&lt;/param-name&gt;&lt;!-- 只对指定过滤参数后缀进行过滤 --&gt; &lt;param-value&gt;.do;.jsp&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;redirectPath&lt;/param-name&gt;&lt;!-- 未通过跳转到登录界面 --&gt; &lt;param-value&gt;/index.jsp&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;disabletestfilter&lt;/param-name&gt;&lt;!-- Y:过滤无效 --&gt; &lt;param-value&gt;N&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;SessionFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 接着编写FilterServlet： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.action.login;import java.io.IOException;import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpServletResponseWrapper;/** * 判断用户是否登录,未登录则退出系统 */public class SessionFilter implements Filter &#123; public FilterConfig config; public void destroy() &#123; this.config = null; &#125; public static boolean isContains(String container, String[] regx) &#123; boolean result = false; for (int i = 0; i &lt; regx.length; i++) &#123; if (container.indexOf(regx[i]) != -1) &#123; return true; &#125; &#125; return result; &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest hrequest = (HttpServletRequest)request; HttpServletResponseWrapper wrapper = new HttpServletResponseWrapper((HttpServletResponse) response); String logonStrings = config.getInitParameter(\"logonStrings\"); // 登录登陆页面 String includeStrings = config.getInitParameter(\"includeStrings\"); // 过滤资源后缀参数 String redirectPath = hrequest.getContextPath() + config.getInitParameter(\"redirectPath\");// 没有登陆转向页面 String disabletestfilter = config.getInitParameter(\"disabletestfilter\");// 过滤器是否有效 if (disabletestfilter.toUpperCase().equals(\"Y\")) &#123; // 过滤无效 chain.doFilter(request, response); return; &#125; String[] logonList = logonStrings.split(\";\"); String[] includeList = includeStrings.split(\";\"); if (!this.isContains(hrequest.getRequestURI(), includeList)) &#123;// 只对指定过滤参数后缀进行过滤 chain.doFilter(request, response); return; &#125; if (this.isContains(hrequest.getRequestURI(), logonList)) &#123;// 对登录页面不进行过滤 chain.doFilter(request, response); return; &#125; String user = ( String ) hrequest.getSession().getAttribute(\"useronly\");//判断用户是否登录 if (user == null) &#123; wrapper.sendRedirect(redirectPath); return; &#125;else &#123; chain.doFilter(request, response); return; &#125; &#125; public void init(FilterConfig filterConfig) throws ServletException &#123; config = filterConfig; &#125;&#125; 这样既可完成对用户所有请求，均要经过这个Filter进行验证用户登录。 2、防止中文乱码过滤器 项目使用spring框架时。当前台JSP页面和JAVA代码中使用了不同的字符集进行编码的时候就会出现表单提交的数据或者上传/下载中文名称文件出现乱码的问题，那就可以使用这个过滤器。 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt;&lt;!--用来指定一个具体的字符集--&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt;&lt;!--true：无论request是否指定了字符集，都是用encoding；false：如果request已指定一个字符集，则不使用encoding--&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 3、Spring+Hibernate的OpenSessionInViewFilter控制session的开关 当 hibernate+spring 配合使用的时候，如果设置了lazy=true（延迟加载）,那么在读取数据的时候，当读取了父数据后，hibernate 会自动关闭 session，这样，当要使用与之关联数据、子数据的时候，系统会抛出lazyinit的错误，这时就需要使用 spring 提供的 OpenSessionInViewFilter 过滤器。 OpenSessionInViewFilter主要是保持 Session 状态直到 request 将全部页面发送到客户端，直到请求结束后才关闭 session，这样就可以解决延迟加载带来的问题。 注意：OpenSessionInViewFilter 配置要写在struts2的配置前面。因为 tomcat 容器在加载过滤器的时候是按照顺序加载的，如果配置文件先写的是 struts2 的过滤器配置，然后才是 OpenSessionInViewFilter 过滤器配置，所以加载的顺序导致，action 在获得数据的时候 session 并没有被 spring 管理。 1234567891011121314151617&lt;!-- lazy loading enabled in spring --&gt;&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.hibernate3.support.OpenSessionInViewFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt;&lt;!-- 可缺省。默认是从spring容器中找id为sessionFactory的bean，如果id不为sessionFactory，则需要配置如下，此处SessionFactory为spring容器中的bean。 --&gt; &lt;param-value&gt;sessionFactory&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;singleSession&lt;/param-name&gt;&lt;!-- singleSession默认为true,若设为false则等于没用OpenSessionInView --&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 4、Struts2的web.xml配置 项目中使用Struts2同样需要在web.xml配置过滤器，用来截取请求，转到Struts2的Action进行处理。 注意：如果在2.1.3以前的Struts2版本，过滤器使用org.apache.struts2.dispatcher.FilterDispatcher。否则使用org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter。从Struts2.1.3开始，将废弃ActionContextCleanUp过滤器，而在StrutsPrepareAndExecuteFilter过滤器中包含相应的功能。 三个初始化参数配置： config参数：指定要加载的配置文件。逗号分割。 actionPackages参数：指定Action类所在的包空间。逗号分割。 configProviders参数：自定义配置文件提供者，需要实现ConfigurationProvider接口类。逗号分割。 123456789&lt;!-- struts 2.x filter --&gt;&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt;","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Filter过滤器","slug":"Filter过滤器","permalink":"http://www.54tianzhisheng.cn/tags/Filter过滤器/"}]},{"title":"详细深入分析 Java ClassLoader 工作机制","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/详细深入分析 Java ClassLoader 工作机制/","text":"什么是 ClassLoader ？大家都知道，当我们写好一个 Java 程序之后，不是管是 C/S 还是 B/S 应用，都是由若干个 .class 文件组织而成的一个完整的 Java 应用程序，当程序在运行时，即会调用该程序的一个入口函数来调用系统的相关功能，而这些功能都被封装在不同的 class 文件当中，所以经常要从这个 class 文件中要调用另外一个 class 文件中的方法，如果另外一个文件不存在的，则会引发系统异常。而程序在启动的时候，并不会一次性加载程序所要用的所有class文件，而是根据程序的需要，通过Java的类加载机制（ClassLoader）来动态加载某个 class 文件到内存当中的，从而只有 class 文件被载入到了内存之后，才能被其它 class 所引用。所以 ClassLoader 就是用来动态加载 class 文件到内存当中用的。 ClassLoader 作用： 负责将 Class 加载到 JVM 中 审查每个类由谁加载（父优先的等级加载机制） 将 Class 字节码重新解析成 JVM 统一要求的对象格式 1、ClassLoader 类结构分析为了更好的理解类的加载机制，我们来深入研究一下 ClassLoader 和他的方法。 public abstract class ClassLoader ClassLoader类是一个抽象类，sun公司是这么解释这个类的： 1234567/** * A class loader is an object that is responsible for loading classes. The * class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to * locate or generate data that constitutes a definition for the class. A * typical strategy is to transform the name into a file name and then read a * &quot;class file&quot; of that name from a file system.**/ 大致意思如下： class loader 是一个负责加载 classes 的对象，ClassLoader 类是一个抽象类，需要给出类的二进制名称，class loader 尝试定位或者产生一个 class 的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 以下是 ClassLoader 常用到的几个方法及其重载方法： ClassLoader defineClass(byte[], int, int) 把字节数组 b中的内容转换成 Java 类，返回的结果是java.lang.Class类的实例。这个方法被声明为final的 findClass(String name) 查找名称为 name的类，返回的结果是java.lang.Class类的实例 loadClass(String name) 加载名称为 name的类，返回的结果是java.lang.Class类的实例 resolveClass(Class&lt;?&gt;) 链接指定的 Java 类 其中 defineClass 方法用来将 byte 字节流解析成 JVM 能够识别的 Class 对象，有了这个方法意味着我们不仅仅可以通过 class 文件实例化对象，还可以通过其他方式实例化对象，如果我们通过网络接收到一个类的字节码，拿到这个字节码流直接创建类的 Class 对象形式实例化对象。如果直接调用这个方法生成类的 Class 对象，这个类的 Class 对象还没有 resolve ，这个 resolve 将会在这个对象真正实例化时才进行。 接下来我们看loadClass方法的实现方式： 123456789101112131415161718192021222324252627282930313233343536protected Class&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 该方法大概意思： 使用指定的二进制名称来加载类，这个方法的默认实现按照以下顺序查找类： 调用findLoadedClass(String) 方法检查这个类是否被加载过 使用父加载器调用 loadClass(String) 方法，如果父加载器为 Null，类加载器装载虚拟机内置的加载器调用 findClass(String) 方法装载类， 如果，按照以上的步骤成功的找到对应的类，并且该方法接收的 resolve 参数的值为 true,那么就调用resolveClass(Class) 方法来处理类。 ClassLoader 的子类最好覆盖 findClass(String) 而不是这个方法。 除非被重写，这个方法默认在整个装载过程中都是同步的（线程安全的）。 2、ClassLoader 的等级加载机制Java默认提供的三个ClassLoader BootStrap ClassLoader：称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： 12345678910public class BootStrapTest&#123; public static void main(String[] args) &#123; URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &#125;&#125; 以下内容是上述程序从本机JDK环境所获得的结果： 其实上述结果也是通过查找 sun.boot.class.path 这个系统属性所得知的。 1System.out.println(System.getProperty(\"sun.boot.class.path\")); 1打印结果：C:\\Java\\jdk1.8.0_60\\jre\\lib\\resources.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\rt.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\sunrsasign.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jsse.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jce.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\charsets.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jfr.jar;C:\\Java\\jdk1.8.0_60\\jre\\classes Extension ClassLoader：称为扩展类加载器，负责加载Java的扩展类库，Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。默认加载JAVA_HOME/jre/lib/ext/目下的所有jar。 App ClassLoader：称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。 ​ 除了系统提供的类加载器以外，开发人员可以通过继承java.lang.ClassLoader类的方式实现自己的类加载器，以满足一些特殊的需求。 除了引导类加载器之外，所有的类加载器都有一个父类加载器。 给出的 getParent()方法可以得到。对于系统提供的类加载器来说，系统类加载器的父类加载器是扩展类加载器，而扩展类加载器的父类加载器是引导类加载器；对于开发人员编写的类加载器来说，其父类加载器是加载此类加载器 Java 类的类加载器。因为类加载器 Java 类如同其它的 Java 类一样，也是要由类加载器来加载的。一般来说，开发人员编写的类加载器的父类加载器是系统类加载器。类加载器通过这种方式组织起来，形成树状结构。树的根节点就是引导类加载器。 ​ ClassLoader加载类的原理1. 原理介绍ClassLoader使用的是双亲委托模型来搜索类的，每个ClassLoader实例都有一个父类加载器的引用（不是继承的关系，是一个包含的关系），虚拟机内置的类加载器（Bootstrap ClassLoader）本身没有父类加载器，但可以用作其它ClassLoader实例的的父类加载器。当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象。 2、为什么要使用双亲委托这种模型呢？因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要 ClassLoader再加载一次。考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义的类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时就被引导类加载器（Bootstrcp ClassLoader）加载，所以用户自定义的ClassLoader永远也无法加载一个自己写的String，除非你改变JDK中ClassLoader搜索类的默认算法。 3、 但是JVM在搜索类的时候，又是如何判定两个class是相同的呢？JVM在判定两个class是否相同时，不仅要判断两个类名是否相同，而且要判断是否由同一个类加载器实例加载的。只有两者同时满足的情况下，JVM才认为这两个class是相同的。就算两个class是同一份class字节码，如果被两个不同的ClassLoader实例所加载，JVM也会认为它们是两个不同class。比如网络上的一个Java类org.classloader.simple.NetClassLoaderSimple，javac编译之后生成字节码文件NetClassLoaderSimple.class，ClassLoaderA和ClassLoaderB这两个类加载器并读取了NetClassLoaderSimple.class文件，并分别定义出了java.lang.Class实例来表示这个类，对于JVM来说，它们是两个不同的实例对象，但它们确实是同一份字节码文件，如果试图将这个Class实例生成具体的对象进行转换时，就会抛运行时异常java.lang.ClassCaseException，提示这是两个不同的类型。现在通过实例来验证上述所描述的是否正确：1）、在web服务器上建一个org.classloader.simple.NetClassLoaderSimple.java类 1234567public class NetClassLoaderSimple&#123; private NetClassLoaderSimple instance; public void setNetClassLoaderSimple(Object object)&#123; this.instance = (NetClassLoaderSimple)object; &#125;&#125; org.classloader.simple.NetClassLoaderSimple类的setNetClassLoaderSimple方法接收一个Object类型参数，并将它强制转换成org.classloader.simple.NetClassLoaderSimple类型。 2）、测试两个class是否相同 NetWorkClassLoader.java 12345678910111213141516171819202122package classloader;public class NewworkClassLoaderTest &#123; public static void main(String[] args) &#123; try &#123; //测试加载网络中的class文件 String rootUrl = &quot;http://localhost:8080/httpweb/classes&quot;; String className = &quot;org.classloader.simple.NetClassLoaderSimple&quot;; NetworkClassLoader ncl1 = new NetworkClassLoader(rootUrl); NetworkClassLoader ncl2 = new NetworkClassLoader(rootUrl); Class&lt;?&gt; clazz1 = ncl1.loadClass(className); Class&lt;?&gt; clazz2 = ncl2.loadClass(className); Object obj1 = clazz1.newInstance(); Object obj2 = clazz2.newInstance(); clazz1.getMethod(&quot;setNetClassLoaderSimple&quot;, Object.class).invoke(obj1, obj2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 首先获得网络上一个class文件的二进制名称，然后通过自定义的类加载器NetworkClassLoader创建两个实例，并根据网络地址分别加载这份class，并得到这两个ClassLoader实例加载后生成的Class实例clazz1和clazz2，最后将这两个Class实例分别生成具体的实例对象obj1和obj2，再通过反射调用clazz1中的setNetClassLoaderSimple方法。 3）、查看测试结果 结论：从结果中可以看出，运行时抛出了java.lang.ClassCastException异常。虽然两个对象obj1和 obj2的类的名字相同，但是这两个类是由不同的类加载器实例来加载的，所以JVM认为它们就是两个不同的类。 了解了这一点之后，就可以理解代理模式的设计动机了。代理模式是为了保证 Java 核心库的类型安全。所有 Java 应用都至少需要引用 java.lang.Object类，也就是说在运行的时候，java.lang.Object这个类需要被加载到 Java 虚拟机中。如果这个加载过程由 Java 应用自己的类加载器来完成的话，很可能就存在多个版本的 java.lang.Object类，而且这些类之间是不兼容的。通过代理模式，对于 Java 核心库的类的加载工作由引导类加载器来统一完成，保证了Java 应用所使用的都是同一个版本的 Java 核心库的类，是互相兼容的。 不同的类加载器为相同名称的类创建了额外的名称空间。相同名称的类可以并存在 Java 虚拟机中，只需要用不同的类加载器来加载它们即可。不同类加载器加载的类之间是不兼容的，这就相当于在 Java 虚拟机内部创建了一个个相互隔离的 Java 类空间。 ClassLoader的体系架构： 类加载器的树状组织结构测试一： 1234567891011public class ClassLoaderTree&#123; public static void main(String[] args) &#123; ClassLoader loader = ClassLoaderTree.class.getClassLoader(); while (loader!=null)&#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; System.out.println(loader); &#125;&#125; 每个 Java 类都维护着一个指向定义它的类加载器的引用，通过 getClassLoader()方法就可以获取到此引用。代码中通过递归调用 getParent()方法来输出全部的父类加载器。 结果是： 第一个输出的是ClassLoaderTree类的类加载器，即系统类加载器。它是sun.misc.Launcher$AppClassLoader类的实例；第二个输出的是扩展类加载器，是sun.misc.Launcher$ExtClassLoader类的实例。需要注意的是这里并没有输出引导类加载器，这是由于有些 JDK 的实现对于父类加载器是引导类加载器的情况，getParent()方法返回 null。第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试二： 将ClassLoaderTree.class打包成ClassLoaderTree.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 此处我在 IDEA 中的运行结果还和上面的一样，与文章 深入分析Java ClassLoader原理 中的有差距，具体原因未弄清楚，还希望读者能够亲自测试。 那文章中的结果是： 打印结果分析：为什么第一行的结果是ExtClassLoader呢？ 因为 ClassLoader 的委托模型机制，当我们要用 ClassLoaderTest.class 这个类的时候，AppClassLoader 在试图加载之前，先委托给 Bootstrcp ClassLoader，Bootstracp ClassLoader 发现自己没找到，它就告诉 ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后 Extension ClassLoader 拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar 这样一个文件中包含 ClassLoaderTest.class 这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的 Class 实例对象，最后把这个实例返回。所以 ClassLoaderTest.class 的类加载器是 ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 JVM加载class文件的两种方法； 隐式加载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 显式加载， 通过class.forname()、this.getClass.getClassLoader().loadClass()等方法显式加载需要的类，或者我们自己实现的 ClassLoader 的 findlass() 方法。 下面介绍下 class.forName的加载类方法： Class.forName是一个静态方法，同样可以用来加载类。该方法有两种形式：Class.forName(String name,boolean initialize, ClassLoader loader)和Class.forName(String className)。第一种形式的参数 name表示的是类的全名；initialize表示是否初始化类；loader表示加载时使用的类加载器。第二种形式则相当于设置了参数 initialize的值为 true，loader的值为当前类的类加载器。Class.forName的一个很常见的用法是在加载数据库驱动的时候。如Class.forName(&quot;org.apache.derby.jdbc.EmbeddedDriver&quot;)用来加载 Apache Derby 数据库的驱动。 类加载的动态性体现：一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现。 3、如何加载 class 文件 第一阶段找到 .class 文件并把这个文件包含的字节码加载到内存中。 第二阶段中分三步，字节码验证；class 类数据结构分析及相应的内存分配；最后的符号表的链接。 第三阶段是类中静态属性和初始化赋值，以及静态块的执行等。 3.1 、加载字节码到内存。。 3.2 、验证与分析 字节码验证，类装入器对于类的字节码要做很多检测，以确保格式正确，行为正确。 类装备，准备代表每个类中定义的字段、方法和实现接口所必须的数据结构。 解析，装入器装入类所引用的其他所有类。 4、常见加载类错误分析4.1 、 ClassNotFoundExecptionClassNotFoundExecption 异常是平常碰到的最多的。这个异常通常发生在显示加载类的时候。 12345678910public class ClassNotFoundExceptionTest&#123; public static void main(String[] args) &#123; try &#123; Class.forName(&quot;NotFoundClass&quot;); &#125;catch (ClassNotFoundException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 显示加载一个类通常有： 通过类 Class 中的 forName() 方法 通过类 ClassLoader 中的 loadClass() 方法 通过类 ClassLoader 中的 findSystemClass() 方法 出现这种错误其实就是当 JVM 要加载指定文件的字节码到内存时，并没有找到这个文件对应的字节码，也就是这个文件并不存在。解决方法就是检查在当前的 classpath 目录下有没有指定的文件。 4.2 、 NoClassDefFoundError在JavaDoc中对NoClassDefFoundError的产生可能的情况就是使用new关键字、属性引用某个类、继承了某个接口或者类，以及方法的某个参数中引用了某个类，这时就会触发JVM或者类加载器实例尝试加载类型的定义，但是该定义却没有找到，影响了执行路径。换句话说，在编译时这个类是能够被找到的，但是在执行时却没有找到。 解决这个错误的方法就是确保每个类引用的类都在当前的classpath下面。 4.3 、 UnsatisfiedLinkError该错误通常是在 JVM 启动的时候，如果 JVM 中的某个 lib 删除了，就有可能报这个错误。 12345678910public class UnsatisfiedLinkErrorTest&#123; public native void nativeMethod(); static &#123; System.loadLibrary(\"NoLib\"); &#125; public static void main(String[] args) &#123; new UnsatisfiedLinkErrorTest().nativeMethod(); //解析native标识的方法时JVM找不到对应的库文件 &#125;&#125; 4.4 、 ClassCastException该错误通常出现强制类型转换时出现这个错误。 123456789101112public class ClassCastExceptionTest&#123; public static Map m = new HashMap()&#123; &#123; put(\"a\", \"2\"); &#125; &#125;; public static void main(String[] args) &#123; Integer integer = (Integer) m.get(\"a\"); //将m强制转换成Integer类型 System.out.println(integer); &#125;&#125; 注意：JVM 在做类型转换时的规则： 对于普通对象，对象必须是目标类的实例或目标类的子类的实例。如果目标类是接口，那么会把它当作实现了该接口的一个子类。 对于数组类型，目标类必须是数组类型或 java.lang.Object、java.lang.Cloneable、java.io.Serializable。 如果不满足上面的规则，JVM 就会报错，有两种方式可避免错误： 在容器类型中显式的指明这个容器所包含的对象类型。 先通过 instanceof 检查是不是目标类型，然后再进行强制类型的转换。 上面代码中改成如下就可以避免错误了： 4.5 、 ExceptionInInitializerError12345678910public class ExceptionInInitializerErrorTest&#123; public static Map m = new HashMap()&#123;&#123; m.put(\"a\", \"2\"); &#125;&#125;; public static void main(String[] args) &#123; Integer integer = (Integer) m.get(\"a\"); System.out.println(integer); &#125;&#125; 在初始化这个类时，给静态属性 m 赋值时出现了异常导致抛出错误 ExceptionInInitializerError。 4.6 NoSuchMethodErrorNoSuchMethodError代表这个类型确实存在，但是一个不正确的版本被加载了。为了解决这个问题我们可以使用 ‘­verbose:class’ 来判断该JVM加载的到底是哪个版本。 4.7 LinkageError有时候事情会变得更糟，和 ClassCastException 本质一样，加载自不同位置的相同类在同一段逻辑（比如：方法）中交互时，会出现 LinkageError 。 LinkageError 需要观察哪个类被不同的类加载器加载了，在哪个方法或者调用处发生（交汇）的，然后才能想解决方法，解决方法无外乎两种。第一，还是不同的类加载器加载，但是相互不再交汇影响，这里需要针对发生问题的地方做一些改动，比如更换实现方式，避免出现上述问题；第二，冲突的类需要由一个Parent类加载器进行加载。LinkageError 和ClassCastException 本质是一样的，加载自不同类加载器的类型，在同一个类的方法或者调用中出现，如果有转型操作那么就会抛 ClassCastException ，如果是直接的方法调用处的参数或者返回值解析，那么就会产生 LinkageError 。 5、常用的 ClassLoader 分析。。参见书籍《深入分析Java Web技术内幕》 6、如何实现自己的 ClassLoaderClassLoader 能够完成的事情有以下情况： 在自定义路径下查找自定义的class类文件。 对我们自己要加载的类做特殊处理。 可以定义类的实现机制。 虽然在绝大多数情况下，系统默认提供的类加载器实现已经可以满足需求。但是在某些情况下，您还是需要为应用开发出自己的类加载器。比如您的应用通过网络来传输 Java 类的字节代码，为了保证安全性，这些字节代码经过了加密处理。这个时候您就需要自己的类加载器来从某个网络地址上读取加密后的字节代码，接着进行解密和验证，最后定义出要在 Java 虚拟机中运行的类来。 定义自已的类加载器分为两步：1、继承java.lang.ClassLoader2、重写父类的findClass方法 6.1 、文件系统类加载器加载存储在文件系统上的 Java 字节代码。 123456789101112131415161718192021222324252627282930313233343536373839404142public class FileSystemClassLoader extends ClassLoader&#123; private String rootDir; public FileSystemClassLoader(String rootDir)&#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null)&#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead = 0; while ((bytesNumRead = ins.read(buffer)) != -1)&#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + \".class\"; &#125;&#125; 类 FileSystemClassLoader继承自类java.lang.ClassLoader。java.lang.ClassLoader类的方法loadClass()封装了前面提到的代理模式的实现。该方法会首先调用 findLoadedClass()方法来检查该类是否已经被加载过；如果没有加载过的话，会调用父类加载器的loadClass()方法来尝试加载该类；如果父类加载器无法加载该类的话，就调用 findClass()方法来查找该类。因此，为了保证类加载器都正确实现代理模式，在开发自己的类加载器时，最好不要覆写 loadClass()方法，而是覆写findClass()方法。 类 FileSystemClassLoader的 findClass()方法首先根据类的全名在硬盘上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass()方法来把这些字节代码转换成 java.lang.Class类的实例。 6.2 、 网络类加载器一个网络类加载器来说明如何通过类加载器来实现组件的动态更新。即基本的场景是：Java 字节代码（.class）文件存放在服务器上，客户端通过网络的方式获取字节代码并执行。当有版本更新的时候，只需要替换掉服务器上保存的文件即可。通过类加载器可以比较简单的实现这种需求。 类 NetworkClassLoader 负责通过网络下载 Java 类字节代码并定义出 Java 类。它的实现与FileSystemClassLoader 类似。在通过 NetworkClassLoader 加载了某个版本的类之后，一般有两种做法来使用它。第一种做法是使用 Java 反射 API。另外一种做法是使用接口。需要注意的是，并不能直接在客户端代码中引用从服务器上下载的类，因为客户端代码的类加载器找不到这些类。使用 Java 反射 API 可以直接调用 Java 类的方法。而使用接口的做法则是把接口的类放在客户端中，从服务器上加载实现此接口的不同版本的类。在客户端通过相同的接口来使用这些实现类。 网络类加载器的代码：ClassLoader 7、类加载器与Web容器对于运行在 Java EE™容器中的 Web 应用来说，类加载器的实现方式与一般的 Java 应用有所不同。不同的 Web 容器的实现方式也会有所不同。以 Apache Tomcat 来说，每个Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 绝大多数情况下，Web 应用的开发人员不需要考虑与类加载器相关的细节。下面给出几条简单的原则： 每个 Web 应用自己的 Java 类文件和使用的库的 jar 包，分别放在 WEB-INF/classes和 WEB-INF/lib目录下面。 多个应用共享的 Java 类文件和 jar 包，分别放在 Web 容器指定的由所有 Web 应用共享的目录下面。 当出现找不到类的错误时，检查当前类的类加载器和当前线程的上下文类加载器是否正确 8、总结本篇文章详细深入的介绍了 ClassLoader 的工作机制，还写了如何自己实现所需的 ClassLoader 。 参考资料1、深度分析 Java 的 ClassLoader 机制（源码级别） 2、深入浅出ClassLoader 3、深入探讨 Java 类加载器 4、深入分析Java ClassLoader原理 5、《深入分析 Java Web 技术内幕》修订版 —— 深入分析 ClassLoader 工作机制","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://www.54tianzhisheng.cn/tags/JVM/"},{"name":"类加载机制","slug":"类加载机制","permalink":"http://www.54tianzhisheng.cn/tags/类加载机制/"}]},{"title":"通过项目逐步深入了解Spring MVC（一）","date":"2017-06-15T16:00:00.000Z","path":"2017/06/16/通过项目逐步深入了解Spring MVC（一）/","text":"相关阅读：本文档和项目代码地址：https://github.com/zhisheng17/springmvc 了解 Spring： Spring 官网：http://spring.io/ 一个好的东西一般都会有一个好的文档解释说明，如果你英语还行，建议还是看官方文档。 Spring MVC基础知识什么是Spring MVC？ Spring MVC框架原理（掌握） ​ 前端控制器、处理器映射器、处理器适配器、试图解析器Spring MVC 入门程序 ​ 目的：对前端控制器、处理器映射器、处理器适配器、试图解析器学习 ​ 非注解的处理器映射器、处理器适配器 ​ 注解的处理器映射器、处理器适配器（掌握） Spring MVC 和 Mybatis 整合（掌握） Spring MVC 注解开发：（掌握） ​ 常用的注解学习 ​ 参数绑定（简单类型，pojo类型、集合类型） ​ 自定义的参数绑定（掌握） Spring MVC 和 Struts2区别 Spring MVC高级应用参数绑定（集合类型） 数据回显 上传图片 json 数据交互 RESTful 支持 拦截器 Spring MVC 框架什么是Spring MVC？springmvc是spring框架的一个模块，springmvc和spring无需通过中间整合层进行整合。springmvc是一个基于mvc的web框架。 Web MVCMVC 设计模式在 B/S 系统下应用： 1、 用户发起request请求至控制器(Controller) 控制接收用户请求的数据，委托给模型进行处理 2、 控制器通过模型(Model)处理数据并得到处理结果 模型通常是指业务逻辑 3、 模型处理结果返回给控制器 4、 控制器将模型数据在视图(View)中展示 web中模型无法将数据直接在视图上显示，需要通过控制器完成。如果在C/S应用中模型是可以将数据在视图中展示的。 5、 控制器将视图response响应给用户 通过视图展示给用户要的数据或处理结果。 Spring MVC 框架 第一步：发起请求到前端控制器(DispatcherServlet) 第二步：前端控制器请求HandlerMapping查找 Handler 可以根据xml配置、注解进行查找 第三步：处理器映射器HandlerMapping向前端控制器返回Handler 第四步：前端控制器调用处理器适配器去执行Handler 第五步：处理器适配器去执行Handler 第六步：Handler执行完成给适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView ModelAndView是springmvc框架的一个底层对象，包括Model和view 第八步：前端控制器请求视图解析器去进行视图解析 根据逻辑视图名解析成真正的视图(jsp) 第九步：视图解析器向前端控制器返回View 第十步：前端控制器进行视图渲染 视图渲染将模型数据(在ModelAndView对象中)填充到request域 第十一步：前端控制器向用户响应结果 组件： 1、前端控制器DispatcherServlet（不需要程序员开发） 作用接收请求，响应结果，相当于转发器，中央处理器。 有了DispatcherServlet减少了其它组件之间的耦合度。 2、处理器映射器HandlerMapping(不需要程序员开发) 作用：根据请求的url查找Handler 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 4、处理器Handler(需要程序员开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler 5、视图解析器View resolver(不需要程序员开发) 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） 6、视图View(需要程序员开发jsp) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…）","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"通过项目逐步深入了解Mybatis（四）","date":"2017-06-14T16:00:00.000Z","path":"2017/06/15/通过项目逐步深入了解Mybatis(四)/","text":"相关阅读： 1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、通过项目逐步深入了解Mybatis&lt;二&gt; 3、通过项目逐步深入了解Mybatis&lt;三&gt; 本项目所有代码及文档都托管在 Github地址：https://github.com/zhisheng17/mybatis 延迟加载什么是延迟加载？resultMap可以实现高级映射（使用association、collection实现一对一及一对多映射），association、collection具备延迟加载功能。需求：如果查询订单并且关联查询用户信息。如果先查询订单信息即可满足要求，当我们需要查询用户信息时再查询用户信息。把对用户信息的按需去查询就是延迟加载。 延迟加载：先从单表查询、需要时再从关联表去关联查询，大大提高 数据库性能，因为查询单表要比关联查询多张表速度要快。 打开延迟加载开关在mybatis核心配置文件中配置： lazyLoadingEnabled、aggressiveLazyLoading 设置项 描述 允许值 默认值 lazyLoadingEnabled 全局性设置懒加载。如果设为‘false’，则所有相关联的都会被初始化加载。 true \\ false false aggressiveLazyLoading 当设置为‘true’的时候，懒加载的对象可能被任何懒属性全部加载。否则，每个属性都按需加载。 true \\ false true 1234&lt;settings&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"aggressiveLazyLoading\" value=\"false\"/&gt;&lt;/settings&gt; 使用 association 实现延迟加载需求：查询订单并且关联查询用户信息 Mapper.xml需要定义两个 mapper 的方法对应的 statement。 1、只查询订单信息 SQL 语句： select * from orders 在查询订单的 statement 中使用 association 去延迟加载（执行）下边的 statement (关联查询用户信息) 1234&lt;!--查询订单并且关联查询用户信息，关联用户信息需要通过 association 延迟加载--&gt; &lt;select id=\"findOrdersUserLazyLoading\" resultMap=\"OrdersUserLazyLoadingResultMap\"&gt; select * from orders &lt;/select&gt; 2、关联查询用户信息 通过上面查询订单信息中的 user_id 来关联查询用户信息。使用 UserMapper.xml 中的 findUserById SQL语句：select * from user where id = user_id 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;value&#125; &lt;/select&gt; 上边先去执行 findOrdersUserLazyLoading，当需要去查询用户的时候再去执行 findUserById ，通过 resultMap的定义将延迟加载执行配置起来。也就是通过 resultMap 去加载 UserMapper.xml 文件中的 select = findUserById 延迟加载的 resultMap1234567891011121314151617181920&lt;!--定义 关联用户信息（通过 association 延迟加载）的resultMap--&gt; &lt;resultMap id=\"OrdersUserLazyLoadingResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;!--对订单信息映射--&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!-- 实现对用户信息进行延迟加载 select：指定延迟加载需要执行的statement的id（是根据user_id查询用户信息的statement） 要使用userMapper.xml中findUserById完成根据用户id(user_id)用户信息的查询，如果findUserById不在本mapper中需要前边加namespace column：订单信息中关联用户信息查询的列，是user_id 关联查询的sql理解为： SELECT orders.*, (SELECT username FROM USER WHERE orders.user_id = user.id)username, (SELECT sex FROM USER WHERE orders.user_id = user.id)sex FROM orders--&gt; &lt;association property=\"user\" javaType=\"cn.zhisheng.mybatis.po.User\" select=\"cn.zhisheng.mybatis.mapper.UserMapper.findUserById\" column=\"user_id\"&gt; &lt;/association&gt; &lt;/resultMap&gt; OrderMapperCustom.java1public List&lt;Orders&gt; findOrdersUserLazyLoading() throws Exception; 测试代码：1234567891011121314151617@Test public void testFindOrdersUserLazyLoading() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //查询订单信息 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersUserLazyLoading(); //遍历所查询的的订单信息 for (Orders orders : list) &#123; //查询用户信息 User user = orders.getUser(); System.out.println(user); &#125; sqlSession.close(); &#125; 测试结果： 整个延迟加载的思路： 1、执行上边mapper方法（findOrdersUserLazyLoading），内部去调用cn.zhisheng.mybatis.mapper.OrdersMapperCustom 中的 findOrdersUserLazyLoading 只查询 orders 信息（单表）。 2、在程序中去遍历上一步骤查询出的 List，当我们调用 Orders 中的 getUser 方法时，开始进行延迟加载。 3、延迟加载，去调用 UserMapper.xml 中 findUserbyId 这个方法获取用户信息。 思考：不使用 mybatis 提供的 association 及 collection 中的延迟加载功能，如何实现延迟加载？？ 实现方法如下： 定义两个mapper方法： 1、查询订单列表 2、根据用户id查询用户信息 实现思路： 先去查询第一个mapper方法，获取订单信息列表 在程序中（service），按需去调用第二个mapper方法去查询用户信息。 总之： 使用延迟加载方法，先去查询 简单的 sql（最好单表，也可以关联查询），再去按需要加载关联查询的其它信息。 一对多延迟加载上面的那个案例是一对一延迟加载，那么如果我们想一对多进行延迟加载呢，其实也是很简单的。 一对多延迟加载的方法同一对一延迟加载，在collection标签中配置select内容。 延迟加载总结：作用： 当需要查询关联信息时再去数据库查询，默认不去关联查询，提高数据库性能。只有使用resultMap支持延迟加载设置。 场合： 当只有部分记录需要关联查询其它信息时，此时可按需延迟加载，需要关联查询时再向数据库发出sql，以提高数据库性能。 当全部需要关联查询信息时，此时不用延迟加载，直接将关联查询信息全部返回即可，可使用resultType或resultMap完成映射。 查询缓存什么是查询缓存？mybatis提供查询缓存，用于减轻数据压力，提高数据库性能。 mybaits提供一级缓存，和二级缓存。 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。 二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。 为什么要用缓存？ 如果缓存中有数据就不用从数据库中获取，大大提高系统性能。 一级缓存工作原理： 第一次发起查询用户id为1的用户信息，先去找缓存中是否有id为1的用户信息，如果没有，从数据库查询用户信息。 得到用户信息，将用户信息存储到一级缓存中。 如果sqlSession去执行commit操作（执行插入、更新、删除），清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。 第二次发起查询用户id为1的用户信息，先去找缓存中是否有id为1的用户信息，缓存中有，直接从缓存中获取用户信息。 一级缓存测试 Mybatis 默认支持一级缓存，不需要在配置文件中配置。 所以我们直接按照上面的步骤进行测试： 123456789101112131415//一级缓存测试 @Test public void testCache1() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建UserMapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //查询使用的是同一个session //第一次发起请求，查询Id 为1的用户信息 User user1 = userMapper.findUserById(1); System.out.println(user1); //第二次发起请求，查询Id 为1的用户信息 User user2 = userMapper.findUserById(1); System.out.println(user2); sqlSession.close(); &#125; 通过结果可以看出第二次没有发出sql查询请求， 所以我们需要在中间执行 commit 操作 123456789//如果sqlSession去执行commit操作（执行插入、更新、删除），// 清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。//更新user1的信息，user1.setUsername(\"李飞\");//user1.setSex(\"男\");//user1.setAddress(\"北京\");userMapper.updateUserById(user1);//提交事务,才会去清空缓存sqlSession.commit(); 测试 一级缓存应用 正式开发，是将 mybatis 和 spring 进行整合开发，事务控制在 service 中。 一个 service 方法中包括很多 mapper 方法调用。 service{ //开始执行时，开启事务，创建SqlSession对象 //第一次调用mapper的方法findUserById(1) //第二次调用mapper的方法findUserById(1)，从一级缓存中取数据 //方法结束，sqlSession关闭 } 如果是执行两次service调用查询相同的用户信息，不走一级缓存，因为session方法结束，sqlSession就关闭，一级缓存就清空。 二级缓存原理 首先开启mybatis的二级缓存。 sqlSession1去查询用户id为1的用户信息，查询到用户信息会将查询数据存储到二级缓存中。 如果SqlSession3去执行相同 mapper下sql，执行commit提交，清空该 mapper下的二级缓存区域的数据。 sqlSession2去查询用户id为1的用户信息，去缓存中找是否存在数据，如果存在直接从缓存中取出数据。 二级缓存与一级缓存区别，二级缓存的范围更大，多个sqlSession可以共享一个UserMapper的二级缓存区域。 UserMapper有一个二级缓存区域（按namespace分） ，其它mapper也有自己的二级缓存区域（按namespace分）。 每一个namespace的mapper都有一个二缓存区域，两个mapper的namespace如果相同，这两个mapper执行sql查询到数据将存在相同的二级缓存区域中。 开启二级缓存： mybaits的二级缓存是mapper范围级别，除了在SqlMapConfig.xml设置二级缓存的总开关，还要在具体的mapper.xml中开启二级缓存 在 SqlMapConfig.xml 开启二级开关 12&lt;!-- 开启二级缓存 --&gt;&lt;setting name=\"cacheEnabled\" value=\"true\"/&gt; 然后在你的 Mapper 映射文件中添加一行： ，表示此 mapper 开启二级缓存。 调用 pojo 类实现序列化接口： 二级缓存需要查询结果映射的pojo对象实现java.io.Serializable接口实现序列化和反序列化操作（因为二级缓存数据存储介质多种多样，在内存不一样），注意如果存在父类、成员pojo都需要实现序列化接口。 12public class Orders implements Serializablepublic class User implements Serializable 测试 12345678910111213141516171819202122232425262728293031323334//二级缓存测试 @Test public void testCache2() throws Exception &#123; SqlSession sqlSession1 = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); SqlSession sqlSession3 = sqlSessionFactory.openSession(); //创建UserMapper对象,mybatis自动生成代理对象 UserMapper userMapper1 = sqlSession1.getMapper(UserMapper.class); //sqlSession1 执行查询 写入缓存(第一次查询请求) User user1 = userMapper1.findUserById(1); System.out.println(user1); sqlSession1.close(); //sqlSession3 执行提交 清空缓存 UserMapper userMapper3 = sqlSession3.getMapper(UserMapper.class); User user3 = userMapper3.findUserById(1); user3.setSex(\"女\"); user3.setAddress(\"山东济南\"); user3.setUsername(\"崔建\"); userMapper3.updateUserById(user3); //提交事务，清空缓存 sqlSession3.commit(); sqlSession3.close(); //sqlSession2 执行查询(第二次查询请求) UserMapper userMapper2 = sqlSession2.getMapper(UserMapper.class); User user2 = userMapper2.findUserById(1); System.out.println(user2); sqlSession2.close(); &#125; 结果： useCache 配置 在 statement 中设置 useCache=false 可以禁用当前 select 语句的二级缓存，即每次查询都会发出sql去查询，默认情况是true，即该sql使用二级缓存。 1&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\" useCache=\"false\"&gt; 总结：针对每次查询都需要最新的数据sql，要设置成useCache=false，禁用二级缓存。 刷新缓存（清空缓存） 在mapper的同一个namespace中，如果有其它insert、update、delete操作数据后需要刷新缓存，如果不执行刷新缓存会出现脏读。 设置statement配置中的flushCache=”true” 属性，默认情况下为true即刷新缓存，如果改成false则不会刷新。使用缓存时如果手动修改数据库表中的查询数据会出现脏读。 如下： 1&lt;insert id=\"insetrUser\" parameterType=\"cn.zhisheng.mybatis.po.User\" flushCache=\"true\"&gt; 一般下执行完commit操作都需要刷新缓存，flushCache=true表示刷新缓存，这样可以避免数据库脏读。 Mybatis Cache参数flushInterval（刷新间隔）可以被设置为任意的正整数，而且它们代表一个合理的毫秒形式的时间段。默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新。 size（引用数目）可以被设置为任意正整数，要记住你缓存的对象数目和你运行环境的可用内存资源数目。默认值是1024。 readOnly（只读）属性可以被设置为true或false。只读的缓存会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。可读写的缓存会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。 如下例子： 1&lt;cache eviction=\"FIFO\" flushInterval=\"60000\" size=\"512\" readOnly=\"true\"/&gt; 这个更高级的配置创建了一个 FIFO 缓存,并每隔 60 秒刷新,存数结果对象或列表的 512 个引用,而且返回的对象被认为是只读的,因此在不同线程中的调用者之间修改它们会导致冲突。可用的收回策略有, 默认的是 LRU: LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 Mybatis 整合 ehcacheehcache 是一个分布式缓存框架。 分布缓存 我们系统为了提高系统并发，性能、一般对系统进行分布式部署（集群部署方式） 不使用分布缓存，缓存的数据在各各服务单独存储，不方便系统 开发。所以要使用分布式缓存对缓存数据进行集中管理。 mybatis无法实现分布式缓存，需要和其它分布式缓存框架进行整合。 整合方法 mybatis 提供了一个二级缓存 cache 接口（org.apache.ibatis.cache 下的 Cache），如果要实现自己的缓存逻辑，实现cache接口开发即可。 12345678910import java.util.concurrent.locks.ReadWriteLock;public interface Cache &#123; String getId(); void putObject(Object var1, Object var2); Object getObject(Object var1); Object removeObject(Object var1); void clear(); int getSize(); ReadWriteLock getReadWriteLock();&#125; mybatis和ehcache整合，mybatis 和 ehcache 整合包中提供了一个 cache 接口的实现类(org.apache.ibatis.cache.impl 下的 PerpetualCache)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package org.apache.ibatis.cache.impl;import java.util.HashMap;import java.util.Map;import java.util.concurrent.locks.ReadWriteLock;import org.apache.ibatis.cache.Cache;import org.apache.ibatis.cache.CacheException;public class PerpetualCache implements Cache &#123; private String id; private Map&lt;Object, Object&gt; cache = new HashMap(); public PerpetualCache(String id) &#123; this.id = id; &#125; public String getId() &#123; return this.id; &#125; public int getSize() &#123; return this.cache.size(); &#125; public void putObject(Object key, Object value) &#123; this.cache.put(key, value); &#125; public Object getObject(Object key) &#123; return this.cache.get(key); &#125; public Object removeObject(Object key) &#123; return this.cache.remove(key); &#125; public void clear() &#123; this.cache.clear(); &#125; public ReadWriteLock getReadWriteLock() &#123; return null; &#125; public boolean equals(Object o) &#123; if(this.getId() == null) &#123; throw new CacheException(\"Cache instances require an ID.\"); &#125; else if(this == o) &#123; return true; &#125; else if(!(o instanceof Cache)) &#123; return false; &#125; else &#123; Cache otherCache = (Cache)o; return this.getId().equals(otherCache.getId()); &#125; &#125; public int hashCode() &#123; if(this.getId() == null) &#123; throw new CacheException(\"Cache instances require an ID.\"); &#125; else &#123; return this.getId().hashCode(); &#125; &#125;&#125; 通过实现 Cache 接口可以实现 mybatis 缓存数据通过其它缓存数据库整合，mybatis 的特长是sql操作，缓存数据的管理不是 mybatis 的特长，为了提高缓存的性能将 mybatis 和第三方的缓存数据库整合，比如 ehcache、memcache、redis等。 引入依赖包 ehcache-core-2.6.5.jar 和 mybatis-ehcache-1.0.2.jar 引入缓存配置文件 classpath下添加：ehcache.xml 内容如下： 1234567891011121314&lt;ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://ehcache.org/ehcache.xsd\"&gt; &lt;diskStore path=\"C:\\JetBrains\\IDEAProject\\ehcache\" /&gt; &lt;defaultCache maxElementsInMemory=\"1000\" maxElementsOnDisk=\"10000000\" eternal=\"false\" overflowToDisk=\"false\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" diskExpiryThreadIntervalSeconds=\"120\" memoryStoreEvictionPolicy=\"LRU\"&gt; &lt;/defaultCache&gt;&lt;/ehcache&gt; 属性说明： diskStore：指定数据在磁盘中的存储位置。 defaultCache：当借助 CacheManager.add(“demoCache”) 创建Cache时，EhCache 便会采用指定的的管理策略 以下属性是必须的： maxElementsInMemory - 在内存中缓存的element的最大数目 maxElementsOnDisk - 在磁盘上缓存的element的最大数目，若是0表示无穷大 eternal - 设定缓存的elements是否永远不过期。如果为true，则缓存的数据始终有效，如果为false那么还要根据timeToIdleSeconds，timeToLiveSeconds判断 overflowToDisk- 设定当内存缓存溢出的时候是否将过期的element缓存到磁盘上 以下属性是可选的： timeToIdleSeconds - 当缓存在EhCache中的数据前后两次访问的时间超过timeToIdleSeconds的属性取值时，这些数据便会删除，默认值是0,也就是可闲置时间无穷大 timeToLiveSeconds - 缓存element的有效生命期，默认是0.,也就是element存活时间无穷大 diskSpoolBufferSizeMB 这个参数设置DiskStore(磁盘缓存)的缓存区大小.默认是30MB.每个Cache都应该有自己的一个缓冲区. diskPersistent- 在VM重启的时候是否启用磁盘保存EhCache中的数据，默认是false。 diskExpiryThreadIntervalSeconds - 磁盘缓存的清理线程运行间隔，默认是120秒。每个120s，相应的线程会进行一次EhCache中数据的清理工作 memoryStoreEvictionPolicy - 当内存缓存达到最大，有新的element加入的时候， 移除缓存中element的策略。默认是LRU（最近最少使用），可选的有LFU（最不常使用）和FIFO（先进先出） 开启ehcache缓存 EhcacheCache 是ehcache对Cache接口的实现；修改mapper.xml文件，在cache中指定EhcacheCache。 根据需求调整缓存参数： 123456789&lt;cache type=\"org.mybatis.caches.ehcache.EhcacheCache\" &gt; &lt;property name=\"timeToIdleSeconds\" value=\"3600\"/&gt; &lt;property name=\"timeToLiveSeconds\" value=\"3600\"/&gt; &lt;!-- 同ehcache参数maxElementsInMemory --&gt; &lt;property name=\"maxEntriesLocalHeap\" value=\"1000\"/&gt; &lt;!-- 同ehcache参数maxElementsOnDisk --&gt; &lt;property name=\"maxEntriesLocalDisk\" value=\"10000000\"/&gt; &lt;property name=\"memoryStoreEvictionPolicy\" value=\"LRU\"/&gt; &lt;/cache&gt; 测试 ：(这命中率就代表成功将ehcache 与 mybatis 整合了) 应用场景对于访问多的查询请求且用户对查询结果实时性要求不高，此时可采用 mybatis 二级缓存技术降低数据库访问量，提高访问速度，业务场景比如：耗时较高的统计分析sql、电话账单查询sql等。 实现方法如下：通过设置刷新间隔时间，由 mybatis 每隔一段时间自动清空缓存，根据数据变化频率设置缓存刷新间隔 flushInterval，比如设置为30分钟、60分钟、24小时等，根据需求而定。 局限性mybatis 二级缓存对细粒度的数据级别的缓存实现不好，比如如下需求：对商品信息进行缓存，由于商品信息查询访问量大，但是要求用户每次都能查询最新的商品信息，此时如果使用 mybatis 的二级缓存就无法实现当一个商品变化时只刷新该商品的缓存信息而不刷新其它商品的信息，因为 mybaits 的二级缓存区域以 mapper 为单位划分，当一个商品信息变化会将所有商品信息的缓存数据全部清空。解决此类问题需要在业务层根据需求对数据有针对性缓存。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":"通过项目逐步深入了解Mybatis（三）","date":"2017-06-13T16:00:00.000Z","path":"2017/06/14/通过项目逐步深入了解Mybatis(三)/","text":"相关阅读：1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、 通过项目逐步深入了解Mybatis&lt;二&gt; 本项目所有代码及文档都托管在 Github地址：https://github.com/zhisheng17/mybatis Mybatis 高级知识安排：对订单商品数据模型进行分析 订单商品数据模型 数据模型分析思路：1、每张表记录的数据内容（分模块对每张表记录的内容进行熟悉，相当于学习系统需求的过程） 2、每张表重要的的字段设置（非空字段、外键字段） 3、数据库级别表与表之间的关系（外键关系） 4、表与表业务之间的关系（要建立在每个业务意义的基础上去分析） 数据模型分析模型 用户表 user：记录购买商品的用户信息 订单表 order：记录用户所创建的订单(购买商品的订单) 订单明细表 orderdetail：（记录了订单的详细信息即购买商品的信息） 商品表 items：记录了商品信息 表与表业务之间的关系： 在分析表与表之间的业务关系时需要建立在某个业务意义基础上去分析。 先分析数据级别之间有关系的表之间的业务关系： 1、usre和orders： user —&gt; orders：一个用户可以创建多个订单，一对多 orders —&gt; user：一个订单只由一个用户创建，一对一 2、 orders和orderdetail： orders —&gt; orderdetail：一个订单可以包括 多个订单明细，因为一个订单可以购买多个商品，每个商品的购买信息在orderdetail记录，一对多关系 orderdetail —&gt; orders：一个订单明细只能包括在一个订单中，一对一 3、 orderdetail 和 itesm： orderdetail —&gt; itesms：一个订单明细只对应一个商品信息，一对一 items —&gt; orderdetail:一个商品可以包括在多个订单明细 ，一对多 再分析数据库级别没有关系的表之间是否有业务关系： 4、 orders 和 items： orders 和 items 之间可以通过 orderdetail 表建立 关系。 一对一查询需求：查询订单信息，关联查询创建订单的用户信息 使用 resultType sql 语句 确定查询的主表：订单表 确定查询的关联表：用户表 关联查询使用内链接？还是外链接？ 由于orders表中有一个外键（user_id），通过外键关联查询用户表只能查询出一条记录，可以使用内链接。 1SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id 创建 pojo Orders.java 123456789101112public class Orders &#123; private Integer id; private Integer userId; private String number; private Date createtime; private String note; //用户信息 private User user; //订单明细 private List&lt;Orderdetail&gt; orderdetails; //getter and setter&#125; OrderCustom.java 1234567891011//通过此类映射订单和用户查询的结果，让此类继承包括 字段较多的pojo类public class OrdersCustom extends Orders&#123; //添加用户属性 /*USER.username, USER.sex, USER.address */ private String username; private String sex; private String address; //getter and setter&#125; 映射文件 OrdersMapperCustom.xml 1234&lt;!--查询订单关联查询用户信息--&gt; &lt;select id=\"findOrdersUser\" resultType=\"cn.zhisheng.mybatis.po.OrdersCustom\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id &lt;/select&gt; Mapper 文件 OrdersMapperCustom.java 1234public interface OrdersMapperCustom&#123; public OrdersCustom findOrdersUser() throws Exception;&#125; 测试代码（记得在 SqlConfig.xml中添加载 OrdersMapperCustom.xml 文件） 1234567891011@Test public void testFindOrdersUser() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;OrdersCustom&gt; list = ordersMapperCustom.findOrdersUser(); System.out.println(list); sqlSession.close(); &#125; 测试结果 ​ ​ 使用 resultMap sql 语句（和上面的一致） 使用 resultMap 映射思路 使用 resultMap 将查询结果中的订单信息映射到 Orders 对象中，在 orders 类中添加 User 属性，将关联查询出来的用户信息映射到 orders 对象中的 user 属性中。 12//用户信息private User user; 映射文件 OrdersMapperCustom.xml 先定义 resultMap 1234567891011121314151617181920212223242526272829303132&lt;!--定义查询订单关联查询用户信息的resultMap 将整个查询结果映射到cn.zhisheng.mybatis.po.Orders --&gt; &lt;resultMap id=\"OrdersUserResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;!--配置映射的订单信息--&gt; &lt;!--id表示查询结果中的唯一标识 在这里是订单的唯一标识 如果是由多列组成的唯一标识，那么就需要配置多个id column：id 是订单信息中的唯一标识列 property：id 是订单信息唯一标识列所映射到orders中的id属性 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!--配置映射的关联用户信息 association 用于映射关联查询单个对象的信息 property 将要关联查询的用户信息映射到 orders中的属性中去 --&gt; &lt;association property=\"user\" javaType=\"cn.zhisheng.mybatis.po.User\"&gt; &lt;!--id 关联用户信息的唯一标识 column: 指定唯一标识用户的信息 property：映射到user的那个属性 --&gt; &lt;id column=\"user_id\" property=\"id\"/&gt; &lt;result column=\"username\" property=\"username\"/&gt; &lt;result column=\"sex\" property=\"sex\"/&gt; &lt;result column=\"address\" property=\"address\"/&gt; &lt;result column=\"birthday\" property=\"birthday\"/&gt; &lt;/association&gt; &lt;/resultMap&gt; 1234&lt;!--查询订单关联查询用户信息, 使用 resultMap--&gt; &lt;select id=\"findOrdersUserResultMap\" resultMap=\"OrdersUserResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id &lt;/select&gt; Mapper 文件 1public List&lt;Orders&gt; findOrdersUserResultMap() throws Exception; 测试代码 1234567891011@Test public void testFindOrdersUserResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersUserResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试结果 使用 resultType 和 resultMap 一对一查询小结 resultType：使用resultType实现较为简单，如果pojo中没有包括查询出来的列名，需要增加列名对应的属性，即可完成映射。如果没有查询结果的特殊要求建议使用resultType。 resultMap：需要单独定义resultMap，实现有点麻烦，如果对查询结果有特殊的要求，使用resultMap可以完成将关联查询映射pojo的属性中。resultMap可以实现延迟加载，resultType无法实现延迟加载。 一对多查询需求：查询订单及订单明细信息 SQL语句： 确定主查询表：订单表 确定关联查询表：订单明细表 在一对一查询基础上添加订单明细表关联即可。 12SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id 分析： 使用 resultType 将上边的查询结果映射到 pojo 中，订单信息的就是重复。 要求： 对 orders 映射不能出现重复记录。 在 orders.java 类中添加 List orderDetails 属性。 最终会将订单信息映射到 orders 中，订单所对应的订单明细映射到 orders 中的 orderDetails 属性中。 映射成的 orders 记录数为两条（orders信息不重复） 每个 orders 中的 orderDetails 属性存储了该订单所对应的订单明细。 映射文件： 首先定义 resultMap 1234567891011121314151617181920&lt;!--定义查询订单及订单明细信息的resultMap使用extends继承，不用在中配置订单信息和用户信息的映射--&gt; &lt;resultMap id=\"OrdersAndOrderDetailResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\" extends=\"OrdersUserResultMap\"&gt; &lt;!-- 订单信息 --&gt; &lt;!-- 用户信息 --&gt; &lt;!-- 使用extends继承，不用在中配置订单信息和用户信息的映射 --&gt; &lt;!-- 订单明细信息 一个订单关联查询出了多条明细，要使用collection进行映射 collection：对关联查询到多条记录映射到集合对象中 property：将关联查询到多条记录映射到cn.zhisheng.mybatis.po.Orders哪个属性 ofType：指定映射到list集合属性中pojo的类型 --&gt; &lt;collection property=\"orderdetails\" ofType=\"cn.zhisheng.mybatis.po.Orderdetail\"&gt; &lt;!-- id：订单明细唯 一标识 property:要将订单明细的唯 一标识 映射到cn.zhisheng.mybatis.po.Orderdetail的哪个属性--&gt; &lt;id column=\"orderdetail_id\" property=\"id\"/&gt; &lt;result column=\"items_id\" property=\"itemsId\"/&gt; &lt;result column=\"items_num\" property=\"itemsNum\"/&gt; &lt;result column=\"orders_id\" property=\"ordersId\"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; 12345&lt;!--查询订单及订单明细信息, 使用 resultMap--&gt; &lt;select id=\"findOrdersAndOrderDetailResultMap\" resultMap=\"OrdersAndOrderDetailResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id &lt;/select&gt; Mapper 文件 1public List&lt;Orders&gt; findOrdersAndOrderDetailResultMap() throws Exception; 测试文件 1234567891011@Test public void testFindOrdersAndOrderDetailResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersAndOrderDetailResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试结果 总结：mybatis使用resultMap的collection对关联查询的多条记录映射到一个list集合属性中。 使用resultType实现：将订单明细映射到orders中的orderdetails中，需要自己处理，使用双重循环遍历，去掉重复记录，将订单明细放在orderdetails中。 多对多查询需求：查询用户及用户购买商品信息。 SQL语句： 查询主表是：用户表 关联表：由于用户和商品没有直接关联，通过订单和订单明细进行关联，所以关联表： orders、orderdetail、items 123SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id,orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id, items.name items_name,items.detail items_detail, items.price items_price FROM orders, USER, orderdetail, items WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id AND orderdetail.items_id = items.id 映射思路： 将用户信息映射到 user 中。在 user 类中添加订单列表属性List orderslist，将用户创建的订单映射到orderslist在Orders中添加订单明细列表属性Listorderdetials，将订单的明细映射到orderdetials在OrderDetail中添加Items属性，将订单明细所对应的商品映射到Items 定义 resultMap：12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!--定义查询用户及用户购买商品信息的 resultMap--&gt; &lt;resultMap id=\"UserAndItemsResultMap\" type=\"cn.zhisheng.mybatis.po.User\"&gt; &lt;!--用户信息--&gt; &lt;id column=\"user_id\" property=\"id\"/&gt; &lt;result column=\"username\" property=\"username\"/&gt; &lt;result column=\"sex\" property=\"sex\"/&gt; &lt;result column=\"birthday\" property=\"birthday\"/&gt; &lt;result column=\"address\" property=\"address\"/&gt; &lt;!--订单信息 一个用户对应多个订单，使用collection映射 --&gt; &lt;collection property=\"ordersList\" ofType=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!-- 订单明细 一个订单包括 多个明细 --&gt; &lt;collection property=\"orderdetails\" ofType=\"cn.zhisheng.mybatis.po.Orderdetail\"&gt; &lt;id column=\"orderdetail_id\" property=\"id\"/&gt; &lt;result column=\"orders_id\" property=\"ordersId\"/&gt; &lt;result column=\"items_id\" property=\"itemsId\"/&gt; &lt;result column=\"items_num\" property=\"itemsNum\"/&gt; &lt;!-- 商品信息 一个订单明细对应一个商品 --&gt; &lt;association property=\"items\" javaType=\"cn.zhisheng.mybatis.po.Items\"&gt; &lt;id column=\"items_id\" property=\"id\"/&gt; &lt;result column=\"items_name\" property=\"name\"/&gt; &lt;result column=\"items_price\" property=\"price\"/&gt; &lt;result column=\"items_pic\" property=\"pic\"/&gt; &lt;result column=\"items_createtime\" property=\"createtime\"/&gt; &lt;result column=\"items_detail\" property=\"detail\"/&gt; &lt;/association&gt; &lt;/collection&gt; &lt;/collection&gt; &lt;/resultMap&gt; 映射文件12345&lt;!--查询用户及用户购买商品信息, 使用 resultMap--&gt; &lt;select id=\"findUserAndItemsResultMap\" resultMap=\"UserAndItemsResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id &lt;/select&gt; Mapper 文件1public List&lt;User&gt; findUserAndItemsResultMap() throws Exception; 测试文件1234567891011@Test public void testFindUserAndItemsResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;User&gt; list = ordersMapperCustom.findUserAndItemsResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试： 我去，竟然报错了，但是不要怕，通过查看报错信息可以知道我忘记在 User.java 中加入 orderlist 属性了，接下来我加上去，并加上 getter 和 setter 方法。 12345678//用户创建的订单列表 private List&lt;Orders&gt; ordersList; public List&lt;Orders&gt; getOrdersList() &#123; return ordersList; &#125; public void setOrdersList(List&lt;Orders&gt; ordersList) &#123; this.ordersList = ordersList; &#125; 再次测试就能成功了。 多对多查询总结将查询用户购买的商品信息明细清单，（用户名、用户地址、购买商品名称、购买商品时间、购买商品数量） 针对上边的需求就使用resultType将查询到的记录映射到一个扩展的pojo中，很简单实现明细清单的功能。 一对多是多对多的特例，如下需求： 查询用户购买的商品信息，用户和商品的关系是多对多关系。 需求1： 查询字段：用户账号、用户名称、用户性别、商品名称、商品价格(最常见) 企业开发中常见明细列表，用户购买商品明细列表， 使用resultType将上边查询列映射到pojo输出。 需求2： 查询字段：用户账号、用户名称、购买商品数量、商品明细（鼠标移上显示明细） 使用resultMap将用户购买的商品明细列表映射到user对象中。 总结： 使用resultMap是针对那些对查询结果映射有特殊要求的功能，，比如特殊要求映射成list中包括多个list。 ResultMap 总结resultType：作用： 将查询结果按照sql列名pojo属性名一致性映射到pojo中。 场合： 常见一些明细记录的展示，比如用户购买商品明细，将关联查询信息全部展示在页面时，此时可直接使用resultType将每一条记录映射到pojo中，在前端页面遍历list（list中是pojo）即可。 resultMap： 使用association和collection完成一对一和一对多高级映射（对结果有特殊的映射要求）。 association：作用： 将关联查询信息映射到一个pojo对象中。 场合： 为了方便查询关联信息可以使用association将关联订单信息映射为用户对象的pojo属性中，比如：查询订单及关联用户信息。使用resultType无法将查询结果映射到pojo对象的pojo属性中，根据对结果集查询遍历的需要选择使用resultType还是resultMap。 collection：作用： 将关联查询信息映射到一个list集合中。 场合： 为了方便查询遍历关联信息可以使用collection将关联信息映射到list集合中，比如：查询用户权限范围模块及模块下的菜单，可使用collection将模块映射到模块list中，将菜单列表映射到模块对象的菜单list属性中，这样的作的目的也是方便对查询结果集进行遍历查询。如果使用resultType无法将查询结果映射到list集合中。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":"Hexo + yilia 主题实现文章目录","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Hexo-yilia-toc/","text":"前提为了方便查看每篇文章的目录结构，可以定位到想看的地方，特地找了下如何实现这个功能。 添加 CSS 样式打开 themes\\yilia\\source 下的 main.234bc0.css 文件，直接在后面添加如下代码：123456789/* 新添加的 */#container .show-toc-btn,#container .toc-article&#123;display:block&#125;.toc-article&#123;z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px&#125;.toc-article .toc-close&#123;font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc&#125;.toc-article .toc-close:hover&#123;color:#000&#125;.toc-article .toc&#123;font-size:12px;padding:0;line-height:20px&#125;.toc-article .toc .toc-number&#123;color:#333&#125;.toc-article .toc .toc-text:hover&#123;text-decoration:underline;color:#2a6496&#125;.toc-article li&#123;list-style-type:none&#125;.toc-article .toc-level-1&#123;margin:4px 0&#125;.toc-article .toc-child&#123;&#125;@-moz-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-webkit-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-o-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;.show-toc-btn&#123;display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9&#125;.show-toc-btn .btn-bg&#123;margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%&#125;.show-toc-btn .btn-text&#123;color:#999;font-size:12px&#125;.show-toc-btn:hover&#123;cursor:pointer&#125;.show-toc-btn:hover .btn-bg&#123;background-position:0 -16px&#125;.show-toc-btn:hover .btn-text&#123;font-size:12px;color:#ea8010&#125;.toc-article li ol, .toc-article li ul &#123; margin-left: 30px;&#125;.toc-article ol, .toc-article ul &#123; margin: 10px 0;&#125; 修改 article.ejs 文件打开 themes\\yilia\\layout\\_partial 文件夹下的 article.ejs 文件, 在 &lt;/header&gt; &lt;% } %&gt; 下面加入如下内容（注意位置） 123456789101112131415161718192021222324252627&lt;!-- 目录内容 --&gt;&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt; &lt;p class=&quot;show-toc-btn&quot; id=&quot;show-toc-btn&quot; onclick=&quot;showToc();&quot; style=&quot;display:none&quot;&gt; &lt;span class=&quot;btn-bg&quot;&gt;&lt;/span&gt; &lt;span class=&quot;btn-text&quot;&gt;文章导航&lt;/span&gt; &lt;/p&gt; &lt;div id=&quot;toc-article&quot; class=&quot;toc-article&quot;&gt; &lt;span id=&quot;toc-close&quot; class=&quot;toc-close&quot; title=&quot;隐藏导航&quot; onclick=&quot;showBtn();&quot;&gt;×&lt;/span&gt; &lt;strong class=&quot;toc-title&quot;&gt;文章目录&lt;/strong&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; function showToc()&#123; var toc_article = document.getElementById(&quot;toc-article&quot;); var show_toc_btn = document.getElementById(&quot;show-toc-btn&quot;); toc_article.setAttribute(&quot;style&quot;,&quot;display:block&quot;); show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:none&quot;); &#125;; function showBtn()&#123; var toc_article = document.getElementById(&quot;toc-article&quot;); var show_toc_btn = document.getElementById(&quot;show-toc-btn&quot;); toc_article.setAttribute(&quot;style&quot;,&quot;display:none&quot;); show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:block&quot;); &#125;; &lt;/script&gt; &lt;% &#125; %&gt;&lt;!-- 目录内容结束 --&gt; 然后若想要文章显示目录，在每篇文章开头加入：toc: true 即可。 参考文章：Hexo+yilia主题实现文章目录和添加视频 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.54tianzhisheng.cn/tags/hexo/"}]},{"title":"Java连接Oracle数据库的三种连接方式","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java连接Oracle数据库的三种连接方式/","text":"背景：这两天在学习Oracle数据库，这里就总结下自己上课所学的知识，同时记录下来，方便整理当天所学下的知识，也同时方便日后自己查询。 SQL语句的话，这里我就不多讲了，感觉和其他的数据库（MySQL、SQL Server）都是类似，区别不大。 今天在这里就写下 Java 连接 Oracle 数据库的三种连接方式。 工具： Oracle Database 10g Express Edition cmd命令窗口 IDEA 2016.1.3 ojdbc6_g.jar（数据库驱动包） jdk 1.8 创建数据库表：首先在本地写好创建的数据库表的创建代码后，然后粘贴在cmd命令窗口下，即可创建成功。（前提是进入安装好了oracle，进入了用户，然后在当前用户下创建这个表） 部门表：tb1_dept （含有id name city三个属性） 12345create table tb1_dept( id number(5) primary key, name varchar2(10) not null, city varchar2(10) not null); 插入数据：然后同样写好插入数据的sql语句，这里我就写三条数据。 123insert into tb1_dept(id, name, city) values(1,&apos;java&apos;, &apos;南昌&apos;);insert into tb1_dept(id, name, city) values(2,&apos;c&apos;, &apos;上海&apos;);insert into tb1_dept(id, name, city) values(3,&apos;java&apos;, &apos;南昌&apos;); 好，数据库表已经创建好了，接下来我们需要准备的是数据库驱动包。 这里我用的是 ojdbc6_g.jar 驱动包。 接下来先了解一些基础知识： JDBC的六大步骤：这里我们就按照jdbc的这六大步骤执行下去： 注册驱动 获取连接 获取执行sql语句对象 执行sql语句 处理结果集 关闭资源 URL：统一资源定位器 oracle URL： jdbc:oracle:thin:@localhost:1521:XE jdbc:oracle:thin:@127.0.0.1:1521:XE MySQL URL：jdbc:mysql://localhost:3306/数据库名称 thin：小型驱动，驱动方式 @localhost 本机ip地址 127.0.0.1 XE：数据库的名字 ipconfig：ip地址查询 URI：统一资源标识符 URN：用特定命名空间的名字标识资源 如果你不知道 URL、 URI、URN三者的区别的话，那么你可以参考下面我推荐的一篇文章。 你知道URL、URI和URN三者之间的区别吗？ 三种连接方式代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.zhisheng.test.jdbc;import oracle.jdbc.driver.OracleDriver;import java.sql.*;import java.util.Properties;/** * Created by 10412 on 2016/12/27. * JDBC的六大步骤 * JAVA连接Oracle的三种方式 */public class JdbcTest&#123; public static void main(String[] args) &#123; Connection connect = null; Statement statement = null; ResultSet resultSet = null; try &#123; //第一步：注册驱动 //第一种方式：类加载(常用) //Class.forName(\"oracle.jdbc.OracleDriver\"); //第二种方式：利用Driver对象 Driver driver = new OracleDriver(); DriverManager.deregisterDriver(driver); //第三种方式:利用系统参数 需在idea中配置program arguments为下面的参数 //-Djdbc.drivers = oracle.jdbc.OracleDriver //第二步：获取连接 //第一种方式：利用DriverManager（常用） //connect = DriverManager.getConnection(\"jdbc:oracle:thin:@localhost:1521:XE\", \"你的oracle数据库用户名\", \"用户名密码\"); //第二种方式：直接使用Driver Properties pro = new Properties(); pro.put(\"user\", \"你的oracle数据库用户名\"); pro.put(\"password\", \"用户名密码\"); connect = driver.connect(\"jdbc:oracle:thin:@localhost:1521:XE\", pro); //测试connect正确与否 System.out.println(connect); //第三步：获取执行sql语句对象 //第一种方式:statement //statement = connect.createStatement(); //第二种方式：PreStatement PreparedStatement preState = connect.prepareStatement(\"select * from tb1_dept where id = ?\"); //第四步：执行sql语句 //第一种方式： //resultSet = statement.executeQuery(\"select * from tb1_dept\"); //第二种方式： preState.setInt(1, 2);//1是指sql语句中第一个？, 2是指第一个？的values值 //resultSet = preState.executeQuery(); //执行查询语句 //查询任何语句，如果有结果集，返回true，没有的话返回false,注意如果是插入一条数据的话，虽然是没有结果集，返回false，但是却能成功的插入一条数据 boolean execute = preState.execute(); System.out.println(execute); //第五步：处理结果集 while (resultSet.next()) &#123; int id = resultSet.getInt(\"id\"); String name = resultSet.getString(\"name\"); String city = resultSet.getString(\"city\"); System.out.println(id+\" \"+name+\" \"+city); //打印输出结果集 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; //第六步：关闭资源 try &#123; if (resultSet!=null) resultSet.close(); if (statement!=null) statement.close(); if (connect!=null) connect.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 注解：1、 第一步：注册驱动 中的第三种方法 利用系统参数 需在idea中配置program arguments为下面的参数 这里我说一下怎么在IDEA中的配置方式吧 运行截图： OK ! 下篇文章将写 JDBC 的封装。","tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.54tianzhisheng.cn/tags/数据库/"},{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Oracle","slug":"Oracle","permalink":"http://www.54tianzhisheng.cn/tags/Oracle/"}]},{"title":"MyBatis的foreach语句详解","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/MyBatis-foreach/","text":"foreach 的主要用在构建in条件中，它可以在SQL语句中进行迭代一个集合。 foreach 元素的属性主要有 item，index，collection，open，separator，close。 item 表示集合中每一个元素进行迭代时的别名， index 指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， open 表示该语句以什么开始， separator 表示在每次进行迭代之间以什么符号作为分隔 符， close 表示以什么结束。 在使用 foreach 的时候最关键的也是最容易出错的就是 collection 属性，该属性是必须指定的，但是在不同情况 下，该属性的值是不一样的，主要有一下3种情况： 如果传入的是单参数且参数类型是一个List的时候，collection 属性值为 list 如果传入的是单参数且参数类型是一个 array 数组的时候，collection 的属性值为 array 如果传入的参数是多个的时候，我们就需要把它们封装成一个 Map 了，当然单参数也可以封装成map，实际上如果你在传入参数的时候，在 breast 里面也是会把它封装成一个 Map 的，map 的 key 就是参数名，所以这个时候 collection 属性值就是传入的 List 或 array 对象在自己封装的 map 里面的 key 。 下面分别来看看上述三种情况的示例代码： 1.单参数 List 的类型： 123456&lt;select id=\"dynamicForeachTest\" resultType=\"Blog\"&gt; select * from t_blog where id in &lt;foreach collection=\"list\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述 collection 的值为list，对应的 Mapper 是这样的 1public List&lt;Blog&gt; dynamicForeachTest(List&lt;Integer&gt; ids); 测试代码： 12345678910111213@Test public void dynamicForeachTest() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); List&lt;Integer&gt; ids = new ArrayList&lt;Integer&gt;(); ids.add(1); ids.add(3); ids.add(6); List&lt;Blog&gt; blogs = blogMapper.dynamicForeachTest(ids); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125; 2.单参数array数组的类型： 123456&lt;select id=\"dynamicForeach2Test\" resultType=\"Blog\"&gt; select * from t_blog where id in &lt;foreach collection=\"array\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述collection为array，对应的Mapper代码： 1public List&lt;Blog&gt; dynamicForeach2Test(int[] ids); 对应的测试代码： 12345678910@Test public void dynamicForeach2Test() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); int[] ids = new int[] &#123;1,3,6,9&#125;; List&lt;Blog&gt; blogs = blogMapper.dynamicForeach2Test(ids); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125; 3.自己把参数封装成Map的类型 123456&lt;select id=\"dynamicForeach3Test\" resultType=\"Blog\"&gt; select * from t_blog where title like \"%\"#&#123;title&#125;\"%\" and id in &lt;foreach collection=\"ids\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述collection的值为ids，是传入的参数Map的key，对应的Mapper代码： 1public List&lt;Blog&gt; dynamicForeach3Test(Map&lt;String, Object&gt; params); 对应测试代码： 12345678910111213141516171819@Test public void dynamicForeach3Test() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); final List&lt;Integer&gt; ids = new ArrayList&lt;Integer&gt;(); ids.add(1); ids.add(2); ids.add(3); ids.add(6); ids.add(7); ids.add(9); Map&lt;String, Object&gt; params = new HashMap&lt;String, Object&gt;(); params.put(\"ids\", ids); params.put(\"title\", \"中国\"); List&lt;Blog&gt; blogs = blogMapper.dynamicForeach3Test(params); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125;","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"foreach","slug":"foreach","permalink":"http://www.54tianzhisheng.cn/tags/foreach/"}]},{"title":"Spring MVC系列文章（一）：Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统 Demo","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统/","text":"SpringBoot 系列文章 相关阅读：1、Spring MVC+Hibernate JPA+ Bootstrap 搭建的博客系统项目中所遇到的坑 由于整个系统不是很难，这里就不详细介绍了，我相信看源码的话，应该能够看得懂。 源码地址：https://github.com/zhisheng17/springmvc数据库：springdemo.sql 下面给出下整个系统的截图吧，觉得不错，可以给个 star ，哈哈！后续继续在这个项目中加入新的项目。 截图：首页 用户管理模块 用户列表 添加用户 用户信息详情 更新用户信息 删除用户 博客管理模块 博客列表 博客详情 添加博客 更新博客 删除博客","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://www.54tianzhisheng.cn/tags/Spring/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"},{"name":"Hibernate JPA","slug":"Hibernate-JPA","permalink":"http://www.54tianzhisheng.cn/tags/Hibernate-JPA/"},{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"}]},{"title":"关于String s = new String(\"xyz\"); 创建几个对象的问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/String-new/","text":"你知道在 java 中除了 8 种基本类型外，其他的都是类对象以及其引用。所以 &quot;xyz &quot;在 java 中它是一个String 对象.对于 string 类对象来说他的对象值是不能修改的，也就是具有不变性。 看：1234String s= &quot;hello &quot;; s= &quot;Java &quot;; String s1= &quot;hello &quot;; String s2=new String( &quot;hello &quot;); 结果如下图： 啊，s 所引用的 string 对象不是被修改了吗？之前所说的不变性，去那里了啊？你别着急，让我告诉你说发生了什么事情：在 jvm 的工作过程中，会创建一片的内存空间专门存入 string 对象。我们把这片内存空间叫做 string 池。 String s = “hello “; 当 jvm 看到 “hello”，在 string 池创建 string 对象存储它，并将他的引用返回给s。s = “java “，当 jvm 看到 “java “，在 string 池创建新的 string 对象存储它，再把新建的 string 对象的引用返回给s。而原先的 “hello”仍然在 string 池内。没有消失，他是不能被修改的。 所以我们仅仅是改变了s的引用，而没有改变他所引用的对象，因为 string 对象的值是不能被修改的。 String s1 = “hello” ; jvm 首先在 string 池内里面看找不找得到字符串 “hello”, 如果找得到,返回他的引用给 s1，否则的话就会创建新的 string 对象，放到 string 池里。这里由于 s = “hello”了,对象已经被引用，所以依据规则 s 和 s1 都是引用同一个对象。所以 s == s1 将返回 true。( == 对于非基本类型，是比较两引用是否引用内存中的同一个对象，这里先不区分 == 和 equals 的区别 ) String s2 = new String( “hello”); jvm 首先在 string 池内里面看找不找得到字符串 “hello”, 如果找得到, 不做任何事情，否则的话就会创建新的 string 对象，放到 string 池里面。由于遇到了 new，还会在内存上（不是 string 池里面）创建 string 对象存储 “hello”，并将内存上的（不是 string 池内的）string 对象返回给 s2。所以 s == s2 将返回 false，不是引用同一个对象。 好现在我们看题目：String s = new String( “xyz “);首先在 string 池内找，找到？不创建 string 对象，否则创建一个对象，遇到 new 运算符号了，在内存上创建 string 对象，并将其返回给 s，又一个对象 所以总共是 1个 或者 2个对象 。 而为什么在网上都说 String s=new String(“hello”); 创建了2个对象？那可能因为它就写这么一句代码，误让人默认的认为执行代码之前并不实例任何一个 String 对象过（也许 很多人不会这么想，），跟着别人或者不经思考的就说2个，斟是说存放在栈内存中专门存放 String 对象引用的 s 变量是一个对象！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"String","slug":"String","permalink":"http://www.54tianzhisheng.cn/tags/String/"}]},{"title":"【字符串】判断两字符串是否互为旋转词？","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/【字符串】判断两字符串是否互为旋转词？/","text":"相关阅读：字符串逆序问题的解决方法题目： 如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。 比如A=”12345”,A的旋转词有”12345”,”23451”,”34512”,”45123”和”51234”。 对于两个字符串A和B，请判断A和B是否互为旋转词。 给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。 测试样例： “cdab”,4,”abcd”,4 返回：true 通过代码： 1234567891011121314import java.util.*;public class Rotation&#123; public static boolean chkRotation(String A, int lena, String B, int lenb) &#123; // write code here if (lena != lenb)&#123; return false; &#125;else &#123; String str = A + A; return str.contains(B); &#125; &#125;&#125; 也可以使用 indexOf。 其区别是： contains 是找指定字符串是否包含一个字符串，返回值的 boolean 类型，即只有 true 和 false indexOf 有多个重载，但无论哪个，都是做一定的匹配，然后把匹配的第一个字符的位置返回，返回的是 int 类型，如果没找到，那么返回 -1 稍微再深究一下的我看了下 contains 的源码，结果发现他调用的是 indexOf 方法。 源码如下： 1234567891011/** * Returns true if and only if this string contains the specified * sequence of char values. * * @param s the sequence to search for * @return true if this string contains &#123;@code s&#125;, false otherwise * @since 1.5 */ public boolean contains(CharSequence s) &#123; return indexOf(s.toString()) &gt; -1; &#125; 意思就是如上面的区别所说的，他只有两个返回值 true 和 false。 于是我们继续看一下 indexOf 方法的源码： 1234567891011121314151617/** * Returns the index within this string of the first occurrence of the * specified substring. * * &lt;p&gt;The returned index is the smallest value &lt;i&gt;k&lt;/i&gt; for which: * &lt;blockquote&gt;&lt;pre&gt; * this.startsWith(str, &lt;i&gt;k&lt;/i&gt;) * &lt;/pre&gt;&lt;/blockquote&gt; * If no such value of &lt;i&gt;k&lt;/i&gt; exists, then &#123;@code -1&#125; is returned. * * @param str the substring to search for. * @return the index of the first occurrence of the specified substring, * or &#123;@code -1&#125; if there is no such occurrence. public int indexOf(String str) &#123; return indexOf(str, 0); &#125; 继续可以发现他又调用了 indexOf 的两个参数方法，只不过索引是 0 。 然后我继续看带有两个参数的 indexOf 方法源码如下： 123456789101112131415161718192021/** * Returns the index within this string of the first occurrence of the * specified substring, starting at the specified index. * * &lt;p&gt;The returned index is the smallest value &lt;i&gt;k&lt;/i&gt; for which: * &lt;blockquote&gt;&lt;pre&gt; * &lt;i&gt;k&lt;/i&gt; &amp;gt;= fromIndex &#123;@code &amp;&amp;&#125; this.startsWith(str, &lt;i&gt;k&lt;/i&gt;) * &lt;/pre&gt;&lt;/blockquote&gt; * If no such value of &lt;i&gt;k&lt;/i&gt; exists, then &#123;@code -1&#125; is returned. * * @param str the substring to search for. * @param fromIndex the index from which to start the search. * @return the index of the first occurrence of the specified substring, * starting at the specified index, * or &#123;@code -1&#125; if there is no such occurrence. */ public int indexOf(String str, int fromIndex) &#123; return indexOf(value, 0, value.length, str.value, 0, str.value.length, fromIndex); &#125; 哈哈，发现他又调用了 indexOf 的方法，这次终于我们可以看到最后的 查找算法 如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Code shared by String and StringBuffer to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param targetOffset offset of the target string. * @param targetCount count of the target string. * @param fromIndex the index to begin searching from. */static int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) &#123; if (fromIndex &gt;= sourceCount) &#123; return (targetCount == 0 ? sourceCount : -1); &#125; if (fromIndex &lt; 0) &#123; fromIndex = 0; &#125; if (targetCount == 0) &#123; return fromIndex; &#125; char first = target[targetOffset]; int max = sourceOffset + (sourceCount - targetCount); for (int i = sourceOffset + fromIndex; i &lt;= max; i++) &#123; /* Look for first character. */ if (source[i] != first) &#123; while (++i &lt;= max &amp;&amp; source[i] != first); &#125; /* Found first character, now look at the rest of v2 */ if (i &lt;= max) &#123; int j = i + 1; int end = j + targetCount - 1; for (int k = targetOffset + 1; j &lt; end &amp;&amp; source[j] == target[k]; j++, k++); if (j == end) &#123; /* Found whole string. */ return i - sourceOffset; &#125; &#125; &#125; return -1;&#125; 总结：遇到这种问题多查看源码，想深入就得从底层做起！","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://www.54tianzhisheng.cn/tags/算法/"},{"name":"字符串","slug":"字符串","permalink":"http://www.54tianzhisheng.cn/tags/字符串/"},{"name":"旋转词","slug":"旋转词","permalink":"http://www.54tianzhisheng.cn/tags/旋转词/"}]},{"title":"字符串","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/【字符串】字符串逆序/","text":"题目一：如果一个字符串 str ，把字符串 str 前面的任意部分挪到后面去形成的字符串叫做 str 的旋转词。比如 str = “ 1234 ” ， 那么 str 的旋转词有 “ 1234 ” ， “ 2341 ” ， “ 3412 ” ， “ 4123 ” 。给定两个字符串 a 和 b ，请判断 a 和 b 是否互为旋转词？举例： a = “ cdab “ , b = “ abcd “ 。返回 true。 a = “ 1ab2 “ , b = “ ab12 “ 。返回 false。 a = “ 2ab1 “ , b= “ ab12 “ 。 返回 true。 思路：最优解时间复杂度为 O(N) 先判断字符串 a 和 b 是否长度相等。 如果长度相等，生成 a + a 的大字符串。 然后判断大字符串中是否包含 b 字符串。（使用 kmp 算法判断）如果大字符串中包含字符串 b ，那么字符串 a 和 b 就互为旋转词。 举例： a = “ 1234 “ a + a = “ 12341234 “ 很明显发现，如果字符串 a 的长度为 N，在 a + a 的大字符串中，任意一个长度为 N 的子串都是 a 的旋转词。 题目二：给定一个字符串 a， 请在单词间做逆序调整。 举例： “ pig loves dog “ 逆序成 “ dog loves pig “ 。 “ I’m a student. “ 逆序成 “ student. a I’m “ 思路： 实现将字符串局部所有字符逆序的函数 f 利用 f 将字符串所有字符逆序 找到逆序后的字符串中每一个单词的区域，利用 f 将每一个单词的区域逆序 题目三：给定一个字符串 a 和一个整数 i。N为字符串的长度，i 为 a 中的位置，将 a [ 0 … i ] 移到右侧，a [ i + 1 … N - 1 ]移到左侧。 举例： a = “ ABCDE “ ，i = 2 。将 str 调整为 “ DEABC “ 。 要求：时间复杂度为 O(N)，额外空间复杂度为 O(1)。 思路： 先将 a[ 0 … i ] 部分的字符逆序 再将 a[ i + 1 … N - 1 ] 部分的字符逆序 最后将整体的字符 a 逆序 题目四：给定一个字符串类型的数组 strs，请找到一种拼接顺序，使得将所有的字符串拼接起来组成的大字符串是所有可能性中字典顺序最小的，并返回这个字符串。 举例： strs = [ “ abc “ , “ de “ ]，可以拼接成 “ abcde “，也可以拼接成 “ deabc “，但是前者的字典顺序更小，所以返回 “ abcde “ 。 strs = [ “ b “, “ ba “ ], 可以拼接成 “ bba “, 也可以拼接成 “ bab “,但是后者的字典顺序更小，所以返回 “ bab “。 思路：最优解的时间复杂度O(N*logN)，其实质是一种排序的实现。 方案二中是比较两个字符串彼此拼接后的字典顺序，所以能成功。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://www.54tianzhisheng.cn/tags/算法/"},{"name":"字符串","slug":"字符串","permalink":"http://www.54tianzhisheng.cn/tags/字符串/"}]},{"title":"MySQL 处理海量数据时的一些优化查询速度方法","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/MySQL-select-good/","text":"在参与实际项目中，当 MySQL 表的数据量达到百万级时，普通的 SQL 查询效率呈直线下降，而且如果 where 中的查询条件较多时，其查询速度无法容忍。想想可知，假如我们查询淘宝的一个订单详情，如果查询时间高达几十秒，这么高的查询延时，任何用户都会抓狂。因此如何提高 SQL 语句查询效率，显得十分重要。 查询速度慢的原因1、没有索引或者没有用到索引（这是查询慢最常见的问题，是程序设计的缺陷） 2、I/O 吞吐量小，形成了瓶颈效应。 3、没有创建计算列导致查询不优化。 4、内存不足 5、网络速度慢 6、查询出的数据量过大（可采用多次查询，其他的方法降低数据量） 7、锁或者死锁（这是查询慢最常见的问题，是程序设计的缺陷） 8、sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。 9、返回了不必要的行和列 10、查询语句不好，没有优化 30 种 SQL 查询语句的优化方法：1、应尽量避免在 where 子句中使用 != 或者 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。 2、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num is null; 可以在 num 上设置默认值 0 ，确保表中 num 列没有 null 值，然后这样查询： 1select id from t where num = 0; 3、对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 4、尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num = 10 or num = 20; 可以这样查询： 123select id from t where num = 10union allselect id from t where num = 20; 5、下面的查询也将导致全表扫描：（不能前置百分号） 1select id from t where name like '%abc%'; 若要提高效率，可以考虑全文检索。 6、in 和 not in 也要慎用，否则会导致全表扫描，如： 1select id from t where num in(1, 2, 3); 对于连续的数值，能用 between 就不要用 in 了： 1select id from t where num between 1 and 3; 12345select xx,phone FROM send a JOIN ( select '13891030091' phone union select '13992085916' ………… UNION SELECT '13619100234' ) b on a.Phone=b.phone--替代下面 很多数据隔开的时候in('13891030091','13992085916','13619100234'…………) 7、如果在 where 子句中使用参数，也会导致全表扫描。因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择到运行时；它必须在编译时进行选择。然而，如果在编译时简历访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： 1select id from t where num = @num; 可以改为强制查询使用索引： 1select id from t with(index(索引名)) where num = @num; 8、应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： 1select id from t where num/2 = 100; 应改为： 1select id from t where num = 100 * 2; 9、应尽量避免在 where 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： 12select id from t where substring(name, 1, 3) = ’abc’–name; //以abc开头的idselect id from t where datediff(day,createdate,’2005-11-30′) = 0–’2005-11-30′; //生成的id 应改为: 12select id from t where name like ‘abc%’select id from t where createdate &gt;= ’2005-11-30′ and createdate &lt; ’2005-12-1′; 10、不要在 where 子句中的 “=” 左边进行函数，算术运算或者其他表达式运算，否则系统将可能无法正确使用索引。 11、在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12、不要些一些没有意义的查询，如需要生成一个空表结构： 1select col1,col2 into #t from t where 1=0; 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： 1create table #t(…) 13、很多时候用 exists 代替 in 是一个好的选择： 1select num from a where num in(select num from b); 用下面的语句替换： 1select num from a where exists(select 1 from b where num=a.num); 14、并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 15、索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 16、应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17、尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18、尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19、任何地方都不要使用 select * from t ，用具体的字段列表代替 *，不要返回用不到的任何字段。 20、尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21、避免频繁创建和删除临时表，以减少系统表资源的消耗。 22、临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23、在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先 create table，然后 insert。 24、如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26、使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 27、与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28、在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 29、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 30、尽量避免大事务操作，提高系统并发能力。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"}]},{"title":"Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子/","text":"背景： PySpider：一个国人编写的强大的网络爬虫系统并带有强大的WebUI。采用Python语言编写，分布式架构，支持多种数据库后端，强大的WebUI支持脚本编辑器，任务监视器，项目管理器以及结果查看器。在线示例： http://demo.pyspider.org/ 官方文档： http://docs.pyspider.org/en/latest/ Github : https://github.com/binux/pyspider 本文爬虫代码 Github 地址：https://github.com/zhisheng17/Python-Projects/blob/master/v2ex/V2EX.py 说了这么多，我们还是来看正文吧！ 前提: 你已经安装好了Pyspider 和 MySQL-python（保存数据） 如果你还没安装的话，请看看我的前一篇文章，防止你也走弯路。 Pyspider 框架学习时走过的一些坑 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误 我所遇到的一些错误： 首先，本爬虫目标：使用 Pyspider 框架爬取 V2EX 网站的帖子中的问题和内容，然后将爬取的数据保存在本地。 V2EX 中大部分的帖子查看是不需要登录的，当然也有些帖子是需要登陆后才能够查看的。（因为后来爬取的时候发现一直 error ，查看具体原因后才知道是需要登录的才可以查看那些帖子的）所以我觉得没必要用到 Cookie，当然如果你非得要登录，那也很简单，简单地方法就是添加你登录后的 cookie 了。 我们在 https://www.v2ex.com/ 扫了一遍，发现并没有一个列表能包含所有的帖子，只能退而求其次，通过抓取分类下的所有的标签列表页，来遍历所有的帖子： https://www.v2ex.com/?tab=tech 然后是 https://www.v2ex.com/go/programmer 最后每个帖子的详情地址是 （举例）： https://www.v2ex.com/t/314683#reply1 创建一个项目 在 pyspider 的 dashboard 的右下角，点击 “Create” 按钮 替换 on_start 函数的 self.crawl 的 URL： 123@every(minutes=24 * 60) def on_start(self): self.crawl(&apos;https://www.v2ex.com/&apos;, callback=self.index_page, validate_cert=False) self.crawl 告诉 pyspider 抓取指定页面，然后使用 callback 函数对结果进行解析。 @every) 修饰器，表示 on_start 每天会执行一次，这样就能抓到最新的帖子了。 validate_cert=False 一定要这样，否则会报 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误 首页： 点击绿色的 run 执行，你会看到 follows 上面有一个红色的 1，切换到 follows 面板，点击绿色的播放按钮： 第二张截图一开始是出现这个问题了，解决办法看前面写的文章，后来问题就不再会出现了。 Tab 列表页 : 在 tab 列表页 中，我们需要提取出所有的主题列表页 的 URL。你可能已经发现了，sample handler 已经提取了非常多大的 URL 代码：1234@config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/?tab=&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.tab_page, validate_cert=False) 由于帖子列表页和 tab列表页长的并不一样，在这里新建了一个 callback 为 self.tab_page @config(age=10 24 60 * 60) 在这表示我们认为 10 天内页面有效，不会再次进行更新抓取 Go列表页 : 代码： 1234@config(age=10 * 24 * 60 * 60) def tab_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/go/&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) 帖子详情页（T）: 你可以看到结果里面出现了一些reply的东西，对于这些我们是可以不需要的，我们可以去掉。 同时我们还需要让他自己实现自动翻页功能。 代码：123456789@config(age=10 * 24 * 60 * 60) def board_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/t/&quot;]&apos;).items(): url = each.attr.href if url.find(&apos;#reply&apos;)&gt;0: url = url[0:url.find(&apos;#&apos;)] self.crawl(url, callback=self.detail_page, validate_cert=False) for each in response.doc(&apos;a.page_normal&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) #实现自动翻页功能 去掉后的运行截图： 实现自动翻页后的截图： 此时我们已经可以匹配了所有的帖子的 url 了。 点击每个帖子后面的按钮就可以查看帖子具体详情了。 代码： 12345678910@config(priority=2) def detail_page(self, response): title = response.doc(&apos;h1&apos;).text() content = response.doc(&apos;div.topic_content&apos;).html().replace(&apos;&quot;&apos;, &apos;\\\\&quot;&apos;) self.add_question(title, content) #插入数据库 return &#123; &quot;url&quot;: response.url, &quot;title&quot;: title, &quot;content&quot;: content, &#125; 插入数据库的话，需要我们在之前定义一个add_question函数。 123456789101112131415#连接数据库def __init__(self): self.db = MySQLdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;wenda&apos;, charset=&apos;utf8&apos;) def add_question(self, title, content): try: cursor = self.db.cursor() sql = &apos;insert into question(title, content, user_id, created_date, comment_count) values (&quot;%s&quot;,&quot;%s&quot;,%d, %s, 0)&apos; % (title, content, random.randint(1, 10) , &apos;now()&apos;); #插入数据库的SQL语句 print sql cursor.execute(sql) print cursor.lastrowid self.db.commit() except Exception, e: print e self.db.rollback() 查看爬虫运行结果： 先debug下，再调成running。pyspider框架在windows下的bug 设置跑的速度，建议不要跑的太快，否则很容易被发现是爬虫的，人家就会把你的IP给封掉的 查看运行工作 查看爬取下来的内容 然后再本地数据库GUI软件上查询下就可以看到数据已经保存到本地了。 自己需要用的话就可以导入出来了。 在开头我就告诉大家爬虫的代码了，如果详细的看看那个project，你就会找到我上传的爬取数据了。（仅供学习使用，切勿商用！） 当然你还会看到其他的爬虫代码的了，如果你觉得不错可以给个 Star，或者你也感兴趣的话，你可以fork我的项目，和我一起学习，这个项目长期更新下去。 最后： 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# created by 10412# !/usr/bin/env python# -*- encoding: utf-8 -*-# Created on 2016-10-20 20:43:00# Project: V2EXfrom pyspider.libs.base_handler import *import reimport randomimport MySQLdbclass Handler(BaseHandler): crawl_config = &#123; &#125; def __init__(self): self.db = MySQLdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;wenda&apos;, charset=&apos;utf8&apos;) def add_question(self, title, content): try: cursor = self.db.cursor() sql = &apos;insert into question(title, content, user_id, created_date, comment_count) values (&quot;%s&quot;,&quot;%s&quot;,%d, %s, 0)&apos; % (title, content, random.randint(1, 10) , &apos;now()&apos;); print sql cursor.execute(sql) print cursor.lastrowid self.db.commit() except Exception, e: print e self.db.rollback() @every(minutes=24 * 60) def on_start(self): self.crawl(&apos;https://www.v2ex.com/&apos;, callback=self.index_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/?tab=&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.tab_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def tab_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/go/&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def board_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/t/&quot;]&apos;).items(): url = each.attr.href if url.find(&apos;#reply&apos;)&gt;0: url = url[0:url.find(&apos;#&apos;)] self.crawl(url, callback=self.detail_page, validate_cert=False) for each in response.doc(&apos;a.page_normal&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) @config(priority=2) def detail_page(self, response): title = response.doc(&apos;h1&apos;).text() content = response.doc(&apos;div.topic_content&apos;).html().replace(&apos;&quot;&apos;, &apos;\\\\&quot;&apos;) self.add_question(title, content) #插入数据库 return &#123; &quot;url&quot;: response.url, &quot;title&quot;: title, &quot;content&quot;: content, &#125;","tags":[{"name":"Python","slug":"Python","permalink":"http://www.54tianzhisheng.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.54tianzhisheng.cn/tags/爬虫/"},{"name":"Pyspider","slug":"Pyspider","permalink":"http://www.54tianzhisheng.cn/tags/Pyspider/"}]},{"title":"Spring MVC系列文章（二）：Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑/","text":"SpringBoot 系列文章 项目代码地址：https://github.com/zhisheng17/springmvc最近在学习 Spring MVC ，其中在做一个简单的博客系统demo，是使用 SpringMVC 集成 Spring Data JPA（由 Hibernate JPA 提供），来进行强大的数据库访问。结果其中遇到的坑不 是一点点啊，我差点崩溃了，其中最大的原因就是由于 Hibernate JPA 中的bug了，反正一开始 还不知道是这个问题，导致折腾了快一天的时间。想想都可怕啊。 mvc-dispatch-servlet.xml代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:jpa=\"http://www.springframework.org/schema/data/jpa\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&gt; &lt;!--指明 controller 所在包，并扫描其中的注解--&gt; &lt;context:component-scan base-package=\"cn.zhisheng.controller\"/&gt; &lt;!-- 静态资源(js、image等)的访问 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 开启注解 --&gt; &lt;mvc:annotation-driven/&gt; &lt;!--ViewResolver 视图解析器--&gt; &lt;!--用于支持Servlet、JSP视图解析--&gt; &lt;bean id=\"jspViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/pages/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!-- 表示JPA Repository所在的包 --&gt; &lt;jpa:repositories base-package=\"cn.zhisheng.repository\"/&gt; &lt;bean id=\"entityManagerFactory\" class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\"&gt; &lt;property name=\"persistenceUnitName\" value=\"defaultPersistenceUnit\"/&gt; &lt;property name=\"packagesToScan\" value=\"cn.zhisheng.model\" /&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"/&gt; &lt;/property&gt; &lt;property name=\"jpaProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.connection.driver_class\"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;prop key=\"hibernate.connection.url\"&gt;jdbc:mysql://localhost:3306/springdemo?useSSL=false&lt;/prop&gt; &lt;prop key=\"hibernate.connection.username\"&gt;root&lt;/prop&gt; &lt;prop key=\"hibernate.connection.password\"&gt;root&lt;/prop&gt; &lt;prop key=\"hibernate.show_sql\"&gt;false&lt;/prop&gt; &lt;prop key=\"hibernate.connection.useUnicode\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.connection.characterEncoding\"&gt;UTF-8&lt;/prop&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.use_sql_comments\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;prop key=\"hibernate.connection.autoReconnect\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQL5Dialect&lt;/prop&gt; &lt;prop key=\"connection.autoReconnectForPools\"&gt;true&lt;/prop&gt; &lt;prop key=\"connection.is-connection-validation-required\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.validate\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.connection.provider_class\"&gt;org.hibernate.service.jdbc.connections.internal.C3P0ConnectionProvider&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.min_size\"&gt;5&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.max_size\"&gt;600&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.timeout\"&gt;1800&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.max_statements\"&gt;50&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.preferredTestQuery\"&gt;SELECT 1;&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.testConnectionOnCheckout\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.idle_test_period\"&gt;3000&lt;/prop&gt; &lt;prop key=\"javax.persistence.validation.mode\"&gt;none&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 事务管理 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.orm.jpa.JpaTransactionManager\"&gt; &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt; &lt;/bean&gt; &lt;!-- 开启事务管理注解 --&gt; &lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt;&lt;/beans&gt; pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;springmvc&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;springmvc Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;spring.version&gt;4.2.6.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;5.1.0.Final&lt;/hibernate.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.10.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-c3p0&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;springmvc&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 一开始我是用默认的在resources文件里面生成了persistence.xml配置文件进行数据库配置的，后来由于用那种方法，碰到的问题有很多，自己搞了好几个个小时都没弄好，只好换种方法，没想到竟然还是这种效果（泪崩），看来是不治标也不治本。 无奈，只好硬刚了，碰到错误，百度+google，看了大量的的解决方法，都是没用，慢慢的我所加的jar包越来越多，用maven管理的依赖的也变得多起来了，但终究是不能够解决问题的。 其实这时我看了这么多的博客和解决方法，我已经知道了是 Hibernate JPA 的bug问题，途中自己也换了一些版本，还是没能解决办法。 最后在吃完完晚饭后，又折腾了快三小时，终于找到可靠有用的解决方案了。 运行成功后，我当时就激动起来了。马丹，老子终于将你解决了。 所以在这里立马就将自己这次的血崩历史纪录下来。 下面写下遇到的问题：（其中有些可能还不记得写了） java.lang.ClassNotFoundException: javax.persistence.EntityManager java.lang.NoSuchMethodError: javax.persistence.JoinColumn.foreignKey()Ljavax/persistence/ForeignKey; javax.persistence.PersistenceException: No Persistence provider for EntityManager named defaultPersistenceUnit javax.persistence.PersistenceException: No Persistence provider for EntityManager named defaultPersi java.lang.NoClassDefFoundError: org/hibernate/ejb/HibernatePersistence java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory java.lang.ClassNotFoundException: org.hibernate.MappingException NoSuchMethodError: javax.persistence.xxx 等，还有几个，忘记了。。 首先通过报错信息可以知道有些是因为jar包的问题，但是并不是光是缺少jar包的问题，很大的原 因就是因为jar包的版本不同，刚好那个jar包又是有问题的（自身有bug）。 就比如错误： java.lang.NoSuchMethodError: javax.persistence.JoinColumn.foreignKey()Ljavax/persistence/ForeignKey; 就是因为JAVAEE6.0中的 javax.persistence.jar与 hibernate4.3.8中的hibernate-jpa-2.1-api-1.0.0.Final.jar冲突 JoinColumn.foreignKey() was introduced with JPA 2.1, which was not implemented by Hibernate 4 until version 4.3. If you’re using an older version of Hibernate 4 then try upgrading to 4.3.x. If you’re already using Hibernate 4.3 then make sure you’re also using JPA 2.1 to make sure the API and implementation match up. 图片来自 : http://stackoverflow.com/questions/24588860/error-javax-persistence-joincolumn-foreignkeyljavax-persistence-foreignkey-wi I finally solved this similar problem, there was an old version(hibernate-jpa-2.0-api-1.0.0-Final.jar) in my lib folder which I guess has been preventing maven dependency from loading. So after I manually deleted it and added (hibernate-jpa-2.1-api-1.0.0-Final.jar) everything started to work. 意思大概就是： 因为JAVAEE6.0中的 javax.persistence.jar与 hibernate4.3.8中的hibernate-jpa-2.1-api-1.0.0.Final.jar冲突 ，我们在pom文件下添加依赖后，竟然没发现在 springmvc（项目名称）\\target\\springmvc（项目名称）\\WEB-INF\\lib 下看到 javax.persistence.jar 文件，结果竟然在 springmvc\\lib下找到他了。 解决办法就是在 pom文件和 mvc-dispatcher-servlet.xml 都配置好的情况下，将 springmvc\\lib下的 javax.persistence.jar 删除。 最后再说一句：Though the error drove almost crazy, hold on, you wil get smile ！ Fighting","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://www.54tianzhisheng.cn/tags/Spring/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"},{"name":"Hibernate JPA","slug":"Hibernate-JPA","permalink":"http://www.54tianzhisheng.cn/tags/Hibernate-JPA/"},{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"}]},{"title":"记录下自己第一次坐飞机的感受","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/feiji/","text":"前提因为学校的原因，需要在大三的时候出去实训几个月，然后选择的是在昆山，公司规定要在 2017 年 4 月 19 号之前赶到，所以我提前订的机票是 17 号，之所以会订机票，其实很大一部分原因是由于自己之前没有坐过飞机，所以想体验一下坐飞机的感脚（嘿嘿），再加上这是机票的淡季，所以坐飞机的价钱比做高铁还便宜。整个行程：学校 ——&gt; 南昌机场 ——&gt; 上海虹桥机场 ——&gt; 公司 学校出发10 点左右去食堂吃了个早餐，然后打包好自己的行李，由于第一次坐飞机，所以打算提前时间去机场，然后在机场熟悉熟悉环境，不然的话错过飞机，那就 gg 了。大概 11：30 了，就拖着行李出门，在学校门口叫了辆 滴滴车 去机场（因为公交车要坐好久并且去机场的路好烂），叫的滴滴车他走的是高速，很快。 南昌机场不到半小时就到了机场，首先干的事就是取票。 然后将行李箱免费托运（小于20公斤可免费托运，超过的话那就准备好 Money 吧），一个同行的同学就超重了。干完了这些事就没啥事了，然后就坐在凳子上看起了最近很火的电视剧《人民的名义》，的确很好看的，没看过的话，可以去看看。 大概 13：40 时，准备进站了，这里要说的就比较有趣了。我就具体的说下： 一开始排队（人不多），轮到我，把机票和身份证给她，然后对着摄像头拍了个照就进去检查随身携带的物品了。 这里之前我就听说了：说是飞机上不能携带液体，不知道我书包带的洗发液能不能过关？同学和我之前还在一起开玩笑说：1、如果不能带，我就当场一口闷了它。（这也行，下图随意找了个表情包） 2、我免费请大家洗个头 电脑和雨伞而外检查 书包和随身携带物品都拿出来过安检 人扫描（检查的挺细致的，比火车严多了） 结果检查人员告知我：把 洗发液 和 两个螺丝刀 拿出来。（我当场就震惊了，我那么小的螺丝刀都被检查到了，这几乎把我的书包给透视了一个遍） 然后呢，自己只好将违禁物品都拿出来，再次检查就可以通过了。 走进去的时候拍的一张照片, （电梯好长，不过后来在上海机场下车才发现那个也还好了） 进去后，又有一个小的进站室，又在那里等待了会。 又开始检票，检完票后坐公交车去飞机下面（因为飞机场太大，我坐的那架飞机不是靠近站台的） 上飞机后从窗口拍的一张照片 飞机上看见飞机的空姐和空少了。。（空姐很漂亮，嘻嘻，没拍照片） 飞机起飞的时候，机舱里有语音播放叫我们系上安全带，并且关闭手机，所以后面就没照片了。 起飞感觉走了好远，然后突然一加速，飞机就慢慢的向上飞起，整个人重心都是往后倒的。 不一会，飞机就飞的好高了，从窗口望下去，下面的感觉好美，一路上的风景感觉都挺漂亮的，有时还可以看到白云。 大概一个小时后，空姐推着个车，提供给我们点零食（面包、饼干、榨菜）和饮料（橙汁、苹果醋、矿泉水、咖啡，可自选一样，可再加一杯），服务还是挺贴心的，我在这里给中国东方航空点个赞。 飞机中途有时会颠簸，机舱里语音说是遇到空气气流的影响。（大部分时间还是很好的） 说说飞机下降的时候吧，同样，感觉声音很大，耳朵有点受不了。不知道空姐、空少们怎么长期受得了这这声音，长期下去，那估计耳朵都要出毛病的吧？ 上海虹桥机场机场给人的感觉很大，走了那个电梯都走了很远。 就只有一张照片。 然后再去取我的行李箱，有个专门来自南昌的取行李处，行李箱一个的放在传送带上，自己拿自己的行李出去。这里有个疑问，如果行李被别人提走的话，那该咋办，我看出口都没设置检查行李是否和本人的匹配？ 公司公司提前有大巴来接我们，坐上大巴，然后在车上躺了下，就睡着了，直到到达公司。 住宿地址和公司有点远，所以买了辆 死飞自行车，方便出行，自己周末也可以出去玩玩。 总结第一次坐飞机，感觉还是挺好的。流程也不复杂，所以不麻烦。 不过自己要注意一些东西： 不要携带飞机上违禁物品（液体、超过多大的充电宝、刀枪等） 一定要提前动身，以防发生突发事件 注意保管好自己的身份证 最后这是我在我博客上写的第一篇随笔文章，不知道咋写，就随便写。 因为星期六（昨天）出去玩了，所以今天早上起来写下来这篇，有些事还是得赶紧做掉去，不然一直拖，拖着拖着就没有想做的欲望了。 公司周围环境挺好的，水乡之地，有个阳澄湖，盛产大闸蟹，周围的大闸蟹庄，那叫一个字：多。有空的时候我尽量多去逛逛，拍点照片回来。 今后也会多写点随笔，记录生活中一些有趣且值得纪念的点滴。 感谢阅读！！！","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"从对象深入分析 Java 中实例变量和类变量的区别","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/java-var/","text":"实例变量 和 类变量局部变量特点：作用时间短，存储在方法的栈内存中 种类： 形参：方法签名中定义的局部变量，由方法调用者负责为其赋值，随方法结束而消亡 方法内的局部变量：方法内定义的局部变量，必须在方法内对其进行显示初始化，从初始化后开始生效，随方法结束而消亡 代码块内的局部变量：在代码块中定义的局部变量，必须在代码块中进行显示初始化，从初始化后开始生效，随代码块结束而消亡 成员变量类体内定义的变量，如果该成员变量没有使用 static 修饰，那该成员变量又被称为非静态变量或实例变量，如果使用 static 修饰，则该成员变量又可被称为静态变量或类变量。 实例变量和类变量的属性使用 static 修饰的成员变量是类变量，属于该类本身，没有使用 static 修饰的成员变量是实例变量，属于该类的实例，在同一个类中，每一个类只对应一个 Class 对象，但每个类可以创建多个对象。 由于同一个 JVM 内的每个类只对应一个 CLass 对象，因此同一个 JVM 内的一个类的类变量只需要一块内存空间；但对于实例变量而言，该类每创建一次实例，就需要为该实例变量分配一块内存空间。也就是说，程序中创建了几个实例，实例变量就需要几块内存空间。 这里我想到一道面试题目： 123456789101112public class A&#123; &#123; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); &#125;&#125; 结果： 123我是静态代码块我是代码块我是代码块 静态代码块只执行一次，而代码块每创建一个实例，就会打印一次。 实例变量的初始化时机程序可在3个地方对实例变量执行初始化： 定义实例变量时指定初始值 非静态初始化块中对实例变量指定初始值 构造器中对实例变量指定初始值 上面第一种和第二种方式比第三种方式更早执行，但第一、二种方式的执行顺序与他们在源程序中的排列顺序相同。 同样在上面那个代码上加上一个变量 weight 的成员变量，我们来验证下上面的初始化顺序： 1、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之后: 123456789101112131415public class A&#123; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; double weight = 2.0; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果是： 1234我是静态代码块我是代码块我是代码块2.0 2、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之前: 123456789101112131415public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果为： 1234我是静态代码块我是代码块我是代码块2.1 大家有没有觉得很奇怪？ 我来好好说清楚下： 定义实例变量时指定的初始值、初始代码块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将会被提取到构造器中。也就是说，这条语句 double weight = 2.0; 实际上会被分成如下 2 次执行： double weight; : 创建 Java 对象时系统根据该语句为该对象分配内存。 weight = 2.1; : 这条语句将会被提取到 Java 类的构造器中执行。 只说原理，大家肯定不怎么信，那么还有拿出源码来，这样才有信服能力的吗？是不？ 这里我直接使用软件将代码的字节码文件反编译过来，看看里面是怎样的组成？ 第一个代码的反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.1D; System.out.println(\"我是代码块\"); this.weight = 2.0D; &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 第二个代码反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 这下子满意了吧！ 通过反编译的源码可以看到该类定义的 weight 实例变量时不再有初始值，为 weight 指定初始值的代码也被提到了构造器中去了，但是我们也可以发现之前规则也是满足的。 他们的赋值语句都被合并到构造器中，在合并过程中，定义的变量语句转换得到的赋值语句，初始代码块中的语句都转换得到的赋值语句，总是位于构造器的所有语句之前，合并后，两种赋值语句的顺序也保持了它们在 Java 源代码中的顺序。 大致过程应该了解了吧？如果还不怎么清楚的，建议还是自己将怎个过程在自己的电脑上操作一遍，毕竟光看不练假把式。 类变量的初始化时机JVM 对每一个 Java 类只初始化一次，因此 Java 程序每运行一次，系统只为类变量分配一次内存空间，执行一次初始化。程序可在两个地方对类变量执行初始化： 定义类变量时指定初始值 静态初始化代码块中对类变量指定初始值 这两种方式的执行顺序与它们在源代码中的排列顺序相同。 还是用上面那个示例，我们在其基础上加个被 static 修饰的变量 height： 1、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后： 123456789101112131415161718public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.0 2、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前： 123456789101112131415161718public class A&#123; static double height = 10.0; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.1 其运行结果正如我们预料，但是我们还是看看反编译后的代码吧！ 第一种情况下反编译的代码： 1234567891011121314151617181920212223public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0D; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 第二种情况下反编译的代码： 123456789101112131415161718192021222324public class A&#123; static double height = 10.0D; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; height = 10.1D; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 通过反编译源码，可以看到第一种情况下(定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后): 我们在 静态初始化代码块中对类变量指定初始值 已经不存在了，只有一个类变量指定的初始值 static double height = 10.0D; , 而在第二种情况下（定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前）和之前的源代码顺序是一样的，没啥区别。 上面的代码中充分的展示了类变量的两种初始化方式 ：每次运行该程序时，系统会为 A 类执行初始化，先为所有类变量分配内存空间，再按照源代码中的排列顺序执行静态初始代码块中所指定的初始值和定义类变量时所指定的初始值。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"Java读取文件","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/java读取文件/","text":"以字节为单位读取文件 以字符为单位读取文件 以行为单位读取文件 随机读取文件内容 ReadFromFile.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263package cn.zhisheng.io;import java.io.*;/** * java读取文件 * Created by 10412 on 2016/12/29. */public class ReadFromFile&#123; /** * 以字节为单位读取文件，常用于读二进制文件，如图片、声音、影像等文件 * @param fileName 文件名 */ public static void readFileByBytes(String fileName) &#123; File file = new File(fileName); InputStream in = null; try &#123; System.out.println(\"以字节为单位读取文件内容，一次读取一个字节\"); //一次读一个字节 in = new FileInputStream(file); int tempbyte; while ((tempbyte = in.read()) != -1) &#123; System.out.println(tempbyte); &#125; in.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); return; &#125; try &#123; System.out.println(\"以字节为单位读取文件内容，一次读取多个字节\"); //一次读取多个字节 byte[] tempbytes = new byte[100]; int byteread = 0; in = new FileInputStream(fileName); ReadFromFile.showAvailableBytes(in); // 读入多个字节到字节数组中，byteread为一次读入的字节数 while ((byteread = in.read(tempbytes)) != -1) &#123; System.out.write(tempbytes, 0, byteread); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (in != null) &#123; try &#123; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 以字符为单位读取文件，常用于读文本，数字等类型的文件 * @param fileName 文件名 */ public static void readFileByChars(String fileName) &#123; File file = new File(fileName); Reader reader = null; try &#123; System.out.println(\"以字符为单位读取文件内容，一次读一个字符：\"); // 一次读一个字符 reader = new InputStreamReader(new FileInputStream(file)); int tempchar; while ((tempchar = reader.read()) != -1) &#123; // 对于windows下，\\r\\n这两个字符在一起时，表示一个换行。 // 但如果这两个字符分开显示时，会换两次行。 // 因此，屏蔽掉\\r，或者屏蔽\\n。否则，将会多出很多空行。 if (((char)tempchar) != '\\r') &#123; System.out.print((char) tempchar); &#125; &#125; reader.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; System.out.println(\"以字符为单位读取文件内容，一次读多个字符：\"); //一次读多个字符 char[] tempchars = new char[30]; int charread = 0; reader = new InputStreamReader(new FileInputStream(fileName)); // 读入多个字符到字符数组中，charread为一次读取字符数 while ((charread = reader.read(tempchars)) != -1) &#123; // 同样屏蔽掉\\r不显示 if ((charread == tempchars.length) &amp;&amp; (tempchars[tempchars.length - 1]) != '\\r') &#123; System.out.print(tempchars); &#125; else &#123; for (int i = 0; i &lt; charread; i++ ) &#123; if (tempchars[i] == '\\r') &#123; continue; &#125; else &#123; System.out.print(tempchars[i]); &#125; &#125; &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 以行为单位读取文件，常用于读面向行的格式化文件 * @param fileName 文件名 */ public static void readFileByLines(String fileName) &#123; File file = new File(fileName); BufferedReader reader =null; try &#123; System.out.println(\"以行为单位读取文件内容，一次读一整行：\"); reader = new BufferedReader(new FileReader(file)); String tempString = null; int line = 1; // 一次读入一行，直到读入null为文件结束 while ((tempString = reader.readLine()) != null) &#123; // 显示行号 System.out.println(\"line \"+line+\": \"+tempString); line++; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 随机读取文件内容 * @param fileName 文件名 */ public static void readFileBRandomAccess(String fileName) &#123; RandomAccessFile randomFile = null; try &#123; System.out.println(\"随机读取一段文件内容：\"); // 打开一个随机访问文件流，按只读方式 randomFile = new RandomAccessFile(fileName, \"r\"); // 文件长度，字节数 long fileLength = randomFile.length(); // 读文件的起始位置 int beginIndex = (fileLength &gt; 4) ? 4 : 0; // 将读文件的开始位置移到beginIndex位置 randomFile.seek(beginIndex); byte[] bytes = new byte[10]; int byteread = 0; // 一次读10个字节，如果文件内容不足10个字节，则读剩下的字节。 // 将一次读取的字节数赋给byteread while ((byteread = randomFile.read(bytes)) != -1) &#123; System.out.write(bytes, 0, byteread); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (randomFile != null) try &#123; randomFile.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 显示输入流中剩余的字节数 * @param in */ public static void showAvailableBytes(InputStream in) &#123; try &#123; System.out.println(\"当前字节流输入流中剩余的字节数为:\"+in.available()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\1.txt\"; //文本文件 //String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\sp20161227_204413.png\"; //图片文件 //readFileByBytes(fileName); //readFileByChars(fileName); //readFileByLines(fileName); readFileBRandomAccess(fileName); &#125;&#125; 文件追加内容AppendToFile.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.zhisheng.io;import java.io.FileNotFoundException;import java.io.FileWriter;import java.io.IOException;import java.io.RandomAccessFile;/** * 追加内容到文件尾部 * Created by 10412 on 2016/12/29. */public class AppendToFile&#123; /** * 第一种方法追加文件：使用RandomAccessFile * @param fileName 文件名 * @param content 追加内容 */ public static void appendMethod1(String fileName, String content) &#123; try &#123; // 打开一个随机访问文件流，按读写方式 RandomAccessFile randomFile = new RandomAccessFile(fileName, \"rw\"); // 文件长度，字节数 long fileLength = randomFile.length(); //将写文件指针移到文件尾 randomFile.seek(fileLength); randomFile.writeBytes(content); randomFile.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 第二种方法追加文件：使用FileWriter * @param fileName 文件名 * @param content 追加内容 */ public static void appendMethod2(String fileName, String content) &#123; try &#123; //打开一个写文件器，构造函数中的第二个参数true表示以追加形式写文件 FileWriter writer = new FileWriter(fileName, true); writer.write(content); writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\1.txt\"; //文本文件 String content = \"new append!\"; //按方法1追加文件// AppendToFile.appendMethod1(fileName, content);// AppendToFile.appendMethod1(fileName, \"\\new append. 第一种方法\\n\"); //按照方法2追加文件 AppendToFile.appendMethod2(fileName, content); AppendToFile.appendMethod2(fileName, \"\\nnew append. 第二种方法\\n\"); //显示文件内容 ReadFromFile.readFileByLines(fileName); &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"文件","slug":"文件","permalink":"http://www.54tianzhisheng.cn/tags/文件/"}]},{"title":"奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True?","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True-/","text":"这是我们今天要讨论的话题，因为我觉得它非常的有趣。 如果你运行如下代码： 12345678910class A&#123; public static void main(String[] args) &#123; Integer a = 128, b = 128; System.out.println(a == b); Integer c = 127, d = 127; System.out.println(c == d); &#125;&#125; 你会得到如下结果： 12falsetrue 我们知道，如果两个引用指向同一个对象，那么==就成立；反之，如果两个引用指向的不是同一个对象，那么==就不成立，即便两个引用的内容是一样的。因此，结果就会出现false。 这是非常有趣的地方。如果你查看Integer.java类，你会找到IntegerCache.java这个内部私有类，它为-128到127之间的所有整数对象提供缓存。 这个东西为那些数值比较小的整数提供内部缓存，当进行如此声明时： 1Integer c = 127 它的内部就是这样的： 1Integer var3 = Integer.valueOf(127); 其实我通过将A.class文件反编译后，代码如下图： 如果我们观察valueOf()类函数，我们可以看到： 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 通过看源码能够知道，整数类型在-128～127之间时，会使用缓存，造成的效果就是，如果已经创建了一个相同的整数，使用valueOf创建第二次时，不会使用new关键字，而用已经缓存的对象。所以使用valueOf方法创建两次对象，若对应的数值相同，且数值在-128～127之间时，两个对象都指向同一个地址。 因此。。。 1Integer c = 127, d = 127; 两者指向同样的对象。 这就是为什么下面这段代码的结果为true了： 1System.out.println(c == d); 现在你可能会问，为什么会为-128到127之间的所有整数设置缓存？ 这是因为在这个范围内的小数值整数在日常生活中的使用频率要比其它的大得多，多次使用相同的底层对象这一特性可以通过该设置进行有效的内存优化。你可以使用reflection API任意使用这个功能。 运行下面的这段代码，你就会明白它的神奇所在了。 12345678910111213public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Class cache = Integer.class.getDeclaredClasses()[0]; Field myCache = cache.getDeclaredField(&quot;cache&quot;); myCache.setAccessible(true); Integer[] newCache = (Integer[]) myCache.get(cache); newCache[132] = newCache[133]; int a = 2; int b = a + a; System.out.printf(&quot;%d + %d = %d&quot;, a, a, b); // &#125; 打印结果竟然是： 12 + 2 = 5 我们再次看一下反汇编代码： 是不是又和上面的是同一个问题呢？ 但是结果为什么是 2 + 2 = 5 呢？ 我们继续去看一下 Integer 源码，去深入了解 Integer 缓存机制，下面截个图： 根据源码可以发现最后修改 Integer 缓存上限时候的方法有点小瑕疵。我们看看Api给我们怎么建议的一段话：1the size of the cache may be controlled by the &#123;@code -XX:AutoBoxCacheMax=&lt;size&gt;&#125; option. 原来我们只需要：运行时设置 -XX:AutoBoxCacheMax=133 就OK。 参考文章： 奇怪的Java题：为什么1000 == 1000返回为False，而100 == 100会返回为True?","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"程序访问文件的几种方式","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/程序访问文件的几种方式/","text":"IO程序访问文件的几种方式 读取和写入文件 I/O 操作都调用操作系统提供的接口。因为磁盘设备是由操作系统管理的，应用程序要访问物理设备只能通过系统调用的方式来工作。读和写分别对应 read() 和 write() 两个系统调用。而只要是系统调用就可能存在内核空间地址和用户空间地址切换的问题，这也是为什么操作系统为了保护系统本身的运行安全而将内核程序运行使用的内存空间和用户程序运行的内存空间进行隔离造成的。虽然这样可以保证内核程序运行的安全性，但是也存在数据可能需要从内核空间向用户空间复制的问题。 如果遇到非常耗时的操作，如磁盘 I/O， 数据从磁盘复制到内核空间，然后又从内核空间复制到用户空间，将会非常缓慢。这时操作系统为了加速 I/O 访问，在内核空间使用缓存机制，也就是将从磁盘读取的文件按照一定的组织方式进行缓存，如果用户程序访问的是同一段磁盘地址的空间数据，那么操作系统将从内核缓存中直接取出返回给用户程序，这样就可以减小 I/O 的响应时间。 1. 标准访问文件的方式标准访问文件的方式就是当应用程序调用 read() 接口时，操作系统检查在内核的高速缓存中有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回，如果没有，则从磁盘中读取，然后缓存在操作系统的缓存中。 写入的方式是，用户的应用程序调用 write() 接口将数据从用户地址空间复制到内核地址空间的缓存中。这时对用户程序来说写操作就已经完成了，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用 sync 同步命令。 标准访问文件的方式如下图所示： 2. 直接 I/O 的方式直接 I/O 方式就是应用程序直接访问磁盘数据，而不经过操作系统内核数据缓冲区，这样做的目的就是减少一次从内核缓冲区到用户程序缓存的数据复制。此种方式通常是在对数据的缓存管理由应用程序实现的数据库管理系统中。如在数据库管理系统中，系统明确的知道应该缓存哪些数据，应该失效哪些数据，还可以对一些热点的数据进行预加载，提前将热点数据加载到内存，可以加速数据的访问效率。在这些情况下，如果是由操作系统进行缓存，则很难做到，因为操作系统并不知道哪些是热点数据，哪些数据是访问一次后再也不会访问了，操作系统就是简单的缓存最近一次从磁盘读取的数据。 但是直接 I/O 也有负面的影响，如果访问的数据不再应用程序缓存中，则每次数据的加载都需要从磁盘读取，样加载的话速度非常的慢，通常是直接 I/O 与 异步 I/O 结合使用，会得到较好的性能。 直接 I/O 的方式如下图所示： 3. 同步访问文件的方式同步访问文件的方式就是数据的读取和写入都是同步操作的，它与标准访问文件的方式不同的是，只有当数据被成功写到磁盘时才返回给应用程序成功的标志。 这种访问文件的方式性能比较差，只有在一些数据安全性要求比较高的场景中才会使用，而且通常这种方式的硬件都是定制的。 同步访问文件的方式如下图所示： 4. 异步访问文件的方式异步访问文件的方式就是当访问数据的线程发出请求之后，线程会接着去处理其他事情，而不是阻塞等待，当请求的数据返回后继续处理下面的操作。这种方式可以明显的提高应用程序的效率，但是不会改变访问文件的效率。 异步访问文件的方式如下图所示： 5. 内存映射的方式内存映射的方式是指操作系统将内存中的某一块区域与磁盘中的文件关联起来，当要访问内存中的一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据从内核空间缓存到用户空间缓存的数据复制操作，因为这两个空间的数据是共享的。 内存映射的方式如下图所示： 注：以上参考书籍《深入分析Java Web 技术内幕修订版》许令波，更多精彩知识还请看原书。","tags":[{"name":"IO","slug":"IO","permalink":"http://www.54tianzhisheng.cn/tags/IO/"}]},{"title":"HashMap、Hashtable、HashSet 和 ConcurrentHashMap 的比较","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/HashMap-Hashtable/","text":"HashMap 和 Hashtable 的比较是 Java 面试中的常见问题，用来考验程序员是否能够正确使用集合类以及是否可以随机应变使用多种思路解决问题。HashMap 的工作原理、ArrayList 与 Vector 的比较以及这个问题是有关 Java 集合框架的最经典的问题。Hashtable 是个过时的集合类，存在于 Java API 中很久了。在 Java 4 中被重写了，实现了 Map 接口，所以自此以后也成了 Java 集合框架中的一部分。Hashtable 和 HashMap 在 Java 面试中相当容易被问到，甚至成为了集合框架面试题中最常被考的问题，所以在参加任何 Java 面试之前，都不要忘了准备这一题。这篇文章中，我们不仅将会看到 HashMap 和 Hashtable 的区别，还将看到它们之间的相似之处。 HashMap 和 Hashtable 的区别HashMap 和 Hashtable 都实现了 Map 接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性，同步 (synchronization)，以及速度。 HashMap 几乎可以等价于 Hashtable，除了 HashMap 是非 synchronized 的，并可以接受 null(HashMap 可以接受为 null 的键值 (key) 和值 (value)，而 Hashtable 则不行)。 HashMap 是非 synchronized，而 Hashtable 是 synchronized，这意味着 Hashtable 是线程安全的，多个线程可以共享一个 Hashtable；而如果没有正确的同步的话，多个线程是不能共享 HashMap 的。Java 5 提供了 ConcurrentHashMap，它是 HashTable 的替代，比 HashTable 的扩展性更好。 另一个区别是 HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器，而 Hashtable 的 enumerator 迭代器不是 fail-fast 的。所以当有其它线程改变了 HashMap 的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的 remove() 方法移除元素则不会抛出ConcurrentModificationException 异常。但这并不是一个一定发生的行为，要看 JVM。这条同样也是Enumeration 和 Iterato r的区别。 由于 Hashtable 是线程安全的也是 synchronized，所以在单线程环境下它比 HashMap 要慢。如果你不需要同步，只需要单一线程，那么使用 HashMap 性能要好过 Hashtable。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 要注意的一些重要术语：1) sychronized 意味着在一次仅有一个线程能够更改 Hashtable。就是说任何线程要更新 Hashtable 时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新 Hashtable。 2) Fail-safe 和 iterator 迭代器相关。如果某个集合对象创建了 Iterator 或者 ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出 ConcurrentModificationException 异常。但其它线程可以通过 set() 方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用 set() 方法，将会抛出 IllegalArgumentException 异常。 3) 结构上的更改指的是删除或者插入一个元素，这样会影响到 map 的结构。 我们能否让 HashMap 同步？HashMap 可以通过下面的语句进行同步：Map m = Collections.synchronizeMap(hashMap); 结论Hashtable 和 HashMap 有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用 Java 5 或以上的话，请使用 ConcurrentHashMap 吧。 转载自：HashMap和Hashtable的区别 关于 HashMap 线程不安全这一点，《Java并发编程的艺术》一书中是这样说的： HashMap 在并发执行 put 操作时会引起死循环，导致 CPU 利用率接近 100%。因为多线程会导致 HashMap 的 Node 链表形成环形数据结构，一旦形成环形数据结构，Node 的 next 节点永远不为空，就会在获取 Node 时产生死循环。 原因： 疫苗：JAVA HASHMAP的死循环 —— 酷壳 HashMap在java并发中如何发生死循环 How does a HashMap work in JAVA 下面的是自己有道云笔记中记录的： HashMap ， HashTable 和 HashSet 区别 关于 HashMap 的一些说法： a) HashMap 实际上是一个“链表散列”的数据结构，即数组和链表的结合体。HashMap 的底层结构是一个数组，数组中的每一项是一条链表。 b) HashMap 的实例有俩个参数影响其性能： “初始容量” 和 装填因子。 c) HashMap 实现不同步，线程不安全。 HashTable 线程安全 d) HashMap 中的 key-value 都是存储在 Entry 中的。 e) HashMap 可以存 null 键和 null 值，不保证元素的顺序恒久不变，它的底层使用的是数组和链表，通过hashCode() 方法和 equals 方法保证键的唯一性 f) 解决冲突主要有三种方法：定址法，拉链法，再散列法。HashMap 是采用拉链法解决哈希冲突的。 注： 链表法是将相同 hash 值的对象组成一个链表放在 hash 值对应的槽位； 用开放定址法解决冲突的做法是：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。 沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。 拉链法解决冲突的做法是： 将所有关键字为同义词的结点链接在同一个单链表中 。若选定的散列表长度为m，则可将散列表定义为一个由m个头指针组成的指针数 组T[0..m-1]。凡是散列地址为i的结点，均插入到以T[i]为头指针的单链表中。T中各分量的初值均应为空指针。在拉链法中，装填因子α可以大于1，但一般均取α≤1。拉链法适合未规定元素的大小。 Hashtable 和 HashMap 的区别： a) 继承不同。 public class Hashtable extends Dictionary implements Map public class HashMap extends AbstractMap implements Map b) Hashtable 中的方法是同步的，而 HashMap 中的方法在缺省情况下是非同步的。在多线程并发的环境下，可以直接使用 Hashtable，但是要使用 HashMap 的话就要自己增加同步处理了。 c) Hashtable 中， key 和 value 都不允许出现 null 值。 在 HashMap 中， null 可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为 null 。当 get() 方法返回 null 值时，即可以表示 HashMap 中没有该键，也可以表示该键所对应的值为 null 。因此，在 HashMap 中不能由 get() 方法来判断 HashMap 中是否存在某个键， 而应该用 containsKey() 方法来判断。 d) 两个遍历方式的内部实现上不同。Hashtable、HashMap 都使用了Iterator。而由于历史原因，Hashtable还使用了 Enumeration 的方式 。 e) 哈希值的使用不同，HashTable 直接使用对象的 hashCode。而 HashMap 重新计算 hash 值。 f) Hashtable 和 HashMap 它们两个内部实现方式的数组的初始大小和扩容的方式。HashTable 中 hash 数组默认大小是11，增加的方式是 old*2+1。HashMap 中 hash 数组的默认大小是 16，而且一定是2的指数。 注： HashSet 子类依靠 hashCode() 和 equal() 方法来区分重复元素。 HashSet 内部使用 Map 保存数据，即将 HashSet 的数据作为 Map 的 key 值保存，这也是 HashSet 中元素不能重复的原因。而 Map 中保存 key 值的,会去判断当前 Map 中是否含有该 Key 对象，内部是先通过 key 的hashCode, 确定有相同的 hashCode 之后，再通过 equals 方法判断是否相同。 《HashMap 的工作原理》 HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道 Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！ 先来些简单的问题“你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？” 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。 “如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？” 除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了： “你了解重新调整HashMap大小存在什么问题吗？” 你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 我们可以使用CocurrentHashMap来代替Hashtable吗？ 这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看 这篇博客 查看Hashtable和ConcurrentHashMap的区别。 我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念 HashMap中解决碰撞的方法 equals()和hashCode()的应用，以及它们在HashMap中的重要性 不可变对象的好处 HashMap多线程的条件竞争 重新调整HashMap的大小 总结HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。你可以查看更多的关于HashMap的文章: HashMap和Hashtable的区别 HashMap和HashSet的区别 转载自：HashMap的工作原理 其他的 HashMap 学习资料： jdk7中HashMap知识点整理 HashMap源码分析（四）put-jdk8-红黑树的引入 JDK7与JDK8中HashMap的实现 JDK1.8HashMap原理和源码分析(java面试收藏) 谈谈ConcurrentHashMap1.7和1.8的不同实现 jdk1.8的HashMap和ConcurrentHashMap ConcurrentHashMap源码分析（JDK8版本） 最后谢谢阅读，如果可以的话欢迎大家转发和点赞。如需转载注明原地址就行。 群 528776268 欢迎各位大牛进群一起讨论。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解/","text":"javap 和 javac javac -verbose 类名.java java -verbose 类名 javap -c 类名 javap -verbose 类名 javap -help用法: javap 其中, 可能的选项包括: -help –help -? 输出此用法消息 -version 版本信息 -v -verbose 输出附加信息 -l 输出行号和本地变量表 -public 仅显示公共类和成员 -protected 显示受保护的/公共类和成员 -package 显示程序包/受保护的/公共类 和成员 (默认) -p -private 显示所有类和成员 -c 对代码进行反汇编 -s 输出内部类型签名 -sysinfo 显示正在处理的类的 系统信息 (路径, 大小, 日期, MD5 散列) -constants 显示最终常量 -classpath 指定查找用户类文件的位置 -cp 指定查找用户类文件的位置 -bootclasspath 覆盖引导类文件的位置 javac -help用法: javac 其中, 可能的选项包括: -g 生成所有调试信息 -g:none 不生成任何调试信息 -g:{lines,vars,source} 只生成某些调试信息 -nowarn 不生成任何警告 -verbose 输出有关编译器正在执行的操作的消息 -deprecation 输出使用已过时的 API 的源位置 -classpath &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -cp &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -sourcepath &lt;路径&gt; 指定查找输入源文件的位置 -bootclasspath &lt;路径&gt; 覆盖引导类文件的位置 -extdirs &lt;目录&gt; 覆盖所安装扩展的位置 -endorseddirs &lt;目录&gt; 覆盖签名的标准路径的位置 -proc:{none,only} 控制是否执行注释处理和/或编译。 -processor [,,…] 要运行的注释处理程序的名称; 绕过默认的搜索进程 -processorpath &lt;路径&gt; 指定查找注释处理程序的位置 -parameters 生成元数据以用于方法参数的反射 -d &lt;目录&gt; 指定放置生成的类文件的位置 -s &lt;目录&gt; 指定放置生成的源文件的位置 -h &lt;目录&gt; 指定放置生成的本机标头文件的位置 -implicit:{none,class} 指定是否为隐式引用文件生成类文件 -encoding &lt;编码&gt; 指定源文件使用的字符编码 -source &lt;发行版&gt; 提供与指定发行版的源兼容性 -target &lt;发行版&gt; 生成特定 VM 版本的类文件 -profile &lt;配置文件&gt; 请确保使用的 API 在指定的配置文件中可用 -version 版本信息 -help 输出标准选项的提要 -A关键字[=值] 传递给注释处理程序的选项 -X 输出非标准选项的提要 -J&lt;标记&gt; 直接将 &lt;标记&gt; 传递给运行时系统 -Werror 出现警告时终止编译 @&lt;文件名&gt; 从文件读取选项和文件名 jps用来查看基于HotSpot的JVM里面中，所有具有访问权限的Java进程的具体状态, 包括进程ID，进程启动的路径及启动参数等等，与unix上的ps类似，只不过jps是用来显示java进程，可以把jps理解为ps的一个子集。 使用jps时，如果没有指定hostid，它只会显示本地环境中所有的Java进程；如果指定了hostid，它就会显示指定hostid上面的java进程，不过这需要远程服务上开启了jstatd服务。 jps -helpusage: jps [-help] jps [-q] [-mlvV] [&lt;hostid&gt;] Definitions: &lt;hostid&gt;: &lt;hostname&gt;[:&lt;port&gt;] -q：忽略输出的类名、Jar名以及传递给main方法的参数，只输出pid。 -m：输出传递给main方法的参数，如果是内嵌的JVM则输出为null。 -l：输出完全的包名，应用主类名，jar的完全路径名 -v：输出传给jvm的参数 -V：输出通过标记的文件传递给JVM的参数（.hotspotrc文件，或者是通过参数-XX:Flags=指定的文件）。 -J 用于传递jvm选项到由javac调用的java加载器中，例如，“-J-Xms48m”将把启动内存设置为48M，使用-J选项可以非常方便的向基于Java的开发的底层虚拟机应用程序传递参数。 jstackjstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息，如果是在64位机器上，需要指定选项”-J-d64”，Windows的jstack使用方式只支持以下的这种方式： jstack [-l] pid 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack -helpUsage: jstack [-l] &lt;pid&gt; (to connect to running process) jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server) Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung)(当’jstack [-l] pid’没有相应的时候强制打印栈信息) -m to print both java and native frames (mixed mode)(打印java和native c/c++框架的所有栈信息.) -l long listing. Prints additional information about locks (长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表.) -h or -help to print this help message (打印帮助信息) jstatJstat 用于监控基于HotSpot的JVM，对其堆的使用情况进行实时的命令行的统计，使用jstat我们可以对指定的JVM做如下监控： 类的加载及卸载情况 查看新生代、老生代及持久代的容量及使用情况 查看新生代、老生代及持久代的垃圾收集情况，包括垃圾回收的次数及垃圾回收所占用的时间 查看新生代中Eden区及Survior区中容量及分配情况等 jstat -help Usage: jstat -help|-options jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] Definitions:&gt; An option reported by the -options option Virtual Machine Identifier. A vmid takes the following form: [@[:]] Where is the local vm identifier for the target Java virtual machine, typically a process id; is the name of the host running the target Java virtual machine; and is the port number for the rmiregistry on the target host. See the jvmstat documentation for a more complete description of the Virtual Machine Identifier. Number of samples between header lines. Sampling interval. The following forms are allowed: [“ms”|”s”] Where is an integer and the suffix specifies the units as milliseconds(“ms”) or seconds(“s”). The default units are “ms”. Number of samples to take before terminating. -J Pass directly to the runtime system. 参考文章1、jstat命令详解 2、jstat命令(Java Virtual Machine Statistics Monitoring Tool) 3、http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jstat.html#class_option jmap打印出某个java进程（使用pid）内存内的，所有‘对象’的情况（如：产生那些对象，及其数量）。 可以输出所有内存中对象的工具，甚至可以将VM 中的heap，以二进制输出成文本。使用方法 jmap -histo pid 如果连用SHELL jmap -histo pid&gt;a.log 可以将其保存到文本中去，在一段时间后，使用文本对比工具，可以对比出GC回收了哪些对象。 jmap -dump:format=b,file=outfile 3024 可以将3024进程的内存heap输出出来到outfile文件里，再配合MAT（内存分析工具(Memory Analysis Tool），使用参见：http://blog.csdn.net/fenglibing/archive/2011/04/02/6298326.aspx）或与jhat (Java Heap Analysis Tool)一起使用，能够以图像的形式直观的展示当前内存是否有问题。 64位机上使用需要使用如下方式： jmap -J-d64 -heap pid jmap -helpUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the &quot;live&quot; suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The &quot;live&quot; suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system 参数说明 1)、options： executable Java executable from which the core dump was produced.(可能是产生core dump的java可执行程序) core 将被打印信息的core dump文件 remote-hostname-or-IP 远程debug服务的主机名或ip server-id 唯一id,假如一台主机上多个远程debug服务 2）、基本参数： -dump:[live,]format=b,file= 使用hprof二进制形式,输出jvm的heap内容到文件=. live子选项是可选的，假如指定live选项,那么只输出活的对象到文件. -finalizerinfo 打印正等候回收的对象的信息. -heap 打印heap的概要信息，GC使用的算法，heap的配置及wise heap的使用情况. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量. -permstat 打印classload和jvm heap长久层的信息. 包含每个classloader的名字,活泼性,地址,父classloader和加载的class数量. 另外,内部String的数量和占用内存数也会打印出来. -F 强迫.在pid没有相应的时候使用-dump或者-histo参数. 在这个模式下,live子参数无效. -h | -help 打印辅助信息 -J 传递参数给jmap启动的jvm. pid 需要被打印配相信息的java进程id,创业与打工的区别 - 博文预览,可以用jps查问. jinfojinfo 可以输出并修改运行时的java 进程的opts。 用处比较简单，用于输出JAVA系统参数及命令行参数。 用法是 jinfo -opt pid 如：查看2788的MaxPerm大小可以用 jinfo -flag MaxPermSize 2788。 jinfo -help Usage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message jconsole一个java GUI监视工具，可以以图表化的形式显示各种数据。并可通过远程连接监视远程的服务器VM。用java写的GUI程序，用来监控VM，并可监控远程的VM，非常易用，而且功能非常强。命令行里打 jconsole，选则进程就可以了。 需要注意的就是在运行jconsole之前，必须要先设置环境变量DISPLAY，否则会报错误，Linux下设置环境变量如下： export DISPLAY=:0.0 可以这里选择查看本地进程的状况，还是远程进程的状况 通过这张图可以看到内存、线程、类及CPU使用的一些情况。 jvisualvm参考文章： 程序员必备利器—Java程序性能分析工具Java VisualVM（Visual GC） jhat用于对JAVA heap进行离线分析的工具，他可以对不同虚拟机中导出的heap信息文件进行分析，如Linux上导出的文件可以拿到WINDOWS上进行分析，可以查找诸如内存方面的问题，使用方式可以查看这篇文章： jhat命令 不过jhat和MAT比较起来，就没有MAT那么直观了，MAT是以图形界面的方式展现结果，MAT的使用方式可以参看文章： MAT(Memory Analyzer Tool)工具入门介绍 Usage:jhat [-stack ] [-refs ] [-port ] [-baseline ] [-debug ] [-version] [-h|-help] -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system. For example, -J-mx512m to use a maximum heap size of 512MB -stack false: Turn off tracking object allocation call stack. -refs false: Turn off tracking of references to objects -port &lt;port&gt;: Set the port for the HTTP server. Defaults to 7000 -exclude &lt;file&gt;: Specify a file that lists data members that should be excluded from the reachableFrom query. -baseline &lt;file&gt;: Specify a baseline object dump. Objects in both heap dumps with the same ID and same class will be marked as not being &quot;new&quot;. -debug &lt;int&gt;: Set debug level. 0: No debug output 1: Debug hprof file parsing 2: Debug hprof file parsing, no server -version Report version number -h|-help Print this help and exit &lt;file&gt; The file to read jdb用来对core文件和正在运行的Java进程进行实时地调试，里面包含了丰富的命令帮助您进行调试，它的功能和Sun studio里面所带的dbx非常相似，但 jdb是专门用来针对Java应用程序的。 jstatdjstatd是一个基于RMI（Remove Method Invocation）的服务程序，它用于监控基于HotSpot的JVM中资源的创建及销毁，并且提供了一个远程接口允许远程的监控工具连接到本地的JVM执行命令。 jstatd是基于RMI的，所以在运行jstatd的服务器上必须存在RMI注册中心，如果没有通过选项”-p port”指定要连接的端口，jstatd会尝试连接RMI注册中心的默认端口。 用法： jstatd [-nr] [-p port] [-n rminame] -nr 如果RMI注册中心没有找到，不会创建一个内部的RMI注册中心。 -p port RMI注册中心的端口号，默认为1099。 -n rminame 默认为JStatRemoteHost；如果同一台主机上同时运行了多个jstatd服务，rminame可以用于唯一确定一个jstatd服务；这里需要注意一下，如果开启了这个选项，那么监控客户端远程连接时，必须同时指定hostid及vmid，才可以唯一确定要连接的服务，这个可以参看jps章节中列出远程服务器上Java进程的示例。 -J 用于传递jvm选项到由javac调用的java加载器中，例如，“-J-Xms48m”将把启动内存设置为48M，使用-J选项可以非常方便的向基于Java的开发的底层虚拟机应用程序传递参数。 参考文章 JDK内置工具使用","tags":[{"name":"JVM","slug":"JVM","permalink":"http://www.54tianzhisheng.cn/tags/JVM/"},{"name":"性能调优工具","slug":"性能调优工具","permalink":"http://www.54tianzhisheng.cn/tags/性能调优工具/"}]},{"title":"Python爬虫实战之爬取百度贴吧帖子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Python爬虫实战之爬取百度贴吧帖子/","text":"大家好，上次我们实验了爬取了糗事百科的段子，那么这次我们来尝试一下爬取百度贴吧的帖子。与上一篇不同的是，这次我们需要用到文件的相关操作。 本篇目标 对百度贴吧的任意帖子进行抓取 指定是否只抓取楼主发帖内容 将抓取到的内容分析并保存到文件 1. URL格式的确定首先，我们先观察一下百度贴吧的任意一个帖子。 比如：http://tieba.baidu.com/p/3138733512?see_lz=1&amp;pn=1，这是一个关于NBA50大的盘点，分析一下这个地址。 http:// 代表资源传输使用http协议 tieba.baidu.com 是百度的二级域名，指向百度贴吧的服务器。 /p/3138733512 是服务器某个资源，即这个帖子的地址定位符 see_lz 和 pn 是该 URL 的两个参数，分别代表了只看楼主和帖子页码，等于1表示该条件为真 所以我们可以把URL分为两部分，一部分为基础部分，一部分为参数部分。 例如，上面的URL我们划分基础部分是 http://tieba.baidu.com/p/3138733512，参数部分是 ?see_lz=1&amp;pn=1 2. 页面的抓取熟悉了URL的格式，那就让我们用urllib2库来试着抓取页面内容吧。上一篇糗事百科我们最后改成了面向对象的编码方式，这次我们直接尝试一下，定义一个类名叫BDTB(百度贴吧)，一个初始化方法，一个获取页面的方法。 其中，有些帖子我们想指定给程序是否要只看楼主，所以我们把只看楼主的参数初始化放在类的初始化上，即init方法。另外，获取页面的方法我们需要知道一个参数就是帖子页码，所以这个参数的指定我们放在该方法中。 综上，我们初步构建出基础代码如下： 1234567891011121314151617181920212223242526#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self, baseUrl, seeLZ): self.baseURL = baseUrl self.seeLZ = &apos;?see_lz=&apos; + str(seeLZ) #传入页码，获取该页帖子的代码 def getPage(self, pageNum): try: url = self.baseURL + self.seeLZ + &apos;&amp;pn=&apos; + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) print response.read() return response except urllib2.URLError, e: if hasattr(e, &quot;reason&quot;): print u&quot;连接百度贴吧失败,错误原因&quot;,e.reason return NonebaseURL = &apos;http://tieba.baidu.com/p/3138733512&apos;bdtb = BDTB(baseURL, 1)bdtb.getPage(1) 运行代码，我们可以看到屏幕上打印出了这个帖子第一页楼主发言的所有内容，形式为HTML代码。 3. 提取相关信息1)提取帖子标题在浏览器中审查元素，或者按F12，查看页面源代码，我们找到标题所在的代码段如下: 1&lt;h3 class=\"core_title_txt pull-left text-overflow \" title=\"纯原创我心中的NBA2014-2015赛季现役50大\" style=\"width: 416px\"&gt;纯原创我心中的NBA2014-2015赛季现役50大&lt;/h3&gt; 所以我们要提取 &lt;h3&gt; 中的内容，因为一开始可以查看整个界面的原代码，查看里面含有 &lt;h3&gt;标签的不止一个。所以需要写正则表达式来匹配，如下： 1&lt;h3 class=&quot;core_title_txt.*?&gt;(.*?)&lt;/h3&gt; 然后，我们可以写个获取标题的方法 12345678910# 获取帖子标题 def getTitle(self): page = self.getPage(1) pattern = re.compile(&apos;&lt;h3 class=&quot;core_title_txt.*?&gt;(.*?)&lt;/h3&gt;&apos;, re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None 2）提取帖子页数同样地，帖子总页数我们也可以通过分析页面中的共?页来获取。 1&lt;li class=\"l_reply_num\" style=\"margin-left:8px\"&gt;&lt;span class=\"red\" style=\"margin-right:3px\"&gt;4784&lt;/span&gt;回复贴，共&lt;span class=\"red\"&gt;36&lt;/span&gt;页&lt;/li&gt; 所以我们的获取总页数的方法如下 12345678910#获取帖子一共有多少页def getPageNum(self): page = self.getPage(1) pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: #print result.group(1) #测试输出 return result.group(1).strip() else: return None 3）提取正文内容审查元素，可以看到百度贴吧每一层楼的主要内容都在标签里面，所以我们可以写如下的正则表达式 1&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt; 所以提取正文内容的方法： 123456#获取每一层楼的内容,传入页面内容def getContent(self,page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) for item in items: print item 运行截图如下： 可以看到有很多的换行符和图片符，既然出现这样的情况，那肯定不是我们想要的结果。那我们就必须要将文本进行处理，将各种复杂的标签给剔除，还原帖子的原来面貌。可以使用一个方法或者类将这个处理文本的实现，不过为了更好的代码重用和架构，还是建议使用一个类。 我们将这个类命名为Too（工具类），里面定义一个replace方法，替换各种标签。然后在类中定义几个正则表达式，利用re.sub方法对文本进行匹配后然后替换。 123456789101112131415161718192021222324252627import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,\"\",x) x = re.sub(self.removeAddr,\"\",x) x = re.sub(self.replaceLine,\"\\n\",x) x = re.sub(self.replaceTD,\"\\t\",x) x = re.sub(self.replacePara,\"\\n \",x) x = re.sub(self.replaceBR,\"\\n\",x) x = re.sub(self.removeExtraTag,\"\",x) #strip()将前后多余内容删除 return x.strip() 在使用时，我们只需要初始化一下这个类，然后调用replace方法即可。 现在整体代码是如下这样子的，现在我的代码是写到这样子的: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re# 处理页面标签类class Tool: # 去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') # 删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') # 把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') # 将表格制表&lt;td&gt;替换为\\t replaceTD = re.compile('&lt;td&gt;') # 把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') # 将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') # 将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self, x): x = re.sub(self.removeImg, \"\", x) x = re.sub(self.removeAddr, \"\", x) x = re.sub(self.replaceLine, \"\\n\", x) x = re.sub(self.replaceTD, \"\\t\", x) x = re.sub(self.replacePara, \"\\n \", x) x = re.sub(self.replaceBR, \"\\n\", x) x = re.sub(self.removeExtraTag, \"\", x) # strip()将前后多余内容删除 return x.strip()# 百度贴吧爬虫类class BDTB: # 初始化，传入基地址，是否只看楼主的参数 def __init__(self, baseUrl, seeLZ): self.baseURL = baseUrl self.seeLZ = '?see_lz=' + str(seeLZ) self.tool = Tool() # 传入页码，获取该页帖子的代码 def getPage(self, pageNum): try: url = self.baseURL + self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) return response.read().decode('utf-8') except urllib2.URLError, e: if hasattr(e, \"reason\"): print u\"连接百度贴吧失败,错误原因\", e.reason return None # 获取帖子标题 def getTitle(self): page = self.getPage(1) pattern = re.compile('&lt;h1 class=\"core_title_txt.*?&gt;(.*?)&lt;/h1&gt;', re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None # 获取帖子一共有多少页 def getPageNum(self): page = self.getPage(1) pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;', re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None # 获取每一层楼的内容,传入页面内容 def getContent(self, page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;', re.S) items = re.findall(pattern, page) # for item in items: # print item print self.tool.replace(items[1])baseURL = 'http://tieba.baidu.com/p/3138733512'bdtb = BDTB(baseURL, 1)bdtb.getContent(bdtb.getPage(1)) 运行截图如下： 4）替换楼层至于这个问题，我感觉直接提取楼层没什么必要呀，因为只看楼主的话，有些楼层的编号是间隔的，所以我们得到的楼层序号是不连续的，这样我们保存下来也没什么用。 所以可以尝试下面的方法： 1.每打印输出一段楼层，写入一行横线来间隔，或者换行符也好。 2.试着重新编一个楼层，按照顺序，设置一个变量，每打印出一个结果变量加一，打印出这个变量当做楼层。 将getContent方法修改如下： 123456789#获取每一层楼的内容,传入页面内容def getContent(self,page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) floor = 1 for item in items: print floor,u\"楼------------------------------------------------------------------------------------------------------------------------------------\\n\" print self.tool.replace(item) floor += 1 运行结果截图如下： 4. 写入文件代码： 12file = open(“tb.txt”,”w”)file.writelines(obj) 5. 完善代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,\"\",x) x = re.sub(self.removeAddr,\"\",x) x = re.sub(self.replaceLine,\"\\n\",x) x = re.sub(self.replaceTD,\"\\t\",x) x = re.sub(self.replacePara,\"\\n \",x) x = re.sub(self.replaceBR,\"\\n\",x) x = re.sub(self.removeExtraTag,\"\",x) #strip()将前后多余内容删除 return x.strip()#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self,baseUrl,seeLZ,floorTag): #base链接地址 self.baseURL = baseUrl #是否只看楼主 self.seeLZ = '?see_lz='+str(seeLZ) #HTML标签剔除工具类对象 self.tool = Tool() #全局file变量，文件写入操作对象 self.file = None #楼层标号，初始为1 self.floor = 1 #默认的标题，如果没有成功获取到标题的话则会用这个标题 self.defaultTitle = u\"百度贴吧\" #是否写入楼分隔符的标记 self.floorTag = floorTag #传入页码，获取该页帖子的代码 def getPage(self,pageNum): try: #构建URL url = self.baseURL+ self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) #返回UTF-8格式编码内容 return response.read().decode('utf-8') #无法连接，报错 except urllib2.URLError, e: if hasattr(e,\"reason\"): print u\"连接百度贴吧失败,错误原因\",e.reason return None #获取帖子标题 def getTitle(self,page): #得到标题的正则表达式 pattern = re.compile('&lt;h1 class=\"core_title_txt.*?&gt;(.*?)&lt;/h1&gt;',re.S) result = re.search(pattern,page) if result: #如果存在，则返回标题 return result.group(1).strip() else: return None #获取帖子一共有多少页 def getPageNum(self,page): #获取帖子页数的正则表达式 pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: return result.group(1).strip() else: return None #获取每一层楼的内容,传入页面内容 def getContent(self,page): #匹配所有楼层的内容 pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) contents = [] for item in items: #将文本进行去除标签处理，同时在前后加入换行符 content = \"\\n\"+self.tool.replace(item)+\"\\n\" contents.append(content.encode('utf-8')) return contents def setFileTitle(self,title): #如果标题不是为None，即成功获取到标题 if title is not None: self.file = open(title + \".txt\",\"w+\") else: self.file = open(self.defaultTitle + \".txt\",\"w+\") def writeData(self,contents): #向文件写入每一楼的信息 for item in contents: if self.floorTag == '1': #楼之间的分隔符 floorLine = \"\\n\" + str(self.floor) + u\"-----------------------------------------------------------------------------------------\\n\" self.file.write(floorLine) self.file.write(item) self.floor += 1 def start(self): indexPage = self.getPage(1) pageNum = self.getPageNum(indexPage) title = self.getTitle(indexPage) self.setFileTitle(title) if pageNum == None: print \"URL已失效，请重试\" return try: print \"该帖子共有\" + str(pageNum) + \"页\" for i in range(1,int(pageNum)+1): print \"正在写入第\" + str(i) + \"页数据\" page = self.getPage(i) contents = self.getContent(page) self.writeData(contents) #出现写入异常 except IOError,e: print \"写入异常，原因\" + e.message finally: print \"写入任务完成\"print u\"请输入帖子代号\"baseURL = 'http://tieba.baidu.com/p/' + str(raw_input(u'http://tieba.baidu.com/p/'))seeLZ = raw_input(\"是否只获取楼主发言，是输入1，否输入0\\n\")floorTag = raw_input(\"是否写入楼层信息，是输入1，否输入0\\n\")bdtb = BDTB(baseURL,seeLZ,floorTag)bdtb.start() 运行后截图如下： 备注： 运行后注意输入帖子的代号先在网址后空格，再输入帖子代号，输入完再把刚才的空格 删除，只有这样才不会报错。 Traceback (most recent call last):File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 149,in &lt; module &gt; bdtb.start()File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 123, in startpageNum = self.getPageNum(indexPage)File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 86, in getPageNumresult = re.search(pattern,page)File “C:\\Python27\\lib\\re.py”, line 146, in searchreturn _compile(pattern, flags).search(string)TypeError: expected string or buffer","tags":[{"name":"Python","slug":"Python","permalink":"http://www.54tianzhisheng.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.54tianzhisheng.cn/tags/爬虫/"}]},{"title":"Python爬虫实战之爬取糗事百科段子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Python爬虫实战之爬取糗事百科段子/","text":"完整代码地址：Python爬虫实战之爬取糗事百科段子程序代码详解： Spider1-qiushibaike.py：爬取糗事百科的8小时最新页的段子。包含的信息有作者名称，觉得好笑人数，评论人数，发布的内容。如果发布的内容中含有图片的话，则过滤图片，内容依然显示出来。 Spider2-qiushibaike.py：在Spider1-qiushibaike.py基础上，引入类和方法，进行优化和封装，爬取糗事百科的24小时热门页的段子。进一步优化，每按一次回车更新一条内容，当前页的内容抓取完毕后，自动抓取下一页的内容，按‘q’退出。 Spider3-qiushibaike.py：在Spiders-qiushibaike.py基础上，爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！ 本爬虫目标： 抓取糗事百科热门段子 过滤带有图片的段子 实现每按一次回车显示一个段子的发布时间，发布人，段子内容，点赞数，评论人数。 糗事百科是不需要登录的，所以也没必要用到Cookie，另外糗事百科有的段子是附图的，我们把图抓下来图片不便于显示，那么我们就尝试过滤掉有图的段子吧。 好，现在我们尝试抓取一下糗事百科的热门段子吧，每按下一次回车我们显示一个段子。 1.确定URL并抓取页面代码首先我们确定好页面的URL是 http://www.qiushibaike.com/hot/page/1，其中最后一个数字1代表页数，我们可以传入不同的值来获得某一页的段子内容。 2.提取某一页的所有段子好，获取了HTML代码之后，我们开始分析怎样获取某一页的所有段子。 首先我们审查元素看一下，按浏览器的F12，截图如下: 我们可以看到，每一个段子都是 &lt;div class=”article block untagged mb15″ id=”…”&gt;…&lt;/div&gt; 包裹的内容。 现在我们想获取发布人，发布日期，段子内容，点赞人数和评论人数。不过另外注意的是，段子有些是带图片的，如果我们想在控制台显示图片是不现实的，所以我们直接把带有图片的段子给它剔除掉，只保存仅含文本的段子。 所以我们加入如下正则表达式来匹配一下，用到的方法是 re.findall 是找寻所有匹配的内容。方法的用法详情可以看前面说的正则表达式的介绍。 好，我们的正则表达式匹配语句书写如下，在原来的基础上追加如下代码： 123456789#正则表达式匹配 pattern = re.compile(&apos;&lt;div.*?author.*?&gt;.*?&lt;img.*?&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?&apos;+ &apos;content&quot;&gt;(.*?)&lt;/div&gt;(.*?)&lt;div.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern,content) for item in items: haveImg = re.search(&quot;img&quot;,item[2]) if not haveImg: print item[0],item[3],item[4],item[1] #item[0]是作者名称 item[3]好笑人数 item[4]评论人数 item[1]内容 item[2]是内容后面的东西，如果含有图片，过滤掉 现在正则表达式在这里稍作说明 1） .*? 是一个固定的搭配， . 和 * 代表可以匹配任意无限多个字符，加上 ？ 表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配，以后我们还会大量用到 .*? 的搭配。 2）(.*?) 代表一个分组，在这个正则表达式中我们匹配了五个分组，在后面的遍历 item 中，item[0] 就代表第一个 (.*?) 所指代的内容，item[1] 就代表第二个 (.*?) 所指代的内容，以此类推。 3）re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。 这样我们就获取了发布人，发布时间，发布内容，附加图片以及点赞数。 在这里注意一下，我们要获取的内容如果是带有图片，直接输出出来比较繁琐，所以这里我们只获取不带图片的段子就好了。 所以，在这里我们就需要对带图片的段子进行过滤。 我们可以发现，带有图片的段子会带有类似下面的代码，而不带图片的则没有，所以，我们的正则表达式的 item[2] 就是获取了下面的内容，如果不带图片，item[2]获取的内容便是空，所以我们只需要判断 item[2]中是否含有 img 标签就可以了。 整体代码如下： 12345678910111213141516171819202122232425262728293031323334#-*-coding:utf8-*-#created by 10412 2016/8/23#爬取糗事百科的8小时最新页的段子。包含的信息有作者名称，觉得好笑人数，评论人数，发布的内容。#如果发布的内容中含有图片的话，则过滤图片，内容依然显示出来。import urllibimport urllib2import re#自定义输入爬取的页数page = raw_input(&quot;please enter the page number:&quot;)url = &apos;http://www.qiushibaike.com/8hr/page/&apos;+ page +&apos;/?s=4880477&apos;user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;headers = &#123; &apos;User-Agent&apos; : user_agent &#125;try: request = urllib2.Request(url,headers = headers) response = urllib2.urlopen(request) content = response.read().decode(&apos;utf-8&apos;) #正则表达式匹配 pattern = re.compile(&apos;&lt;div.*?author.*?&gt;.*?&lt;img.*?&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?&apos;+ &apos;content&quot;&gt;(.*?)&lt;/div&gt;(.*?)&lt;div.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern,content) for item in items: haveImg = re.search(&quot;img&quot;,item[2]) if not haveImg: print item[0],item[3],item[4],item[1] #item[0]是作者名称 item[3]好笑人数 item[4]评论人数 item[1]内容 item[2]是内容后面的东西，如果含有图片，过滤掉except urllib2.URLError, e: if hasattr(e,&quot;code&quot;): print e.code if hasattr(e,&quot;reason&quot;): print e.reason 运行一下看下效果: 恩，带有图片的段子已经被剔除啦。 3.完善交互，设计面向对象模式好啦，现在最核心的部分我们已经完成啦，剩下的就是修一下边边角角的东西，我们想达到的目的是： 按下回车，读取一个段子，显示出段子的发布人，内容，点赞个数及评论数量。 另外我们需要设计面向对象模式，引入类和方法，将代码做一下优化和封装，最后，我们的代码如下所示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#-*-coding:utf8-*-#created by 10412# 在Spider1-qiushibaike.py基础上，引入类和方法，进行优化和封装，爬取糗事百科的24小时热门页的段子。# 进一步优化，每按一次回车更新一条内容，当前页的内容抓取完毕后，自动抓取下一页的内容，按‘q’退出。import urllib2import reclass QSBK: def __init__(self): self.pageIndex = 1 self.user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos; self.headers = &#123;&apos;User-Agent&apos; : self.user_agent&#125; self.stories = [] # 存放程序是否继续运行的变量 self.enable = False # 传入某一页的索引获得页面代码 def getPage(self, pageIndex): try: url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(pageIndex) request = urllib2.Request(url, headers=self.headers) response = urllib2.urlopen(request) pageCode = response.read().decode(&apos;utf-8&apos;) return pageCode except urllib2.URLError, e: if hasattr(e, &quot;reason&quot;): print u&quot;连接糗事百科失败,错误原因&quot;, e.reason return None # 传入某一页代码，返回本页不带图片的段子列表 def getPageItems(self, pageIndex): pageCode = self.getPage(pageIndex) if not pageCode: print u&quot;出错了&quot; return None pattern = re.compile(&apos;&lt;div class=&quot;author.*?href.*?&lt;img src.*?title=.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div class=&quot;content&quot;&gt;(.*?)&lt;/div&gt;.*?&lt;i class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern, pageCode) pageStories = [] for item in items: replaceBR = re.compile(&apos;&lt;br/&gt;&apos;) text = re.sub(replaceBR, &quot;\\n&quot;, item [1] ) pageStories.append([item[0].strip(), text.strip(), item[2].strip(), item[3].strip()]) return pageStories # 加载并提取页面内容，加入到列表中 def loadPage(self): if self.enable == True: if len(self.stories) &lt; 2: # 获取新一页 pageStories = self.getPageItems(self.pageIndex) if pageStories: self.stories.append(pageStories) self.pageIndex += 1 # 调用该方法，回车打印一个段子 def getOneStory(self, pageStories, page): for story in pageStories: input = raw_input() self.loadPage() if input == &quot;Q&quot;: self.enable = False return print u&quot;第%d页\\t发布人:%s\\t赞:%s\\t评论:%s\\n%s&quot; %(page, story[0], story[2], story[2], story [1]) # 开始方法 def start(self): print u&quot;正在读取糗事百科,按回车查看新段子，Q退出&quot; # 使变量为True，程序可以正常运行 self.enable = True # 先加载一页内容 self.loadPage() # 局部变量，控制当前读到了第几页 nowPage = 0 while self.enable: if len(self.stories) &gt; 0: # 从全局list中获取一页的段子 pageStories = self.stories[0] # 当前读到的页数加一 nowPage += 1 # 将全局list中第一个元素删除，因为已经取出 del self.stories[0] # 输出该页的段子 self.getOneStory(pageStories, nowPage)spider = QSBK()spider.start() 好啦，大家来测试一下吧，点一下回车会输出一个段子，包括第几页，发布人，段子内容，点赞数以及评论数量，是不是感觉爽爆了！ 完善更新版爬虫代码在上面爬虫的基础上，还增加爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198#-*-coding:utf8-*-#created by 10412#在Spiders-qiushibaike.py基础上，爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，# 换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！import urllibimport urllib2import reimport os.pathhtmlCharacterMap = &#123; &apos;&lt;br/&gt;&apos; : &apos;\\n&apos;, &apos;&amp;quot;&apos; : &apos;&quot;&apos;, &apos;&amp;nbsp;&apos; : &apos; &apos;, &apos;&amp;gt;&apos; : &apos;&gt;&apos;, &apos;&amp;lt;&apos; : &apos;&lt;&apos;, &apos;&amp;amp;&apos;: &apos;&amp;&apos;, &apos;&amp;#39&apos;:&quot;&apos;&quot;,&#125;class QSBK(object): &quot;&quot;&quot;糗事百科的爬虫&quot;&quot;&quot; def __init__(self): self.pageIndex = 1 self.pagetotal = 9999 self.user_agent = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&apos; self.headers = &#123;&apos;User-Agent&apos; : self.user_agent&#125; self.stories = [] self.comments = [] self.currentStoryId = &apos;&apos; #是否要退出了 self.enable = False #记录当前是否在查看评论 self.viewComment = False def getPageContent(self, pageIndex): try: url = &apos;http://www.qiushibaike.com/8hr/page/%d/&apos; % pageIndex request = urllib2.Request(url, headers=self.headers) print u&apos;开始加载%02d页&apos; % pageIndex response = urllib2.urlopen(request, timeout=5) print u&apos;成功加载%02d页&apos; % pageIndex pageContent = response.read().decode(&apos;utf-8&apos;) return pageContent except urllib2.URLError, e: if hasattr(e, &apos;reason&apos;): print u&quot;连接糗事百科失败，错误原因：&quot;, e.reason return None def getCommentsContent(self, storyId): # 得到段子的评论 try: url = &apos;http://www.qiushibaike.com/article/%s&apos; % storyId request = urllib2.Request(url, headers=self.headers) response = urllib2.urlopen(request, timeout=5) pageContent = response.read().decode(&apos;utf-8&apos;) return pageContent except urllib2.URLError, e: if hasattr(e, &apos;reason&apos;): print u&quot;连接糗事百科失败，错误原因：&quot;, e.reason return None def getPageTotal(self, content): # 得到总页数 if self.pagetotal != 9999: # print u&apos;加载第%d页&apos; % self.pageIndex return pattrenStr = &apos;&lt;span class=&quot;page-numbers&quot;&gt;(?P&lt;pagetotal&gt;.*?)&lt;/span&gt;&apos; pattern = re.compile(pattrenStr, re.S) items = re.findall(pattern, content) if len(items)&gt;0: self.pagetotal = int(items[-1].strip()) print u&apos;总共有%d页&apos; % self.pagetotal def getPageItems(self, pageIndex): pageContent = self.getPageContent(pageIndex) with open(&apos;temp%02d.html&apos; % pageIndex, &apos;w&apos;) as f: f.write(pageContent.encode(&apos;utf-8&apos;)) if not pageContent: print &quot;页面加载失败...&quot; return None self.getPageTotal(pageContent) pattrenStr = r&apos;&lt;h2&gt;(?P&lt;authorname&gt;.*?)&lt;/h2&gt;.*?&apos;\\ r&apos;&lt;div class=&quot;content&quot;&gt;(?P&lt;content&gt;.*?)&lt;/div&gt;&apos;\\ r&apos;(?P&lt;maybehaveimage&gt;.*?)&apos;\\ r&apos;&lt;i class=&quot;number&quot;&gt;(?P&lt;numbervote&gt;.*?)&lt;/i&gt;.*?&apos;\\ r&apos;&lt;span class=&quot;stats-comments&quot;&gt;(?P&lt;comments&gt;.*?)&lt;/div&gt;&apos; pattern = re.compile(pattrenStr, re.S) items = re.findall(pattern, pageContent) return items def getCurrentStoryComments(self, storyId): #切换到查看评论模式 self.viewComment = True content = self.getCommentsContent(storyId) if not content: print &quot;页面加载失败...&quot; return None reStr = r&apos;&lt;div id=&quot;comment-.*?&apos;\\ r&apos;&lt;a href=&quot;/users/.*?/&quot; class=&quot;userlogin&quot; target=&quot;_blank&quot; title=&quot;(?P&lt;username&gt;.*?)&quot;&gt;(?P=username)&lt;/a&gt;.*?&apos;\\ r&apos;&lt;span class=&quot;body&quot;&gt;(?P&lt;comment&gt;.*?)&lt;/span&gt;.*?&apos;\\ r&apos;&lt;div class=&quot;report&quot;&gt;(?P&lt;index&gt;.*?)&lt;/div&gt;&apos; pattern = re.compile(reStr, re.S) items = re.findall(pattern, content) del self.comments[:] for item in items: comentstr = item[0]+&apos;(&apos;+ item[2] + u&apos;楼)&apos; + &apos;\\n&apos; + item[1] + &apos;\\n&apos; for (k,v) in htmlCharacterMap.items(): re.sub(re.compile(k), v, comentstr) self.comments.append(comentstr) if len(self.comments)&gt;0: print &apos;已切换到查看评论，换回车显示下一个评论,按Q退出回到查看糗事&apos; else: print &apos;当前糗事没有评论&apos; self.viewComment = False def getNextPage(self): if self.pageIndex &gt; self.pagetotal: self.enable = False print &quot;你已经看完所有的糗事，现在自动退出！&quot; return items = self.getPageItems(self.pageIndex) self.pageIndex += 1 for item in items: #如果有图片直接跳过，因为图片在终端显示不了 if re.search(&apos;img&apos;, item[2]): continue content = item[1].strip() #转换html的特殊字符 for (k,v) in htmlCharacterMap.items(): content = re.sub(re.compile(k), v, content) authorname = item[0].strip() for (k,v) in htmlCharacterMap.items(): authorname = re.sub(re.compile(k), v, authorname) #找出评论个数，没有为0 pattern = re.compile(r&apos;.*?&lt;a href=&quot;/article/(?P&lt;id&gt;.*?)&quot;.*?&lt;i class=&quot;number&quot;&gt;(?P&lt;number&gt;.*?)&lt;/i&gt;.*?&apos;, re.S) result = re.match(pattern, item[4]) commentnumbers = 0 articleId = &apos;&apos; if result: commentnumbers = result.groupdict().get(&apos;number&apos;, &apos;0&apos;) articleId = result.groupdict().get(&apos;id&apos;, &apos;&apos;) self.stories.append(authorname + &apos;(&apos; + item[3].strip() + u&apos;好笑·&apos; + str(commentnumbers) + u&apos;评论)&apos; + &apos;\\n&apos; + content + &apos;\\n&apos;) self.stories.append(articleId) def getNextComment(self): print self.comments[0] self.comments.pop(0) if len(self.comments)==0: print &apos;你已查看完这个糗事的所有评论,现在自动退出到查看糗事&apos; self.viewComment = False def getOneStory(self): #防止有的页面全是带图片的 while (len(self.stories)==0 and self.enable): self.getNextPage() story = self.stories[0] self.currentStoryId = self.stories[1] print story self.stories.pop(0) self.stories.pop(0) if len(self.stories)==0: self.getNextPage() def start(self): #先删除临时保存的网页 tempfiles = [x for x in os.listdir(&apos;.&apos;) if os.path.isfile(x) and os.path.splitext(x)[1]==&apos;.html&apos; and x.startswith(&apos;temp&apos;)] for file in tempfiles: os.remove(file) print u&quot;正在读取糗事百科，按回车查看下一个糗事，按C查看当前这个糗事的评论，按Q退出或返回&quot; self.enable = True self.getNextPage() while self.enable: input = raw_input() if input.upper() == &quot;Q&quot;: if not self.viewComment: self.enable = False else: self.viewComment = False print &apos;现在退出到查看糗事了&apos; elif input.upper() == &quot;C&quot;: #查看当前看到的糗事的评论 if len(self.currentStoryId)&gt;0: self.getCurrentStoryComments(self.currentStoryId) else: print &apos;这条糗事没有评论&apos; else: if not self.viewComment: self.getOneStory() else: self.getNextComment()if __name__ == &apos;__main__&apos;: spider = QSBK() spider.start()","tags":[{"name":"Python","slug":"Python","permalink":"http://www.54tianzhisheng.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.54tianzhisheng.cn/tags/爬虫/"}]},{"title":"利用Github Page 搭建个人博客网站","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/利用Github Page 搭建个人博客网站/","text":"前言最近这几天，没事干，想找点事折腾下，于是自己便想到了自己一直想干的一件事：搭建一个属于自己的博客网站。目前搭建个人 blog 网站最好的是用 wordpress ，但是那个折腾起来好像还挺麻烦的，再加上还需要自己修改些前端代码和用 PHP 做（虽然我学了几天拍黄片，但是早已忘了），然后就是用 Github Page 吧，自己也一直在这个最大的交友网站装 X 。想想就用这个吧（后来好像觉得这个还挺省事的） 再说说拥有个人博客网站的好处吧： 装 X（如果网站够炫） 很好的用来总结自己所学的知识 面试加分（在简历上放上自己的个人网站链接，面试官就可以更好的了解你，知道你所学知识的深度和广度） 不再受其他博客平台的规则所束缚 如果你现在还没有自己个人博客网站的话，那么我觉得你看完本篇博客后，强烈的建议你去折腾折腾下，搞个自己的，让自己也能够体验装 X 的感觉。 要想用搭建一个个人博客网站，首先你得有一个域名，这样别人才可以通过域名访问，其次你还要一个空间来存放你的页面。 域名 域名的话，你可以在万网、阿里云、腾讯云等注册，我的域名 www.54tianzhisheng.cn 就是在腾讯云注册的，记得是腾讯云一元钱（一个域名+主机）搞的，这是腾讯云对学生才有这优惠。 .cn 的域名需要备案，备案的审核速度我觉得还是挺快的，还需要上传证件。当然你也可以买其他的那些不需要备案的域名，省得麻烦事。 空间 空间有免费的空间，也有收费的空间。免费的当然就不够稳定了，收费的就很贵了，终究是很不爽，有没有什么地方是既免费又稳定的空间呢？有，Github 。它允许上传个人网站项目并自定义你的域名，而且又有稳定的服务，实在是不能够在好了。 下面就一起跟着我来一步一步的利用 Github 搭建个人博客网站吧！ 1. 拥有一个域名这个步骤我就不详述了。 举例： 打开腾讯云官网 搜索你想要的域名，下单买一个 2. 拥有一个 Github账号互联网崇尚自由与分享。Github 是一个全世界程序员聚集的地方，大家相互分享自己写的代码，提升别人，也提升自己。大家都在为着开源社区努力着。因为我从开源项目中学到很多知识，所以我也非常愿意分享我的所见所学所得，我的 Github 主页：https://github.com/zhisheng17 （欢迎 follow 和对我的项目给个 star 或者 fork 我的项目一起来和我完善项目） 如果还没有 Github 账号的话你就先去注册一个吧，有的话，直接登录就行，后面的操作都要用到 Github 的。 3. Github 上新建个人网站项目登录 GitHub 之后，在页面右上角点击 + 加号按钮，点击 New repository。 由于我们是新建一个个人网站项目，所有仓库的名称需要安装 GitHub 个人网站项目的规定来写。 规则就是： YOUR-GITHUB-USERNAME.github.io 比如我的 GitHub 用户名是 zhisheng17，那我就要填写 zhisheng17.github.io。然后选择公开模式，接着点击创建仓库按钮。 创建成功之后，进入了项目主页面。点击设置按钮。 进入之后，滚动页面到下方。点击页面自动生成器按钮。 点击右下方继续去布局按钮。 选择一个模板，点击发布页面按钮。 这个时候，你就可以通过YOUR-GITHUB-USERNAME.github.io来访问此页面了。 4. 上传个人网页到 Github自动生成页面，肯定不符合我们的要求，我们希望能够自己设计自己的个人网站。我们可以自己编写一个网页文件，命名为 index.html。然后上传到 GitHub个人网站项目上。这里为了节约时间，可以先下载我的个人网站项目代码，然后修改为你的网页上传到 GitHub。 下面介绍详细步骤。 进入此项目https://github.com/zhisheng17/zhisheng17.github.io，然后下载源码。解压之后，拿到里面的index.html文件。 然后进入自己的个人网站项目主页 YOUR-GITHUB-USERNAME/YOUR-GITHUB-USERNAME.github.io。点击上传文件按钮，进入上传文件页面，将 index.html 文件拖入蓝色大圈圈区域，点击提交按钮即可提交成功。此时打开网址 YOUR-GITHUB-USERNAME.github.io 就可以看到主页已经改变为我们自己的网页了。 通过 zhisheng17.github.io 查看效果： 5. 域名CNAME到个人网站项目网页上传成功了，我们不想一直通过YOUR-GITHUB-USERNAME.github.io来访问我们的个人网站，而是希望通过自己的域名来访问。 下面讲述详细步骤。 点击我们的个人网站项目设置选项卡，滚动到下面，就会发现一个自定义域名卡片。输入我们买的域名，然后点击保存。 接着我们还要将我们的域名解析到这个个人网站项目上。因为我的域名是在腾讯云上面买的，所以我打开腾讯云域名管理页面，进行相关的设置。 接着，点击添加一条域名解析记录，主机填写www，代表你是一级域名来访问，指向填写YOUR-GITHUB-USERNAME.github.io，然后点击保存按钮。应该要等会，域名的解析时间可能不一样，我的腾讯云就是很慢的 6. 访问你的域名所有这些步骤做完之后，在浏览器里输入自己的域名，回车键一按，就会返回我们刚刚上传到 GitHub 的index.html 页面了。 这里只是入门了 GitHub 搭建个人网站的功能，GitHub 官方推荐 Jekyll 博客系统来发布自己的页面。以后有数据更新，都可以通过 Jekyll 来重新编译整个网站。（期待后续我的使用 Jekyll 博客系统发布自己博客的文章吧） 7. 注意事项尽管GitHub个人网站项目是免费的，但是却有一些限制。总体来说，完全够用，甚至太多了。 单个仓库大小不超过1GB，上传单个文件大小不能超过100MB，如果通过浏览器上传不能超过25MB 个人网站项目也不例外，最大空间1GB 个人网站项目每个月访问请求数不能超过10万次，总流量不能超过100GB 个人网站项目一小时创建数量不能超过10个 当然了，这些政策可能随时改变，可以通过此网页查看最新政策。 https://help.github.com/articles/what-is-github-pages/#recommended-limits 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"Github Page","slug":"Github-Page","permalink":"http://www.54tianzhisheng.cn/tags/Github-Page/"},{"name":"博客网站","slug":"博客网站","permalink":"http://www.54tianzhisheng.cn/tags/博客网站/"}]},{"title":"解决jdk1.8中发送邮件失败（handshake_failure）问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/解决jdk1.8中发送邮件失败（handshake_failure）问题/","text":"暑假在家做一个类似知乎的问答型网站（代码可见：Github/wenda 喜欢的可以给个star或者自己fork然后修改，目前功能还未很完善），其中有一个站内邮件通知系统（这里简单的讲一个例子：如果用户登录的时候出现异常，那么就会通过邮件发送通知用户）。然而却碰到一个问题。问题错误信息如下： 发送邮件失败Mail server connection failed; nested exception is javax.mail.MessagingException: Could not connect to SMTP host: smtp.qq.com, port: 465;nested exception is: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure. Failed messages: javax.mail. MessagingException: Could not connect to SMTP host: smtp.qq.com, port: 465;nested exception is: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure 自己在将错误信息代码google了一下，找了很久发现很多解决方案，包括stackoverflow上的一些解决方案，但还是没用。然后呢用百度试了下，结果在第一条是开源中国的一篇博客:javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure。 点进去是这样的：（如下图） 结果就是：这个问题是jdk导致的，jdk1.8里面有一个jce的包，安全性机制导致的访问https会报错，官网上有替代的jar包，如果替换掉就可以了。问题的解决方法还可以就是在整个项目中把你的jdk换成是1.7去，同样也可以解决这个我问题。 这两个jar包的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html 然后下载之后，把这个压缩文件解压，得到两个jar包去覆盖jdk安装目录下的jre\\lib\\security\\下相同的jar包就能解决java8的邮件发送问题。 接着用QQ邮箱我亲测有用，但是要注意一点就是：开启SMTP服务后要记得将你的16位授权码作为你的qq邮箱登录密码。 MailSender.java中mailSender.setPassword(“16位授权码”); mailSender.setHost(“smtp.qq.com”);mailSender.setPort(465); 下面把完整代码发布出来： 1. LoginExceptionHandler.java 12345678910111213141516171819202122232425262728293031323334package com.nowcoder.async.handler;import com.nowcoder.async.EventHandler;import com.nowcoder.async.EventModel;import com.nowcoder.async.EventType;import com.nowcoder.util.MailSender;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.Arrays;import java.util.HashMap;import java.util.List;import java.util.Map;/** * Created by 10412 on 2016/8/10. */@Componentpublic class LoginExceptionHandler implements EventHandler&#123; @Autowired MailSender mailSender; @Override public void doHandle(EventModel model) &#123; // xxxx判断发现这个用户登陆异常 Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(\"username\", model.getExt(\"username\")); mailSender.sendWithHTMLTemplate(model.getExt(\"email\"), \"登陆IP异常\", \"mails/login_exception.html\", map); &#125; @Override public List&lt;EventType&gt; getSupportEventTypes() &#123; return Arrays.asList(EventType.LOGIN); &#125;&#125; 2. LoginController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.nowcoder.controller;import com.nowcoder.async.EventModel;import com.nowcoder.async.EventProducer;import com.nowcoder.async.EventType;import com.nowcoder.service.UserService;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.CookieValue;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletResponse;import java.util.Map;/** * Created by 10412 on 2016/7/2. */@Controllerpublic class LoginController &#123; private static final Logger logger = LoggerFactory.getLogger(LoginController.class); @Autowired UserService userService; @Autowired EventProducer eventProducer; @RequestMapping(path = &#123;&quot;/reg/&quot;&#125;, method = &#123;RequestMethod.POST&#125;) public String reg(Model model, @RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(&quot;next&quot;) String next, @RequestParam(value=&quot;rememberme&quot;, defaultValue = &quot;false&quot;) boolean rememberme, HttpServletResponse response) &#123; try &#123; Map&lt;String, Object&gt; map = userService.register(username, password); if (map.containsKey(&quot;ticket&quot;)) &#123; Cookie cookie = new Cookie(&quot;ticket&quot;, map.get(&quot;ticket&quot;).toString()); cookie.setPath(&quot;/&quot;); if (rememberme) &#123; cookie.setMaxAge(3600*24*5); &#125; response.addCookie(cookie); if (StringUtils.isNotBlank(next)) &#123; return &quot;redirect:&quot; + next; &#125; return &quot;redirect:/&quot;; &#125; else &#123; model.addAttribute(&quot;msg&quot;, map.get(&quot;msg&quot;)); return &quot;login&quot;; &#125; &#125; catch (Exception e) &#123; logger.error(&quot;注册异常&quot; + e.getMessage()); model.addAttribute(&quot;msg&quot;, &quot;服务器错误&quot;); return &quot;login&quot;; &#125; &#125; @RequestMapping(path = &#123;&quot;/reglogin&quot;&#125;, method = &#123;RequestMethod.GET&#125;) public String regloginPage(Model model, @RequestParam(value = &quot;next&quot;, required = false) String next) &#123; model.addAttribute(&quot;next&quot;, next); return &quot;login&quot;; &#125; @RequestMapping(path = &#123;&quot;/login/&quot;&#125;, method = &#123;RequestMethod.POST&#125;) public String login(Model model, @RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(value=&quot;next&quot;, required = false) String next, @RequestParam(value=&quot;rememberme&quot;, defaultValue = &quot;false&quot;) boolean rememberme, HttpServletResponse response) &#123; try &#123; Map&lt;String, Object&gt; map = userService.login(username, password); if (map.containsKey(&quot;ticket&quot;)) &#123; Cookie cookie = new Cookie(&quot;ticket&quot;, map.get(&quot;ticket&quot;).toString()); cookie.setPath(&quot;/&quot;); if (rememberme) &#123; cookie.setMaxAge(3600*24*5); &#125; response.addCookie(cookie); eventProducer.fireEvent(new EventModel(EventType.LOGIN) .setExt(&quot;username&quot;, username).setExt(&quot;email&quot;, &quot;****@qq.com&quot;) .setActorId((int)map.get(&quot;userId&quot;))); if (StringUtils.isNotBlank(next)) &#123; return &quot;redirect:&quot; + next; &#125; return &quot;redirect:/&quot;; &#125; else &#123; model.addAttribute(&quot;msg&quot;, map.get(&quot;msg&quot;)); return &quot;login&quot;; &#125; &#125; catch (Exception e) &#123; logger.error(&quot;登陆异常&quot; + e.getMessage()); return &quot;login&quot;; &#125; &#125; @RequestMapping(path = &#123;&quot;/logout&quot;&#125;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;) public String logout(@CookieValue(&quot;ticket&quot;) String ticket) &#123; userService.logout(ticket); return &quot;redirect:/&quot;; &#125;&#125; 3. EventHandler.java1234567891011121314package com.nowcoder.async;import java.util.List;/** * Created by 10412 on 2016/8/10. */public interface EventHandler&#123; void doHandle(EventModel model); List&lt;EventType&gt; getSupportEventTypes();&#125; 4. MailSender.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.nowcoder.util;import org.apache.velocity.app.VelocityEngine;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.mail.javamail.JavaMailSenderImpl;import org.springframework.mail.javamail.MimeMessageHelper;import org.springframework.stereotype.Service;import org.springframework.ui.velocity.VelocityEngineUtils;import javax.mail.internet.InternetAddress;import javax.mail.internet.MimeMessage;import javax.mail.internet.MimeUtility;import java.util.Map;import java.util.Properties;/** * Created by 10412 on 2016/8/10. // ***@qq.com wnppafhsbrcgbfbh（16位授权码） */@Servicepublic class MailSender implements InitializingBean &#123; private static final Logger logger = LoggerFactory.getLogger(MailSender.class); private JavaMailSenderImpl mailSender; @Autowired private VelocityEngine velocityEngine; public boolean sendWithHTMLTemplate(String to, String subject, String template, Map&lt;String, Object&gt; model) &#123; try &#123; String nick = MimeUtility.encodeText(\"***\"); InternetAddress from = new InternetAddress(nick + \"&lt;****@qq.com&gt;\"); MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper mimeMessageHelper = new MimeMessageHelper(mimeMessage); String result = VelocityEngineUtils .mergeTemplateIntoString(velocityEngine, template, \"UTF-8\", model); mimeMessageHelper.setTo(to); mimeMessageHelper.setFrom(from); mimeMessageHelper.setSubject(subject); mimeMessageHelper.setText(result, true); mailSender.send(mimeMessage); return true; &#125; catch (Exception e) &#123; logger.error(\"发送邮件失败\" + e.getMessage()); return false; &#125; &#125; @Override public void afterPropertiesSet() throws Exception &#123; mailSender = new JavaMailSenderImpl(); mailSender.setUsername(\"***@qq.com\"); mailSender.setPassword(\"wnppafhsbrcgbfbh\"); //qq邮箱开启smtp服务后使用16位授权码在第三方登录// mailSender.setHost(\"smtp.exmail.qq.com\"); mailSender.setHost(\"smtp.qq.com\"); mailSender.setPort(465);// mailSender.setHost(\"smtp.163.com\"); //163邮箱// mailSender.setPort(25); mailSender.setProtocol(\"smtps\"); mailSender.setDefaultEncoding(\"utf8\"); Properties javaMailProperties = new Properties(); javaMailProperties.put(\"mail.smtp.ssl.enable\", true); //javaMailProperties.put(\"mail.smtp.auth\", true); //javaMailProperties.put(\"mail.smtp.starttls.enable\", true); mailSender.setJavaMailProperties(javaMailProperties); &#125;&#125; 5. login_exception.html 发送消息模板（可自定义） 1你好$username，你的登陆有问题! 一切都好了，运行。 登录。 发送邮件过来了。 总结来说：这个错误就是jdk1.8中的一个jce的包，安全性机制导致访问https会报错。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"邮件发送","slug":"邮件发送","permalink":"http://www.54tianzhisheng.cn/tags/邮件发送/"}]},{"title":"深入分析 Java Web 中的中文编码问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/深入分析 Java Web 中的中文编码问题/","text":"背景： 编码问题一直困扰着程序开发人员，尤其是在 Java 中更加明显，因为 Java 是跨平台的语言，在不同平台的编码之间的切换较多。接下来将介绍 Java 编码问题出现的根本原因；在 Java 中经常遇到的几种编码格式的区别；在 Java 中经常需要编码的场景；出现中文问题的原因分析；在开发 Java Web 中可能存在编码的几个地方；一个 HTTP 请求怎么控制编码格式；如何避免出现中文编码问题等。 1、几种常见的编码格式1.1 为什么要编码 在计算机中存储信息的最小单元是 1 个字节，即 8 个 bit， 所以能表示的字符范围是 0 ~ 255 个。 要表示的符号太多，无法用 1 个字节来完全表示。 1.2 如何翻译计算机中提供多种翻译方式，常见的有 ASCII、ISO-8859-1、GB2312、GBK、UTF-8、UTF-16等。这些都规定了转化的规则，按照这个规则就可以让计算机正确的表示我们的字符。下面介绍这几种编码格式： ASCII 码 总共有 128 个，用 1 个字节的低 7 位表示， 0 ~ 31 是控制字符如换行、回车、删除等，32 ~ 126 是打印字符，可以通过键盘输入并且能够显示出来。 ISO-8859-1 128 个字符显然是不够用的，所以 ISO 组织在 ASCII 的基础上扩展，他们是 ISO-8859-1 至 ISO-8859-15，前者涵盖大多数字符，应用最广。ISO-8859-1 仍是单字节编码，它总归能表示 256 个字符。 GB2312 它是双字节编码，总的编码范围是 A1 ~ F7，其中 A1 ~ A9 是符号区，总共包含 682 个符号；B0 ~ F7 是汉字区，包含 6763 个汉字。 GBk GBK 为《汉字内码扩展规范》，为 GB2312 的扩展，它的编码范围是 8140 ~ FEFE（去掉XX7F），总共有 23940 个码位，能表示 21003 个汉字，和 GB2312的编码兼容，不会有乱码。 UTF-16 它具体定义了 Unicode 字符在计算机中的存取方法。UTF-16 用两个字节来表示 Unicode 的转化格式，它采用定长的表示方法，即不论什么字符用两个字节表示。两个字节是 16 个 bit，所以叫 UTF-16。它表示字符非常方便，没两个字节表示一个字符，这就大大简化了字符串操作。 UTF-8 虽说 UTF-16 统一采用两个字节表示一个字符很简单方便，但是很大一部分字符用一个字节就可以表示，如果用两个字节表示，存储空间放大了一倍，在网络带宽有限的情况下会增加网络传输的流量。UTF-8 采用了一种变长技术，每个编码区域有不同的字码长度不同类型的字符可以由 1 ~ 6 个字节组成。 UTF-8 有以下编码规则： 如果是 1 个字节，最高位（第 8 位）为 0，则表示这是一个 ASCII 字符（00 ~ 7F） 如果是 1 个字节，以 11 开头，则连续的 1 的个数暗示这个字符的字节数 如果是 1 个字节，以 10 开头，表示它不是首字节，则需要向前查找才能得到当前字符的首字节 ​ 2、在 Java 中需要编码的场景2.1 在 I/O 操作中存在的编码 如上图：Reader 类是在 Java 的 I/O 中读取符的父类，而 InputStream 类是读字节的父类， InputStreamReader 类就是关联字节到字符的桥梁，它负责在 I/O 过程中处理读取字节到字符的转换，而对具体字节到字符的解码实现，它又委托 StreamDecoder 去做，在 StreamDecoder 解码过程中必须由用户指定 Charset 编码格式。值得注意的是，如果你没有指定 Charset，则将使用本地环境中默认的字符集，如在中文环境中将使用 GBK 编码。 如下面一段代码，实现了文件的读写功能： 12345678910111213141516171819202122232425String file = \"c:/stream.txt\";String charset = \"UTF-8\";// 写字符换转成字节流FileOutputStream outputStream = new FileOutputStream(file);OutputStreamWriter writer = new OutputStreamWriter(outputStream, charset);try &#123; writer.write(\"这是要保存的中文字符\");&#125; finally &#123; writer.close();&#125;// 读取字节转换成字符FileInputStream inputStream = new FileInputStream(file);InputStreamReader reader = new InputStreamReader(inputStream, charset);StringBuffer buffer = new StringBuffer();char[] buf = new char[64];int count = 0;try &#123; while ((count = reader.read(buf)) != -1) &#123; buffer.append(buffer, 0, count); &#125;&#125; finally &#123; reader.close();&#125; 在我们的应用程序中涉及 I/O 操作时，只要注意指定统一的编解码 Charset 字符集，一般不会出现乱码问题。 2.2 在内存操作中的编码在内存中进行从字符到字节的数据类型转换。 1、String 类提供字符串转换到字节的方法，也支持将字节转换成字符串的构造函数。 123String s = \"字符串\"；byte[] b = s.getBytes(\"UTF-8\");String n = new String(b, \"UTF-8\"); 2、Charset 提供 encode 与 decode，分别对应 char[] 到 byte[] 的编码 和 byte[] 到 char[] 的解码。 123Charset charset = Charset.forName(\"UTF-8\");ByteBuffer byteBuffer = charset.encode(string);CharBuffer charBuffer = charset.decode(byteBuffer); … 3、在 Java 中如何编解码Java 编码类图 首先根据指定的 charsetName 通过 Charset.forName(charsetName) 设置 Charset 类，然后根据 Charset 创建 CharsetEncoder 对象，再调用 CharsetEncoder.encode 对字符串进行编码，不同的编码类型都会对应到一个类中，实际的编码过程是在这些类中完成的。下面是 String. getBytes(charsetName) 编码过程的时序图 Java 编码时序图 从上图可以看出根据 charsetName 找到 Charset 类，然后根据这个字符集编码生成 CharsetEncoder，这个类是所有字符编码的父类，针对不同的字符编码集在其子类中定义了如何实现编码，有了 CharsetEncoder 对象后就可以调用 encode 方法去实现编码了。这个是 String.getBytes 编码方法，其它的如 StreamEncoder 中也是类似的方式。 经常会出现中文变成“？”很可能就是错误的使用了 ISO-8859-1 这个编码导致的。中文字符经过 ISO-8859-1 编码会丢失信息，通常我们称之为“黑洞”，它会把不认识的字符吸收掉。由于现在大部分基础的 Java 框架或系统默认的字符集编码都是 ISO-8859-1，所以很容易出现乱码问题，后面将会分析不同的乱码形式是怎么出现的。 几种编码格式的比较对中文字符后面四种编码格式都能处理，GB2312 与 GBK 编码规则类似，但是 GBK 范围更大，它能处理所有汉字字符，所以 GB2312 与 GBK 比较应该选择 GBK。UTF-16 与 UTF-8 都是处理 Unicode 编码，它们的编码规则不太相同，相对来说 UTF-16 编码效率最高，字符到字节相互转换更简单，进行字符串操作也更好。它适合在本地磁盘和内存之间使用，可以进行字符和字节之间快速切换，如 Java 的内存编码就是采用 UTF-16 编码。但是它不适合在网络之间传输，因为网络传输容易损坏字节流，一旦字节流损坏将很难恢复，想比较而言 UTF-8 更适合网络传输，对 ASCII 字符采用单字节存储，另外单个字符损坏也不会影响后面其它字符，在编码效率上介于 GBK 和 UTF-16 之间，所以 UTF-8 在编码效率上和编码安全性上做了平衡，是理想的中文编码方式。 4、在 Java Web 中涉及的编解码对于使用中文来说，有 I/O 的地方就会涉及到编码，前面已经提到了 I/O 操作会引起编码，而大部分 I/O 引起的乱码都是网络 I/O，因为现在几乎所有的应用程序都涉及到网络操作，而数据经过网络传输都是以字节为单位的，所以所有的数据都必须能够被序列化为字节。在 Java 中数据被序列化必须继承 Serializable 接口。 一段文本它的实际大小应该怎么计算，我曾经碰到过一个问题：就是要想办法压缩 Cookie 大小，减少网络传输量，当时有选择不同的压缩算法，发现压缩后字符数是减少了，但是并没有减少字节数。所谓的压缩只是将多个单字节字符通过编码转变成一个多字节字符。减少的是 String.length()，而并没有减少最终的字节数。例如将“ab”两个字符通过某种编码转变成一个奇怪的字符，虽然字符数从两个变成一个，但是如果采用 UTF-8 编码这个奇怪的字符最后经过编码可能又会变成三个或更多的字节。同样的道理比如整型数字 1234567 如果当成字符来存储，采用 UTF-8 来编码占用 7 个 byte，采用 UTF-16 编码将会占用 14 个 byte，但是把它当成 int 型数字来存储只需要 4 个 byte 来存储。所以看一段文本的大小，看字符本身的长度是没有意义的，即使是一样的字符采用不同的编码最终存储的大小也会不同，所以从字符到字节一定要看编码类型。 我们能够看到的汉字都是以字符形式出现的，例如在 Java 中“淘宝”两个字符，它在计算机中的数值 10 进制是 28120 和 23453，16 进制是 6bd8 和 5d9d，也就是这两个字符是由这两个数字唯一表示的。Java 中一个 char 是 16 个 bit 相当于两个字节，所以两个汉字用 char 表示在内存中占用相当于四个字节的空间。 这两个问题搞清楚后，我们看一下 Java Web 中那些地方可能会存在编码转换？ 用户从浏览器端发起一个 HTTP 请求，需要存在编码的地方是 URL、Cookie、Parameter。服务器端接受到 HTTP 请求后要解析 HTTP 协议，其中 URI、Cookie 和 POST 表单参数需要解码，服务器端可能还需要读取数据库中的数据，本地或网络中其它地方的文本文件，这些数据都可能存在编码问题，当 Servlet 处理完所有请求的数据后，需要将这些数据再编码通过 Socket 发送到用户请求的浏览器里，再经过浏览器解码成为文本。这些过程如下图所示： 一次 HTTP 请求的编码示例 4.1 URL 的编解码用户提交一个 URL，这个 URL 中可能存在中文，因此需要编码，如何对这个 URL 进行编码？根据什么规则来编码？有如何来解码？如下图一个 URL： 上图中以 Tomcat 作为 Servlet Engine 为例，它们分别对应到下面这些配置文件中：Port 对应在 Tomcat 的 中配置，而 Context Path 在 中配置，Servlet Path 在 Web 应用的 web.xml 中的 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;junshanExample&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlets/servlet/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 中配置，PathInfo 是我们请求的具体的 Servlet，QueryString 是要传递的参数，注意这里是在浏览器里直接输入 URL 所以是通过 Get 方法请求的，如果是 POST 方法请求的话，QueryString 将通过表单方式提交到服务器端。 上图中 PathInfo 和 QueryString 出现了中文，当我们在浏览器中直接输入这个 URL 时，在浏览器端和服务端会如何编码和解析这个 URL 呢？为了验证浏览器是怎么编码 URL 的我选择的是360极速浏览器并通过 Postman 插件观察我们请求的 URL 的实际的内容，以下是 URL： HTTP://localhost:8080/examples/servlets/servlet/君山?author=君山 君山的编码结果是：e5 90 9b e5 b1 b1，和《深入分析 Java Web 技术内幕》中的结果不一样，这是因为我使用的浏览器和插件和原作者是有区别的，那么这些浏览器之间的默认编码是不一样的，原文中的结果是： 君山的编码结果分别是：e5 90 9b e5 b1 b1，be fd c9 bd，查阅上一届的编码可知，PathInfo 是 UTF-8 编码而 QueryString 是经过 GBK 编码，至于为什么会有“%”？查阅 URL 的编码规范 RFC3986 可知浏览器编码 URL 是将非 ASCII 字符按照某种编码格式编码成 16 进制数字然后将每个 16 进制表示的字节前加上“%”，所以最终的 URL 就成了上图的格式了。 从上面测试结果可知浏览器对 PathInfo 和 QueryString 的编码是不一样的，不同浏览器对 PathInfo 也可能不一样，这就对服务器的解码造成很大的困难，下面我们以 Tomcat 为例看一下，Tomcat 接受到这个 URL 是如何解码的。 解析请求的 URL 是在 org.apache.coyote.HTTP11.InternalInputBuffer 的 parseRequestLine 方法中，这个方法把传过来的 URL 的 byte[] 设置到 org.apache.coyote.Request 的相应的属性中。这里的 URL 仍然是 byte 格式，转成 char 是在 org.apache.catalina.connector.CoyoteAdapter 的 convertURI 方法中完成的： 12345678910111213141516171819202122232425262728293031323334protected void convertURI(MessageBytes uri, Request request) throws Exception &#123; ByteChunk bc = uri.getByteChunk(); int length = bc.getLength(); CharChunk cc = uri.getCharChunk(); cc.allocate(length, -1); String enc = connector.getURIEncoding(); if (enc != null) &#123; B2CConverter conv = request.getURIConverter(); try &#123; if (conv == null) &#123; conv = new B2CConverter(enc); request.setURIConverter(conv); &#125; &#125; catch (IOException e) &#123;...&#125; if (conv != null) &#123; try &#123; conv.convert(bc, cc, cc.getBuffer().length - cc.getEnd()); uri.setChars(cc.getBuffer(), cc.getStart(), cc.getLength()); return; &#125; catch (IOException e) &#123;...&#125; &#125; &#125; // Default encoding: fast conversion byte[] bbuf = bc.getBuffer(); char[] cbuf = cc.getBuffer(); int start = bc.getStart(); for (int i = 0; i &lt; length; i++) &#123; cbuf[i] = (char) (bbuf[i + start] &amp; 0xff); &#125; uri.setChars(cbuf, 0, length); &#125; 从上面的代码中可以知道对 URL 的 URI 部分进行解码的字符集是在 connector 的 中定义的，如果没有定义，那么将以默认编码 ISO-8859-1 解析。所以如果有中文 URL 时最好把 URIEncoding 设置成 UTF-8 编码。 QueryString 又如何解析？ GET 方式 HTTP 请求的 QueryString 与 POST 方式 HTTP 请求的表单参数都是作为 Parameters 保存，都是通过 request.getParameter 获取参数值。对它们的解码是在 request.getParameter 方法第一次被调用时进行的。request.getParameter 方法被调用时将会调用 org.apache.catalina.connector.Request 的 parseParameters 方法。这个方法将会对 GET 和 POST 方式传递的参数进行解码，但是它们的解码字符集有可能不一样。POST 表单的解码将在后面介绍，QueryString 的解码字符集是在哪定义的呢？它本身是通过 HTTP 的 Header 传到服务端的，并且也在 URL 中，是否和 URI 的解码字符集一样呢？从前面浏览器对 PathInfo 和 QueryString 的编码采取不同的编码格式不同可以猜测到解码字符集肯定也不会是一致的。的确是这样 QueryString 的解码字符集要么是 Header 中 ContentType 中定义的 Charset 要么就是默认的 ISO-8859-1，要使用 ContentType 中定义的编码就要设置 connector 的 中的 useBodyEncodingForURI 设置为 true。这个配置项的名字有点让人产生混淆，它并不是对整个 URI 都采用 BodyEncoding 进行解码而仅仅是对 QueryString 使用 BodyEncoding 解码，这一点还要特别注意。 从上面的 URL 编码和解码过程来看，比较复杂，而且编码和解码并不是我们在应用程序中能完全控制的，所以在我们的应用程序中应该尽量避免在 URL 中使用非 ASCII 字符，不然很可能会碰到乱码问题，当然在我们的服务器端最好设置 中的 URIEncoding 和 useBodyEncodingForURI 两个参数。 4.2 HTTP Header 的编解码当客户端发起一个 HTTP 请求除了上面的 URL 外还可能会在 Header 中传递其它参数如 Cookie、redirectPath 等，这些用户设置的值很可能也会存在编码问题，Tomcat 对它们又是怎么解码的呢？ 对 Header 中的项进行解码也是在调用 request.getHeader 是进行的，如果请求的 Header 项没有解码则调用 MessageBytes 的 toString 方法，这个方法将从 byte 到 char 的转化使用的默认编码也是 ISO-8859-1，而我们也不能设置 Header 的其它解码格式，所以如果你设置 Header 中有非 ASCII 字符解码肯定会有乱码。 我们在添加 Header 时也是同样的道理，不要在 Header 中传递非 ASCII 字符，如果一定要传递的话，我们可以先将这些字符用 org.apache.catalina.util.URLEncoder 编码然后再添加到 Header 中，这样在浏览器到服务器的传递过程中就不会丢失信息了，如果我们要访问这些项时再按照相应的字符集解码就好了。 4.3 POST 表单的编解码在前面提到了 POST 表单提交的参数的解码是在第一次调用 request.getParameter 发生的，POST 表单参数传递方式与 QueryString 不同，它是通过 HTTP 的 BODY 传递到服务端的。当我们在页面上点击 submit 按钮时浏览器首先将根据 ContentType 的 Charset 编码格式对表单填的参数进行编码然后提交到服务器端，在服务器端同样也是用 ContentType 中字符集进行解码。所以通过 POST 表单提交的参数一般不会出现问题，而且这个字符集编码是我们自己设置的，可以通过 request.setCharacterEncoding(charset) 来设置。 另外针对 multipart/form-data 类型的参数，也就是上传的文件编码同样也是使用 ContentType 定义的字符集编码，值得注意的地方是上传文件是用字节流的方式传输到服务器的本地临时目录，这个过程并没有涉及到字符编码，而真正编码是在将文件内容添加到 parameters 中，如果用这个编码不能编码时将会用默认编码 ISO-8859-1 来编码。 4.4 HTTP BODY 的编解码当用户请求的资源已经成功获取后，这些内容将通过 Response 返回给客户端浏览器，这个过程先要经过编码再到浏览器进行解码。这个过程的编解码字符集可以通过 response.setCharacterEncoding 来设置，它将会覆盖 request.getCharacterEncoding 的值，并且通过 Header 的 Content-Type 返回客户端，浏览器接受到返回的 socket 流时将通过 Content-Type 的 charset 来解码，如果返回的 HTTP Header 中 Content-Type 没有设置 charset，那么浏览器将根据 Html 的 中的 charset 来解码。如果也没有定义的话，那么浏览器将使用默认的编码来解码。 4.5 其它需要编码的地方除了 URL 和参数编码问题外，在服务端还有很多地方可能存在编码，如可能需要读取 xml、velocity 模版引擎、JSP 或者从数据库读取数据等。xml 文件可以通过设置头来制定编码格式 1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; Velocity 模版设置编码格式： 1services.VelocityService.input.encoding=UTF-8 JSP 设置编码格式： 1&lt;%@page contentType=&quot;text/html; charset=UTF-8&quot;%&gt; 访问数据库都是通过客户端 JDBC 驱动来完成，用 JDBC 来存取数据要和数据的内置编码保持一致，可以通过设置 JDBC URL 来制定如 MySQL：url=”jdbc:mysql://localhost:3306/DB?useUnicode=true&amp;characterEncoding=GBK”。 5、常见问题分析下面看一下，当我们碰到一些乱码时，应该怎么处理这些问题？出现乱码问题唯一的原因都是在 char 到 byte 或 byte 到 char 转换中编码和解码的字符集不一致导致的，由于往往一次操作涉及到多次编解码，所以出现乱码时很难查找到底是哪个环节出现了问题，下面就几种常见的现象进行分析。 5.1 中文变成了看不懂的字符例如，字符串“淘！我喜欢！”变成了“Ì Ô £ ¡Î Ò Ï²»¶ £ ¡”编码过程如下图所示： 字符串在解码时所用的字符集与编码字符集不一致导致汉字变成了看不懂的乱码，而且是一个汉字字符变成两个乱码字符。 5.2 一个汉字变成一个问号例如，字符串“淘！我喜欢！”变成了“？？？？？？”编码过程如下图所示: 将中文和中文符号经过不支持中文的 ISO-8859-1 编码后，所有字符变成了“？”，这是因为用 ISO-8859-1 进行编解码时遇到不在码值范围内的字符时统一用 3f 表示，这也就是通常所说的“黑洞”，所有 ISO-8859-1 不认识的字符都变成了“？”。 5.3 一个汉字变成两个问号例如，字符串“淘！我喜欢！”变成了“？？？？？？？？？？？？”编码过程如下图所示: 这种情况比较复杂，中文经过多次编码，但是其中有一次编码或者解码不对仍然会出现中文字符变成“？”现象，出现这种情况要仔细查看中间的编码环节，找出出现编码错误的地方。 5.4 一种不正常的正确编码还有一种情况是在我们通过 request.getParameter 获取参数值时，当我们直接调用 String value = request.getParameter(name); 会出现乱码，但是如果用下面的方式 String value = String(request.getParameter(name).getBytes(&quot; ISO-8859-1&quot;), &quot;GBK&quot;); 解析时取得的 value 会是正确的汉字字符，这种情况是怎么造成的呢？ 看下如所示： 这种情况是这样的，ISO-8859-1 字符集的编码范围是 0000-00FF，正好和一个字节的编码范围相对应。这种特性保证了使用 ISO-8859-1 进行编码和解码可以保持编码数值“不变”。虽然中文字符在经过网络传输时，被错误地“拆”成了两个欧洲字符，但由于输出时也是用 ISO-8859-1，结果被“拆”开的中文字的两半又被合并在一起，从而又刚好组成了一个正确的汉字。虽然最终能取得正确的汉字，但是还是不建议用这种不正常的方式取得参数值，因为这中间增加了一次额外的编码与解码，这种情况出现乱码时因为 Tomcat 的配置文件中 useBodyEncodingForURI 配置项没有设置为”true”，从而造成第一次解析式用 ISO-8859-1 来解析才造成乱码的。 6、总结本文首先总结了几种常见编码格式的区别，然后介绍了支持中文的几种编码格式，并比较了它们的使用场景。接着介绍了 Java 那些地方会涉及到编码问题，已经 Java 中如何对编码的支持。并以网络 I/O 为例重点介绍了 HTTP 请求中的存在编码的地方，以及 Tomcat 对 HTTP 协议的解析，最后分析了我们平常遇到的乱码问题出现的原因。 综上所述，要解决中文问题，首先要搞清楚哪些地方会引起字符到字节的编码以及字节到字符的解码，最常见的地方就是读取会存储数据到磁盘，或者数据要经过网络传输。然后针对这些地方搞清楚操作这些数据的框架的或系统是如何控制编码的，正确设置编码格式，避免使用软件默认的或者是操作系统平台默认的编码格式。 注明：文章大部分参考书籍《深入 Java Web 技术内幕》第三章，自己有删减，二次转载请也务必注明此出处。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"编码","slug":"编码","permalink":"http://www.54tianzhisheng.cn/tags/编码/"}]},{"title":"深度探究Java 中 finally 语句块","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/深度探究Java 中 finally 语句块/","text":"乍看这个题目，是不是有人会问，这个谁不知道啊，大凡熟悉 Java 编程的人都知道 finally 语句块的作用和用法。有什么可深度辨析的呢？事实并非如此，我发现即使写了很多年 Java 程序的人，也不一定能够透彻的理解 finally 语句块。本篇将以生动形象的案例来带您由浅入深的来分析一下这个小小的 finally，希望这篇文章能够让您真正的理解 finally 语句块的本质，至少阅读完本篇文章后，没有觉得浪费了时间。 可不能小看这个简单的 finally，看似简单的问题背后，却隐藏了无数的玄机。接下来我就带您一步一步的揭开这个 finally 的神秘面纱。 问题分析首先来问大家一个问题：finally 语句块一定会执行吗？很多人都认为 finally 语句块是肯定要执行的，其中也包括一些很有经验的 Java 程序员。可惜并不像大多人所认为的那样，对于这个问题，答案当然是否定的，我们先来看下面这个例子。 清单 1.12345678910111213141516171819public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"return value of test(): \" + test()); &#125;public static int test() &#123;int i = 1;// if(i == 1)// return 0;System.out.println(\"the previous statement of try block\");i = i / 0;try &#123; System.out.println(\"try block\"); return i; &#125;finally &#123; System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 1 的执行结果如下： 1234the previous statement of try block Exception in thread &quot;main&quot; java.lang.ArithmeticException: / by zero at com.bj.charlie.Test.test(Test.java:15) at com.bj.charlie.Test.main(Test.java:6) 另外，如果去掉上例中被注释的两条语句前的注释符，执行结果则是： 1return value of test(): 0 在以上两种情况下，finally 语句块都没有执行，说明什么问题呢？只有与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块才会执行。以上两种情况，都是在 try 语句块之前返回（return）或者抛出异常，所以 try 对应的 finally 语句块没有执行。 那好，即使与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块一定会执行吗？不好意思，这次可能又让大家失望了，答案仍然是否定的。请看下面这个例子（清单 2）。 清单 2.12345678910111213141516public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"return value of test(): \" + test()); &#125;public static int test() &#123;int i = 1;try &#123;System.out.println(\"try block\");System.exit(0);return i;&#125;finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 2 的执行结果如下： 1try block finally 语句块还是没有执行，为什么呢？因为我们在 try 语句块中执行了 System.exit (0) 语句，终止了 Java 虚拟机的运行。那有人说了，在一般的 Java 应用中基本上是不会调用这个 System.exit(0) 方法的。OK ！没有问题，我们不调用 System.exit(0) 这个方法，那么 finally 语句块就一定会执行吗？ 再一次让大家失望了，答案还是否定的。当一个线程在执行 try 语句块或者 catch 语句块时被打断（interrupted）或者被终止（killed），与其相对应的 finally 语句块可能不会执行。还有更极端的情况，就是在线程运行 try 语句块或者 catch 语句块时，突然死机或者断电，finally 语句块肯定不会执行了。可能有人认为死机、断电这些理由有些强词夺理，没有关系，我们只是为了说明这个问题。 finally 语句剖析说了这么多，还是让我们拿出些有说服力的证据吧！还有什么证据比官方的文档更具说服力呢？让我们来看看官方网站上的《The Java Tutorials》中是怎样来描述 finally 语句块的吧！以下内容原封不动的摘自于《 The Java Tutorials 》文档。 The finally BlockThe finally block always executes when the try block exits. This ensures that the finally block is executed even if an unexpected exception occurs. But finally is useful for more than just exception handling — it allows the programmer to avoid having cleanup code accidentally bypassed by a return,continue, or break. Putting cleanup code in a finally block is always a good practice, even when no exceptions are anticipated.Note: If the JVM exits while the try or catch code is being executed, then the finally block may not execute. Likewise, if the thread executing the try or catch code is interrupted or killed, the finally block may not execute even though the application as a whole continues. 请仔细阅读并认真体会一下以上两段英文，当你真正的理解了这两段英文的确切含义，你就可以非常自信的来回答“finally 语句块是否一定会执行？”这样的问题。看来，大多时候，并不是 Java 语言本身有多么高深，而是我们忽略了对基础知识的深入理解。 接下来，我们看一下 finally 语句块是怎样执行的。在排除了以上 finally 语句块不执行的情况后，finally 语句块就得保证要执行，既然 finally 语句块一定要执行，那么它和 try 语句块与 catch 语句块的执行顺序又是怎样的呢？还有，如果 try 语句块中有 return 语句，那么 finally 语句块是在 return 之前执行，还是在 return 之后执行呢？带着这样一些问题，我们还是以具体的案例来讲解。 关于 try、catch、finally 的执行顺序问题，我们还是来看看权威的论述吧！以下内容摘自 Java 语言规范第四版（《The Java™ Programming Language, Fourth Edition》）中对于 try，catch，和 finally 的描述。 12.4. Try, catch, and finallyYou catch exceptions by enclosing code in Try blocks. The basic syntax for a Try block is:try {statements} catch (exception_type1 identifier1) {statements} catch (exception_type2 identifier2) {statements…} finally {statements} where either at least one catch clause, or the finally clause, must be present. The body of the try statement is executed until either an exception is thrown or the body finishes successfully. If an exception is thrown, each catch clause is examined in turn, from first to last, to see whether the type of the exception object is assignable to the type declared in the catch. When an assignable catch clause is found, its block is executed with its identifier set to reference the exception object. No other catch clause will be executed. Any number of catch clauses, including zero, can be associated with a particular TRy as long as each clause catches a different type of exception. If no appropriate catch is found, the exception percolates out of the try statement into any outer try that might have a catch clause to handle it. If a finally clause is present with a try, its code is executed after all other processing in the try is complete. This happens no matter how completion was achieved, whether normally, through an exception, or through a control flow statement such as return or break. 上面这段文字的大体意思是说，不管 try 语句块正常结束还是异常结束，finally 语句块是保证要执行的。如果 try 语句块正常结束，那么在 try 语句块中的语句都执行完之后，再执行 finally 语句块。如果 try 中有控制转移语句（return、break、continue）呢？那 finally 语句块是在控制转移语句之前执行，还是之后执行呢？似乎从上面的描述中我们还看不出任何端倪，不要着急，后面的讲解中我们会分析这个问题。如果 try 语句块异常结束，应该先去相应的 catch 块做异常处理，然后执行 finally 语句块。同样的问题，如果 catch 语句块中包含控制转移语句呢？ finally 语句块是在这些控制转移语句之前，还是之后执行呢？我们也会在后续讨论中提到。 其实，关于 try，catch，finally 的执行流程远非这么简单，有兴趣的读者可以参考 Java 语言规范第三版（《The Java™ Language Specification, Third Edition》）中对于 Execution of try-catch-finally 的描述，非常复杂的一个流程。限于篇幅的原因，本文不做摘录，请感兴趣的读者自行阅读。 finally 语句示例说明下面，我们先来看一个简单的例子（清单 3）。 清单 3.12345678910public class Test &#123;public static void main(String[] args) &#123;try &#123;System.out.println(\"try block\");return ;&#125; finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 3 的执行结果为：12try blockfinally block 清单 3 说明 finally 语句块在 try 语句块中的 return 语句之前执行。我们再来看另一个例子（清单 4）。 清单 4.123456789101112131415161718public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"reture value of test() : \" + test()); &#125;public static int test()&#123;int i = 1;try &#123; System.out.println(\"try block\"); i = 1 / 0; return 1;&#125;catch (Exception e)&#123;System.out.println(\"exception block\");return 2;&#125;finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 4 的执行结果为：1234try blockexception blockfinally blockreture value of test() : 2 清单 4 说明了 finally 语句块在 catch 语句块中的 return 语句之前执行。 从上面的清单 3 和清单 4，我们可以看出，其实 finally 语句块是在 try 或者 catch 中的 return 语句之前执行的。更加一般的说法是，finally 语句块应该是在控制转移语句之前执行，控制转移语句除了 return 外，还有 break 和 continue。另外，throw 语句也属于控制转移语句。虽然 return、throw、break 和 continue 都是控制转移语句，但是它们之间是有区别的。其中 return 和 throw 把程序控制权转交给它们的调用者（invoker），而 break 和 continue 的控制权是在当前方法内转移。请大家先记住它们的区别，在后续的分析中我们还会谈到。 还是得来点有说服力的证据，下面这段摘自 Java 语言规范第四版（《The Java™ Programming Language, Fourth Edition》），请读者自己体会一下其含义。 Afinallyclause can also be used to clean up forbreak,continue, andreturn, which is one reason you will sometimes see atryclause with nocatchclauses. When any control transfer statement is executed, all relevantfinallyclauses are executed. There is no way to leave atryblock without executing itsfinallyclause. 好了，看到这里，是不是有人认为自己已经掌握了 finally 的用法了？先别忙着下结论，我们再来看两个例子 – 清单 5 和清单 6。 清单 5.123456789101112public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125; public static int getValue() &#123; try &#123; return 0; &#125; finally &#123; return 1; &#125; &#125; &#125; 清单 5 的执行结果：1return value of getValue(): 1 清单 6.12345678910111213public class Test &#123;public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125;public static int getValue() &#123; int i = 1; try &#123; return i; &#125; finally &#123; i++; &#125; &#125;&#125; 清单 6 的执行结果：1return value of getValue(): 1 利用我们上面分析得出的结论：finally 语句块是在 try 或者 catch 中的 return 语句之前执行的。 由此，可以轻松的理解清单 5 的执行结果是 1。因为 finally 中的 return 1；语句要在 try 中的 return 0；语句之前执行，那么 finally 中的 return 1；语句执行后，把程序的控制权转交给了它的调用者 main（）函数，并且返回值为 1。那为什么清单 6 的返回值不是 2，而是 1 呢？按照清单 5 的分析逻辑，finally 中的 i++；语句应该在 try 中的 return i；之前执行啊？ i 的初始值为 1，那么执行 i++；之后为 2，再执行 return i；那不就应该是 2 吗？怎么变成 1 了呢？ 关于 Java 虚拟机是如何编译 finally 语句块的问题，有兴趣的读者可以参考《 The JavaTM Virtual Machine Specification, Second Edition 》中 7.13 节 Compiling finally。那里详细介绍了 Java 虚拟机是如何编译 finally 语句块。实际上，Java 虚拟机会把 finally 语句块作为 subroutine（对于这个 subroutine 不知该如何翻译为好，干脆就不翻译了，免得产生歧义和误解。）直接插入到 try 语句块或者 catch 语句块的控制转移语句之前。但是，还有另外一个不可忽视的因素，那就是在执行 subroutine（也就是 finally 语句块）之前，try 或者 catch 语句块会保留其返回值到本地变量表（Local Variable Table）中。待 subroutine 执行完毕之后，再恢复保留的返回值到操作数栈中，然后通过 return 或者 throw 语句将其返回给该方法的调用者（invoker）。请注意，前文中我们曾经提到过 return、throw 和 break、continue 的区别，对于这条规则（保留返回值），只适用于 return 和 throw 语句，不适用于 break 和 continue 语句，因为它们根本就没有返回值。 是不是不太好理解，那我们就用具体的例子来做形象的说明吧！ 为了能够解释清单 6 的执行结果，我们来分析一下清单 6 的字节码（byte-code）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Compiled from &quot;Test.java&quot; public class Test extends java.lang.Object&#123; public Test(); Code: 0: aload_0 1:invokespecial#1; //Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 1: 0 public static void main(java.lang.String[]); Code: 0: getstatic #2; //Field java/lang/System.out:Ljava/io/PrintStream; 3: new #3; //class java/lang/StringBuilder 6: dup 7: invokespecial #4; //Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 10: ldc #5; //String return value of getValue(): 12: invokevirtual #6; //Method java/lang/StringBuilder.append:( Ljava/lang/String;)Ljava/lang/StringBuilder; 15: invokestatic #7; //Method getValue:()I 18: invokevirtual #8; //Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 21: invokevirtual #9; //Method java/lang/StringBuilder.toString:()Ljava/lang/String; 24: invokevirtual #10; //Method java/io/PrintStream.println:(Ljava/lang/String;)V 27: return public static int getValue(); Code: 0: iconst_1 1: istore_0 2: iload_0 3: istore_1 4: iinc 0, 1 7: iload_1 8: ireturn 9: astore_2 10: iinc 0, 1 13: aload_2 14: athrow Exception table: from to target type 2 4 9 any 9 10 9 any &#125; 对于 Test（）构造方法与 main（）方法，在这里，我们不做过多解释。让我们来分析一下 getValue（）方法的执行。在这之前，先让我把 getValue（）中用到的虚拟机指令解释一下，以便读者能够正确的理解该函数的执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445461. iconst_Description: Push the int constant (-1, 0, 1, 2, 3, 4 or 5) onto the operand stack.Forms: iconst_m1 = 2 (0x2) iconst_0 = 3 (0x3) iconst_1 = 4 (0x4)iconst_2 = 5 (0x5) iconst_3 = 6 (0x6) iconst_4 = 7 (0x7) iconst_5 = 8 (0x8)2. istore_Description: Store int into local variable. The must be an index into thelocal variable array of the current frame.Forms: istore_0 = 59 (0x3b) istore_1 = 60 (0x3c) istore_2 = 61 (0x3d)istore_3 = 62 (0x3e)3. iload_Description: Load int from local variable. The must be an index into thelocal variable array of the current frame.Forms: iload_0 = 26 (0x1a) iload_1 = 27 (0x1b) iload_2 = 28 (0x1c) iload_3 = 29 (0x1d)4. iinc index, constDescription: Increment local variable by constant. The index is an unsigned byte thatmust be an index into the local variable array of the current frame. The const is animmediate signed byte. The local variable at index must contain an int. The valueconst is first sign-extended to an int, and then the local variable at index isincremented by that amount.Forms: iinc = 132 (0x84)Format:iincindexconst5. ireturnDescription: Return int from method.Forms: ireturn = 172 (0xac)6. astore_Description: Store reference into local variable. The must be an index into thelocal variable array of the current frame.Forms: astore_0 = 75 (0x4b) astore_1 = 76 (0x4c) astore_2 =77 (0x4d) astore_3 =78 (0x4e)7. aload_Description: Load reference from local variable. The must be an index into thelocal variable array of the current frame.Forms: aload_0 = 42 (0x2a) aload_1 = 43 (0x2b) aload_2 = 44 (0x2c) aload_3 = 45 (0x2d)8. athrowDescription: Throw exception or error.Forms: athrow = 191 (0xbf) 有了以上的 Java 虚拟机指令，我们来分析一下其执行顺序：分为正常执行（没有 exception）和异常执行（有 exception）两种情况。我们先来看一下正常执行的情况，如图 1 所示： ###图 1. getValue（）函数正常执行的情况 由上图，我们可以清晰的看出，在 finally 语句块（iinc 0, 1）执行之前，getValue（）方法保存了其返回值（1）到本地表量表中 1 的位置，完成这个任务的指令是 istore_1；然后执行 finally 语句块（iinc 0, 1），finally 语句块把位于 0 这个位置的本地变量表中的值加 1，变成 2；待 finally 语句块执行完毕之后，把本地表量表中 1 的位置上值恢复到操作数栈（iload _1），最后执行 ireturn 指令把当前操作数栈中的值（1）返回给其调用者（main）。这就是为什么清单 6 的执行结果是 1，而不是 2 的原因。 再让我们来看看异常执行的情况。是不是有人会问，你的清单 6 中都没有 catch 语句，哪来的异常处理呢？我觉得这是一个好问题，其实，即使没有 catch 语句，Java 编译器编译出的字节码中还是有默认的异常处理的，别忘了，除了需要捕获的异常，还可能有不需捕获的异常（如：RunTimeException 和 Error）。 从 getValue（）方法的字节码中，我们可以看到它的异常处理表（exception table）， 如下：123Exception table:from to target type2 4 9 any 它的意思是说：如果从 2 到 4 这段指令出现异常，则由从 9 开始的指令来处理。 图 2. getValue（）函数异常执行的情况 先说明一点，上图中的 exception 其实应该是 exception 对象的引用，为了方便说明，我直接把它写成 exception 了。 由上图（图 2）可知，当从 2 到 4 这段指令出现异常时，将会产生一个 exception 对象，并且把它压入当前操作数栈的栈顶。接下来是 astore_2 这条指令，它负责把 exception 对象保存到本地变量表中 2 的位置，然后执行 finally 语句块，待 finally 语句块执行完毕后，再由 aload _2 这条指令把预先存储的 exception 对象恢复到操作数栈中，最后由 athrow 指令将其返回给该方法的调用者（main）。 通过以上的分析，大家应该已经清楚 try-catch-finally 语句块的执行流程了吧！为了更具说服力，我们还是来引经据典吧！下面这段仍然摘自 Java 语言规范第四版 《The Java™ Programming Language, Fourth Edition》，请读者自己体会吧！ a finally clause is always entered with a reason. That reason may be that the try code finished normally, that it executed a control flow statement such as return, or that an exception was thrown in code executed in the Try block. The reason is remembered when the finally clause exits by falling out the bottom. However, if the finally block creates its own reason to leave by executing a control flow statement (such as break or return) or by throwing an exception, that reason supersedes the original one, and the original reason is forgotten. For example, consider the following code:try {// … do something …return 1;} finally {return 2;}When the Try block executes its return, the finally block is entered with the “reason” of returning the value 1. However, inside the finally block the value 2 is returned, so the initial intention is forgotten. In fact, if any of the other code in the try block had thrown an exception, the result would still be to return 2. If the finally block did not return a value but simply fell out the bottom, the “return the value 1 ″ reason would be remembered and carried out. 好了，有了以上的知识，让我们再来看以下 3 个例子。 清单 7.123456789101112131415public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125; @SuppressWarnings(\"finally\") public static int getValue() &#123; int i = 1; try &#123; i = 4; &#125; finally &#123; i++; return i; &#125; &#125; &#125; 清单 7 的执行结果：1return value of getValue(): 5 清单 8.1234567891011121314public class Test &#123;public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125;public static int getValue() &#123; int i = 1; try &#123; i = 4; &#125; finally &#123; i++; &#125; return i; &#125;&#125; 清单 8 的执行结果：1return value of getValue(): 5 清单 7 和清单 8 应该还比较简单吧！利用我们上面讲解的知识，很容易分析出其结果。让我们再来看一个稍微复杂一点的例子 – 清单 9。我建议大家最好先不要看执行结果，运用学过的知识来分析一下，看是否能推断出正确的结果。 清单 9.1234567891011121314151617public class Test &#123;public static void main(String[] args) &#123;System.out.println(test()); &#125;public static String test() &#123;try &#123;System.out.println(\"try block\");return test1();&#125; finally &#123;System.out.println(\"finally block\"); &#125; &#125;public static String test1() &#123;System.out.println(\"return statement\");return \"after return\"; &#125;&#125; 清单 9 的结果：1234try blockreturn statementfinally blockafter return 你分析对了吗？其实这个案例也不算很难，return test1();这条语句等同于 : 12String tmp = test1();return tmp; 这样，就应该清楚为什么是上面所示的执行结果了吧！ 如果还是不怎么清楚可以在 IDE 下试用下 Debug，然后查看具体的运行步骤。 好了，就写到这吧！希望大家看完这篇文章能够有所收获！ 总结没想到吧！一个小小的、看似简单的 finally 语句块背后居然隐藏了这么多玄机。看来，我们平时还是应该认真的阅读 Java 相关的基础文档，比如：Java 语言规范、Java 虚拟机规范等，很多棘手的问题都可以从中得到答案。只有真正的吃透了基础知识，才能达到运用自如的境界！ 参考资料参考 The Java Tutorials，查看对 finally 语句块的描述。 参考 《The Java Programming Language, Fourth Edition》，查看 Java 语言规范第四版中对 finally 语句块的解释。 参考 《The Java Language Specification, Third Edition》，查看 Java 语言规范第三版中对 finally 语句块执行流程的具体描述。 参考 《The Java Virtual Machine Specification, Second Edition》，查看 Java 虚拟机是如何来编译 finally 语句块的知识。 查看文章 《Java 的异常处理机制(try … catch … finally)》，了解更多关于 Java 中 finally 语句块的分析。 查看文章 《Java 中 finally 的辨析》，了解更多关于 Java 中 finally 语句块的分析。 查看文章《关于 Java 中 finally 语句块的深度辨析》","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"finally","slug":"finally","permalink":"http://www.54tianzhisheng.cn/tags/finally/"}]},{"title":"《Java 多线程编程核心技术》学习笔记及总结","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java-Thread/","text":"第一章 —— Java 多线程技能线程技术点： 线程的启动 如何使线程暂停 如何使线程停止 线程的优先级 线程安全相关问题 进程和线程的概念及多线程的优点进程：比如我们电脑运行的 QQ.exe 程序，是操作系统管理的基本运行单元 线程：在进程中独立运行的子任务，比如 QQ.exe 进程中就有很多线程在运行，下载文件线程、发送消息线程、语音线程、视频线程等。 多线程优点：我们电脑可以同时操作不同的软件，边听着歌，敲着代码，查看 pdf 文档，浏览网页等，CPU 在这些任务之间不停的切换，切换非常快，所以我们就觉得他们是在同时运行的。 使用多线程继承 Thread 类JDK 源码注释（Thread.java）如下： 12345678910111213141516171819One is to declare a class to be a subclass(子类) of &lt;code&gt;Thread&lt;/code&gt;. This subclass should override the &lt;code&gt;run&lt;/code&gt; method of class &lt;code&gt;Thread&lt;/code&gt;. An instance of the subclass can then be allocated and started. For example, a thread that computes primeslarger than a stated value could be written as follows://继承 Thread 类class PrimeThread extends Thread &#123; long minPrime; PrimeThread(long minPrime) &#123; this.minPrime = minPrime; &#125; public void run() &#123; // compute primes larger than minPrime 重写 Thread 类的 run 方法 &#125; &#125;The following code would then create a thread and start it running://开启线程 PrimeThread p = new PrimeThread(143); p.start(); 实现 Runnable 接口JDK 源码注释（Thread.java）如下： 12345678910111213141516171819The other way to create a thread is to declare a class that implements the &lt;code&gt;Runnable&lt;/code&gt; interface. That class then implements the &lt;code&gt;run&lt;/code&gt; method. An instance of the class can then be allocated, passed as an argument when creating&lt;code&gt;Thread&lt;/code&gt;, and started. The same example in this other style looks like the following://实现 Runnable 接口 class PrimeRun implements Runnable &#123; long minPrime; PrimeRun(long minPrime) &#123; this.minPrime = minPrime; &#125; public void run() &#123; // compute primes larger than minPrime //重写 run 方法 &#125; &#125;The following code would then create a thread and start it running://开启线程 PrimeRun p = new PrimeRun(143); new Thread(p).start(); currentThread() 方法该方法返回代码段正在被哪个线程调用的信息。 isAlive() 方法判断当前线程是否处于活动状态（已经启动但未终止） sleep() 方法在指定的毫秒数内让当前“正在执行的线程（this.currentThread() 返回的线程）”休眠（暂停执行）。 getId() 方法获取线程的唯一标识 停止线程可以使用 Thread.stop() 方法，但最好不要用，因为这个方法是不安全的，已经弃用作废了。 大多数停止一个线程是使用 Thread.interrupt() 方法 判断线程是否是停止状态 interrupted() 12345//测试当前线程是否已经中断了，这个线程的中断状态会被这个方法清除。//换句话说，如果连续两次调用了这个方法，第二次调用的时候将会返回 false ，public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; isInterrupted() 1234567891011 //测试线程是否已经中断了，线程的状态不会受这个方法的影响 //线程中断被忽略，因为线程处于中断下不处于活动状态的线程由此返回false的方法反映出来 public boolean isInterrupted() &#123; return isInterrupted(false); &#125; /*** Tests if some Thread has been interrupted. The interrupted state* is reset or not based on the value of ClearInterrupted that is* passed.*/private native boolean isInterrupted(boolean ClearInterrupted); 在沉睡中停止1234567891011121314151617181920212223242526public class MyThread2 extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"run start\"); Thread.sleep(20000); System.out.println(\"run end\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"run catch \"+this.isInterrupted()); e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; try &#123; MyThread2 t2 = new MyThread2(); t2.start(); Thread.sleep(200); t2.interrupt(); &#125; catch (InterruptedException e) &#123; System.out.println(\"main catch\"); e.printStackTrace(); &#125; System.out.println(\"main end\"); &#125;&#125; 运行结果： 123456run startmain endrun catch falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.zhisheng.thread.thread1.MyThread2.run(MyThread2.java:12) 从运行结果来看，如果在 sleep 状态下停止某一线程，会进入 catch 语句，并清除停止状态值，使之变成 false。 在停止中沉睡12345678910111213141516171819public class MyThread3 extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"run start\"); Thread.sleep(20000); System.out.println(\"run end\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"run catch \"+this.isInterrupted()); e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; MyThread3 t3 = new MyThread3(); t3.start(); t3.interrupt(); &#125;&#125; 运行结果： 12345run startrun catch falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.zhisheng.thread.thread1.MyThread3.run(MyThread3.java:12) 能停止的线程 —— 暴力停止使用 stop() 方法停止线程 暂停线程可使用 suspend 方法暂停线程，使用 resume() 方法恢复线程的执行。 suspend 和 resume 方法的使用1234567891011121314151617181920212223242526272829public class MyThread4 extends Thread&#123; private int i; public int getI() &#123; return i; &#125; public void setI(int i) &#123; this.i = i; &#125; @Override public void run() &#123; while (true) &#123; i++; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyThread4 t4 = new MyThread4(); t4.start(); System.out.println(\"A----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); Thread.sleep(2000); System.out.println(\"A----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); t4.suspend(); Thread.sleep(2000); t4.resume(); System.out.println(\"B----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); Thread.sleep(2000); System.out.println(\"B----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); &#125;&#125; 从运行结果来看，线程的确能够暂停和恢复。 但是 suspend 和 resume 方法的缺点就是：不同步，因为线程的暂停导致数据的不同步。 yield 方法123456789101112131415161718/** * A hint to the scheduler that the current thread is willing to yield * its current use of a processor. The scheduler is free to ignore this * hint. * * &lt;p&gt; Yield is a heuristic attempt to improve relative progression * between threads that would otherwise over-utilise a CPU. Its use * should be combined with detailed profiling and benchmarking to * ensure that it actually has the desired effect. * * &lt;p&gt; It is rarely appropriate to use this method. It may be useful * for debugging or testing purposes, where it may help to reproduce * bugs due to race conditions. It may also be useful when designing * concurrency control constructs such as the ones in the * &#123;@link java.util.concurrent.locks&#125; package. */ //暂停当前正在执行的线程对象，并执行其他线程。暂停的时间不确定。 public static native void yield(); 1234567891011121314151617public class MyThread5 extends Thread&#123; @Override public void run() &#123; double start = System.currentTimeMillis(); for (int i = 0; i &lt; 200000; i++) &#123; //yield();//暂停的时间不确定 i++; &#125; double end = System.currentTimeMillis(); System.out.println(\"time is \"+(end - start)); &#125; public static void main(String[] args) &#123; MyThread5 t5 = new MyThread5(); t5.start(); &#125;&#125; 线程的优先级设置优先级的方法：setPriority() 方法 12345678910111213public final void setPriority(int newPriority) &#123; ThreadGroup g; checkAccess(); if (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123; throw new IllegalArgumentException(); &#125; if((g = getThreadGroup()) != null) &#123; if (newPriority &gt; g.getMaxPriority()) &#123; newPriority = g.getMaxPriority(); &#125; setPriority0(priority = newPriority); &#125; &#125; 不一定优先级高的线程就先执行。 守护线程当进程中不存在非守护线程了，则守护线程自动销毁。垃圾回收线程就是典型的守护线程，当进程中没有非守护线程了，则垃圾回收线程也就没有存在的必要了，自动销毁。 12345678910111213141516171819202122/** * Marks this thread as either a &#123;@linkplain #isDaemon daemon&#125; thread * or a user thread. The Java Virtual Machine exits when the only * threads running are all daemon threads. * * &lt;p&gt; This method must be invoked before the thread is started. * * @param on * if &#123;@code true&#125;, marks this thread as a daemon thread * @throws IllegalThreadStateException * if this thread is &#123;@linkplain #isAlive alive&#125; * @throws SecurityException * if &#123;@link #checkAccess&#125; determines that the current * thread cannot modify this thread */ public final void setDaemon(boolean on) &#123; checkAccess(); if (isAlive()) &#123; throw new IllegalThreadStateException(); &#125; daemon = on; &#125; 第二章 —— 对象及变量的并发访问技术点： synchronized 对象监视器为 Object 时的使用 synchronized 对象监视器为 Class 时的使用 非线程安全是如何出现的 关键字 volatile 的主要作用 关键字 volatile 与 synchronized 的区别及使用情况 synchronized 同步方法方法内的变量为线程安全“非线程安全”问题存在于“实例变量”中，如果是方法内部的私有变量，则不存在“非线程安全”问题，所得结果也就是“线程安全”了。 实例变量非线程安全如果多线程共同访问一个对象中的实例变量，则有可能出现“非线程安全”问题。 在两个线程访问同一个对象中的同步方法时一定是线程安全的。 脏读发生脏读的情况是在读取实例变量时，此值已经被其他线程更改过了。 如下例子就可以说明，如果不加 synchronized 关键字在 setValue 和 getValue 方法上，就会出现数据脏读。 123456789101112131415161718192021222324252627282930313233343536373839404142class VarName&#123; private String userName = \"A\"; private String password = \"AA\"; synchronized public void setValue(String userName, String password) &#123; try &#123; this.userName = userName; Thread.sleep(500); this.password = password; System.out.println(\"setValue method Thread name is : \" + Thread.currentThread().getName() + \" userName = \" + userName + \" password = \" + password); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //synchronized public void getValue() &#123; System.out.println(\"getValue method Thread name is : \" + Thread.currentThread().getName() + \" userName = \" + userName + \" password = \" + password); &#125;&#125;class Thread1 extends Thread&#123; private VarName varName; public Thread1(VarName varName) &#123; this.varName = varName; &#125; @Override public void run() &#123; varName.setValue(\"B\", \"BB\"); &#125;&#125;public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; VarName v = new VarName(); Thread1 thread1 = new Thread1(v); thread1.start(); Thread.sleep(200);//打印结果受睡眠时间的影响 v.getValue(); &#125;&#125; synchronized 锁重入关键字 synchronized 拥有锁重入的功能，也就是在使用 synchronized 时，当一个线程得到一个对象锁后，再次请求此对象锁是可以再次得到该对象的锁的。这也证明了在一个 synchronized 方法/块的内部调用本类的其他 synchronized 方法/块时，是永远可以得到锁的。 12345678910111213141516171819202122232425262728293031class Service&#123; synchronized public void service1() &#123; System.out.println(\"service 1\"); service2(); &#125; synchronized public void service2() &#123; System.out.println(\"service 2\"); service3(); &#125; synchronized public void service3() &#123; System.out.println(\"service 3\"); &#125;&#125;class Thread2 extends Thread&#123; @Override public void run() &#123; Service s = new Service(); s.service1(); &#125;&#125;public class Test2&#123; public static void main(String[] args) &#123; Thread2 t2 = new Thread2(); t2.start(); &#125;&#125; 运行结果： 123service 1service 2service 3 同步不具有继承性同步不可以继承。 synchronized 同步语句块synchronized 代码块间的同步性当一个线程访问 object 的一个 synchronized(this) 同步代码块时，其他线程对同一个 object 中所有其他 synchronized(this) 同步代码块的访问将被阻塞，这说明 synchronized 使用的 “对象监视器” 是一个。 将任意对象作为对象监视器多个线程调用同一个对象中的不同名称的 synchronized 同步方法或者 synchronized(this) 同步代码块时，调用的效果就是按顺序执行，也就是同步的，阻塞的。 静态同步 synchronized 方法与 synchronized(class) 代码块关键字 synchronized 还可以应用在 static 静态方法上，如果这样写就是对当前的 *.java 文件对应的 Class 类进行加锁。而 synchronized 关键字加到非 static 静态方法上就是给对象加锁。 多线程的死锁volatile 关键字作用：使变量在多个线程间可见。 通过使用 volatile 关键字，强制的从公共内存中读取变量的值。使用 volatile 关键字增加了实例变量在多个线程之间的可见性，但 volatile 关键字最致命的缺点就是不支持原子性。 关键字 synchronized 和 volatile 比较： 关键字 volatile 是线程同步的轻量实现，所以 volatile 性能肯定要比 synchronized 要好，并且 volatile 只能修饰于变量，而 synchronized 可以修饰方法，以及代码块。 多线程访问 volatile 不会发生阻塞，而 synchronized 会出现阻塞。 volatile 能保证数据的可见性，但不能保证原子性；而 synchronized 可以保证原子性，也可以间接保证可见性，因为它会将私有内存和公有内存中的数据做同步。 关键字 volatile 解决的是变量在多个线程之间的可见性；而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 ​ 第三章 —— 线程间通信技术点： 使用 wait/notify 实现线程间的通信 生产者/消费者模式的实现 方法 join 的使用 ThreadLocal 类的使用 等待/通知机制wait 使线程停止运行，notify 使停止的线程继续运行。 关键字 synchronized 可以将任何一个 Object 对象作为同步对象看待，而 Java 为每个 Object 都实现了 wait() 和 notify() 方法，他们必须用在被 synchronized 同步的 Object 的临界区内。通过调用 wait 方法可以使处于临界区内的线程进入等待状态，同时释放被同步对象的锁。而 notify 操作可以唤醒一个因调用了 wait 方法而处于阻塞状态的线程，使其进入就绪状态。被重新唤醒的线程会试图重新获得临界区的控制权，继续执行临界区内 wait 之后的代码。 wait 方法可以使调用该方法的线程释放共享资源的锁，从运行状态退出，进入等待状态，直到再次被唤醒。 notify() 方法可以随机唤醒等待对列中等待同一共享资源的一个线程，并使该线程退出等待状态，进入可运行状态。 notifyAll() 方法可以随机唤醒等待对列中等待同一共享资源的所有线程，并使这些线程退出等待状态，进入可运行状态。 线程状态示意图： 新创建一个线程对象后，在调用它的 start() 方法，系统会为此线程分配 CPU 资源，使其处于 Runnable（可运行）状态，如果线程抢占到 CPU 资源，此线程就会处于 Running （运行）状态 Runnable 和 Running 状态之间可以相互切换，因为线程有可能运行一段时间后，有其他优先级高的线程抢占了 CPU 资源，此时线程就从 Running 状态变成了 Runnable 状态。 线程进入 Runnable 状态有如下几种情况： 调用 sleep() 方法后经过的时间超过了指定的休眠时间 线程调用的阻塞 IO 已经返回，阻塞方法执行完毕 线程成功的获得了试图同步的监视器 线程正在等待某个通知，其他线程发出了通知 处于挂状态的线程调用了 resume 恢复方法 Blocked 是阻塞的意思，例如线程遇到一个 IO 操作，此时 CPU 处于空闲状态，可能会转而把 CPU 时间片分配给其他线程，这时也可以称为 “暂停”状态。Blocked 状态结束之后，进入 Runnable 状态，等待系统重新分配资源。 出现阻塞状态的有如下几种情况： 线程调用 sleep 方法，主动放弃占用的处理器资源 线程调用了阻塞式 IO 方法，在该方法返回之前，该线程被阻塞 线程试图获得一个同步监视器，但该同步监视器正在被其他线程所持有 线程等待某个通知 程序调用了 suspend 方法将该线程挂起 run 方法运行结束后进入销毁阶段，整个线程执行完毕。 生产者/消费者模式实现一个生产者，一个消费者 存储值对象： 12345678910package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 存储值对象 */public class ValueObject&#123; public static String value = \"\";&#125; 生产者： 123456789101112131415161718192021222324252627282930package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 生产者 */public class Product&#123; private String lock; public Product(String lock) &#123; this.lock = lock; &#125; public void setValue() &#123; synchronized (lock) &#123; if (!ValueObject.value.equals(\"\")) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; String value = System.currentTimeMillis() + \"_\" + System.nanoTime(); System.out.println(\"生产者 set 的值是：\" + value); ValueObject.value = value; lock.notify(); &#125; &#125;&#125; 消费者： 1234567891011121314151617181920212223242526272829package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 消费者 */public class Resume&#123; private String lock; public Resume(String lock) &#123; this.lock = lock; &#125; public void getValue() &#123; synchronized (lock) &#123; if (ValueObject.value.equals(\"\")) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"消费者 get 的值：\" + ValueObject.value); ValueObject.value = \"\"; lock.notify(); &#125; &#125;&#125; 生产者线程： 123456789101112131415161718192021package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 生产者线程 */public class ProductThread extends Thread&#123; private Product p; public ProductThread(Product p) &#123; this.p = p; &#125; @Override public void run() &#123; while (true) &#123; p.setValue(); &#125; &#125;&#125; 消费者线程： 123456789101112131415161718192021package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 消费者线程 */public class ResumeThread extends Thread&#123; private Resume r; public ResumeThread(Resume r) &#123; this.r = r; &#125; @Override public void run() &#123; while (true) &#123; r.getValue(); &#125; &#125;&#125; 主函数： 123456789101112131415161718package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 一个生产者一个消费者测试 */public class Test&#123; public static void main(String[] args) &#123; String str = new String(\"\"); Product p = new Product(str); Resume r = new Resume(str);; ProductThread pt = new ProductThread(p); ResumeThread rt = new ResumeThread(r); pt.start(); rt.start(); &#125;&#125; 题目：创建20个线程，其中10个线程是将数据备份到数据库A，另外10个线程将数据备份到数据库B中去，并且备份数据库A和备份数据库B是交叉进行的。 工具类： 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. * 创建20个线程，其中10个线程是将数据备份到数据库A，另外10个线程将数据备份到数据库B中去，并且 * 备份数据库A和备份数据库B是交叉进行的 */public class DBTools&#123; volatile private boolean prevIsA = false; //确保A备份先进行 synchronized public void backA() &#123; while (prevIsA == true) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"AAAAA\"); &#125; prevIsA = true; notifyAll(); &#125; synchronized public void backB() &#123; while (prevIsA == false) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"BBBBB\"); &#125; prevIsA = false; notifyAll(); &#125;&#125; 备份A先线程： 123456789101112131415161718package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class ThreadA extends Thread&#123; private DBTools dbTools; public ThreadA(DBTools dbTools) &#123; this.dbTools = dbTools; &#125; @Override public void run() &#123; dbTools.backA(); &#125;&#125; 备份B线程： 123456789101112131415161718package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class ThreadB extends Thread&#123; private DBTools dbTools; public ThreadB(DBTools dbTools) &#123; this.dbTools = dbTools; &#125; @Override public void run() &#123; dbTools.backB(); &#125;&#125; 测试： 1234567891011121314151617package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class Test&#123; public static void main(String[] args) &#123; DBTools dbTools = new DBTools(); for (int i = 0; i &lt; 20; i++) &#123; ThreadB tb = new ThreadB(dbTools); tb.start(); ThreadA ta = new ThreadA(dbTools); ta.start(); &#125; &#125;&#125; Join 方法的使用作用：等待线程对象销毁 join 方法具有使线程排队运行的作用，有些类似同步的运行效果。join 与 synchronized 的区别是：join 在内部使用 wait() 方法进行等待，而 synchronized 关键字使用的是 “对象监视器” 原理做为同步。 在 join 过程中，如果当前线程对象被中断，则当前线程出现异常。 方法 join(long) 中的参数是设定等待的时间。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 等待该线程终止的时间最长为 millis 毫秒。超时为 0 意味着要一直等下去。 * Waits at most &#123;@code millis&#125; milliseconds for this thread to * die. A timeout of &#123;@code 0&#125; means to wait forever. * * &lt;p&gt; This implementation uses a loop of &#123;@code this.wait&#125; calls * conditioned on &#123;@code this.isAlive&#125;. As a thread terminates the * &#123;@code this.notifyAll&#125; method is invoked. It is recommended that * applications not use &#123;@code wait&#125;, &#123;@code notify&#125;, or * &#123;@code notifyAll&#125; on &#123;@code Thread&#125; instances. * * @param millis * the time to wait in milliseconds * * @throws IllegalArgumentException * if the value of &#123;@code millis&#125; is negative * * @throws InterruptedException * if any thread has interrupted the current thread. The * &lt;i&gt;interrupted status&lt;/i&gt; of the current thread is * cleared when this exception is thrown. */ public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException(\"timeout value is negative\"); if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 类 ThreadLocal 的使用该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal 实例通常是类中的private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。 get() 方法12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; 返回此线程局部变量的当前线程副本中的值。如果变量没有用于当前线程的值，则先将其初始化为调用 initialValue() 方法返回的值。 InheritableThreadLocal 类的使用该类扩展了 ThreadLocal，为子线程提供从父线程那里继承的值：在创建子线程时，子线程会接收所有可继承的线程局部变量的初始值，以获得父线程所具有的值。通常，子线程的值与父线程的值是一致的；但是，通过重写这个类中的 childValue 方法，子线程的值可以作为父线程值的一个任意函数。 当必须将变量（如用户 ID 和 事务 ID）中维护的每线程属性（per-thread-attribute）自动传送给创建的所有子线程时，应尽可能地采用可继承的线程局部变量，而不是采用普通的线程局部变量。 第四章 —— Lock 的使用使用 ReentrantLock 类一个可重入的互斥锁 Lock，它具有与使用 synchronized 方法和语句所访问的隐式监视器锁相同的一些基本行为和语义，但功能更强大。 ReentrantLock 将由最近成功获得锁，并且还没有释放该锁的线程所拥有。当锁没有被另一个线程所拥有时，调用 lock 的线程将成功获取该锁并返回。如果当前线程已经拥有该锁，此方法将立即返回。可以使用 isHeldByCurrentThread()和 getHoldCount()方法来检查此情况是否发生。 此类的构造方法接受一个可选的公平 参数。当设置为 true 时，在多个线程的争用下，这些锁倾向于将访问权授予等待时间最长的线程。否则此锁将无法保证任何特定访问顺序。与采用默认设置（使用不公平锁）相比，使用公平锁的程序在许多线程访问时表现为很低的总体吞吐量（即速度很慢，常常极其慢），但是在获得锁和保证锁分配的均衡性时差异较小。不过要注意的是，公平锁不能保证线程调度的公平性。因此，使用公平锁的众多线程中的一员可能获得多倍的成功机会，这种情况发生在其他活动线程没有被处理并且目前并未持有锁时。还要注意的是，未定时的 tryLock方法并没有使用公平设置。因为即使其他线程正在等待，只要该锁是可用的，此方法就可以获得成功。 建议总是 立即实践，使用 lock 块来调用 try，在之前/之后的构造中，最典型的代码如下： 12345678910111213class X &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void m() &#123; lock.lock(); // block until condition holds try &#123; // ... method body &#125; finally &#123; lock.unlock() &#125; &#125; &#125; ConditionCondition 将 Object 监视器方法（wait、notify 和 notifyAll）分解成截然不同的对象，以便通过将这些对象与任意 Lock 实现组合使用，为每个对象提供多个等待 set（wait-set）。其中，Lock 替代了 synchronized 方法和语句的使用，Condition 替代了 Object 监视器方法的使用。 假定有一个绑定的缓冲区，它支持 put 和 take 方法。如果试图在空的缓冲区上执行 take 操作，则在某一个项变得可用之前，线程将一直阻塞；如果试图在满的缓冲区上执行 put 操作，则在有空间变得可用之前，线程将一直阻塞。我们喜欢在单独的等待 set 中保存 put 线程和 take 线程，这样就可以在缓冲区中的项或空间变得可用时利用最佳规划，一次只通知一个线程。可以使用两个 Condition 实例来做到这一点。 12345678910111213141516171819202122232425262728293031323334353637class BoundedBuffer &#123; final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(Object x) throws InterruptedException &#123; lock.lock(); try &#123; while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) notEmpty.await(); Object x = items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); return x; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 正确使用 Condition 实现等待/通知MyService.java 123456789101112131415161718192021222324252627282930313233343536package com.zhisheng.thread.Thread9;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created by 10412 on 2017/6/4. */public class MyService&#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void await() &#123; lock.lock(); try &#123; System.out.println(\"await A\"); condition.await();//使当前执行的线程处于等待状态 waiting System.out.println(\"await B\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); System.out.println(\"释放锁\"); &#125; &#125; public void signal() &#123; lock.lock(); System.out.println(\"signal A\"); condition.signal(); System.out.println(\"signal B\"); lock.unlock(); &#125;&#125; ThreadA.java 123456789101112131415161718package com.zhisheng.thread.Thread9;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.await(); &#125;&#125; Test.java 123456789101112131415package com.zhisheng.thread.Thread9;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ta.start(); Thread.sleep(5000); service.signal(); &#125;&#125; 运行结果： 12345await Asignal Asignal Bawait B释放锁 Object 类中的 wait() 方法相当于 Condition 类中 await() 方法 Object 类中的 wait(long time) 方法相当于 Condition 类中 await(long time, TimeUnit unit) 方法 Object 类中的 notify() 方法相当于 Condition 类中 signal() 方法 Object 类中的 notifyAll() 方法相当于 Condition 类中 signalAll() 方法 题目：实现生产者与消费者 一对一交替打印 MyService.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.zhisheng.thread.thread10;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created by 10412 on 2017/6/4. * 实现生产者与消费者 一对一·交替打印 */public class MyService&#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); private boolean flag = false; public void setValue() &#123; lock.lock(); while (flag == true) &#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"SetValue AAAAAA\"); flag = true; condition.signal(); lock.unlock(); &#125; public void getValue() &#123; lock.lock(); while (flag == false) &#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"GetValue BBBB\"); flag = false; condition.signal(); lock.unlock(); &#125;&#125; ThreadA.java 1234567891011121314151617181920package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; service.setValue(); &#125; &#125;&#125; ThreadB.java 1234567891011121314151617181920package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class ThreadB extends Thread&#123; private MyService service; public ThreadB(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; service.getValue(); &#125; &#125;&#125; Test.java 123456789101112131415package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ThreadB tb = new ThreadB(service); ta.start(); tb.start(); &#125;&#125; getHoldCount() 查询当前线程保持此锁定的个数，也就是调用 lock() 的方法 getQueueLength() 返回正等待获取此锁定的线程估计数 getWaitQueueLength() 返回等待与此锁定相关的给定条件 Condition 的线程估计数 hasQueuedThread() 查询指定的线程是否正在等待获取此锁定 hasQueuedThreads() 查询是否有线程正在等待获取此锁定 hasWaiters() 查询是否有线程正在等待与此锁定有关的 condition 条件 isFair() 判断是否是公平锁（默认下 ReentrantLock类使用的是非公平锁） isHeldByCurrentThread() 查询当前线程是否保持此锁定 isLocked() 查询此锁定是否由任意线程保持 lockInterruptibly() 如果当前线程未被中断，则获取锁定，如果已经被中断则出现异常 tryLock() 仅在调用时锁定未被另一个线程保持的情况下，才获取该锁定 tryLock(long time, TimeUtil util) 如果锁定在给定的等待时间内没有被另一个线程保持，且当前线程未被中断，则获取该锁定。 使用 ReentrantReadWriteLock 类读写互斥： MyService.java 123456789101112131415161718192021222324252627282930313233package com.zhisheng.thread.Thread11;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * Created by 10412 on 2017/6/4. */public class MyService&#123; private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); public void read() &#123; lock.readLock().lock(); System.out.println(Thread.currentThread().getName() + \" Read AAA \" + System.currentTimeMillis()); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.readLock().unlock(); &#125; public void write() &#123; lock.writeLock().lock(); System.out.println(Thread.currentThread().getName() + \" write BBB \" + System.currentTimeMillis()); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.writeLock().unlock(); &#125;&#125; ThreadA.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.read(); &#125;&#125; ThreadB.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class ThreadB extends Thread&#123; private MyService service; public ThreadB(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.write(); &#125;&#125; Test.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ta.setName(\"A\"); ta.start(); Thread.sleep(1000); ThreadB tb = new ThreadB(service); tb.setName(\"B\"); tb.start(); &#125;&#125; 运行结果： 12A Read AAA 1496556770402B write BBB 1496556780402 第六章 —— 单例模式与多线程推荐文章 《深入浅出单实例Singleton设计模式》 立即加载模式 / “饿汉模式”立即加载：使用类的时候已经将对象创建完毕，new 实例化 123456789public class MyObject&#123; private static MyObject object = new MyObject(); private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return object; &#125;&#125; 延迟加载 / “ 懒汉模式 ”就是在调用 get 的时候实例才被创建。在 get() 方法中进行 new 实例化。 12345678910111213public class MyObject&#123; private static MyObject object; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; if (object != null) &#123; &#125; else &#123; object = new MyObject(); &#125; return object; &#125;&#125; 使用 DCL 双重检查锁，解决“懒汉模式”遇到的多线程问题 123456789101112131415161718public class MyObject&#123; private volatile static MyObject object; private MyObject() &#123; &#125; //synchronized public static MyObject getInstance() &#123; if (object != null) &#123; &#125; else &#123; synchronized (MyObject.class) &#123; if (object == null) &#123; object = new MyObject(); &#125; &#125; &#125; return object; &#125;&#125; 使用静态内部类实现单例模式123456789101112public class MyObject&#123; private static class MyObjectHandler &#123; private static MyObject object = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.object; &#125;&#125; 序列化与反序列化的单例模式实现MyObject.java 12345678910111213141516171819202122232425package com.zhisheng.thread.thread15;import java.io.ObjectStreamException;import java.io.Serializable;/** * Created by 10412 on 2017/6/4. */public class MyObject implements Serializable&#123; private static final long serialVersionUID = 888L; private static class MyObjectHandler &#123; private static final MyObject object = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.object; &#125; protected Object readResolve() throws ObjectStreamException &#123; System.out.println(\"调用了readResolve方法！\"); return MyObjectHandler.object; &#125;&#125; SaveAndRead.java 12345678910111213141516171819202122232425262728293031323334353637383940package com.zhisheng.thread.thread15;import java.io.*;/** * Created by 10412 on 2017/6/4. */public class SaveAndRead&#123; public static void main(String[] args) &#123; try &#123; MyObject object = MyObject.getInstance(); FileOutputStream fos = new FileOutputStream(new File(\"fos.txt\")); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(object); oos.close(); fos.close(); System.out.println(object.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; FileInputStream fis = new FileInputStream(new File(\"fos.txt\")); ObjectInputStream ois = new ObjectInputStream(fis); MyObject o = (MyObject) ois.readObject(); ois.close(); fis.close(); System.out.println(o.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这里主要要指出 MyObject.java 中 readResolve 方法 1234protected Object readResolve() throws ObjectStreamException &#123; System.out.println(\"调用了readResolve方法！\"); return MyObjectHandler.object; &#125; 方法 readResolve 允许 class 在反序列化返回对象前替换、解析在流中读出来的对象。实现 readResolve 方法，一个 class 可以直接控制反序化返回的类型和对象引用。 方法 readResolve 会在 ObjectInputStream 已经读取一个对象并在准备返回前调用。ObjectInputStream 会检查对象的 class 是否定义了 readResolve 方法。如果定义了，将由 readResolve 方法指定返回的对象。返回对象的类型一定要是兼容的，否则会抛出 ClassCastException 。 使用 static 代码块实现单例模式1234567891011121314151617package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class MyObject&#123; private static MyObject instance = null; private MyObject() &#123; &#125; static &#123; instance = new MyObject(); &#125; public static MyObject getInstance() &#123; return instance; &#125;&#125; ThreadA.java 1234567891011121314package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getInstance().hashCode()); &#125; &#125;&#125; Test.java 12345678910111213141516package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) &#123; ThreadA ta1 = new ThreadA(); ThreadA ta2 = new ThreadA(); ThreadA ta3 = new ThreadA(); ta1.start(); ta2.start(); ta3.start(); &#125;&#125; 使用枚举数据类型实现单例模式在使用枚举类时，构造方法会被自动调用，也可以应用这个特性实现单例模式。 123456789101112131415public class MyObject &#123; private enum MyEnumSingleton&#123; INSTANCE; private Resource resource; private MyEnumSingleton()&#123; resource = new Resource(); &#125; public Resource getResource()&#123; return resource; &#125; &#125; public static Resource getResource()&#123; return MyEnumSingleton.INSTANCE.getResource(); &#125;&#125; 测试： 123456789101112131415161718192021import test.MyObject;public class Run &#123; class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getResource().hashCode()); &#125; &#125; &#125; public static void main(String[] args) &#123; Run.MyThread t1 = new Run().new MyThread(); Run.MyThread t2 = new Run().new MyThread(); Run.MyThread t3 = new Run().new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 这里再推荐一篇 stackoverflow 上的一个问题回答： What is an efficient way to implement a singleton pattern in Java? 总结本篇文章是我读 《Java多线程编程核心技术》 的笔记及自己的一些总结，觉得不错，欢迎点赞和转发。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.54tianzhisheng.cn/tags/多线程/"}]},{"title":"Java NIO 系列教程","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java NIO 系列教程/","text":"Java NIO（New IO）是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。 Java NIO提供了与标准IO不同的IO工作方式： Channels and Buffers（通道和缓冲区）：标准的IO基于字节流和字符流进行操作的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。 Asynchronous IO（异步IO）：Java NIO可以让你异步的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Selectors（选择器）：Java NIO引入了选择器的概念，选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 下面就来详细介绍Java NIO的相关知识。 1、Java NIO 概述Java NIO 由以下几个核心部分组成： Channels Buffers Selectors 虽然 Java NIO 中除此之外还有很多类和组件，但在我看来，Channel，Buffer 和 Selector 构成了核心的 API。其它组件，如 Pipe 和 FileLock，只不过是与三个核心组件共同使用的工具类。因此，在概述中我将集中在这三个组件上。其它组件会在单独的章节中讲到。 Channel 和 Buffer基本上，所有的 IO 在NIO 中都从一个 Channel 开始。Channel 有点象流。 数据可以从 Channel 读到 Buffer 中，也可以从 Buffer 写到 Channel 中。这里有个图示： Channel 和 Buffer 有好几种类型。下面是 JAVA NIO 中的一些主要 Channel 的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel 正如你所看到的，这些通道涵盖了 UDP 和 TCP 网络 IO，以及文件 IO。 与这些类一起的有一些有趣的接口，但为简单起见，我尽量在概述中不提到它们。本教程其它章节与它们相关的地方我会进行解释。 以下是 Java NIO 里关键的 Buffer 实现： ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些 Buffer 覆盖了你能通过 IO 发送的基本数据类型：byte, short, int, long, float, double 和 char。 Java NIO 还有个 MappedByteBuffer，用于表示内存映射文件， 我也不打算在概述中说明。 SelectorSelector 允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用 Selector 就会很方便。例如，在一个聊天服务器中。 这是在一个单线程中使用一个 Selector 处理3个 Channel 的图示： 要使用 Selector，得向 Selector 注册 Channel，然后调用它的 select() 方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。 2、ChannelJava NIO 的通道类似流，但又有些不同： 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。 通道可以异步地读写。 通道中的数据总是要先读到一个 Buffer，或者总是要从一个 Buffer 中写入。 正如上面所说，从通道读取数据到缓冲区，从缓冲区写入数据到通道。如下图所示： Channel 的实现这些是 Java NIO 中最重要的通道的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel FileChannel 从文件中读写数据。 DatagramChannel 能通过 UDP 读写网络中的数据。 SocketChannel 能通过 TCP 读写网络中的数据。 ServerSocketChannel 可以监听新进来的 TCP 连接，像 Web 服务器那样。对每一个新进来的连接都会创建一个 SocketChannel。 基本的 Channel 示例下面是一个使用 FileChannel 读取数据到 Buffer 中的示例： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf);while (bytesRead != -1) &#123;System.out.println(&quot;Read &quot; + bytesRead);buf.flip();while(buf.hasRemaining())&#123;System.out.print((char) buf.get());&#125;buf.clear();bytesRead = inChannel.read(buf);&#125;aFile.close(); 注意 buf.flip() 的调用，首先读取数据到Buffer，然后反转Buffer,接着再从Buffer中读取数据。下一节会深入讲解Buffer的更多细节。 3、BufferJava NIO 中的 Buffer 用于和 NIO 通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。 Buffer 的基本用法使用 Buffer 读写数据一般遵循以下四个步骤： 写入数据到 Buffer 调用 flip() 方法 从 Buffer 中读取数据 调用 clear() 方法或者 compact() 方法 当向 buffer 写入数据时，buffer 会记录下写了多少数据。一旦要读取数据，需要先通过 flip() 方法将 Buffer 从写模式切换到读模式。在读模式下，可以读取之前写入到 buffer 的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用 clear() 或 compact() 方法。clear() 方法会清空整个缓冲区。compact() 方法只会清除已经读过的数据，任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 下面是一个使用Buffer的例子： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();//create buffer with capacity of 48 bytesByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); //read into buffer.while (bytesRead != -1) &#123; buf.flip(); //make buffer ready for read while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); // read 1 byte at a time &#125; buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf);&#125;aFile.close(); Buffer 的 capacity、 position 和 limit缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。 为了理解 Buffer 的工作原理，需要熟悉它的三个属性： capacity position limit position 和 limit 的含义取决于 Buffer 处在读模式还是写模式。不管 Buffer 处在什么模式，capacity 的含义总是一样的。 这里有一个关于 capacity，position 和 limit 在读写模式中的说明，详细的解释在插图后面。 capacity 作为一个内存块，Buffer 有一个固定的大小值，也叫“capacity”.你只能往里写 capacity 个byte、long，char 等类型。一旦 Buffer 满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 position 当你写数据到 Buffer 中时，position 表示当前的位置。初始的 position 值为 0。当一个 byte、long 等数据写到 Buffer 后， position 会向前移动到下一个可插入数据的 Buffer 单元。position 最大可为 capacity – 1. 当读取数据时，也是从某个特定位置读。当将 Buffer 从写模式切换到读模式，position 会被重置为 0. 当从 Buffer 的 position 处读取数据时，position 向前移动到下一个可读的位置。 limit 在写模式下，Buffer 的 limit 表示你最多能往 Buffer 里写多少数据。 写模式下，limit 等于Buffer 的 capacity。 当切换 Buffer 到读模式时， limit 表示你最多能读到多少数据。因此，当切换 Buffer 到读模式时，limit 会被设置成写模式下的 position 值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是 position） Buffer的类型Java NIO 有以下Buffer类型 ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 如你所见，这些Buffer类型代表了不同的数据类型。换句话说，就是可以通过char，short，int，long，float 或 double类型来操作缓冲区中的字节。 MappedByteBuffer 有些特别，在涉及它的专门章节中再讲。 Buffer 的分配要想获得一个 Buffer 对象首先要进行分配。 每一个 Buffer 类都有一个 allocate 方法。下面是一个分配48字节 capacity 的 ByteBuffer 的例子。 1ByteBuffer buf = ByteBuffer.allocate(48); 这是分配一个可存储1024个字符的 CharBuffer： 1CharBuffer buf = CharBuffer.allocate(1024); 向 Buffer 中写数据写数据到 Buffer 有两种方式： 从 Channel 写到 Buffer。 通过 Buffer 的 put() 方法写到 Buffer 里。 从 Channel 写到 Buffer 的例子： 1int bytesRead = inChannel.read(buf); //read into buffer. 通过put方法写Buffer的例子： 1buf.put(127); put 方法有很多版本，允许你以不同的方式把数据写入到 Buffer 中。例如， 写到一个指定的位置，或者把一个字节数组写入到 Buffer。 更多Buffer实现的细节参考JavaDoc。 flip() 方法 flip() 方法将 Buffer 从写模式切换到读模式。调用 flip() 方法会将 position 设回 0，并将 limit 设置成之前 position 的值。 换句话说，position 现在用于标记读的位置，limit 表示之前写进了多少个 byte、char等 —— 现在能读取多少个byte、char等。 从Buffer中读取数据从Buffer中读取数据有两种方式： 从Buffer读取数据到Channel。 使用get()方法从Buffer中读取数据。 从Buffer读取数据到Channel的例子： 12//read from buffer into channel.int bytesWritten = inChannel.write(buf); 使用get()方法从Buffer中读取数据的例子 1byte aByte = buf.get(); get方法有很多版本，允许你以不同的方式从Buffer中读取数据。例如，从指定position读取，或者从Buffer中读取数据到字节数组。更多Buffer实现的细节参考JavaDoc。 rewind()方法Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等）。 clear()与compact()方法一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。 如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。 compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 mark()与reset()方法通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如： 123buffer.mark();//call buffer.get() a couple of times, e.g. during parsing.buffer.reset(); //set position back to mark. equals()与compareTo()方法可以使用equals()和compareTo()方法两个Buffer。 equals() 当满足下列条件时，表示两个Buffer相等： 有相同的类型（byte、char、int等）。 Buffer中剩余的byte、char等的个数相等。 Buffer中所有剩余的byte、char等都相同。 如你所见，equals只是比较Buffer的一部分，不是每一个在它里面的元素都比较。实际上，它只比较Buffer中的剩余元素。 compareTo()方法 compareTo()方法比较两个Buffer的剩余元素(byte、char等)， 如果满足下列条件，则认为一个Buffer“小于”另一个Buffer： 第一个不相等的元素小于另一个Buffer中对应的元素 。 所有元素都相等，但第一个Buffer比另一个先耗尽(第一个Buffer的元素个数比另一个少)。 （译注：剩余元素是从 position到limit之间的元素） 4、Scatter/GatherJava NIO 开始支持 scatter/gather，scatter/gather 用于描述从 Channel（译者注：Channel 在中文经常翻译为通道）中读取或者写入到 Channel 的操作。 分散（scatter）从 Channel 中读取是指在读操作时将读取的数据写入多个 buffer 中。因此，Channel 将从 Channel 中读取的数据“分散（scatter）”到多个Buffer中。 聚集（gather）写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 Scattering ReadsScattering Reads是指数据从一个channel读取到多个buffer中。如下图描述： 代码示例如下： 123456ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);ByteBuffer[] bufferArray = &#123; header, body &#125;;channel.read(bufferArray); 注意buffer首先被插入到数组，然后再将数组作为channel.read() 的输入参数。read()方法按照buffer在数组中的顺序将从channel中读取的数据写入到buffer，当一个buffer被写满后，channel紧接着向另一个buffer中写。 Scattering Reads在移动下一个buffer前，必须填满当前的buffer，这也意味着它不适用于动态消息(译者注：消息大小不固定)。换句话说，如果存在消息头和消息体，消息头必须完成填充（例如 128byte），Scattering Reads才能正常工作。 Gathering WritesGathering Writes是指数据从多个buffer写入到同一个channel。如下图描述： 代码示例如下： 12345678ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);//write data into buffersByteBuffer[] bufferArray = &#123; header, body &#125;;channel.write(bufferArray); buffers数组是write()方法的入参，write()方法会按照buffer在数组中的顺序，将数据写入到channel，注意只有position和limit之间的数据才会被写入。因此，如果一个buffer的容量为128byte，但是仅仅包含58byte的数据，那么这58byte的数据将被写入到channel中。因此与Scattering Reads相反，Gathering Writes能较好的处理动态消息。 5、通道之间的数据传输在Java NIO中，如果两个通道中有一个是FileChannel，那你可以直接将数据从一个channel（译者注：channel中文常译作通道）传输到另外一个channel。 transferFrom()FileChannel的transferFrom()方法可以将数据从源通道传输到FileChannel中（译者注：这个方法在JDK文档中的解释为将字节从给定的可读取字节通道传输到此通道的文件中）。 下面是一个简单的例子： 12345678910RandomAccessFile fromFile = new RandomAccessFile(&quot;fromFile.txt&quot;, &quot;rw&quot;);FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(&quot;toFile.txt&quot;, &quot;rw&quot;);FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();toChannel.transferFrom(position, count, fromChannel); 方法的输入参数position表示从position处开始向目标文件写入数据，count表示最多传输的字节数。如果源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数。 此外要注意，在SoketChannel的实现中，SocketChannel只会传输此刻准备好的数据（可能不足count字节）。因此，SocketChannel可能不会将请求的所有数据(count个字节)全部传输到FileChannel中。 transferTo()transferTo()方法将数据从FileChannel传输到其他的channel中。 下面是一个简单的例子： 12345678910RandomAccessFile fromFile = new RandomAccessFile(&quot;fromFile.txt&quot;, &quot;rw&quot;);FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(&quot;toFile.txt&quot;, &quot;rw&quot;);FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();fromChannel.transferTo(position, count, toChannel); 是不是发现这个例子和前面那个例子特别相似？除了调用方法的FileChannel对象不一样外，其他的都一样。 上面所说的关于SocketChannel的问题在transferTo()方法中同样存在。SocketChannel会一直传输数据直到目标buffer被填满。 6、SelectorSelector（选择器）是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。 为什么使用Selector?仅用单个线程来处理多个Channels的好处是，只需要更少的线程来处理通道。事实上，可以只用一个线程处理所有的通道。对于操作系统来说，线程之间上下文切换的开销很大，而且每个线程都要占用系统的一些资源（如内存）。因此，使用的线程越少越好。 但是，需要记住，现代的操作系统和CPU在多任务方面表现的越来越好，所以多线程的开销随着时间的推移，变得越来越小了。实际上，如果一个CPU有多个内核，不使用多任务可能是在浪费CPU能力。不管怎么说，关于那种设计的讨论应该放在另一篇不同的文章中。在这里，只要知道使用Selector能够处理多个通道就足够了。 下面是单线程使用一个Selector处理3个channel的示例图： Selector的创建通过调用Selector.open()方法创建一个Selector，如下： 1Selector selector = Selector.open(); 向Selector注册通道为了将Channel和Selector配合使用，必须将channel注册到selector上。通过SelectableChannel.register()方法来实现，如下： 123channel.configureBlocking(false);SelectionKey key = channel.register(selector, Selectionkey.OP_READ); 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 注意register()方法的第二个参数。这是一个“interest集合”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件： Connect Accept Read Write 通道触发了一个事件意思是该事件已经就绪。所以，某个channel成功连接到另一个服务器称为“连接就绪”。一个server socket channel准备好接收新进入的连接称为“接收就绪”。一个有数据可读的通道可以说是“读就绪”。等待写数据的通道可以说是“写就绪”。 这四种事件用SelectionKey的四个常量来表示： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果你对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来，如下： 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 在下面还会继续提到interest集合。 SelectionKey在上一小节中，当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些你感兴趣的属性： interest集合 ready集合 Channel Selector 附加的对象（可选）下面我会描述这些属性。 interest集合就像向Selector注册通道一节中所描述的，interest集合是你所选择的感兴趣的事件集合。可以通过SelectionKey读写interest集合，像这样： 123456int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT；boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; 可以看到，用“位与”操作interest 集合和给定的SelectionKey常量，可以确定某个确定的事件是否在interest 集合中。 ready集合ready 集合是通道已经准备就绪的操作的集合。在一次选择(Selection)之后，你会首先访问这个ready set。Selection将在下一小节进行解释。可以这样访问ready集合： 1int readySet = selectionKey.readyOps(); 可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型： 1234selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); Channel + Selector从SelectionKey访问Channel和Selector很简单。如下： 12Channel channel = selectionKey.channel();Selector selector = selectionKey.selector(); 附加的对象可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下： 12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如： 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 通过Selector选择通道一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 下面是select()方法： int select() int select(long timeout) int selectNow() select()阻塞到至少有一个通道在你注册的事件上就绪了。 select(long timeout)和select()一样，除了最长会阻塞timeout毫秒(参数)。 selectNow()不会阻塞，不管什么通道就绪都立刻返回（译者注：此方法执行非阻塞的选择操作。如果自从前一次选择操作后，没有通道变成可选择的，则此方法直接返回零。）。 select()方法返回的int值表示有多少通道已经就绪。亦即，自上次调用select()方法后有多少通道变成就绪状态。如果调用select()方法，因为有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。 selectedKeys()一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。如下所示： 当像Selector注册Channel时，Channel.register()方法会返回一个SelectionKey 对象。这个对象代表了注册到该Selector的通道。可以通过SelectionKey的selectedKeySet()方法访问这些对象。 可以遍历这个已选择的键集合来访问就绪的通道。如下： 123456789101112131415Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator();while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove();&#125; 这个循环遍历已选择键集中的每个键，并检测各个键所对应的通道的就绪事件。 注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。 SelectionKey.channel()方法返回的通道需要转型成你要处理的类型，如ServerSocketChannel或SocketChannel等。 wakeUp()某个线程调用select()方法后阻塞了，即使没有通道已经就绪，也有办法让其从select()方法返回。只要让其它线程在第一个线程调用select()方法的那个对象上调用Selector.wakeup()方法即可。阻塞在select()方法上的线程会立马返回。 如果有其它线程调用了wakeup()方法，但当前没有线程阻塞在select()方法上，下个调用select()方法的线程会立即“醒来（wake up）”。 close()用完Selector后调用其close()方法会关闭该Selector，且使注册到该Selector上的所有SelectionKey实例无效。通道本身并不会关闭。 完整的示例这里有一个完整的示例，打开一个Selector，注册一个通道注册到这个Selector上(通道的初始化过程略去),然后持续监控这个Selector的四种事件（接受，连接，读，写）是否就绪。 12345678910111213141516171819202122Selector selector = Selector.open();channel.configureBlocking(false);SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123; int readyChannels = selector.select(); if(readyChannels == 0) continue; Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125;&#125; 7、FileChannelJava NIO中的FileChannel是一个连接到文件的通道。可以通过文件通道读写文件。 FileChannel无法设置为非阻塞模式，它总是运行在阻塞模式下。 打开FileChannel在使用FileChannel之前，必须先打开它。但是，我们无法直接打开一个FileChannel，需要通过使用一个InputStream、OutputStream或RandomAccessFile来获取一个FileChannel实例。下面是通过RandomAccessFile打开FileChannel的示例： 12RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel(); 从FileChannel读取数据调用多个read()方法之一从FileChannel中读取数据。如： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); 首先，分配一个Buffer。从FileChannel中读取的数据将被读到Buffer中。 然后，调用FileChannel.read()方法。该方法将数据从FileChannel读取到Buffer中。read()方法返回的int值表示了有多少字节被读到了Buffer中。如果返回-1，表示到了文件末尾。 向FileChannel写数据使用FileChannel.write()方法向FileChannel写数据，该方法的参数是一个Buffer。如： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意FileChannel.write()是在while循环中调用的。因为无法保证write()方法一次能向FileChannel写入多少字节，因此需要重复调用write()方法，直到Buffer中已经没有尚未写入通道的字节。 关闭FileChannel用完FileChannel后必须将其关闭。如：1channel.close(); FileChannel的position方法有时可能需要在FileChannel的某个特定位置进行数据的读/写操作。可以通过调用position()方法获取FileChannel的当前位置。 也可以通过调用position(long pos)方法设置FileChannel的当前位置。 这里有两个例子:12long pos = channel.position();channel.position(pos +123); 如果将位置设置在文件结束符之后，然后试图从文件通道中读取数据，读方法将返回-1 —— 文件结束标志。 如果将位置设置在文件结束符之后，然后向通道中写数据，文件将撑大到当前位置并写入数据。这可能导致“文件空洞”，磁盘上物理文件中写入的数据间有空隙。 FileChannel的size方法FileChannel实例的size()方法将返回该实例所关联文件的大小。如:1long fileSize = channel.size(); FileChannel的truncate方法可以使用FileChannel.truncate()方法截取一个文件。截取文件时，文件将中指定长度后面的部分将被删除。如：1channel.truncate(1024); 这个例子截取文件的前1024个字节。 FileChannel的force方法FileChannel.force()方法将通道里尚未写入磁盘的数据强制写到磁盘上。出于性能方面的考虑，操作系统会将数据缓存在内存中，所以无法保证写入到FileChannel里的数据一定会即时写到磁盘上。要保证这一点，需要调用force()方法。 force()方法有一个boolean类型的参数，指明是否同时将文件元数据（权限信息等）写到磁盘上。 下面的例子同时将文件数据和元数据强制写到磁盘上： 查看源代码打印帮助1channel.force(true); 8、SocketChannelJava NIO中的SocketChannel是一个连接到TCP网络套接字的通道。可以通过以下2种方式创建SocketChannel： 打开一个SocketChannel并连接到互联网上的某台服务器。一个新连接到达ServerSocketChannel时，会创建一个SocketChannel。 打开 SocketChannel下面是SocketChannel的打开方式： 12SocketChannel socketChannel = SocketChannel.open();socketChannel.connect(new InetSocketAddress(&quot;http://jenkov.com&quot;, 80)); 关闭 SocketChannel 当用完SocketChannel之后调用SocketChannel.close()关闭SocketChannel： 1socketChannel.close(); 从 SocketChannel 读取数据要从SocketChannel中读取数据，调用一个read()的方法之一。以下是例子： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = socketChannel.read(buf); 首先，分配一个Buffer。从SocketChannel读取到的数据将会放到这个Buffer中。 然后，调用SocketChannel.read()。该方法将数据从SocketChannel 读到Buffer中。read()方法返回的int值表示读了多少字节进Buffer里。如果返回的是-1，表示已经读到了流的末尾（连接关闭了）。 写入 SocketChannel写数据到SocketChannel用的是SocketChannel.write()方法，该方法以一个Buffer作为参数。示例如下： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意SocketChannel.write()方法的调用是在一个while循环中的。Write()方法无法保证能写多少字节到SocketChannel。所以，我们重复调用write()直到Buffer没有要写的字节为止。 非阻塞模式可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。 connect()如果SocketChannel在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用finishConnect()的方法。像这样： 123456socketChannel.configureBlocking(false);socketChannel.connect(new InetSocketAddress(&quot;http://jenkov.com&quot;, 80));while(! socketChannel.finishConnect() )&#123; //wait, or do something else...&#125; write()非阻塞模式下，write()方法在尚未写出任何内容时可能就返回了。所以需要在循环中调用write()。前面已经有例子了，这里就不赘述了。 read()非阻塞模式下,read()方法在尚未读取到任何数据时可能就返回了。所以需要关注它的int返回值，它会告诉你读取了多少字节。 非阻塞模式与选择器非阻塞模式与选择器搭配会工作的更好，通过将一或多个SocketChannel注册到Selector，可以询问选择器哪个通道已经准备好了读取，写入等。Selector与SocketChannel的搭配使用会在后面详讲。 9、ServerSocketChannelJava NIO中的 ServerSocketChannel 是一个可以监听新进来的TCP连接的通道, 就像标准IO中的ServerSocket一样。ServerSocketChannel类在 java.nio.channels包中。 这里有个例子： 12345678910ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(9999));while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; 打开 ServerSocketChannel通过调用 ServerSocketChannel.open() 方法来打开ServerSocketChannel.如： 1ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); 关闭 ServerSocketChannel通过调用ServerSocketChannel.close() 方法来关闭ServerSocketChannel. 如： 1serverSocketChannel.close(); 监听新进来的连接通过 ServerSocketChannel.accept() 方法监听新进来的连接。当 accept()方法返回的时候,它返回一个包含新进来的连接的 SocketChannel。因此, accept()方法会一直阻塞到有新连接到达。 通常不会仅仅只监听一个连接,在while循环中调用 accept()方法. 如下面的例子： 123456while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; 当然,也可以在while循环中使用除了true以外的其它退出准则。 非阻塞模式ServerSocketChannel可以设置成非阻塞模式。在非阻塞模式下，accept() 方法会立刻返回，如果还没有新进来的连接,返回的将是null。 因此，需要检查返回的SocketChannel是否是null.如： 12345678910111213ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(9999));serverSocketChannel.configureBlocking(false);while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); if(socketChannel != null)&#123; //do something with socketChannel... &#125;&#125; 10、Java NIO DatagramChannelJava NIO中的DatagramChannel是一个能收发UDP包的通道。因为UDP是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包。 打开 DatagramChannel下面是 DatagramChannel 的打开方式： 12DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9999)); 这个例子打开的 DatagramChannel可以在UDP端口9999上接收数据包。 接收数据通过receive()方法从DatagramChannel接收数据，如： 123ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();channel.receive(buf); receive()方法会将接收到的数据包内容复制到指定的Buffer. 如果Buffer容不下收到的数据，多出的数据将被丢弃。 发送数据通过send()方法从DatagramChannel发送数据，如: 12345678String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress(&quot;jenkov.com&quot;, 80)); 这个例子发送一串字符到”jenkov.com”服务器的UDP端口80。 因为服务端并没有监控这个端口，所以什么也不会发生。也不会通知你发出的数据包是否已收到，因为UDP在数据传送方面没有任何保证。 连接到特定的地址可以将DatagramChannel“连接”到网络中的特定地址的。由于UDP是无连接的，连接到特定地址并不会像TCP通道那样创建一个真正的连接。而是锁住DatagramChannel ，让其只能从特定地址收发数据。 这里有个例子: 1channel.connect(new InetSocketAddress(&quot;jenkov.com&quot;, 80)); 当连接后，也可以使用read()和write()方法，就像在用传统的通道一样。只是在数据传送方面没有任何保证。这里有几个例子： 12int bytesRead = channel.read(buf);int bytesWritten = channel.write(but); 11、PipeJava NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 这里是Pipe原理的图示： 创建管道通过Pipe.open()方法打开管道。例如： 1Pipe pipe = Pipe.open(); 向管道写数据要向管道写数据，需要访问sink通道。像这样： 1Pipe.SinkChannel sinkChannel = pipe.sink(); 通过调用SinkChannel的write()方法，将数据写入SinkChannel,像这样： 12345678910String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; sinkChannel.write(buf);&#125; 从管道读取数据从读取管道的数据，需要访问source通道，像这样： 1Pipe.SourceChannel sourceChannel = pipe.source(); 调用source通道的read()方法来读取数据，像这样： 123ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = sourceChannel.read(buf); read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 12、Java NIO与IO的对比当学习了Java NIO和IO的API后，一个问题马上涌入脑海： 我应该何时使用IO，何时使用NIO呢？在本文中，我会尽量清晰地解析Java NIO和IO的差异、它们的使用场景，以及它们如何影响您的代码设计。 Java NIO和IO的主要区别下表总结了Java NIO和IO之间的主要差别，我会更详细地描述表中每部分的差异。 IO NIO Stream oriented Buffer oriented Blocking IO Non blocking IO 无 Selectors 面向流与面向缓冲Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 阻塞与非阻塞IOJava IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors）Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 NIO和IO如何影响应用程序的设计无论您选择IO或NIO工具箱，可能会影响您应用程序设计的以下几个方面： 对NIO或IO类的API调用。 数据处理。 用来处理数据的线程数。 API调用当然，使用NIO的API调用时看起来与使用IO时有所不同，但这并不意外，因为并不是仅从一个InputStream逐字节读取，而是数据必须先读入缓冲区再处理。 数据处理使用纯粹的NIO设计相较IO设计，数据处理也受到影响。 在IO设计中，我们从InputStream或 Reader逐字节读取数据。假设你正在处理一基于行的文本数据流，例如： 1234Name: AnnaAge: 25Email: anna@mailserver.comPhone: 1234567890 该文本行的流可以这样处理： 1234567InputStream input = … ; // get the InputStream from the client socketBufferedReader reader = new BufferedReader(new InputStreamReader(input));String nameLine = reader.readLine();String ageLine = reader.readLine();String emailLine = reader.readLine();String phoneLine = reader.readLine(); 请注意处理状态由程序执行多久决定。换句话说，一旦reader.readLine()方法返回，你就知道肯定文本行就已读完， readline()阻塞直到整行读完，这就是原因。你也知道此行包含名称；同样，第二个readline()调用返回的时候，你知道这行包含年龄等。 正如你可以看到，该处理程序仅在有新数据读入时运行，并知道每步的数据是什么。一旦正在运行的线程已处理过读入的某些数据，该线程不会再回退数据（大多如此）。下图也说明了这条原则： 上图：从一个阻塞的流中读数据 而一个NIO的实现会有所不同，下面是一个简单的例子： 123ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer); 注意第二行，从通道读取字节到ByteBuffer。当这个方法调用返回时，你不知道你所需的所有数据是否在缓冲区内。你所知道的是，该缓冲区包含一些字节，这使得处理有点困难。假设第一次 read(buffer)调用后，读入缓冲区的数据只有半行，例如，“Name:An”，你能处理数据吗？显然不能，需要等待，直到整行数据读入缓存，在此之前，对数据的任何处理毫无意义。 所以，你怎么知道是否该缓冲区包含足够的数据可以处理呢？好了，你不知道。发现的方法只能查看缓冲区中的数据。其结果是，在你知道所有数据都在缓冲区里之前，你必须检查几次缓冲区的数据。这不仅效率低下，而且可以使程序设计方案杂乱不堪。例如： 12345ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer);while(! bufferFull(bytesRead) ) &#123;bytesRead = inChannel.read(buffer);&#125; bufferFull()方法必须跟踪有多少数据读入缓冲区，并返回真或假，这取决于缓冲区是否已满。换句话说，如果缓冲区准备好被处理，那么表示缓冲区满了。 bufferFull()方法扫描缓冲区，但必须保持在bufferFull（）方法被调用之前状态相同。如果没有，下一个读入缓冲区的数据可能无法读到正确的位置。这是不可能的，但却是需要注意的又一问题。 如果缓冲区已满，它可以被处理。如果它不满，并且在你的实际案例中有意义，你或许能处理其中的部分数据。但是许多情况下并非如此。下图展示了“缓冲区数据循环就绪”： 上图：从一个通道里读数据，直到所有的数据都读到缓冲区里 总结NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。 如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，实现NIO的服务器可能是一个优势。同样，如果你需要维持许多打开的连接到其他计算机上，如P2P网络中，使用一个单独的线程来管理你所有出站连接，可能是一个优势。一个线程多个连接的设计方案如下图所示： 上图：单线程管理多个连接 如果你有少量的连接使用非常高的带宽，一次发送大量的数据，也许典型的IO服务器实现可能非常契合。下图说明了一个典型的IO服务器设计： 上图：一个典型的IO服务器设计：一个连接通过一个线程处理 注明：文章转载自 NIO|并发编程网，二次转载请务必注明原出处！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"http://www.54tianzhisheng.cn/tags/NIO/"}]},{"title":"通过项目逐步深入了解Mybatis（二）","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/通过项目逐步深入了解Mybatis(二)/","text":"转载请务必注明出处，原创不易！ 相关文章：通过项目逐步深入了解Mybatis&lt;一&gt;本项目全部代码地址：Github-Mybatis Mybatis 解决 jdbc 编程的问题1、 数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 解决：在SqlMapConfig.xml中配置数据链接池，使用连接池管理数据库链接。 2、 Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。 解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。 3、 向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。 解决：Mybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。 4、 对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。 解决：Mybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。 Mybatis 与 Hibernate 不同Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句，不过mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。 Mybatis 开发 dao两种方法 原始 dao 开发方法（程序需要编写 dao 接口和 dao 实现类）（掌握） Mybatis 的 mapper 接口（相当于 dao 接口）代理开发方法（掌握） 需求将下边的功能实现Dao： 根据用户id查询一个用户信息 根据用户名称模糊查询用户信息列表 添加用户信息 Mybatis 配置文件 SqlMapConfig.xml Sqlsession 的使用范围SqlSession 中封装了对数据库的操作，如：查询、插入、更新、删除等。 通过 SqlSessionFactory 创建 SqlSession，而 SqlSessionFactory 是通过 SqlSessionFactoryBuilder 进行创建。 1、SqlSessionFactoryBuilderSqlSessionFactoryBuilder 用于创建 SqlSessionFacoty，SqlSessionFacoty 一旦创建完成就不需要SqlSessionFactoryBuilder 了，因为 SqlSession 是通过 SqlSessionFactory 生产，所以可以将SqlSessionFactoryBuilder 当成一个工具类使用，最佳使用范围是方法范围即方法体内局部变量。 2、SqlSessionFactorySqlSessionFactory 是一个接口，接口中定义了 openSession 的不同重载方法，SqlSessionFactory 的最佳使用范围是整个应用运行期间，一旦创建后可以重复使用，通常以单例模式管理 SqlSessionFactory。 3、SqlSessionSqlSession 是一个面向用户的接口， sqlSession 中定义了数据库操作，默认使用 DefaultSqlSession 实现类。 执行过程如下： 1）、 加载数据源等配置信息 Environment environment = configuration.getEnvironment(); 2）、 创建数据库链接 3）、 创建事务对象 4）、 创建Executor，SqlSession 所有操作都是通过 Executor 完成，mybatis 源码如下： 12345678910if (ExecutorType.BATCH == executorType) &#123; executor = newBatchExecutor(this, transaction); &#125; elseif (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125;if (cacheEnabled) &#123; executor = new CachingExecutor(executor, autoCommit); &#125; 5）、 SqlSession的实现类即 DefaultSqlSession，此对象中对操作数据库实质上用的是 Executor 结论：每个线程都应该有它自己的SqlSession实例。SqlSession的实例不能共享使用，它也是线程不安全的。因此最佳的范围是请求或方法范围(定义局部变量使用)。绝对不能将SqlSession实例的引用放在一个类的静态字段或实例字段中。 打开一个SqlSession；使用完毕就要关闭它。通常把这个关闭操作放到 finally 块中以确保每次都能执行关闭。如下： 123456SqlSession session = sqlSessionFactory.openSession(); try &#123; // do work &#125; finally &#123; session.close();&#125; 原始 Dao 开发方法思路：需要程序员编写 Dao 接口和 Dao 实现类； 需要在 Dao 实现类中注入 SqlsessionFactory ，在方法体内通过 SqlsessionFactory 创建 Sqlsession。 Dao接口1234567891011public interface UserDao //dao接口，用户管理&#123; //根据id查询用户信息 public User findUserById(int id) throws Exception; //添加用户信息 public void addUser(User user) throws Exception; //删除用户信息 public void deleteUser(int id) throws Exception;&#125; Dao 实现类1234567891011121314151617181920212223242526272829303132333435363738394041public class UserDaoImpl implements UserDao //dao接口实现类&#123; //需要在 Dao 实现类中注入 SqlsessionFactory //这里通过构造方法注入 private SqlSessionFactory sqlSessionFactory; public UserDaoImpl(SqlSessionFactory sqlSessionFactory) &#123; this.sqlSessionFactory = sqlSessionFactory; &#125; @Override public User findUserById(int id) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); User user = sqlSession.selectOne(\"test.findUserById\", id); sqlSession.close(); return user; &#125; @Override public void insertUser(User user) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); //执行插入的操作 sqlSession.insert(\"test.insetrUser\", user); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; @Override public void deleteUser(int id) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); sqlSession.delete(\"test.deleteUserById\", id); //提交事务 sqlSession.commit(); sqlSession.close(); &#125;&#125; 测试12345678910111213141516171819202122232425public class UserDaoImplTest&#123; private SqlSessionFactory sqlSessionFactory; //此方法是在 testFindUserById 方法之前执行的 @Before public void setup() throws Exception &#123; //创建sqlSessionFactory //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; @Test public void testFindUserById() throws Exception &#123; //创建UserDao的对象 UserDao userDao = new UserDaoImpl(sqlSessionFactory); //调用UserDao方法 User user = userDao.findUserById(1); System.out.println(user); &#125;&#125; 通过id查询用户信息测试结果如下：（其他的可以自己在写测试代码，原理类似） 问题原始Dao开发中存在以下问题： Dao方法体存在重复代码：通过 SqlSessionFactory 创建 SqlSession，调用 SqlSession 的数据库操作方法 调用 sqlSession 的数据库操作方法需要指定 statement 的i d，这里存在硬编码，不得于开发维护。 调用 sqlSession 的数据库操作方法时传入的变量，由于 sqlsession 方法使用泛型，即使变量类型传入错误，在编译阶段也不报错，不利于程序员开发。 Mybatis 的 mapper 接口思路程序员需要编写 mapper.xml 映射文件 只需要程序员编写Mapper接口（相当于Dao接口），需遵循一些开发规范，mybatis 可以自动生成 mapper 接口类代理对象。 开发规范： 在 mapper.xml 中 namespace 等于 mapper 接口地址 1&lt;mapper namespace=\"cn.zhisheng.mybatis.mapper.UserMapper\"&gt;&lt;/mapper&gt; 在 xxxmapper.java 接口中的方法名要与 xxxMapper.xml 中 statement 的 id 一致。 在 xxxmapper.java 接口中的输入参数类型要与 xxxMapper.xml 中 statement 的 parameterType 指定的参数类型一致。 在 xxxmapper.java 接口中的返回值类型要与 xxxMapper.xml 中 statement 的 resultType 指定的类型一致。 UserMapper.java 12//根据id查询用户信息 public User findUserById(int id) throws Exception; UserMapper.xml 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where id = #&#123;1&#125;&lt;/select&gt; 总结：以上的开发规范主要是对下边的代码进行统一的生成： 1234User user = sqlSession.selectOne(\"test.findUserById\", id);sqlSession.insert(\"test.insetrUser\", user);sqlSession.delete(\"test.deleteUserById\", id);List&lt;User&gt; list = sqlSession.selectList(\"test.findUserByName\", username); 测试测试之前记得在 SqlMapConfig.xml 文件中添加加载映射文件 UserMapper.xml： 1&lt;mapper resource=\"mapper/UserMapper.xml\"/&gt; 测试代码： 1234567891011121314151617181920212223242526public class UserMapperTest&#123; private SqlSessionFactory sqlSessionFactory; //此方法是在 testFindUserById 方法之前执行的 @Before public void setup() throws Exception &#123; //创建sqlSessionFactory //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; @Test public void testFindUserById() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //调用UserMapper的方法 User user = userMapper.findUserById(1); System.out.println(user); &#125;&#125; 通过id查询用户信息测试结果如下：（其他的请自己根据上下文写测试代码，或者去看我 Github-Mybatis学习笔记 上看我这个项目的全部代码） 通过姓名查询用户信息： 代理对象内部调用 selectOne 或者 selectList 如果 mapper 方法返回单个 pojo 对象（非集合对象），代理对象内部通过 selectOne 查询数据库 如果 mapper 方法返回集合对象，代理对象内部通过 selectList 查询数据库 mapper接口方法参数只能有一个是否影响系统开发 mapper 接口方法参数只能有一个，系统是否不利于维护？ 系统框架中，dao层的代码是被业务层公用的。 即使 mapper 接口只有一个参数，可以使用包装类型的 pojo 满足不同的业务方法的需求。 注意：持久层方法的参数可以包装类型、map…. ，service方法中不建议使用包装类型。（不利于业务层的可扩展性） SqlMapConfig.xml 文件Mybatis 的全局配置变量，配置内容和顺序如下： properties（属性） settings（全局配置参数） typeAliases（类型别名） typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境集合属性对象） ​ environment（环境子属性对象） ​ transactionManager（事务管理） ​ dataSource（数据源） mappers（映射器） properties 属性需求：将数据库连接参数单独配置在 db.properties 中，只需要在 SqlMapConfig.xml 中加载该配置文件 db.properties 的属性值。在 SqlMapConfig.xml 中就不需要直接对数据库的连接参数进行硬编码了。方便以后对参数进行统一的管理，其他的xml文件可以引用该 db.properties 。 db.properties 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8jdbc.username=rootjdbc.password=root 那么 SqlMapConfig.xml 中的配置变成如下： 12345678910111213141516&lt;!--加载配置文件--&gt; &lt;properties resource=\"db.properties\"&gt;&lt;/properties&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务由 Mybatis 控制--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池--&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\" /&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 配置完成后我们测试一下是否能够和刚才一样的能够成功呢？那么我就先在db.properties中把数据库密码故意改错，看是否是正确的？不出意外的话是会报错的。 注意： MyBatis 将按照下面的顺序来加载属性： 在 properties 元素体内定义的属性首先被读取。 然后会读取 properties 元素中 resource 或 url 加载的属性，它会覆盖已读取的同名属性。 最后读取 parameterType 传递的属性，它会覆盖已读取的同名属性。 因此，通过parameterType传递的属性具有最高优先级，resource或 url 加载的属性次之，最低优先级的是 properties 元素体内定义的属性。 建议： 不要在 properties 元素体内添加任何属性值，只将属性值定义在 db.properties 文件之中。 在 db.properties 文件之中定义的属性名要有一定的特殊性。如 xxx.xxx.xxx settings（全局配置参数）Mybatis 框架在运行时可以调整一些运行参数 比如：开启二级缓存、开启延迟加载。。。 typeAliases（类型别名）需求： 在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。 如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。 Mybatis支持的别名： 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal 自定义别名： 在 SqlMapConfig.xml 中配置：(设置别名) 1234567&lt;typeAliases&gt; &lt;!-- 单个别名定义 --&gt; &lt;typeAlias alias=\"user\" type=\"cn.zhisheng.mybatis.po.User\"/&gt; &lt;!-- 批量别名定义，扫描整个包下的类，别名为类名（首字母大写或小写都可以） --&gt; &lt;package name=\"cn.zhisheng.mybatis.po\"/&gt; &lt;package name=\"其它包\"/&gt;&lt;/typeAliases&gt; 在 UserMapper.xml 中引用别名：( resultType 为 user ) 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; 测试结果： typeHandlers（类型处理器）mybatis中通过typeHandlers完成jdbc类型和java类型的转换。 通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义. mybatis支持类型处理器： 类型处理器 Java类型 JDBC类型 BooleanTypeHandler Boolean，boolean 任何兼容的布尔值 ByteTypeHandler Byte，byte 任何兼容的数字或字节类型 ShortTypeHandler Short，short 任何兼容的数字或短整型 IntegerTypeHandler Integer，int 任何兼容的数字和整型 LongTypeHandler Long，long 任何兼容的数字或长整型 FloatTypeHandler Float，float 任何兼容的数字或单精度浮点型 DoubleTypeHandler Double，double 任何兼容的数字或双精度浮点型 BigDecimalTypeHandler BigDecimal 任何兼容的数字或十进制小数类型 StringTypeHandler String CHAR和VARCHAR类型 ClobTypeHandler String CLOB和LONGVARCHAR类型 NStringTypeHandler String NVARCHAR和NCHAR类型 NClobTypeHandler String NCLOB类型 ByteArrayTypeHandler byte[] 任何兼容的字节流类型 BlobTypeHandler byte[] BLOB和LONGVARBINARY类型 DateTypeHandler Date（java.util） TIMESTAMP类型 DateOnlyTypeHandler Date（java.util） DATE类型 TimeOnlyTypeHandler Date（java.util） TIME类型 SqlTimestampTypeHandler Timestamp（java.sql） TIMESTAMP类型 SqlDateTypeHandler Date（java.sql） DATE类型 SqlTimeTypeHandler Time（java.sql） TIME类型 ObjectTypeHandler 任意 其他或未指定类型 EnumTypeHandler Enumeration类型 VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。 mappers（映射器） 使用相对于类路径的资源，如： 使用完全限定路径如： 使用 mapper 接口类路径 如： 注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 注册指定包下的所有mapper接口如：注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 Mapper.xml 映射文件Mapper.xml映射文件中定义了操作数据库的sql，每个sql是一个statement，映射文件是mybatis的核心。 输入映射通过 parameterType 指定输入参数的类型，类型可以是简单类型、hashmap、pojo的包装类型。 传递 pojo 包装对象 （重点） 开发中通过pojo传递查询条件 ，查询条件是综合的查询条件，不仅包括用户查询条件还包括其它的查询条件（比如将用户购买商品信息也作为查询条件），这时可以使用包装对象传递输入参数。 定义包装对象 定义包装对象将查询条件(pojo)以类组合的方式包装起来。 UserQueryVo.java 123456789101112131415public class UserQueryVo //用户包装类型&#123; //在这里包装所需要的查询条件 //用户查询条件 private UserCustom userCustom; public UserCustom getUserCustom() &#123; return userCustom; &#125; public void setUserCustom(UserCustom userCustom) &#123; this.userCustom = userCustom; &#125; //还可以包装其他的查询条件，比如订单、商品&#125; UserCustomer.java 1234public class UserCustom extends User //用户的扩展类&#123; //可以扩展用户的信息&#125; UserMapper.xml 文件 1234567&lt;!--用户信息综合查询 #&#123;userCustom.sex&#125; :取出pojo包装对象中的性别值 #&#123;userCustom.username&#125; :取出pojo包装对象中的用户名称 --&gt; &lt;select id=\"findUserList\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"cn.zhisheng.mybatis.po.UserCustom\"&gt; select * from user where user.sex = #&#123;userCustom.sex&#125; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/select&gt; UserMapper.java 12//用户信息综合查询public List&lt;UserCustom&gt; findUserList(UserQueryVo userQueryVo) throws Exception; 测试代码 1234567891011121314151617//测试用户信息综合查询 @Test public void testFindUserList() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //创建包装对象，设置查询条件 UserQueryVo userQueryVo = new UserQueryVo(); UserCustom userCustom = new UserCustom(); userCustom.setSex(\"男\"); userCustom.setUsername(\"张小明\"); userQueryVo.setUserCustom(userCustom); //调用UserMapper的方法 List&lt;UserCustom&gt; list = userMapper.findUserList(userQueryVo); System.out.println(list); &#125; 测试结果 输出映射 resultType 使用 resultType 进行输出映射，只有查询出来的列名和 pojo 中的属性名一致，该列才可以映射成功。 如果查询出来的列名和 pojo 中的属性名全部不一致，没有创建 pojo 对象。 只要查询出来的列名和 pojo 中的属性有一个一致，就会创建 pojo 对象。 输出简单类型需求：用户信息综合查询列表总数，通过查询总数和上边用户综合查询列表才可以实现分页 实现： 1234567&lt;!--用户信息综合查询总数 parameterType:指定输入的类型和findUserList一样 resultType:输出结果类型为 int --&gt; &lt;select id=\"findUserCount\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"int\"&gt; select count(*) from user where user.sex = #&#123;userCustom.sex&#125; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/select&gt; 12//用户信息综合查询总数 public int findUserCount(UserQueryVo userQueryVo) throws Exception; 12345678910111213141516//测试用户信息综合查询总数 @Test public void testFindUserCount() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //创建包装对象，设置查询条件 UserQueryVo userQueryVo = new UserQueryVo(); UserCustom userCustom = new UserCustom(); userCustom.setSex(\"男\"); userCustom.setUsername(\"张小明\"); userQueryVo.setUserCustom(userCustom); //调用UserMapper的方法 System.out.println(userMapper.findUserCount(userQueryVo)); &#125; 注意：查询出来的结果集只有一行且一列，可以使用简单类型进行输出映射。 输出pojo对象和pojo列表 不管是输出的pojo单个对象还是一个列表（list中包括pojo），在mapper.xml中resultType指定的类型是一样的。 在mapper.java指定的方法返回值类型不一样： 1、输出单个pojo对象，方法返回值是单个对象类型 12//根据id查询用户信息 public User findUserById(int id) throws Exception; 2、输出pojo对象list，方法返回值是List 12//根据用户名查询用户信息 public List&lt;User&gt; findUserByUsername(String userName) throws Exception; resultType总结： 输出pojo对象和输出pojo列表在sql中定义的resultType是一样的。 返回单个pojo对象要保证sql查询出来的结果集为单条，内部使用session.selectOne方法调用，mapper接口使用pojo对象作为方法返回值。 返回pojo列表表示查询出来的结果集可能为多条，内部使用session.selectList方法，mapper接口使用List对象作为方法返回值。 resultMap resultType 可以指定 pojo 将查询结果映射为 pojo，但需要 pojo 的属性名和 sql 查询的列名一致方可映射成功。 如果sql查询字段名和pojo的属性名不一致，可以通过resultMap将字段名和属性名作一个对应关系 ，resultMap实质上还需要将查询结果映射到pojo对象中。 resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询。 使用方法： 1、定义 resultMap 2、使用 resultMap 作为 statement 的输出映射类型 将下面的 sql 使用 User 完成映射 1select id id_, username username_ from user where id = #&#123;value&#125; User 类中属性名和上边查询的列名不一致。 所以需要： 1、定义 resultMap 1234567891011121314151617181920&lt;!--定义 resultMap 将select id id_, username username_ from user where id = #&#123;value&#125; 和User类中的属性做一个映射关系 type: resultMap最终映射的java对象类型 id:对resultMap的唯一标识 --&gt; &lt;resultMap id=\"userResultMap\" type=\"user\"&gt; &lt;!--id表示查询结果中的唯一标识 column：查询出来的列名 property：type指定pojo的属性名 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;id column=\"id_\" property=\"id\"/&gt; &lt;!--result: 对普通结果映射定义 column：查询出来的列名 property：type指定pojo的属性名 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;result column=\"username_\" property=\"username\"/&gt; &lt;/resultMap&gt; 2、使用 resultMap 作为 statement 的输出映射类型 12345&lt;!--使用 resultMap 作为输出映射类型 resultMap=\"userResultMap\":其中的userResultMap就是我们刚才定义的 resultMap 的id值,如果这个resultMap在其他的mapper文件中，前边须加上namespace --&gt; &lt;select id=\"findUserByIdResultMap\" parameterType=\"int\" resultMap=\"userResultMap\"&gt; select id id_, username username_ from user where id = #&#123;value&#125; &lt;/select&gt; 3、UserMapper.java 12//根据id查询用户信息，使用 resultMap 输出public User findUserByIdResultMap(int id) throws Exception; 4、测试 1234567891011//测试根据id查询用户信息，使用 resultMap 输出 @Test public void testFindUserByIdResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //调用UserMapper的方法 User user = userMapper.findUserByIdResultMap(1); System.out.println(user); &#125; 5、测试结果 动态 SQL通过mybatis提供的各种标签方法实现动态拼接sql。 需求： 用户信息综合查询列表和用户信息查询列表总数这两个 statement的定义使用动态sql。 对查询条件进行判断，如果输入的参数不为空才进行查询条件拼接。 UserMapper.xml (findUserList的配置如下，那么findUserCount的也是一样的，这里就不全部写出来了) 1234567891011121314&lt;select id=\"findUserList\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"cn.zhisheng.mybatis.po.UserCustom\"&gt; select * from user &lt;!--where可以自动的去掉条件中的第一个and--&gt; &lt;where&gt; &lt;if test=\"userCustom != null\"&gt; &lt;if test=\"userCustom.sex != null and userCustom.sex != ''\"&gt; and user.sex = #&#123;userCustom.sex&#125; &lt;/if&gt; &lt;if test=\"userCustom.username != null\"&gt; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/if&gt; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 测试代码：因为设置了动态的sql，如果不设置某个值，那么条件就不会拼接在sql上 所以我们就注释掉设置username的语句 1//userCustom.setUsername(\"张小明\"); 测试结果： Sql 片段通过上面的其实看到在 where sql语句中有很多重复代码，我们可以将其抽取出来，组成一个sql片段，其他的statement就可以引用这个sql片段，利于系统的开发。 这里我们就拿上边sql 中的where定义一个sq片段如下： 123456789101112131415&lt;!--sql片段 id:唯一标识 经验：是基于单表来定义sql片段，这样的话sql片段的可重用性才高 一般不包含where --&gt; &lt;sql id=\"query_user_where\"&gt; &lt;if test=\"userCustom != null\"&gt; &lt;if test=\"userCustom.sex != null and userCustom.sex != ''\"&gt; and user.sex = #&#123;userCustom.sex&#125; &lt;/if&gt; &lt;if test=\"userCustom.username != null\"&gt; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/if&gt; &lt;/if&gt; &lt;/sql&gt; 那么我们该怎样引用这个sql片段呢？如下： 12345select * from user &lt;where&gt; &lt;!--refid: 指定sql片段的id，如果是写在其他的mapper文件中，则需要在前面加上namespace--&gt; &lt;include refid=\"query_user_where\"/&gt; &lt;/where&gt; 测试的话还是那样了，就不继续说了，前面已经说了很多了。 foreach向sql传递数组或List，mybatis使用foreach解析 需求： 在用户查询列表和查询总数的statement中增加多个id输入查询。 sql语句如下： 123SELECT * FROM USER WHERE id=1 OR id=10 ORid=16或者SELECT * FROM USER WHERE id IN(1,10,16) 在输入参数类型中添加 List ids 传入多个 id 12345public class UserQueryVo //用户包装类型&#123; //传入多个id private List&lt;Integer&gt; ids;&#125; 修改 UserMapper.xml文件 WHERE id=1 OR id=10 OR id=16 在查询条件中，查询条件定义成一个sql片段，需要修改sql片段。 12345678910111213141516171819202122&lt;if test=\"ids!=null\"&gt; &lt;!-- 使用 foreach遍历传入ids collection：指定输入 对象中集合属性 item：每个遍历生成对象中 open：开始遍历时拼接的串 close：结束遍历时拼接的串 separator：遍历的两个对象中需要拼接的串 --&gt; &lt;!-- 使用实现下边的sql拼接： AND (id=1 OR id=10 OR id=16) --&gt; &lt;foreach collection=\"ids\" item=\"user_id\" open=\"AND (\" close=\")\" separator=\"or\"&gt; &lt;!-- 每个遍历需要拼接的串 --&gt; id=#&#123;user_id&#125; &lt;/foreach&gt; &lt;!-- 实现 “ and id IN(1,10,16)”拼接 --&gt; &lt;!-- &lt;foreach collection=\"ids\" item=\"user_id\" open=\"and id IN(\" close=\")\" separator=\",\"&gt; 每个遍历需要拼接的串 #&#123;user_id&#125; &lt;/foreach&gt; --&gt; &lt;/if&gt; 测试代码： 1234567 //传入多个idList&lt;Integer&gt; ids = new ArrayList&lt;&gt;();ids.add(1);ids.add(10);ids.add(16);//将ids传入statement中userQueryVo.setIds(ids); 期待后续的文章吧！","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":"《疯狂 Java 突破程序员基本功的 16 课》读书笔记","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java-16-lession/","text":"第 1 课 —— 数组与内存控制数组初始化数组初始化之后，该数组的长度是不可变的（可通过数组的 length 属性访问数组的长度）。Java 中的数组必须经过初始化（为数组对象的元素分配内存空间，并为每个数组元素指定初始值）才可使用。 数组初始化的形式： 静态初始化：初始化时由程序员显示的指定每个数组的初始值，系统决定数组长度。 动态初始化：初始化时程序员只指定数组的长度，系统为数组元素分配初始值。 使用数组数组元素就是变量：例如 int[] 数组元素相当于 int 类型的变量 当通过索引来使用数组元素时（访问数组元素的值、为数组元素赋值），将该数组元素当成普通变量使用即可。 第 2 课 —— 对象与内存的控制Java 内存管理分为：内存分配和内存回收。 内存分配：创建 Java 对象时 JVM 为该对象在堆内存中所分配的内存空间。 内存回收：当 Java 对象失去引用，变成垃圾，JVM 的垃圾回收机制自动清理该对象，并回收内存 实例变量 和 类变量局部变量特点：作用时间短，存储在方法的栈内存中 种类： 形参：方法签名中定义的局部变量，由方法调用者负责为其赋值，随方法结束而消亡 方法内的局部变量：方法内定义的局部变量，必须在方法内对其进行显示初始化，从初始化后开始生效，随方法结束而消亡 代码块内的局部变量：在代码块中定义的局部变量，必须在代码块中进行显示初始化，从初始化后开始生效，随代码块结束而消亡 成员变量类体内定义的变量，如果该成员变量没有使用 static 修饰，那该成员变量又被称为非静态变量或实例变量，如果使用 static 修饰，则该成员变量又可被称为静态变量或类变量。 实例变量和类变量的属性使用 static 修饰的成员变量是类变量，属于该类本身，没有使用 static 修饰的成员变量是实例变量，属于该类的实例，在同一个类中，每一个类只对应一个 Class 对象，但每个类可以创建多个对象。 由于同一个 JVM 内的每个类只对应一个 CLass 对象，因此同一个 JVM 内的一个类的类变量只需要一块内存空间；但对于实例变量而言，该类每创建一次实例，就需要为该实例变量分配一块内存空间。也就是说，程序中创建了几个实例，实例变量就需要几块内存空间。 这里我想到一道面试题目： 123456789101112public class A&#123; &#123; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); &#125;&#125; 结果： 123我是静态代码块我是代码块我是代码块 静态代码块只执行一次，而代码块每创建一个实例，就会打印一次。 实例变量的初始化时机程序可在3个地方对实例变量执行初始化： 定义实例变量时指定初始值 非静态初始化块中对实例变量指定初始值 构造器中对实例变量指定初始值 上面第一种和第二种方式比第三种方式更早执行，但第一、二种方式的执行顺序与他们在源程序中的排列顺序相同。 同样在上面那个代码上加上一个变量 weight 的成员变量，我们来验证下上面的初始化顺序： 1、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之后: 123456789101112131415public class A&#123; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; double weight = 2.0; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果是： 1234我是静态代码块我是代码块我是代码块2.0 2、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之前: 123456789101112131415public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果为： 1234我是静态代码块我是代码块我是代码块2.1 大家有没有觉得很奇怪？ 我来好好说清楚下： 定义实例变量时指定的初始值、初始代码块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将会被提取到构造器中。也就是说，这条语句 double weight = 2.0; 实际上会被分成如下 2 次执行： double weight; : 创建 Java 对象时系统根据该语句为该对象分配内存。 weight = 2.1; : 这条语句将会被提取到 Java 类的构造器中执行。 只说原理，大家肯定不怎么信，那么还有拿出源码来，这样才有信服能力的吗？是不？ 这里我直接使用软件将代码的字节码文件反编译过来，看看里面是怎样的组成？ 第一个代码的反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.1D; System.out.println(\"我是代码块\"); this.weight = 2.0D; &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 第二个代码反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 这下子满意了吧！ 通过反编译的源码可以看到该类定义的 weight 实例变量时不再有初始值，为 weight 指定初始值的代码也被提到了构造器中去了，但是我们也可以发现之前规则也是满足的。 他们的赋值语句都被合并到构造器中，在合并过程中，定义的变量语句转换得到的赋值语句，初始代码块中的语句都转换得到的赋值语句，总是位于构造器的所有语句之前，合并后，两种赋值语句的顺序也保持了它们在 Java 源代码中的顺序。 大致过程应该了解了吧？如果还不怎么清楚的，建议还是自己将怎个过程在自己的电脑上操作一遍，毕竟光看不练假把式。 类变量的初始化时机JVM 对每一个 Java 类只初始化一次，因此 Java 程序每运行一次，系统只为类变量分配一次内存空间，执行一次初始化。程序可在两个地方对类变量执行初始化： 定义类变量时指定初始值 静态初始化代码块中对类变量指定初始值 这两种方式的执行顺序与它们在源代码中的排列顺序相同。 还是用上面那个示例，我们在其基础上加个被 static 修饰的变量 height： 1、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后： 123456789101112131415161718public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.0 2、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前： 123456789101112131415161718public class A&#123; static double height = 10.0; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.1 其运行结果正如我们预料，但是我们还是看看反编译后的代码吧！ 第一种情况下反编译的代码： 1234567891011121314151617181920212223public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0D; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 第二种情况下反编译的代码： 123456789101112131415161718192021222324public class A&#123; static double height = 10.0D; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; height = 10.1D; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 通过反编译源码，可以看到第一种情况下(定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后): 我们在 静态初始化代码块中对类变量指定初始值 已经不存在了，只有一个类变量指定的初始值 static double height = 10.0D; , 而在第二种情况下（定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前）和之前的源代码顺序是一样的，没啥区别。 上面的代码中充分的展示了类变量的两种初始化方式 ：每次运行该程序时，系统会为 A 类执行初始化，先为所有类变量分配内存空间，再按照源代码中的排列顺序执行静态初始代码块中所指定的初始值和定义类变量时所指定的初始值。 父类构造器当创建任何 Java 对象时，程序总会先依次调用每个父类非静态初始化代码块、父类构造器（总是从 Object 开始）执行初始化，最后才调用本类的非静态初始化代码块、构造器执行初始化。 隐式调用和显示调用当调用某个类的构造器来创建 Java 对象时，系统总会先调用父类的非静态初始化代码块进行初始化。这个调用是隐式执行的，而且父类的静态初始化代码块总是会被执行。接着会调用父类的一个或多个构造器执行初始化，这个调用既可以是通过 super 进行显示调用，也可以是隐式调用。 当所有父类的非静态初始代码块、构造器依次调用完成后，系统调用本类的非静态代码块、构造器执行初始化，最后返回本类的实例。至于调用父类的哪个构造器执行初始化，分以下几种情况： 子类构造器执行体的第一行代码使用 super 显式调用父类构造器，系统将根据 super 调用里传入的实参列表来确定调用父类的哪个构造器； 子类构造器执行体的第一行代码使用 this 显式调用本类中的重载构造器，系统将根据 this 调用里传入的实参列表来确定奔雷的另一个构造器（执行本类中另一个构造器时即进入第一种情况）； 子类构造器中既没有 super 调用，也没有 this 调用，系统将会在执行子类构造器之前，隐式调用父类无参构造器。 注：super 和 this 必须在构造器的第一行，且不能同时存在。 推荐一篇博客：Java初始化顺序 文章从无继承和继承两种情况下分析了 Java 初始化的顺序。 Java初始化顺序如图： 访问子类对象的实例变量调用被子类重写的方法父子实例的内存控制继承成员变量和继承方法的区别方法的行为总是表现出它们实际类型的行为；实例变量的值总是表现出声明这些变量所用类型的行为。 内存中的子类实例父、子类的类变量final 修饰符final 可以修饰变量、方法、类。 修饰变量，变量被赋初始值之后，不能够对他在进行修改 修饰方法，不能够被重写 修饰类，不能够被继承 final 修饰的实例变量只能在如下位置指定初始值： 定义 final 实例变量时指定初始值 在非静态代码块中为 final 实例变量指定初始值 在构造器中为 final 实例变量指定初始值 final 修饰的类变量只能在如下位置指定初始值： 定义 final 类变量时指定初始值 在静态代码块中为 final 类变量指定初始值 第 3 课 —— 常见 Java 集合的实现细节Java 集合框架类图： Set 和 MapSet 代表一种集合元素无序、集合元素不可重复的集合，Map 则代表一种由多个 key-value 对组合的集合，Map 集合类似于传统的关联数组。 Set 和 Map 的关系1、Map 集合中的 key 不能重复且没有顺序。将这些 key 组合起来就是一个 Set 集合。所以有一个 Set&lt;k&gt; keySet() 方法来返回所有 key 组成的 Set 集合。 2、Set 也可以转换成 Map。（在 Set 中将 每一对 key 和 value 存放在一起） HashMap 和 HashSetHashSet：系统采用 Hash 算法决定集合元素的存储位置。（基于 HashMap 实现的） HashMap：系统将 value 当成 key 的附属，系统根据 Hash 算法决定 key 的存储位置。 HashSet 的绝大部分方法都是通过调用 HashMap 的方法实现的，因此 HashSet 和 HashMap 两个集合在实现本质上是相同的。 TreeMap 和 TreeSetTreeSet 底层使用 TreeMap 来包含 Set 集合中的所有元素。 TreeMap 采用的是一种“红黑树”的排序二叉树来保存 Map 中每个 Entry —— 每个 Entry 都被当成 “红黑树” 的一个节点对待。 Map 和 ListMap 的 values() 方法不管是 HashMap 还是 TreeMap ，它们的 values() 方法都可以返回其所有 value 组成的 Collection 集合，其实是一个不存储元素的 Collection 集合，当程序遍历 Collection 集合时，实际上就是遍历 Map 对象的 value。 HashMap 和 TreeMap 的 values() 方法并未把 Map 中的 values 重新组合成一个包含元素的集合对象，这样就可以降低系统内存开销。 Map 和 List 的关系底层实现很相似；用法上很相似。 Map 接口提供 get(K key) 方法允许 Map 对象根据 key 来取得 value； List 接口提供了 get(int index) 方法允许 List 对象根据元素索引来取得 value； ArrayList 和 LinkedListList 集合的实现类，主要有 ArrayList 、Vector 和 LinkedList。 ArrayList 是一个可改变大小的数组.当更多的元素加入到 ArrayList 中时, 其大小将会动态地增长. 内部的元素可以直接通过 get 与 set 方法进行访问, 因为 ArrayList 本质上就是一个数组. LinkedList 是一个双链表, 在添加和删除元素时具有比 ArrayList 更好的性能. 但在 get 与 set 方面弱于ArrayList. 当然, 这些对比都是指数据量很大或者操作很频繁的情况下的对比, 如果数据和运算量很小,那么对比将失去意义. Vector 和 ArrayList 类似, 但属于强同步类。如果你的程序本身是线程安全的(thread-safe,没有在多个线程之间共享同一个集合/对象),那么使用 ArrayList 是更好的选择。 Vector 和 ArrayList 在更多元素添加进来时会请求更大的空间。Vector 每次请求其大小的双倍空间，而 ArrayList每次对 size 增长 50%. 而 LinkedList 还实现了 Queue 接口, 该接口比 List 提供了更多的方法,包括 offer(), peek(), poll()等. 注意: 默认情况下 ArrayList 的初始容量非常小, 所以如果可以预估数据量的话, 分配一个较大的初始值属于最佳实践, 这样可以减少调整大小的开销。 ArrayList与LinkedList性能对比 时间复杂度对比如下: LinkedList 更适用于: 没有大规模的随机读取 大量的增加/删除操作 Iterator 迭代器是一个迭代器接口，专门用于迭代各种 Collection 集合，包括 Set 集合和 List 集合。 第 4 课 —— Java 的内存回收Java 引用的种类对象在内存中的状态JVM 垃圾回收机制，是否回收一个对象的标准在于：是否还有引用变量引用该对象？只要有引用变量引用该对象，垃圾回收机制就不会回收它。 Java 语言对对象的引用有： 强引用 软引用 弱引用 虚引用 强引用程序创建一个对象，并把这个对象赋给一个引用变量，这个引用变量就是强引用。当一个对象被一个或者一个以上的强引用变量所引用时，它处于可达状态，它是不会被系统的垃圾回收机制回收。 软引用软引用需要通过 SoftReference 类来实现，当一个对象只具有软引用时，它有可能会被垃圾回收机制回收。对于只有软引用的对象而言，当系统内存空间足够时，它不会被系统回收，程序也可使用该对象；当系统内存空间不足时，系统将会回收它。 弱引用弱引用和软引用有点相似，区别在于弱引用所引用对象的生存期更短。 虚引用虚引用主要用于跟踪对象被垃圾回收的状态，虚引用不能单独使用，虚引用必须和引用队列联合使用。 Java 的内存泄漏ArrayList.java 中的 remove 方法 1234567891011public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue; &#125; 其中 elementData[--size] = null; // clear to let GC do its work 语句是清除数组元素的引用，避免内存的泄漏，如果没有这句的话，那么就是只有两个作用： 修饰 Stack 的属性，也就是将值减 1； 返回索引为 size -1 的值。 垃圾回收机制 跟踪并监控每个 Java 对象，当某个对象处于不可达状态时，回收该对象所占用的内存。 清理内存分配，回收过程中产生的内存碎片。 垃圾回收的基本算法对于一个垃圾回收器的设计算法来说，大概有如下几个设计： 串行回收 和 并行回收 串行回收：不管系统有多少个 CPU，始终使用一个 CPU 来执行垃圾回收操作 并行回收：把整个回收工作拆分成多部分，每个部分由一个 CPU 负责，从而让多个 CPU 并行回收 并发执行 和 应用程序停止 压缩 和 不压缩 和 复制 复制：将堆内分成两个相同的空间，从根开始访问每一个关联的可达对象，将空间A的可达对象全部复制到空间B，然后一次性回收整个空间A。 标记清除：也就是 不压缩 的回收方式。垃圾回收器先从根开始访问所有可达对象，将它们标记为可达状态，然后再遍历一次整个内存区域，把所有没有标记为可达的对象进行回收处理。 标记压缩：这是压缩方式，这种方式充分利用上述两种算法的优点，垃圾回收器先从根开始访问所有可达对象，将他们标记为可达状态，接下来垃圾回收器会将这些活动对象搬迁在一起，这个过程叫做内存压缩，然后垃圾回收机制再次回收那些不可达对象所占用的内存空间，这样就避免了回收产生的内存碎片。 堆内存的分代回收1、Young 代 2、Old 代 3、Permanent 代 内存管理小技巧 尽量使用直接量 使用 StringBuilder 和 StringBuffer 进行字符串拼接 尽早释放无用对象的引用 尽量少用静态变量 避免在经常调用的方法、循环中创建 Java 对象 缓存经常使用的对象 尽量不要使用 finalize 方法 考虑使用 SoftReference 第 5 课 —— 表达式中的陷阱关于字符串的陷阱JVM 对字符串的处理String java = new String(&quot;Java&quot;) 这句创建了两个字符串对象，一个是 “Java” 这个直接量对应的字符串对象，另外一个是 new String() 构造器返回的字符串对象。 Java 程序中创建对象的方法： 通过 new 调用构造器创建 Java 对象 通过 Class 对象的 newInstance() 方法调用构造器创建 Java 对象 通过 Java 的反序列化机制从 IO 流中恢复 Java 对象 通过 Java 对象提供的 clone() 方法复制一个新的 Java 对象 对于字符串以及 Byte、Short、Int、Long、Character、Float、Double 和 Boolean 这些基本类型的包装类 直接量的方式来创建 Java 对象 Integer in = 5； 通过简单的算法表达式，连接运算来创建 Java 对象 String str = “a” + “b”; （如果这个字符串表达式的值在编译时确定下来，那么 JVM 会在编译时计算该字符串变量的值，并让它指向字符串池中对应的字符串。如果这些算法表达式都是字符串直接量、整数直接量，没有变量和方法参与，那么就可以在编译期就可以确定字符串的值；如果使用了变量、调用了方法，那么只有等到运行时才能确定字符串表达式的值；如果字符串连接运算所有的变量都可执行 “宏替换”（使用 final 修饰的变量），那在编译时期也能确定字符串连接表达式的值） 对于 Java 程序的字符直接量，JVM 会使用一个字符串池来保护它们；当第一次使用某个字符串直接量时，JVM 会将它放入字符串池进行缓存。在一般的情况下，字符串池中的字符串对象不会被垃圾回收器回收，当程序再次需要使用该字符串时，无需重新创建一个新的字符串，而是直接让引用变量指向字符串池中已有的字符串。 不可变的字符串String 类是一个不可变类，当一个 String 对象创建完成后，该 String 类里包含的字符序列就被固定下来，以后永远不能修改。 如果程序需要一个字符序列会发生改变的字符串，那么建议使用 StringBuilder （效率比 StringBuffer 高） 字符串比较如果要比较两个字符串是否相同，用 == 进行判断就行，但如果要判断两个字符串所包含的字符序列是否相同，则应该用 String 重写过的 equals() 方法进行比较。 123456789101112131415161718192021222324252627public boolean equals(Object anObject) &#123; //如果两个字符串相同 if (this == anObject) &#123; return true; &#125; //如果anObject是String类型 if (anObject instanceof String) &#123; String anotherString = (String)anObject; //n代表字符串的长度 int n = value.length; //如果两个字符串长度相等 if (n == anotherString.value.length) &#123; //获取当前字符串、anotherString底层封装的字符数组 char v1[] = value; char v2[] = anotherString.value; int i = 0; //逐一比较v1 和 v2数组中的每个字符 while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false; &#125; 还可以使用 String 提供的 compareTo() 方法返回两个字符串的大小 123456789101112131415161718public int compareTo(String anotherString) &#123; int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k &lt; lim) &#123; char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) &#123; return c1 - c2; &#125; k++; &#125; return len1 - len2; &#125; 表达式类型的陷阱表达式类型的自动提升 所有 byte、short、char类型将被提升到 int 类型参与运算 整个算术表达式的数据类型自动提升到与表达式中最高等级操作数同样的类型，操作数的等级排列如下：char -&gt; int -&gt; long -&gt;float -&gt; double byte -&gt; short -&gt; int -&gt; long -&gt;float -&gt; double 复合赋值运算符的陷阱Java 语言允许所有的双目运算符和 = 一起结合组成复合赋值运算符，如 +=、-=、*=、/=、%= 、&amp;= 等，复合赋值运算符包含了一个隐式的类型转换。 123//下面这两条语句不等价a = a + 5; //a += 5; //实际上等价于 a = (a的类型) (a + 5); 复合赋值运算符会自动的将它计算的结果值强制转换为其左侧变量的类型。 输入法导致的陷阱注释的字符必须合法转义字符的陷阱 慎用字符的 Unicode 转义形式 中止行注释的转义字符 泛型可能引起的错误原始类型变量的赋值 当程序把一个原始类型的变量赋给一个带有泛型信息的变量时，总是可以通过编译（只是会提示警告信息） 当程序试图访问带泛型声明的集合的集合元素时，编译器总是把集合元素当成泛型类型处理（它并不关心集合里集合元素的实际类型） 当程序试图访问带泛型声明的集合的集合元素时，JVM会遍历每个集合元素自动执行强制转型，如果集合元素的实际类型与集合所带的泛型信息不匹配，运行时将引发 ClassCastException 原始类型带来的擦除当把一个具有泛型信息的对象赋给另一个没有泛型信息的变量时，所有在尖括号之间的类型信息都会丢弃。 创建泛型数组的陷阱Java 中不允许创建泛型数组 正则表达式的陷阱有些符号本身就是正则表达式，我们需要对符号做转义运算。 多线程的陷阱不要调用 run 方法开启线程是用 start() 方法，而不是 run() 方法。 静态的同步方法对于同步代码块而言，程序必须显式为它指定同步监视器；对于同步非静态方法而言，该方法的同步监视器是 this —— 即调用该方法的 Java 对象；对于静态的同步方法而言，该方法的同步监视器不是 this，而是该类本身。 第 6 课 —— 流程控制的陷阱switch 语句陷阱break 语句不要忘记写 switch 的表达式类型： byte short int char enum String （Jdk 1.7 以后有 String） 标签引起的陷阱Java 中的标签通常是和循环中的 break 和 continue 结合使用，让 break 直接终止标签所标识的循环，让 continue 语句忽略标签所标识的循环的剩下语句。 。。 第 7 课 —— 面向对象的陷阱instanceof 运算符的陷阱instanceof 它用于判断前面的对象是否是后面的类或其子类、实现类的实例。如果是返回 true，否则返回 false。 instanceof 运算符前面操作数的编译时类型必须是： 要么与后面的类相同 要么是后面类的父类 要么是后面类型的子类 构造器陷阱构造器是 Java 中每个类都会提供的一个“特殊方法”。构造器负责对 Java 对象执行初始化操作，不管是定义实例变量时指定的初始值，还是在非静态初始化代码块中所做的操作，实际上都会被提取到构造器中执行。 构造器不能声明返回值类型，也不能使用void声明构造器没有返回值。 构造器创建对象吗构造器并不会创建 Java 对象，构造器只是负责执行初始化，在构造器执行之前，Java 对象所需要的内存空间，是由 new 关键字申请出来的。绝大部分时候，程序使用 new 关键字为一个 Java 对象申请空间之后，都需要使用构造器为这个对象执行初始化，但在某些时候，程序创建 Java 对象无需调用构造器，如下： 使用反序列化的方式恢复 Java 对象 使用 clone 方法复制 Java 对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.zhisheng.test;import java.io.*;/** * Created by 10412 on 2017/5/31. */class Wolf implements Serializable&#123; private String name; public Wolf(String name) &#123; System.out.println(\"调用了有参构造方法\"); this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Wolf wolf = (Wolf) o; return name != null ? name.equals(wolf.name) : wolf.name == null; &#125; @Override public int hashCode() &#123; return name != null ? name.hashCode() : 0; &#125;&#125;public class SerializableTest&#123; public static void main(String[] args) &#123; Wolf w = new Wolf(\"灰太狼\"); System.out.println(\"对象创建完成\"); Wolf w2 = null; ObjectInputStream ois = null; ObjectOutputStream oos = null; try &#123; //创建输出对象流 oos = new ObjectOutputStream(new FileOutputStream(\"a.bin\")); //创建输入对象流 ois = new ObjectInputStream(new FileInputStream(\"a.bin\")); //序列输出java 对象 oos.writeObject(w); oos.flush(); //反序列化恢复java对象 w2 = (Wolf) ois.readObject(); System.out.println(w); System.out.println(w2); //两个对象的实例变量值完全相等，输出true System.out.println(w.equals(w2)); //两个对象不同，输出false System.out.println(w == w2); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;finally &#123; if (ois!=null) try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (oos!=null) try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 程序运行结果： 123456调用了有参构造方法对象创建完成com.zhisheng.test.Wolf@1b15382com.zhisheng.test.Wolf@1b15382truefalse 正如结果所示：创建 Wolf 对象时，程序调用了相应的构造器来对该对象执行初始化；当程序通过反序列化机制恢复 Java 对象时，系统无需在调用构造器来进行初始化。通过反序列化恢复出来的 Wolf 对象和原来的 Wolf 对象具有完全相同的实例变量值，但系统会产生两个对象。 无限递归构造器12345678910111213public class ConstrutionTest&#123; ConstrutionTest ct; &#123; ct = new ConstrutionTest(); &#125; public ConstrutionTest() &#123; System.out.println(\"无参构造器\"); &#125; public static void main(String[] args) &#123; ConstrutionTest ct = new ConstrutionTest(); &#125;&#125; 运行结果抛出异常 java.lang.StackOverflowError 因为不管定义实例变量时指定的初始值，还是在非静态初始化代码块中执行的初始化操作，最终都将提取到构造器中执行，因为代码中递归调用了类的构造器，最终导致出现 java.lang.StackOverflowError 异常。 到底调用哪个重载方法1、第一阶段 JVM 将会选取所有可获得并匹配调用的方法或者构造器 2、第二个阶段决定到底要调用哪个方法，此时 JVM 会在第一阶段所选取的方法或者构造器中再次选取最精确匹配的那一个。 1234567891011121314151617public class OverrideTest&#123; public void info(Object obj, int a) &#123; System.out.println(\"obj 参数\" + obj); System.out.println(\"整型参数 \" + a); &#125; public void info(Object[] obj, double a) &#123; System.out.println(\"obj 参数\" + obj); System.out.println(\"整型参数 \" + a); &#125; public static void main(String[] args) &#123; OverrideTest o = new OverrideTest(); o.info(null, 5); &#125;&#125; 报错如下： 12Error:(20, 10) java: 对info的引用不明确 com.zhisheng.test.OverrideTest 中的方法 info(java.lang.Object,int) 和 com.zhisheng.test.OverrideTest 中的方法 info(java.lang.Object[],double) 都匹配 在这种复杂的条件下，JVM 无法判断哪个方法更匹配实际调用，将会导致程序编译错误。 方法重写的陷阱无法重写父类 private 方法。如果子类有一个与父类 private 方法具有相同方法名、相同形参列表、相同返回值类型的方法，依然不是重写，只是子类定义了一个与父类相同的方法。 static 关键字static 可以修饰类中定义的成员：field、方法、内部类、初始化代码块、内部枚举类 静态方法属于类被 static 修饰的成员（field、方法、内部类、初始化块、内部枚举类）属于类本身，而不是单个的 Java 对象。静态方法也是属于类。 第 8 课 —— 异常捕捉的陷阱正确关闭资源的方式 使用 finally 块来保证回收，保证关闭操作总是会被执行 关闭每个资源之前首先保证引用该资源的引用变量不为 null 为每个物理资源单独使用 try .. catch 块关闭资源，保证关闭资源时引发的异常不会影响其他资源的关闭。 finally 块陷阱finally 执行顺序，看我以前写的一篇文章《深度探究Java 中 finally 语句块》。 catch 块用法在 try 块后使用 catch 块来捕获多个异常时，程序应该小心多个 catch 块之间的顺序：捕获父类异常的 catch 块都应该排在捕获子类异常的 catch 块之后（先处理小异常，再处理大异常），否则出现编译错误。 继承得到的异常子类重写父类方法时，不能声明抛出比父类方法类型更多、范围更大的异常。 二叉树性质： 二叉树第 i 层上的节点数目至多为 2 ^(i - 1) (i &gt;= 1) 深度为 k 的二叉树至多有 2 ^ k - 1 个节点 在任何一颗二叉树中，如果其叶子结点的数量为 n0，度为 2 的子节点数量为 n2，则 n0 = n2 + 1 具有 n 个节点的完全二叉树的深度为 log n + 1 (log 的底为 2) 对于一棵有 n 个节点的完全二叉树的节点按层自左向右编号，则对任一编号为 i 的节点有如下性质： 当 i == 1 时，节点 i 是二叉树的根；若 i &gt; 1 时，则节点的父节点是 i/2 当 2i &lt;= n，则节点 i 有左孩子，左孩子的编号是 2i，否则，节点无左孩子，并且是叶子结点 若 2i + 1 &lt;= n ，则节点 i 有右孩子，右孩子的编号是 2i + 1；否则，节点无右孩子。 对于一颗 n 个节点的完全二叉树的节点按层自左向右编号，1 ~ n/2 范围的节点都是有孩子节点的非叶子结点，其余的节点全部都是叶子结点。编号为 n/2 的节点有可能只有左节点，也可能既有左节点，又有右节点。 选择排序直接选择排序需要经过 n - 1 趟比较 第一趟比较：程序将记录定位在第一个数据上，拿第一个数据依次和它后面的每个数据进行比较，如果第一个数据大于后面某个数据，交换它们。。依此类推，经过第一趟比较，这组数据中最小的数据被选出来，它被排在第一位。 第二趟比较：程序将记录定位在第二个数据上，拿第二个数据依次和它后面每个数据进行比较，如果第二个数据大于后面某个数据，交换它们。。依次类推，经过第二趟比较，这组数据中第二小的数据被选出，它排在第二位 。。 按此规则一共进行 n-1 趟比较，这组数据中第 n - 1小（第二大）的数据被选出，被排在第 n -1 位（倒数第一位）；剩下的就是最大的数据，它排在最后。 直接选择排序的优点就是算法简单，容易实现，缺点就是每趟只能确定一个元素，n个数组需要进行 n-1 趟比较。 堆排序 建堆 拿堆的根节点和最后一个节点交换 交换排序冒泡排序第一趟：依次比较0和1，1和2，2和3 … n-2 和 n - 1 索引的元素，如果发现第一个数据大于后一个数据，交换它们，经过第一趟，最大的元素排到了最后。 第二趟：依次比较0和1，1和2，2和3 … n-3 和 n - 2 索引的元素，如果发现第一个数据大于后一个数据，交换它们，经过第二趟，第二大的元素排到了倒数第二位 。。 第 n -1 趟：依次比较0和1元素，如果发现第一个数据大于后一个数据，交换它们，经过第 n - 1 趟，第二小的元素排到了第二位。 快速排序从待排的数据序列中任取一个数据作为分界值，所有比它小的数据元素一律放在左边，所有比他大的元素一律放在右边，这样一趟下来，该序列就分成了两个子序列，接下来对两个子序列进行递归，直到每个子序列只剩一个，排序完成。 插入排序直接插入排序依次将待排序的数据元素按其关键字值的大小插入前面的有序序列。 折半插入排序当第 i - 1 趟需要将第 i 个元素插入前面的 0 ~ i -1 个元素序列中时： 计算 0 ~ i - 1 索引的中间点，也就是用 i 索引处的元素和 （0 + i - 1）/2 索引处的元素进行比较，如果 i 索引处的元素大，就直接在 （（0 + i - 1）/2 ） ~ （i - 1）后半个范围内进行搜索，反之在前半个范围搜索。 重复上面步骤 确定第 i 个元素的插入位置，就将该位置的后面所有元素整体后移一位，然后将第 i 个元素放入该位置。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"通过项目逐步深入了解Mybatis（一）","date":"2017-06-11T16:00:00.000Z","path":"2017/06/12/通过项目逐步深入了解Mybatis(一)/","text":"Mybatis 和 SpringMVC 通过订单商品案例驱动 官方中文地址：http://www.mybatis.org/mybatis-3/zh/ 官方托管地址：https://github.com/mybatis/mybatis-3 本项目全部代码地址：https://github.com/zhisheng17/mybatis 如果觉得不错的话，欢迎给个 star ， 如果你想完善这个项目的话，你也可以 fork 后修改然后推送给我。 基础知识：对原生态 jdbc 程序（单独使用 jdbc 开发）问题总结1、环境​ java 环境 ：jdk1.8.0_77 ​ 开发工具 ： IDEA 2016.1 ​ 数据库 ： MySQL 5.7 2、创建数据库​ mybatis_test.sql ​ Tables ：items、orderdetail、orders、user 3、JDBC 程序​ 使用 JDBC 查询 MySQL 数据库中用户表的记录 ​ 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package cn.zhisheng.mybatis.jdbc;/** * Created by 10412 on 2016/11/27. */import java.sql.*;/** *通过单独的jdbc程序来总结问题 */public class JdbcTest&#123; public static void main(String[] args) &#123; //数据库连接 Connection connection = null; //预编译的Statement，使用预编译的Statement可以提高数据库性能 PreparedStatement preparedStatement = null; //结果集 ResultSet resultSet = null; try &#123; //加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\"); //通过驱动管理类获取数据库链接 connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8\", \"root\", \"root\"); //定义sql语句 ?表示占位符（在这里表示username） String sql = \"select * from user where username = ?\"; //获取预处理statement preparedStatement = connection.prepareStatement(sql); //设置参数，第一个参数为sql语句中参数的序号（从1开始），第二个参数为设置的参数值 preparedStatement.setString(1, \"王五\"); //向数据库发出sql执行查询，查询出结果集 resultSet = preparedStatement.executeQuery(); //遍历查询结果集 while(resultSet.next()) &#123; System.out.println(resultSet.getString(\"id\")+\" \"+resultSet.getString(\"username\")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; //释放资源 if(resultSet!=null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; if(preparedStatement!=null) &#123; try &#123; preparedStatement.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; if(connection!=null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 4、问题总结 数据库连接，使用时就创建，不使用立即释放，对数据库频繁连接开启和关闭，造成数据库资源的浪费，影响数据库性能。 解决方法：使用数据库连接池管理数据库连接。 将 sql 语句硬编码到 java 代码中，如果 sql 语句需要修改，那么就需要重新编译 java 代码，不利于系统的维护。 设想：将 sql 语句配置在 xml 配置文件中，即使 sql 语句发生变化，也不需要重新编译 java 代码。 向 preparedStatement 中设置参数，对占位符号位置和设置参数值，硬编码在 java 代码中，同样也不利于系统的维护。 设想：将 sql 语句、占位符、参数值配置在 xml 配置文件中。 从 resultSet 中遍历结果集数据时，存在硬编码，将获取表的字段进行硬编码，不利于系统维护。 设想：将查询的结果集自动映射成 java 对象。 Mybatis框架原理（掌握）1、Mybatis 是什么？​ Mybatis 是一个持久层的架构，是 appach 下的顶级项目。 ​ Mybatis 原先是托管在 googlecode 下，再后来是托管在 Github 上。 ​ Mybatis 让程序员将主要的精力放在 sql 上，通过 Mybatis 提供的映射方式，自由灵活生成（半自动，大部分需要程序员编写 sql ）满足需要 sql 语句。 ​ Mybatis 可以将向 preparedStatement 中的输入参数自动进行输入映射，将查询结果集灵活的映射成 java 对象。（输出映射） 2、Mybatis 框架 注解： SqlMapConfig.xml （Mybatis的全局配置文件，名称不定）配置了数据源、事务等 Mybatis 运行环境 Mapper.xml 映射文件（配置 sql 语句） SqlSessionFactory （会话工厂）根据配置文件配置工厂、创建 SqlSession SqlSession （会话）面向用户的接口、操作数据库（发出 sql 增删改查） Executor （执行器）是一个接口（基本执行器、缓存执行器）、SqlSession 内部通过执行器操作数据库 Mapped Statement （底层封装对象）对操作数据库存储封装，包括 sql 语句、输入参数、输出结果类型 ​ Mybatis入门程序1、需求实现以下功能： 根据用户id查询一个用户信息 根据用户名称模糊查询用户信息列表 添加用户 更新用户 删除用户 2、环境java 环境 ：jdk1.8.0_77 开发工具 ： IDEA 2016.1 数据库 ： MySQL 5.7 Mybatis 运行环境（ jar 包） MySQL 驱动包 其他依赖包 3、 log4j.properties在classpath下创建log4j.properties如下： 1234567# Global logging configuration#在开发环境日志级别要设置为DEBUG、生产环境要设置为INFO或者ERRORlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n Mybatis默认使用log4j作为输出日志信息。 4、工程结构 5、SqlMapConfig.xml配置 Mybatis 的运行环境、数据源、事务等 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务由 Mybatis 控制--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池,由Mybatis管理，数据库名是mybatis_test，Mysql用户名root，密码root --&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"root\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt;&lt;/configuration&gt; 6、创建 po 类Po 类作为 mybatis 进行 sql 映射使用，po 类通常与数据库表对应，User.java 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package cn.zhisheng.mybatis.po;import java.util.Date;/** * Created by 10412 on 2016/11/28. */public class User&#123; private int id; private String username; // 用户姓名 private String sex; // 性别 private Date birthday; // 生日 private String address; // 地址 //getter and setter public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125;&#125; 7、根据用户 id（主键）查询用户信息 映射文件 User.xml（原在 Ibatis 中命名）在 Mybatis 中命名规则为 xxxmapper.xml 在映射文件中配置 sql 语句 User.xml 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapperPUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"test\"&gt;&lt;/mapper&gt; namespace ：命名空间，对 sql 进行分类化管理，用于隔离 sql 语句，后面会讲另一层非常重要的作用。 ​ 在 User.xml 中加入 123456789101112&lt;!--通过select执行数据库查询 id:标识映射文件中的sql 将sql语句封装到mappedStatement对象中，所以id称为Statement的id #&#123;&#125;：表示占位符 #&#123;id&#125;：其中的id表示接收输入的参数，参数名称就是id，如果输入参数是简单类型，那么#&#123;&#125;中的参数名可以任意，可以是value或者其他名称 parameterType：表示指定输入参数的类型 resultType：表示指定sql输出结果的所映射的java对象类型 --&gt;&lt;!-- 根据id获取用户信息 --&gt; &lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt; User.xml 映射文件已经完全写好了，那接下来就需要在 SqlMapConfig.xml中加载映射文件 User.xml 1234&lt;!--加载映射文件--&gt; &lt;mappers&gt; &lt;mapper resource=\"sqlmap/User.xml\"/&gt; &lt;/mappers&gt; ​ 编写程序 `MybatisFirst.java` ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445package cn.zhisheng.mybatis.first;import cn.zhisheng.mybatis.po.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import java.io.IOException;import java.io.InputStream; /*** Created by 10412 on 2016/11/28.*/public class MybatisFirst&#123; //根据id查询用户信息，得到用户的一条记录 @Test public void findUserByIdTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 //sqlSession.selectOne 结果与映射文件中所匹配的resultType类型的对象 User user = sqlSession.selectOne(\"test.findUserById\", 1); System.out.println(user); //释放资源 sqlSession.close(); &#125;&#125; 然后运行一下这个测试，发现结果如下就代表可以了： 8、根据用户名称模糊查询用户信息列表 映射文件 依旧使用 User.xml 文件，只不过要在原来的文件中加入 123456789&lt;!-- 自定义条件查询用户列表 resultType：指定就是单条记录所映射的java对象类型 $&#123;&#125;:表示拼接sql串，将接收到的参数内容不加修饰的拼接在sql中 使用$&#123;&#125;拼接sql，会引起sql注入 $&#123;value&#125;：接收输入参数的内容，如果传入类型是简单类型，$&#123;&#125;中只能够使用value--&gt; &lt;select id=\"findUserByUsername\" parameterType=\"java.lang.String\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where username like '%$&#123;value&#125;%' &lt;/select&gt; 编写程序 依旧直接在刚才那个 MybatisFirst.java 中加入测试代码： 12345678910111213141516171819202122232425262728//根据用户名称模糊查询用户信息列表 @Test public void findUserByUsernameTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 //selectList 查询结果可能多条 //list中的user和映射文件中resultType所指定的类型一致 List&lt;User&gt; list = sqlSession.selectList(\"test.findUserByUsername\", \"小明\"); System.out.println(list); //释放资源 sqlSession.close(); &#125; 同样测试一下findUserByUsernameTest ，如果运行结果如下就代表没问题： 提示：通过这个代码可以发现，其中有一部分代码是冗余的，我们可以将其封装成一个函数。 1234567public void createSqlSessionFactory() throws IOException &#123; // 配置文件 String resource = \"SqlMapConfig.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); // 使用SqlSessionFactoryBuilder从xml配置文件中创建SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; 注意：1、#{ } 和 ${ } 的区别 #{ }表示一个占位符号，通过#{ }可以实现 preparedStatement 向占位符中设置值，自动进行java 类型和 jdbc 类型转换，#{ } 可以有效防止sql注入。#{ } 可以接收简单类型值或 pojo 属性值（通过 OGNL 读取对象中的值，属性.属性.属性..方式获取对象属性值）。 如果 parameterType 传输单个简单类型值，#{ }括号中可以是 value 或其它名称。 ${ } 表示拼接 sql 串，通过${ }可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换， ${ }可以接收简单类型值或 pojo 属性值（（通过 OGNL 读取对象中的值，属性.属性.属性..方式获取对象属性值）），如果 parameterType 传输单个简单类型值，${}括号中只能是 value。 2、parameterType 和 resultType 区别 parameterType：指定输入参数类型，mybatis 通过 ognl 从输入对象中获取参数值拼接在 sql 中。 resultType：指定输出结果类型，mybatis 将 sql 查询结果的一行记录数据映射为 resultType 指定类型的对象。 3、selectOne 和 selectList 区别 selectOne 查询一条记录来进行映射，如果使用selectOne查询多条记录则抛出异常： org.apache.ibatis.exceptions.TooManyResultsException: Expected one result (or null) to bereturned by selectOne(), but found: 3 at selectList 可以查询一条或多条记录来进行映射。 9、添加用户 映射文件 在 User.xml 中加入： 12345678&lt;!-- 添加用户 --&gt; &lt;insert id=\"insetrUser\" parameterType=\"cn.zhisheng.mybatis.po.User\" &gt; &lt;selectKey keyProperty=\"id\" order=\"AFTER\" resultType=\"java.lang.Integer\"&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; insert into user(username, birthday, sex, address) values(#&#123;username&#125;, #&#123;birthday&#125;, #&#123;sex&#125;, #&#123;address&#125;) &lt;/insert&gt; 注意: selectKey将主键返回，需要再返回 添加selectKey实现将主键返回 keyProperty:返回的主键存储在pojo中的哪个属性 order：selectKey的执行顺序，是相对与insert语句来说，由于mysql的自增原理执行完insert语句之后才将主键生成，所以这里selectKey的执行顺序为after resultType:返回的主键是什么类型 LAST_INSERT_ID():是mysql的函数，返回auto_increment自增列新记录id值。 然后在 MybatisFirst.java 中写一个测试函数，代码如下 123456789101112131415161718192021@Test public void insetrUser() throws IOException, ParseException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); User user = new User(); SimpleDateFormat sdf = new SimpleDateFormat (\"yyyy-MM-dd\"); user.setUsername(\"田志声\"); user.setSex(\"男\"); user.setBirthday(sdf.parse(\"2016-11-29\")); user.setAddress(\"江西南昌\"); sqlSession.insert(\"test.insetrUser\", user); sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 然后 run 一下，如果出现的结果如下，那么就是成功了。 同时数据库也能查询到刚插入的用户信息： 10、自增主键返回 与 非自增主键返回 MySQL 自增主键：执行 insert 提交之前自动生成一个自增主键，通过 MySQL 函数获取到刚插入记录的自增主键： LAST_INSERT_ID() ，是在 insert 函数之后调用。 非自增主键返回：使用 MySQL 的 uuid() 函数生成主键，需要修改表中 id 字段类型为 String ，长度设置为 35 位，执行思路：先通过 uuid() 查询到主键，将主键输入到 sql 语句中；执行 uuid() 语句顺序相对于 insert 语句之前执行。 刚才那个插入用户的地方，其实也可以通过 uuid() 来生成主键，如果是这样的话，那么我们就需要在 User.xml 中加入如下代码： 123456789&lt;!--使用 MySQL 的 uuid()生成主键 执行过程： 首先通过uuid()得到主键，将主键设置到user对象的id属性中 其次执行insert时，从user对象中取出id属性值 --&gt;&lt;selectKey keyProperty=\"id\" order=\"BEFORE\" resultType=\"java.lang.String\"&gt; select uuid()&lt;/selectKey&gt;insert into user(id, username, birthday, sex, address) values(#&#123;id&#125;, #&#123;username&#125;, #&#123;birthday&#125;, #&#123;sex&#125;, #&#123;address&#125;) Oracle 使用序列生成主键 首先自定义一个序列且用于生成主键，selectKey使用如下： 12345678&lt;insert id=\"insertUser\" parameterType=\"cn.itcast.mybatis.po.User\"&gt; &lt;selectKey resultType=\"java.lang.Integer\" order=\"BEFORE\" keyProperty=\"id\"&gt; SELECT 自定义序列.NEXTVAL FROM DUAL &lt;/selectKey&gt;insert into user(id,username,birthday,sex,address) values(#&#123;id&#125;,#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;)&lt;/insert&gt; ​ 11、删除用户前面说了这么多了，这里就简单来说明下： 在 User.xml 文件中加入如下代码： 1234&lt;!--删除用户--&gt; &lt;delete id=\"deleteUserById\" parameterType=\"int\"&gt; delete from user where user.id = #&#123;id&#125; &lt;/delete&gt; 在 MybatisFirst.java 文件中加入如下代码： 1234567891011121314151617181920212223242526272829//删除用户 @Test public void deleteUserByIdTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 sqlSession.delete(\"test.deleteUserById\", 26); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 测试结果如下： 之前的数据库 user 表查询结果： 执行完测试代码后，结果如下： 12、更新用户信息在 User.xml 中加入如下代码： 123456789&lt;!--根据id更新用户 需要输入用户的id 传入用户要更新的信息 parameterType指定user对象，包括id和更新信息，id必须存在 #&#123;id&#125;：从输入对象中获取id属性值--&gt;&lt;update id=\"updateUserById\" parameterType=\"cn.zhisheng.mybatis.po.User\"&gt; update user set username = #&#123;username&#125;, birthday = #&#123;birthday&#125;, sex = #&#123;sex&#125;, address = #&#123;address&#125; where user.id = #&#123;id&#125; &lt;/update&gt; 然后在 MybatisFirst.java 中加入 12345678910111213141516171819202122232425262728293031323334353637//根据id更新用户信息 @Test public void updateUserByIdTest() throws IOException, ParseException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //为了设置生日的日期输入 SimpleDateFormat sdf = new SimpleDateFormat (\"yyyy-MM-dd\"); User user = new User(); //根据id更新用户信息 user.setId(24); user.setUsername(\"张四风\"); user.setBirthday(sdf.parse(\"2015-01-12\")); user.setSex(\"女\"); user.setAddress(\"上海黄埔\"); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 sqlSession.update(\"test.updateUserById\", user); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 测试结果如下： 查看数据库，id 为 24 的用户信息是否更新了： 啊，是不是很爽，所有的需求都完成了。 没错，这只是 Mybatis 的一个简单的入门程序，简单的实现了对数据库的增删改查功能，通过这个我们大概可以了解这个编程方式了。 期待接下来的 Mybatis高级知识文章吧！ 更多文章请见 微信公众号：猿blog","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":" 六月 —— 愿你做最美好的自己！","date":"2017-06-01T16:00:00.000Z","path":"2017/06/02/poetry2/","text":"美妙的六月已经到来，送你八封信，愿时光轻缓，愿微风正好，愿你做最美好的自己。 第一封 —— 关于压力 鸡蛋，从外打破是食物，从内打破是生命。人生亦是，从外打破是压力，从内打破是成长。如果你等待别人打破你，那么你注定成为别人的食物；如果能让自己从内打破，那么你会发现自己的成长相当于一种重生。 第二封 —— 关于读书 读书是一种充实人生的艺术，没有书的人生就像空心的竹子一样，空洞无物。犹太人让孩子们亲吻涂有蜂蜜的书本，是为了让他们记住：书本是甜的，要让甜蜜充满人生就要读书。读书是一本人生最难得的存折，一点一滴地积累，最后你会发现：自己是世界上最富有的人。第三封 —— 关于人际关系 你可以要求自己守信，但不能要求别人守信；你可以要求自己对人好，但不能期待别人对你好。你怎样对人，并不代表人家就会怎么对你。如果看不透这一点，你只会徒添不必要的烦恼。 第四封 —— 关于孤独 每个人都要经历一段孤独的日子，每段路都有一段独孤的时光。父母不可能一直帮着你，朋友也不可能一直围着你转。孤独不是孤僻，更不是寂寞。经历过孤独的人，内心更坚强，不管处于什么样的环境都能让自己安静，更好地调整状态，面对环境。 第五封 —— 关于修养 看别人不顺眼，是自己修养不够。人愤怒的那一瞬间，智商是零，过一分钟后恢复正常。人的优雅关键在于控制自己的惰绪，用嘴伤害人，是最愚蠢的一种行为。 第六封 —— 关于现实 现实有太多的不如意，就算生活给你的是垃圾，你同样能把垃圾踩在脚底下登上世界之巅。你要把自己逼出最大的潜能，没有人会为你的未来买单，你要么努力向上爬，要么烂在社会最底层的泥里，这就是生活。 第七封 —— 关于自己 一个人经过不同程度的鍛炼，就获得不同程度的修养。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容。我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。 第八封 —— 关于幸福 常有人说，我现在不幸福，等我结了婚或买了房……就幸福了。事实是，幸福的人在哪儿都幸福，不幸福的人在哪儿都不幸福。所以要先培养自己的幸福力，不论发生什么，别人都动不了你的自在开心。这才是真正强大的气场和自信。 幸福的人生，需要三种姿态：对过去，要淡；对现在，要惜；对未来，要信。愿你拥有幸福的能力，做最美好的自己。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"最近很火的鸡汤，分享给大家","date":"2017-05-11T16:00:00.000Z","path":"2017/05/12/poetry/","text":"&lt;最近很火的一首小诗，分享给大家&gt; 纽约时间比加州时间早三个小时， New York is 3 hours ahead of California, 但加州时间并没有变慢。 but it does not make California slow. 有人22岁就毕业了， Someone graduated at the age of 22, 但等了五年才找到好的工作！ but waited 5 years before securing a good job! 有人25岁就当上CEO， Someone became a CEO at 25, 却在50岁去世。 and died at 50. 也有人迟到50岁才当上CEO， While another became a CEO at 50, 然后活到90岁。 and lived to 90 years. 有人依然单身， Someone is still single, 同时也有人已婚。 while someone else got married. 奥巴马55岁就退休， Obama retires at 55, 川普70岁才开始当总统。 but Trump starts at 70. 世上每个人本来就有自己的发展时区。 Absolutely everyone in this world works based on their Time Zone. 身边有些人看似走在你前面， People around you might seem to go ahead of you, 也有人看似走在你后面。 some might seem to be behind you. 但其实每个人在自己的时区有自己的步程。 But everyone is running their own RACE, in their own TIME. 不用嫉妒或嘲笑他们。 Don’t envy them or mock them. 他们都在自己的时区里，你也是！ They are in their TIME ZONE, and you are in yours! 生命就是等待正确的行动时机。 Life is about waiting for the right moment to act. 所以，放轻松。 So, RELAX. 你没有落后。 You’re not LATE. 你没有领先。 You’re not EARLY. 在命运为你安排的属于自己的时区里，一切都准时。 You are very much ON TIME, and in your TIME ZONE Destiny set up for you.","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Github pages + Hexo 博客 yilia 主题使用畅言评论系统","date":"2017-04-12T16:00:00.000Z","path":"2017/04/13/Hexo-yilia-changyan/","text":"前言Hexo的Yilia主题由于原来使用的是多说的留言板，近期多说公告要停止提供服务了，所以我就把多说换成搜狐的畅言了，下面写一个简单的小教程。 注册畅言进入畅言官网 , 点击右上角 “免费注册”，并填写注册信息。（注意域名需要备案信息） 登录并进入畅言后台注册完后，登录进入畅言官网，获取你的畅言 app id 和 app key。 使用畅言系统下面说下修改评论为畅言的方法，其实方法和多说是差不多的。 1、修改 themes\\yilia\\layout\\_partial\\article.ejs 模板，把如下代码 1234567&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt; &lt;section id=\"comments\"&gt; &lt;div id=\"disqus_thread\"&gt; 这里还有很多代码 &lt;/div&gt; &lt;/section&gt; &lt;% &#125; %&gt; 修改为： 1234567891011121314151617181920 &lt;% if (!index &amp;&amp; post.comments)&#123; %&gt; &lt;section id=&quot;comments&quot;&gt;&lt;!--高速版，加载速度快，使用前需测试页面的兼容性--&gt;&lt;div id=&quot;SOHUCS&quot; sid=&quot;&lt;%= page.title %&gt;&quot;&gt;&lt;/div&gt;&lt;script&gt; (function()&#123; var appid = &apos;你的APP ID&apos;, conf = &apos;你的APP KEY&apos;; var doc = document, s = doc.createElement(&apos;script&apos;), h = doc.getElementsByTagName(&apos;head&apos;)[0] || doc.head || doc.documentElement; s.type = &apos;text/javascript&apos;; s.charset = &apos;utf-8&apos;; s.src = &apos;http://assets.changyan.sohu.com/upload/changyan.js?conf=&apos;+ conf +&apos;&amp;appid=&apos; + appid; h.insertBefore(s,h.firstChild); window.SCS_NO_IFRAME = true; &#125;)()&lt;/script&gt; &lt;/section&gt; &lt;% &#125; %&gt; 上面的APP ID和APP KEY是在畅言设置中得到。 这里需要注意一点的是：sid=&quot;&lt;%= page.title %&gt;&quot;&gt; 这样的话畅言就可以直接根据对应的文章来识别，使得文章有对应的评论，不会都乱在一起。 2、在每篇文章开头的 front-matter 中添加一句comments: true，然后回到博客根目录执行命令 hexo d -g ，重新生成博客并部署博客，然后刷新，任选一篇文章进入下拉，会发现评论功能可以使用了。 修改 BUG但是，这是你会发现一个 Bug，表情按钮点击不了，原因是被左侧的 div 层覆盖了，回到我们刚才改过的代码，找到 &lt;div id=&quot;SOHUCS&quot; 开头的一串代码。并做如下更改 1&lt;div id=\"SOHUCS\" sid=\"&lt;%=title %&gt;\" style=\"padding: 0px 30px 0px 46px;\"&gt;&lt;/div&gt; 加上上面这一段样式代码，即可修复。 参考文章： 1、Hexo博客yilia主题更换畅言评论系统 2、在Hexo中使用畅言评论系统 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.54tianzhisheng.cn/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://www.54tianzhisheng.cn/tags/yilia/"}]}]